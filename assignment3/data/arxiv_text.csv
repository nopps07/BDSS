"...1","Title","Text","nrc_sen","vader_sen"
0,"A Framework for Machine Learning of Model Error in Dynamical Systems"," g u s d h t m v v x r communications american mathematical society volume number pages – s xx framework machine learning model error dynamical systems matthew e levine andrew m stuart abstract development datainformed predictive models dynamical systems widespread interest many disciplines present unifying framework blending mechanistic machinelearning approaches identify dynamical systems noisily partially observed data compare pure datadriven learning hybrid models incorporate imperfect domain knowledge referring discrepancy assumed truth model imperfect mechanistic model model error formulation agnostic chosen machine learning model presented continuous discretetime settings compatible model errors exhibit substantial memory errors memoryless first study memoryless linear wrt parametricdependence model error learning theory perspective deﬁning excess risk generalization error ergodic continuoustime systems prove excess risk generalization error bounded terms diminish squareroot t timeinterval training data speciﬁed secondly study scenarios beneﬁt modeling memory proving universal approximation theorems two classes continuoustime recurrent neural networks rnns can learn memorydependent model error assuming governed ﬁnitedimensional hidden variable together observed hidden variables form continuoustime markovian system addition connect one class rnns reservoir computing thereby relating learning memorydependent error recent work supervised learning banach spaces using random features numerical results presented lorenz ’ lorenz ’ multiscale systems compare purely datadriven hybrid approaches ﬁnding hybrid methods less datahungry parametrically eﬃcient also ﬁnd continuoustime framing allows robustness irregular sampling desirable domaininterpretability discretetime framing can provide similar better predictive performance especially data undersampled vector ﬁeld deﬁning true dynamics identiﬁed finally demonstrate numerically data assimilation can leveraged learn hidden dynamics noisy partiallyobserved data illustrate challenges representing memory approach training models received editors july mathematics subject classiﬁcation primary t m secondary m key words phrases dynamical systems model error statistical learning random features recurrent neural networks reservoir computing authors grateful david albers oliver dunbar ian melbourne fei lu ivan d jimenez rodriguez yisong yue helpful discussions work mel ams supported nih ro lm “mechanistic machine learning” mel also supported national science foundation graduate research fellowship grant dge ams also supported nsf award ags nsf award dms oﬃce naval research award n afosr muri award number fa machine learning physicsbased modeling simulation computations presented conducted resnick high performance center facility supported resnick sustainability institute california institute technology introduction background literature review modeling prediction dy namical systems timeseries important goal numerous domains including biomedicine climatology robotics social sciences traditional approaches modeling systems appeal careful study mechanisms design targeted equations represent carefully built mechanistic models impacted humankind numerous arenas including ability land spacecraft celestial bodies provide highﬁdelity numerical weather prediction artiﬁcially regulate physiologic processes use pacemakers artiﬁcial pancreases example paper focuses learning model error assume imperfect mechanistic model known data used improve introduce framework problem focusing distinctions markovian nonmarkovian model error providing unifying review relevant literature developing underpinning theory related markovian nonmarkovian settings presenting numerical experiments illustrate key ﬁndings set work context ﬁrst review use datadriven methods time dependent problems organizing literature review around four themes comprising sections devoted respectively pure datadriven methods hybrid methods build mechanistic models nonmarkovian models describe memory applications various approaches set work context section detail contributions make describe organization paper datadriven modeling dynamical systems recent wave machine learning successes datadriven modeling especially imaging sciences shown can demand even existing models can design models complex phenomena heretofore traditional models built example low order polynomials andor linearized model reductions may appear limited compared ﬂexible function approximation frameworks provided neural networks kernel methods neural networks example long history success modeling dynamical systems recent developments deep learning operators continue propel trend success neural networks arguably relies balanced expressivity generalizability methods also excel learning parsimonious gen eralizable representations dynamics particularly popular methodology perform sparse regression dictionary vector ﬁelds including use thresholding approaches sindy lregularized polynomial regression nonparametric methods like gaussian process models also used widely modeling nonlinear dynamics good choice kernel often essential success methods recent progress made towards automatic hyperparameter tuning via paramet ric kernel ﬂows successes gaussian process models also extended high dimensional problems using random feature map approximations within context datadriven learning parametric partial diﬀerential equations pdes solution operators advancements datadriven methods based machine learning model error koopman operator theory dynamic mode decomposition also oﬀer exciting new possibilities predicting nonlinear dynamics data important consider whether model discrete continuoustime potential advantages primary positive continuoustime modeling lies ﬂexibility interpretability particular continuoustime approaches readily naturally applied irregularly sampled timeseries data eg electronic health record data discretetime methods furthermore ﬂexibility respect timestep enables simple transferability model learnt discretetime data one timestep new settings diﬀerent timestep indeed variable timestep settings learned righthandside can used generate numerical solutions timestep hand applying discretetime model new timestep either requires exact alignment subsampled data postprocessing interpolation step continuoustime models may also provide greater interpretability discretetime methods righthandside ordinary diﬀerential equation ode physically interpretable object ∆tsolution operator eg equation discovery traditional implementations continuoustime learning require accurate estima tion timederivatives state may circumvented using approaches leverage autodiﬀerentiation software methods learn statistics derived timeseries moments correlation functions keller du du et al provide rigorous analysis demonstrating inference continuoustime model discretetime data must conducted great care prove stable consistent linear multistep methods continuoustime integration may possess guarantees used inverse problem ie discovery dynamics queiruga et al pro vide pathological illustrations phenomenon context rungekutta methods discretetime approaches hand easily deployed train test data sample rates applications data collection easily conﬁgured eg simulated settings available automatic sensors etc discrete time methods typically much easier implement test continuoustime methods moreover allow “nonintrusive” model correction additions applied outside numerical integrator may relevant practical integration complex simulation software addition discretetime approaches can preferable unavoidably large error continuoustime inference chorin lu lu et al nonparametric parametric model classes used learning dynamical systems latter connecting former via representer theorem gaussian process regression used hybrid mechanistic datadriven modeling attempts transform mains relied traditional mechanistic models using purely datadriven ie de novo “learn scratch” approaches often fall short now growing recognition machine learning researchers mechanistic models valuable represent distillation centuries data collected countless studies interpreted domain experts recent studies consistently found advantages hybrid methods blend mechanistic knowledge datadriven techniques willard et al provide thorough review shift amongst scientists engineers hybrid methods improve matthew e levine andrew m stuart predictive performance also reduce data demands improve interpretability trustworthiness essential many applications exempliﬁed work autonomous drone landing helicopter ﬂying well predictions covid mortality risk covid treatment response question best use power data machine learning leverage build upon existing mechanistic knowledge thus widespread current interest question research direction anticipated last thirty years research activity interface dynamical systems machine learning now critical mass eﬀort developing variety studies tackling questions weather climate modeling even imaging sciences pure machine learning spectacularly successful emerging work shows incorporating knowledge underlying physical mechanisms improves performance variety image recognition tasks noted studied ba et al freno carlberg others common highlevel approaches blending machine learning mechanistic knowledge use machine learning learn additive residual corrections mechanistic model use mechanistic model input feature machine learning model use mechanistic knowledge form diﬀerential equation ﬁnal layer neural network representation solution equivalently deﬁne loss function include approximate satisfaction diﬀerential equation use mechanistic intuition constrain inform machine learning architecture many successful studies developed speciﬁc designs hybridize perspectives addition parameter estimation mechanistic models wellstudied topic data assimilation inverse problems mechanistic modeling communities recent approaches leverage machine learning task may create new opportunities accounting temporal parameter variations unknown observation functions important distinction made physicsinformed surrogate modeling refer hybrid modeling surrogate modeling primarily focuses replacing highcost highﬁdelity mechanistic model simulations similarly accurate models cheap evaluate eﬀorts shown great promise training machine learning models expensive highﬁdelity simulation data especially successful underlying physical domainspeciﬁc mechanistic knowledge equations incorporated model training architecture use term hybrid modeling hand indicate ﬁnal learned system involves interaction possibly feedback mechanismbased datadriven models work focus primarily hybrid methods learn residuals imperfect mechanistic model closely follow discretetime hybrid modeling framework developed providing new insights continuoustime modeling perspective beneﬁts form hybrid modeling many others observed yet fully understood theoretical sense intuitively nominal mechanistic models useful encode key nonlinearities readily inferred using general model classes modest amounts data indeed classical approximation theorems ﬁtting polynomials machine learning model error fourier modes common function bases directly reﬂect relationship bounding error respect measure complexity target function eg lipschitz constants moduli continuity sobolev norms etc chapter recent work e et al provides priori error bounds twolayer neural networks kernelbased regressions constants depend explicitly norm target function modelhypothesis space barron space reproducing kernel hilbert space resp time problems mechanistic models capture lowcomplexity trends eg linear may still good candidates hybrid learning purely datadriven accurate linear model reduces parametric burden machinelearning task eﬀect likely accentuated datastarved regimes furthermore even cases datadriven models perform satisfactorily hybrid approach may improve interpretability trustworthiness controllability without sacriﬁcing performance hybrid models often cast markovian memoryfree settings learned dynamical system learned residuals solely dependent observed states approach can highly eﬀective measurements relevant states available inﬂuence unobserved states adequately described function observables perspective employed shi et al learn corrections physical equations motion autonomous vehicle regions state space physics perform poorly— residual errors driven unmodeled turbulence landing can predicted using observable states vehicle ie position velocity acceleration also perspective taken applications highdimensional multiscale dynamical systems wherein subgrid closure models parameterize eﬀects expensive ﬁnescale interactions eg cloud simulations functions coarse variables result hybrid dynamical system physicsbased equation deﬁned coarse variables markovian correction term accounts eﬀects expensive ﬁne scale dynamics nonmarkovian datadriven modeling unobserved unmodeled processes often responsible model errors represented markovian fashion within observed variables alone need driven substantial advances memorybased modeling one approach use delay embeddings methods inherently tied discrete time representations data although successful many applied contexts less value goal datadriven learning ﬁt continuoustime models desirable modeling goal many settings alternative understanding memory via morizwanzig formalism fundamental building block presentation memory hidden variables may employed discretetime continuoustime models although initially developed primarily context statistical mechanics provides basis understanding hidden variables dynamical systems thus underpins many generic computational tools applied setting successfully applied problems ﬂuid turbulence molecular dynamics lin lu demonstrate connections morizwanzig delay embedding theory context nonlinear autoregressive models using koopman operator theory indeed gilani et al shows correspondence morizwanzig representation koopman operator taken’s matthew e levine andrew m stuart delayembedding ﬂow map studies ma et al wang et al demonstrate morizwanzig formalism motivates use recurrent neural networks rnns deep learning approach nonmarkovian closure modeling harlim et al also use morizwanzig formalism deduce nonmarkovian closure model evaluate rnnbased approximations closure dynamics closure modeling using rnns recently emerged new way learn memory based closures although original formulation morizwanzig general purpose approach modeling partially observed systems continuoustime many practical implementations adopt discretetime picture causes learned memory terms depend sampling rates turn can inhibit ﬂexibility interpretability recent advances continuoustime memorybased modeling however may applicable nonmarkovian hybrid model settings theory continuous time rnns ie formulated diﬀerential equations rather recurrence relation studied s albeit equations speciﬁc additive structure structure exploited continuoustime reservoir computing rc approach lu et al reconstructing chaotic attractors data comparisons rnns rc subclass rnns random parameters ﬁxed recurrent state discretetime yielded mixed conclusions terms relative eﬃciencies ability retain memory recent formulations continuoustime rnns departed slightly additive structure focused constraints architectures ensure stability accuracy resulting dynamical system addition signiﬁcant theoretical work performed linear rnns continuoustime nevertheless various methods yet formulated within hybrid modeling framework approximation power carefully evaluated context recent step direction however work gupta lermusiaux tackles nonmarkovian hybrid modeling continuoustime neural networkbased delay diﬀerential equations ddes noisy observations data assimilation work consider settings observations may noisy partial observations may partial either system undersampled time certain variables observed emphasize ideas statistics can used smooth andor interpolate data remove noise deal undersampling deal missing data ideas data assimilation can used remove noise learn unobserved variables experiments will use noisefree data continuoustime clearly expose issues separate noiseinterpolation experiments will use methodologies data assimilation enhance learning applications datadriven modeling order deploy hybrid methods realworld scenarios must also able cope noisy partial observations accommodating learning model error setting well state estimation active area research data assimilation da community learning dynamics noisy data generally nontrivial nonlinear systems— chickenandegg problem accurate state estimation typically relies availability correct models correct models readily identiﬁed machine learning model error using accurate state estimates recent studies addressed challenge attempting jointly learn noise dynamics gottwald reich approach problem data assimilation perspective employ ensemble kalman filter enkf iteratively update parameters dynamics model ﬁlter current state using updated dynamics recent followup work applies daapproach partiallyobserved systems learns model space discretetime delayembeddings similar studies performed brajard et al applied speciﬁcally model error scenarios ayed et al focus learning continuoustime neural network representation ode partial observations learn separate encoder neural network map historical warmup sequence likely initial conditions unobserved space kaheman et al approach problem variational perspective performing single optimization noise sequences dynamical parameterizations nguyen et al use expectationmaximization em perspective compare variational ensemblebased approaches study needed understand tradeoﬀs styles optimization chen et al study enkfbased optimization scheme performs joint rather embased learning running gradient descent architecture backpropagates data assimilator note data assimilators dynamical systems can tuned using optimization machine learning provide accurate state updates eﬃcient state identiﬁcation however learning improved da schemes sometimes viewed strategy coping model error see optimization da correction model errors two separate problems addressed individually connecting models dynamical systems realworld data also essential recognize available observables may live diﬀerent space underlying dynamics recent studies shown ways navigate using autoencoders dynamical systems models jointly learn latent embedding dynamics latent space proof concepts similar approaches primarily focus imagebased inputs potential applications medicine reduction nonlinear pdes contributions despite large recent body work data driven learning methods hybrid modeling strategies many challenges remain understanding best combine mechanistic machinelearned models indeed answer highly dependent application construct mathematical framework uniﬁes many common approaches blending mechanistic machine learning models done provide strong evidence value hybrid approaches contributions listed follows provide overarching framework learning model error possibly noisy data dynamical systems settings studying discrete continuoustime models together memoryless markovian memorydependent representations model error formulation agnostic choice mechanistic model class machine learning functions study markovian learning problem context ergodic continuous time dynamics proving bounds excess risk generalization error matthew e levine andrew m stuart present simple approximation theoretic approach learning memory dependent nonmarkovian model error continuoustime proving form universal approximation two families memorydependent model error deﬁned using recurrent neural networks describe numerical experiments demonstrate utility learning model error comparison pure datadriven learning pure slightly imperfect mechanistic modeling b compare beneﬁts learning discrete versus continuoustime models c demonstrate utility autodiﬀerentiable data assimilation learn dynamics partially observed noisy data d explain issues arising memorydependent model error learning typical situation dimension memory variable unknown section address contribution deﬁning general settings interest dynamical systems continuous discretetime link underlying systems machine learning framework sections former formulate problem setting statistical learning latter deﬁne concrete optimization problems found ﬁnite parameterizations hypothesis class model error sought section focused speciﬁc choices architectures underpinning theory machine learning methods choices analyze linear methods perspective learning theory context ergodic dynamical systems contribution describe approximation theorem continuoustime hybrid recurrent neural networks contribution finally section presents detailed numerical experiments apply methods section exemplar dynamical systems forms outlined section highlight ﬁndings contribution dynamical systems setting following use phrase markovian model error describe model error expressible entirely terms observed variable current time memoryless situation nonmarkovian model error refers need express model error terms past history observed variable present general framework modeling dynamical system markov ian model error ﬁrst continuoustime section discretetime section extend framework setting nonmarkovian model error section including parameter ε enables us smoothly transition scaleseparated problems markovian closure likely accurate problems unobserved variables scaleseparated observed markovian closure likely fail memory needs accounted important note continuoustime formulation necessarily assumes underlying datagenerating process continuous nature discrete time formulation can viewed discretization underlying continuous system can also represent systems truly discrete settings present intended represent classify common situations arise modeling predicting dynamical systems particular stress two key features first point mechanistic models later referred vector ﬁeld f ﬂow map ψ often available may provide predictions reasonable ﬁdelity however models often simpliﬁcations true system thus can improved datadriven approaches nevertheless machine learning model error provide useful starting point can reduce complexity datahunger learning problems context study tradeoﬀs discrete continuoustime framings begin fullyobserved contexts dynamics markovian respect observed state x later note may access partial observations x larger system x y restricting interest prediction observables show latent dynamical process eg rnn power reconstruct correct dynamics observables continuoustime consider following dynamical system ˙x f†x x x f† ∈ c rdx rdx solution deﬁne xs c s rdx x· ∈ xt t tmax tmaxx ∈ r maximal interval existence primary model error scenario envisage section one vector ﬁeld f† can partially known accessed assume f† f m† f known us m† known f ∈ c rdx rdx regardless ﬁdelity exists function m†x ∈ c rdx rdx can rewritten ˙x fx m†x however paper useful think m† small relative f function m† accounts model error approach targeted learning residuals f f† can alternatively reconstructed f diﬀerent function m†x ∈ c rdx rdx using form ˙x m†x fx approaches deﬁned spaces allow perfect reconstruction f† however ﬁrst formulation hypothesizes missing information additive second formulation provides indication ﬁrst approach ensures substantial usage f advantages settings f trusted practitioners model explainability important second approach will likely see advantages settings simple nonadditive form model error including coordinate transformations possibly statedependent nonlinear warping functions nominal physics f note use f representing model error augmentedinput setting includes case leveraging f hence potentially useful simply adopting x−dependent model error requires learning complex function augmentedinput method also connections model stacking bagging perspective can useful n model hypotheses ˙x m†cidx f x f n x θcid residualbased design relates model boosting goal use machine learning approximate corrector functions m† t ∈ xt using nominal knowledge f observations trajectory xtt t tmaxx true system work consider case learning m†x equation now reader may consider xtt t xk ψ†xk matthew e levine andrew m stuart given without noise principle ˙xtt t known may leveraged practice will case example data highfrequency discrete time address issue follows discretetime consider following dynamical system deﬁne xk cid∞cid k rdxcid solution xkk∈z ∈ x∞ cid∞cidz rdxcid continuoustime setting ψ† ∈ crdx rdx map yields assume access approximate mechanistic model ψ ∈ crdx rdx can corrected using additive residual term m† ∈ crdx rdx feeding ψ input corrective warping function m† ∈ crdx rdx xk ψxk m†xk xk m†xk ψxk focus experiments additive residual framing note discretetime formulation can made compatible continuous time data sampled uniformly rate ∆t ie xk∆t xk k ∈ n see let φ†x t xt solution operator φ deﬁned analogously f b can obtained via numerical integration f† f respectively ψ†v φ†v ∆t ψv φv ∆t partially observed systems continuoustime framework sec tions assumes system dynamics markovian respect observable x experiments performed fullyobserved markov ian case however assumption rarely holds realworld systems consider blockonaspring experiment conducted introductory physics laboratory principle system strictly governed position momentum block ie f along scalar parameters however students’ error analysis reports will note dynamics also driven variety external factors like wobbly table poorly greased track magnitude timescale structure inﬂuence diﬀerent factors rarely known yet somehow encoded discrepancy nominal equations motion noisy observations multiscale system thus also consider setting dynamics x markovian consider x observable states markovian system dimension higher dx can write full system b ˙x f†x y g†x y ˙y ε x x y y deﬁne z nonnegative integers including zero machine learning model error f† ∈ c rdx × rdy rdx g† ∈ c rdx × rdy rdy ε constant measuring degree scaleseparation large ε small system yields solution x· ∈ xt y· ∈ yt t tmaxx y ∈ r maximal interval existence view y complicated unresolved unobserved aspects true underlying system f ∈ c rdx rdx regardless ﬁdelity exists function m†x y ∈ c rdx × rdy rdx can rewritten b ˙x fx m†x y ˙y g†x y ε now observe considering solution equation b function history x inﬂuence y· ∈ yt solution x· ∈ xt can captured † t xt × rdy × r cid→ rdx parameterized wrt t family operators m historical trajectory xst s unobserved initial condition y scale separation parameter ε ˙xt f cidxtcid m cidxst s y εcid † t goal use machine learning ﬁnd markovian model x part state variable using nominal knowledge f observations trajectory xtt t ∈ xt t tmaxx y true system note y· observed nothing assumed known vector ﬁeld g† parameter ε note equations equivalent formulations problem identical solutions third formulation points towards two intrinsic diﬃculties unknown “function” learned fact deﬁned † t mapping banach space path history rdx secondly family operators m operator parameterized y unobserved will address † t can arbitrarily wellapproximated ﬁrst issue showing operators m within family diﬀerential equations dimension rdxdy second issue may addressed techniques data assimilation approximating family learned emphasize however investigate practicality approach learning nonmarkovian systems much remains done area † also important note nonmarkovian operators m t can sometimes adequately approximated invoking markovian model x simply learning function m†· section example ε → y dynamics x ﬁxed suﬃciently mixing averaging principle may invoked deduce cidxst s y εcid m†xt † m t lim ε→ m† section fact used section study learning closure models linear gaussian stochastic diﬀerential equations sdes yt deﬁned analogously xt matthew e levine andrew m stuart highly advantageous identify settings markovian modeling † t necessary suﬃcient simpler learning problem ﬁnd learning m signiﬁcant memory required explain dynamics x learning m† suﬃcient memory eﬀects minimal section show markovian closures can perform well certain tasks even scaleseparation factor ε † small section demonstrate family operators m t may represented odes appealing ideas blend continuoustime rnns assumed known vector ﬁeld f partially observed systems discretetime discretetime analog previous setting considers mapping † xk yk xk ψ † yk ψ xk yk b † † ∈ crdx × rdy rdx ψ ∈ crdx × rdy rdy yielding solutions † known ψ † xkk∈z ∈ x∞ ykk∈z ∈ y∞ assume unknown ψ ψ approximate model ψ rewrite b xk ψxk m†xk yk † yk ψ xk yk cidxk cid m cidxsk † k s y cid can analogously write solution space observables xk ψ † k xk × rdy → rdx function historical trajectory xsk s m unobserved initial condition y discretetime system computed time ∆t map ε cid averaging scenarios apply discussed section memoryless model may used statistical learning ergodic dynamical systems present learning theory framework within consider methods discovering model error data outline learning theory continuous time markovian setting using possibly discretely sampled data point analogs discretetime nonmarkovian settings discretetime settings assume access discretely sampled training data xk xk∆tk k ∆t uniform sampling rate assume k∆t t continuoustime settings assume access continuous time training data ˙xt xtt t section discusses important practical question estimating ˙xt xt discrete high frequency data either case consider problem identifying m ∈ m m represents model hypothesis class minimizes loss function quantifying closeness m m† markovian setting choose measure µ rdx deﬁne loss cid rdx lµm m† cidmx − m†xcid dµx assume true m† x· ergodic invariant density µ can exchange time space averages see inﬁnitely long trajectory xtt≥ machine learning model error ∞m lim t→∞ cid cidmxt − m†xtcid dt t cidmx − m†xcid dµx cid t lµm m† rdx cid t since may access trajectory dataset ﬁnite length t natural deﬁne m note ergodicity t cidmxt − m†xtcid dt lim t→∞ m lµm m† cid t finally can use get m cid ˙xt − fxt − mxtcid dt t possibly regularized natural loss function employ continuoustime data available viewed approximating lµm m† can use deﬁnitions frame problem learning model error language statistical learning let m denote hypothesis class seek minimize m may deﬁne m∗ ∞ argmin m∈m lµm m† argmin m∈m ∞m m∗ t argmin m∈m m risk associated seeking approximate m† class m deﬁned lµm∗ ∞ m† noting m† ∈ m risk measures intrinsic error incurred seeking learn m† restricted class m typically include m† approximation theoretic concept encodes richness hypothesis class m risk may decreased increasing expressiveness m thus risk independent data employed empirical risk minimization refers minimizing regularized version rather ∞ involves speciﬁc instance data available quantify eﬀect data volume learning m† empirical risk minimization helpful introduce following two concepts excess risk deﬁned rt ∞m∗ t − ∞m∗ ∞ represents additional approximation error incurred using data deﬁned ﬁnite time horizon t estimate m† generalization error gt m∗ t − ∞m∗ t represents discrepancy training error deﬁned using ﬁnite trajectory idealized test error deﬁned using inﬁnite length trajectory equivalently invariant measure µ evaluated estimate function m† obtained ﬁnite data return study excess risk generalization error context linear terms parametricdependence models m† ergodicity assumptions data generating process section matthew e levine andrew m stuart introduced machine learning framework continuoustime mar kovian setting may adopted discretetime nonmarkovian settings section deﬁne appropriate objective functions cases remark developments describe learning odes can extended case learning sdes see setting consistency large t limit wellunderstood interesting build learning theory perspective described study statistical consistency odes approaches developed work mcgoﬀ et al su mukherjee potentially useful regard cid parameterization loss function section deﬁne explicit optimizations learning approximate model † error functions m† markovian settings model error operators m t nonmarkovian settings continuous discretetime formulations given defer discussion speciﬁc approximation architectures next section make notational transition optimization possibly nonparametric functions m ∈ m functions parameterized θ characterize class m numerical experiments paper study use continuous discretetime approaches model data generated continuoustime process setup section reﬂects setting two key parameters appear t continuoustime horizon data ∆t frequency data latter parameter will always appear discretetime models may also implicit continuoustime models need infer continuoustime quantities discretely sampled data relate t ∆t k∆t t present general forms jt θ optional regularization terms rθ optimization via derivativebased methodology requires either analytic diﬀerentiation dynamical system model respect parameters use autodiﬀerentiable ode solvers dt rθ continuoustime markovian learning approximate mar kovian closure term parameterized function mx θ assuming full knowledge ˙xt xt learn correction term ﬂow ﬁeld minimizing following objective function θ cid t cidcid ˙xt − fxt − mxt θcidcid cidm · θcid rθ thus proposed methodology jt θ t note jt θ regularization empirical risk minimization described preceding section notable examples leverage framing include paper θ coeﬃcients library loworder polynomials rθ sparsity promoting regularization deﬁned sindy framework paper θ parameters deep neural network dnn l regularization applied weights paper θ dnn parameters rθ encodes constraints lipschitz constant m provided spectral normalization paper applies approach lorenz ’ multiscale system using neural networks l regularization weights machine learning model error discretetime markovian learning learn markovian cor rection term minimizing jt θ k cidcidxk − ψxk − mxk θcidcid rθ k−cid k natural discretetime analog may derived analogously starting discrete analog loss lµm m† now µ assumed ergodic measure discrete analog deﬁned parameterization m regularization leads underlying model assumption work farchi et al continuoustime nonmarkovian learning can attempt recreate dynamics x modeling nonmarkovian residual term common approach augment dynamics space variable r ∈ rdr leading model form ˙x fx fr x θ ˙r fr x θ b seek dr large enough parametric models fjr x· j expres sive enough ensure dynamics x reproduced note although model error x nonmarkovian depends history x seeking explain observed x data enlarged model including hidden variables r dynamics x r markovian learning hidden dynamics partial observations must jointly infer missing states rt typically parameterized governing dynamics f f furthermore family parametric models closed respect translation r will also desirable learn r x observed noisily similarly important learn x clarify discussions training data let u x r f concatenation vector ﬁelds given f f f ˙u f u θ solution ut v θ solving equivalently initial condition v ie u v θ v consider observation operators hx hr x hxu r hru deﬁne noisy observations x zt xt ηt η iid observational noise now outline three optimization approaches learning noisily partially observed data z optimization hard constraint missing dynamics since deter ministic may suﬃce jointly learn parameters initial condition u u minimizing cid t cidcidzt − hxut u θcidcid dt rθ jt θ u t similar approach applied initial conditions learnt outputs additional dnn encoder network maps observation sequences ﬁxed length temporal discretization initial conditions matthew e levine andrew m stuart optimization weak constraint missing dynamics hard constraint minimization sensitive large t settings dynamics chaotic can ameliorated extent considering objective function jt cidθ utcid cid t cidcidzt − hxutcidcid cid t cidcid ˙ut − f ut θcidcid dt t t λ dt objective function employed motivated weak constraint variational formulation dvar arising data assimilation optimization data assimilation missing dynamics weak constraint approach may still scale poorly t large still relies gradientbased optimization infer hidden states avoid potential issues follow recent work using ﬁlteringbased methods estimate hidden state implicitly learns initializations removes noise data allows computation gradients resulting loss function back ﬁltering algorithm learn model parameters deﬁne ﬁltered state cid τ ˆv θdyn θdacidzt scidτ cid s ˆutτ ˆut estimate ut τ zt sτ s initialized ˆut ˆv formulation distinguish θdyn parameters modeling dynamics via θda hyperparameters governing speciﬁcs data assimilation scheme examples θda constant gain matrix k must chosen dvar parameters inﬂation localization methods deployed within ensemble kalman filtering parameterizing choices θda can optimize jointly model parameters θdyn let θ θdyn θda minimize jt θ cidcidzt τ s − hxus ˆutτ θcidcid cid t−τ−τ cid τ ds dt t − τ − ττ t s τ denotes length assimilation time used estimate state initializes parameterﬁtting window duration τ parameterﬁtting leads innerintegration s entire picture translated t time units objective function found integrating t optimizing can understood minimization shortterm forecast errors generated assimilation windows inner integral takes ﬁxed start time t applies data assimilation window t t τ estimate initial condition ˆutτ computes shortterm τ prediction error resulting dabased initialization outer integral sums errors available windows long trajectory data length t work perform ﬁltering using simple dvar method whose constant gain can either chosen constant can learnt data constant natural choice k ∝ h t x approach direct equivalence standard warmup strategies employed rnn rc training paper suggests minimization similar objective considers general observation operators h restricts outer integral nonoverlapping windows solves ﬁltering problem enkf known statecovariance structure practice found setting ˆv works well machine learning model error remark motivate learning parameters data assimilation make following observation problems model known ie θdyn ﬁxed observe successes approach identifying dvar gains empirically outperform theoretically derived gains similar expected parameters deﬁning inﬂation localization enkf cid remark speciﬁc functional forms f f corresponding parameter inference strategies reduce various approaches continuoustime rnn analysis discuss section will start considering settings f f approximated expressive function classes neural networks will specify models f linear r independent x whilst f single layer neural network intuitive former may expressive allow smaller dr latter latter connects directly reservoir computing connection make explicitly follows numerical experiments section will performed settings will train models general setting carefully designed experiments will shed light issues arising overparameterization sense choosing learn model dimension higher true observedhidden model working setting linear coupling term f depending r cid remark recent paper proposes interesting computationally tractable approach learning model error presence memory propose learn closure operator mτ · θ xτ → rdx dde ﬁnite memory τ cidxtcid mτ cidxt − sτ s θcid ˙xt f neural networks used learn operator mτ alternatively gaussian processes used ﬁt speciﬁc class stochastic delay diﬀerential equation sdde however although delaybased approaches seen practical success many applications present issues domain interpretability markovian ode pde closures desirable cid discretetime nonmarkovian learning similar spirit section can aim recreate discretetime dynamics x model b xk ψxk ψrk xk θ rk ψrk xk θ objective function jt θ r k st k−cid cidcidxk − ψxk − ψrk xk θcidcid rθ k rkk− k solves b observe estimation initial condition r crucial data similation methods discussed section can adapted discretetime setting functional form ψ ψ corresponding parameter inference strategies reduce various approaches including recurrent neural networks latent odes delayembedding maps eg get delay embedding map ψ shift operator pathak et al use reservoir computing random features analog rnn described next section l regularization study matthew e levine andrew m stuart approach similar included ψxk feature ψ ψ instead using central model upon learn residuals datadriven superparameterization approach also appears follow underlying sumption harlim et al evaluate hybrid models form settings delay embedding closures employed rnnbased approximations via lstms employed underpinning theory section identify speciﬁc hypothesis classes m using random feature maps markovian settings section using recurrent neural networks memorydependent setting discuss problems theoretical standpoint section study excess risk generalization error context linear models setting includes random features model special case conclude discussing use rnns chapter nonmarkovian settings discrete continuoustime section present approximation theorem continuoustime hybrid rnn models throughout section speciﬁc use random feature maps recurrent neural networks illustration models course used markovian modeling random feature maps principle hypothesis class can used learn m† however focus models easily trained largescale complex systems yet proven approximation power functions ﬁnitedimensional euclidean spaces markovian modeling case use random feature maps like traditional neural networks possess arbitrary approximation power beneﬁt quadratic minimization problem training phase kernel gaussian process methods case studies found random feature models suﬃciently expressive found optimization easily implementable found learned models generalized well moreover linearity respect unknown parameters enables straightforward analysis excess risk generalization error section details derivation speciﬁc design choices random feature modeling approach can found section explain sample d random feature functions ϕ rdx → r stack form vectorvalued feature map φ rdx → rd given random function φ deﬁne hypothesis class m m rdx → rdx ∃ c ∈ rdx×d mx cφx continuoustime continuoustime framing markovian closure model uses hypothesis class thus takes form rewrite particular case l regularization parameter λ ∈ r jt c dt cidccid λ employ notation ⊗ b abt outerproduct matrices ∈ rm×n b ∈ rl×n following notation timeaverage ˙x fx cφxt cidcid ˙xt − fxt − cφcidxtcidcidcid cid t t cid t t atdt machine learning model error ∈ l t rm×n objective function quadratic convex c thus globally minimized unique c∗ makes derivative jt zero consequently minimizer c∗ satisﬁes following linear equation derived section ∈ rd×d identity z λic∗t y z φ ⊗ φt ∈ rd×d y φ ⊗ m†t ∈ rd×dx course m† known m†t ˙xt − fxt can computed data summarize algorithm proceeds follows create realization random feature vector φ compute integrals obtain z y solve linear matrix equation c∗ together leads approximation m†x ≈ m∗ t x θ c∗φx discretetime discretetime markovian closure model xk ψxk cφxk learnt minimizing jt θ k k−cid k cidcidxk − ψxk − cφcidxtcidcidcid cidccid λ objective function quadratic c thus globally minimized c∗ section can compute z y solve linear system c∗ approximate m†x ≈ m∗ t x θ c∗φx formulation closely mirrors fully datadriven linear regression approach learning theory markovian models linear hypothesis class subsection provide estimates excess risk generalization error context learning m† trajectory time horizon t study ergodic continuoustime models setting section end consider general linear hypothesis class given m m rdx → rdx ∃ θ ∈ rp mx pcid θcidfcidx cid note fcid iid draws function φ case d dx reduces random features model analysis context statistical learning rely random features structure fact analysis can used provide learning theory linear settings fcid represents dictionary hypothesized features whose coeﬃcients learnt data nonetheless universal approximation random features provides important example approximation class loss function ∞ may made arbitrarily small choice p large enough appropriate choice parameters reader may ﬁnd useful focus case also note theory present subsection readily generalized working hypothesis class make following ergodicity assumption data generation process √ t cid cid t t cid cid rdx cid t ϕcidxtciddt − cid cid t t matthew e levine andrew m stuart assumption equation possesses compact attractor supporting invariant measure µ furthermore dynamical system ergodic respect µ satisﬁes central limit theorem following form h¨older continuous ϕ rdx cid→ r σ σϕ cid ϕcidxcidµdx ⇒ n σ ⇒ denotes convergence distribution respect x ∼ µ furthermore law iterated logarithm holds almost surely respect x ∼ µ ϕcidxtciddt − cid rdx cid ϕcidxcidµdx σ limsupt→∞ log log t remark note ϕ· evaluated compact obviating need boundedness assumptions ϕ· work melbourne coworkers assumption proven hold class diﬀerential equations including lorenz ’ model neighbourhood classical parameter values central limit theorem established continuity σ ϕ proven whilst general diﬃcult prove results given chaotic dynamical system strong empirical evidence results many chaotic dynamical systems arise practice combination theory empirical evidence justify studying learning model error assumption tran ward ﬁrst make use theory melbourne coworkers study learning chaotic diﬀerential equations timeseries cid given m hypothesis class m deﬁned deﬁne ∞ argminθ∈rpi∞cidm· θcid argminθ∈rplµ cidm· θcid t argminθ∈rpit θ∗ θ∗ cidm· θcid regularization needed setting data plentiful— ∞ θ∗ continuoustime trajectory— number parameters ﬁnite θ∗ solve linear systems t ∞θ∗ ∞ b∞ θ∗ t bt cid ∞ij cidfix fjxcid µdx cid t cidxtcidcid dt cidxtcid fj cidfi rdx t b∞j cid rdx t cidm†x fjxcid µdx cid t cidxtcidcid dt cidm†cidxtcid fj ij facts can derived analogously derivation section given θ∗ ∞ θ∗ bt j t also deﬁne ∞ m· θ∗ m∗ ∞ m∗ t m· θ∗ t recall assumed f† f m† c make following assumption regarding vector ﬁelds deﬁning hypothesis class m machine learning model error assumption functions fcidp cid appearing deﬁnition hypothesis class m h¨older continuous rdx addition matrix ∞ invertible √ theorem let assumptions hold scaled excess risk tgt bounded t rt resp scaled generalization error cidercid resp cidegcid random variable er ∈ rp resp eg ∈ rp converges distribution n σr resp n σg wrt x ∼ µ t → ∞ furthermore constant c almost surely wrt x ∼ µ √ cid cid cidrt gtcid ≤ c limsupt→∞ t log log t proof provided section remark convergence distribution shows high probability √ respect initial data excess risk generalization error bounded √ t can improved give almost sure result terms size cost factor log log t theorem shows ignoring log factors acknowledging probabilistic nature statements trajectories length o− required produce bounds excess risk generalization error size o bounds excess risk generalization error also show empirical risk minimization approaches theoretically analyzable concept risk minimization ∞ hypothesis class sum excess risk rt generalization error gt gives et m∗ t − ∞m∗ ∞ t computable approximate solution m∗ note m∗ t identiﬁed thus combined estimate et leads estimate risk associated hypothesis class used approximating space m rich enough approximation theory may combined theorem estimate trajectory error resulting learned dynamical system approach pursued proposition sdes furthermore setting knowledge rate mixingdecay correlations sdes may used quantify constants appearing error bounds interesting pursue analysis chaotic odes known mixing ratesdecay correlations results mixing less well developed however chaotic odes see discussion point recent work work zhang et al demonstrates error bounds learned model error terms can extended bound error reproduction invariant statistics ergodic sdes moreover e et al provide direction proving similar bounds model error learning using nonlinear function classes eg twolayer neural networks finally remark dependence risk generalization error bounds size model error intuitive amount data required learn model error decrease size model error decreases demonstrated numerically section cf figures b comment theorem also exhibits feature examination proof appendix shows upper bounds terms appearing excess generalization error proportional m† m∗ ∞ approximation given matthew e levine andrew m stuart ∞ m† hypothesis class contains inﬁnite amount data note m∗ truth cid nonmarkovian modeling recurrent neural networks recur rent neural networks rnns one de facto tools modeling systems memory show straightforward residual implementations rnns continuous discretetime goal modeling nonmarkovian model error general case equation b coupling constitute general way account memorydependent model error dynamics x fact f f suﬃciently expressive eg random feature functions neural networks polynomials dr ≥ dy solutions can approximate solutions arbitrarily well make type universal approximation theorem concrete theorems start proving theorem rests following assumptions assumption functions f† g† f f f globally lipschitz note implies m† also globally lipschitz cidx ycid ∈ b ρ implies thatcidxt ytcid ∈ b ρt ∀ t ∈ t assumption fix t exist ρ ∈ r ρt ∈ r equation assumption hidden state r ∈ rdr dimension true hidden state y dr dy assumption let functions f· θ ∈ c rdx × rdy rdx f· θ ∈ c rdx × rdy rdy parameterized n ∈ n θ ∈ rn δ exists n θ ∈ rn sup xy∈bρt sup xy∈bρt cidf†x y − fx y θcid ≤ δ cidg†x y − fx y θcid ≤ δ note assumption can satisﬁed parametric function class possessing universal approximation property maps ﬁnitedimensional euclidean spaces neural networks polynomials random feature methods next theorem transfers universal approximation property maps euclidean spaces universal approximation property representation model error memory form inﬁnite dimensional approximation since via dynamics memory variable r maps past history x model error correction term dynamics x theorem let assumptions aa hold fix t ρ let x· y· denote solution ε let xδ· rδ· denote solution parameters θ ∈ rn δ t parameter dimension n nδ ∈ n parameterization θ θδ ∈ rnδ initial condition xδ rδ ∈ rdxdy property initial condition cidx ycid ∈ b ρ cidx − xδcid ≤ δ sup t∈t deﬁne n strictly positive integers machine learning model error proof provided section direct consequence approximation power f f gronwall lemma remark note existence theorem also holds dr dy freezing dynamics excess dimensions initializing example however possible augmentations dr dy introduce numerical instability imperfectly initialized excess dimensions despite provable correctness perfectly initialized see section nevertheless encounter issues training general model class examples considered paper – see section cid linear coupling now study particular form rnn coupling term f appearing linear depends hidden variable b ˙x fx cr ˙r σar bx c σ activation function speciﬁc linear coupling form particular interest connection make see remark reservoir computing goal choose b c c output xtt≥ matches output without observation ytt≥ knowledge m† g† general case preceding subsection inherent choosing matrices b c vector c choice embedding dimension variable r will typically larger dimension y idea create recurrent state r suﬃciently large dimension dr whose evolution equation takes x input ﬁnal linear transformation approximates missing dynamics m†x y existing approximation theory discretetime rnns showing discretetime analog linear coupling setup can used approximate discretetime systems arbitrarily well see also theorem also general approximation theorem using continuoustime rnns proved apply linearcoupling setting thus extend work three papers context residualbased learning state theorem making three assumptions upon rests assumption functions f† g† f globally lipschitz note implies m† also globally lipschitz assumption let σ ∈ c r r bounded monotonic bounded ﬁrst derivative σu deﬁned σui σui satisﬁes σ ∈ c rp rp assumption fix t exist ρ ∈ r ρt ∈ r equation cidx ycid ∈ b ρ implies thatcidxt ytcid ∈ b ρt ∀ t ∈ t theorem let assumptions aa hold fix t ρ let x· y· denote solution ε let xδ· rδ· denote solution parameters θ ∈ rn δ t embedding dimension dr ∈ n parameter dimension n nδ ∈ n parameterization θ θδ aδ bδ cδ cδ property initial conditioncidx ycid ∈ b ρ initial condition xδ rδ ∈ rdxdr cidx − xδcid ≤ δ sup t∈t matthew e levine andrew m stuart complete proof provided section describe basic structure deﬁne mt m†cidxt ytcid aim ﬁnding diﬀerential equation since ˙mt time derivative m†cidxt ytcid x y solve h†x y ∇xm†x yfx m†x y ∇ym†x yg†x y mt recall ε deﬁne vector ﬁeld ˙m h†x y motivated observations now introduce new system autonomous odes variables x y m ∈ rdx × rdy × rdx b c ˙x fx m ˙y g†x y ˙m h†x y follows identity cid d dt avoid proliferation symbols use letters x y solving equation x y solving equation now show m m†x y invariant manifold clearly manifold dynamics x y governed reduces dynamics x y governed thus mt must initialized m†cidx ycid ensure equivalence solution desired invariance manifold m m†x y dynamics cid −∇xm†x ycidm − m†x ycid m − m†x y identity derived noting recalling deﬁnition h† using d dt m h†x y ∇xm†x yfx m†x y ∇ym†x yg†x y ∇xm†x yfx m ∇ym†x yg†x y − ∇xm†x ycidm − m†x ycid m†x y − ∇xm†x ycidm − m†x ycid d dt emphasize calculation performed dynamics deﬁned proof rnn approximation property proceeds approximating vector ﬁelds g†x y h†x y neural networks introducing linear transformations y m rewrite approximate version system form eﬀect approximation vector ﬁelds true solution propagated system eﬀect controlled via straightforward gronwall argument remark details training continuoustime rnns ensure accuracy longtime stability subject current research paper conﬁne training rnns example general setting case linear coupling discretetime rnn training hand much mature produced satisfactory accuracy stability settings uniform sample rates consistent across train testing scenarios form linear coupling widely studied discrete time machine learning model error models furthermore sophisticated variants rnns longshort term memory lstm rnns gated recurrent units gru often eﬀective although similar nature rnns however potential formulation implementation advantages variants continuoustime setting yet understood refer readers background discrete rnn implementations backpropagation time bptt implementations continuoustime rnns common leverage success automatic bptt code written pytorch tensorﬂow discretizing ode solver compatible autodiﬀerentiation tools eg torchdiffeq nbeddyn adenkf compatibility can also achieved use explicit rungekutta schemes note discretization can perhaps much ﬁner data sampling rate ∆t requires reliable estimation xt ˙xt discrete data cid remark need data assimilation learn initialization recurrent neural networks may understood follows since m† known y observed particular y known desired initialization thus also approximations equation g† h† replaced neural networks known hence rnn trained particular trajectory initial condition required accurate approximation unseen initial condition known furthermore invariant manifold m m†x y may unstable numerical approximation however observations trajectory starting new initial condition used data assimilation techniques can potentially learn initialization rnn also stabilize invariant manifold ad hoc initialization methods common practice rely forcing learned rnn short sequence observed data synchronize hidden state success approaches likely rely rnns’ abilities emulate data assimilators however careful treatment initialization problem may enable substantial advances cid remark reservoir computing rc variant rnns advantage leading quadratic optimization problem within context continuoustime rnn correspond randomizing b c b choosing parameter c ﬁt data concrete leads cidxst s r b ccid rt gt gt may viewed random function pathhistory x upto time t initial condition r c determined minimizing quadratic function jt c t cid ˙xt − fxt − crtcid dt cidccid λ cid t may viewed random feature approach banach space xt use random features learning mappings banach spaces studied nelsen stuart connections random features reservoir computing introduced dong et al speciﬁc setting described care will needed choosing probability measure b c ensure wellbehaved map gt furthermore data assimilation ideas will needed learn appropriate r prediction phase discussed remark rnns cid matthew e levine andrew m stuart numerical experiments section present numerical experiments intended test diﬀerent hypotheses utility hybrid mechanistic datadriven modeling summarize ﬁndings section deﬁne overarching experimental setup section introduce criteria evaluating model performance section lorenz ’ l experiments section investigate simple markovian random features model error term can recovered using discrete continuoustime methods methods scale magnitude error data sampling rate availability training data number learned parameters lorenz ’ multiscale lms experiments section take step learning markovian random features closure term scaleseparated system well systems less scaleseparation expected ﬁnd markovian closure approach highly accurate scaleseparated regime also see markovian closure merit even cases reduced scaleseparation however situation clearly beneﬁt learning closure term memory topic turn section demonstrate nonmarkovian closure models can learnt noisy partially observed data lowdimensional cases eg l method training converges return good model high shortterm accuracy longterm statistical validity higherdimensional cases eg lms ﬁnd method hold promise research required general area section demonstrate nonmarkovian closures must carefully initialized andor controlled eg via data assimilation order ensure longterm stability shortterm accuracy summary findings numerical experiments ﬁnd hybrid modeling better predictive performance purely datadriven methods wide range settings see figures b section includes scenarios f highly accurate imperfect scenarios f highly inaccurate nevertheless faithfully encodes much true structure f† ﬁnd hybrid modeling dataeﬃcient purely datadriven approaches figure section ﬁnd hybrid modeling parametereﬃcient purely data driven approaches figure section purely datadriven discretetime modeling can suﬀer instabilities small timestep limit ∆t cid hybrid discretetime approaches can alleviate issue built integrator ψ will necessarily encode correct parametric dependence ∆t cid figure section order leverage standard supervised regression techniques continuous time methods require good estimates derivatives ˙xt data figure section quantiﬁes estimation function data sample rate nonmarkovian model error can captured markovian terms scale separated cases section demonstrates quantitatively figure qualitatively figure beyond scaleseparation limit markovian terms will fail trajectory forecasting however markovian terms may still reproduce invariant statistics dissipative systems example cases machine learning model error short memorylength section demonstrates quantitatively figure figure oﬀers intuition ﬁndings nonmarkovian description model error needed accurately represent problems hidden dynamics scaleseparated observed dynamics section shows partial noisy observations can exploited augmented ode models form noise hidden dynamics learnt implicitly autodiﬀerentiable data assimilation observe highquality reconstruction l system along ﬁrst component choosing correct figure overly enlarged figure hidden dimension also observe promising reconstruction lms system slow components figure however longtime solutions learnt model exhibited instabilities inconsistent true system nonmarkovian models must carefully initialized indeed data similation needed order ensure accuracy section invariant statistics figure longterm stability figure accurate short term predictions figure explain observed phenomena terms properties desired lowerdimensional invariant manifold embedded within higher dimensional system used rnn’s basis approximation learning markovian model errors noisefree data experimental setup markovian error modeling experiments described sections whether using continuous discretetime models train random features model noisefree trajectories true system ode problems study provably compact global attractor provably l empirically lms ergodic invariant distribution supported global attractor captures statistics longtime trajectories ergodicity independent initial condition data trajectories generated using scipy’s implementation rungekutta via solve ivp absolute relative tolerances − maximum step size − order obtain statistical results create training trajectories true system interest initial conditions sampled independently attractor note training trajectory long enough explore attractor used train separate model purpose observe variance learnt models respect randomly sampled paths attractor use sampling procedure generate short independent validation testing trajectories— use validation trajectories testing trajectories short use evaluate model’s short term forecast performance assessing longterm statistics learnt model compare long simulations true system plots use error bars represent empirical estimates mean standard deviation presented performance metric computed ensembling performance models one per training trajectory testing trajectories total random performance evaluations training procedure also involves independent draw random feature functions deﬁned validation step subsequently performed optimize hyperparameters ω β well regularization parameter λ automate validation using bayesian optimization ﬁnd typically matthew e levine andrew m stuart identiﬁes good hyperparameters within iterations entire process entraining model single long training trajectory including hyperparameter validation typically takes approximately minutes single core ghz skylake cpu allocated gb ram given realization random features optimal λ obtain minimizer c∗ using moorepenrose pseudoinverse implemented scipy pinv learned c∗ paired random feature realization used predict unseen testing trajectories given true initial condition testing trajectories implementing continuoustime given high frequency discretetime data two computational issues must addressed extrapolation data continuoustime ii discretization resulting integrals approach adopt avoids “inverse crimes” favourable behaviour observed agreement data generation mechanism speciﬁc integrator approximation objective functions see queiruga et al illustration issue keller du du et al rigorous analysis inversion process context linear multistep integration methods deep learning interpolate data spline obtain continuoustime trajectories discretize integrals using simple riemann sum strikes desirable balance robustness eﬃciency avoids inverse crimes discretetime approaches however able learn modeldiscrepancy also integratorbased discrepancies hence discretetime methods may artiﬁcially appear outperform continuoustime approaches fact performances might simply considered comparable evaluation criteria models evaluated test set ability predict individual trajectories well invariant statistics invariant measure autocorrelation function trajectory validity time given threshold γ ﬁnd ﬁrst time tγ norm discrepancy true approximate solutions reaches γ cid cid tγ argmint∈t t cidxt − xmtcid ≥ γcidxtcid xt true solution xmt learned approximation normed time average cidxtcid approximated training data threshold violated t deﬁne tγ t rare practice take γ ie relative divergence invariant distribution quantify errors reconstruction invari ant measure consider kullbackleibler kl divergence true invariant measure µ invariant measure produced learned model µm approximate divergence cid cid dµ cid dklµ µm log dµ r dµm integrating kernel density estimates respect lebesgue measure autocorrelation compare autocorrelation function acf respect invariant distribution true learned models approximate acf using fastfouriertransform convolutions seabold perktold compare via normalized l norm diﬀerence machine learning model error lorenz ’ l setting l system described following ode ˙ux auy − ux ˙uy bux − uy − uxuz ˙uz −cuz uxuy whose solutions known exhibit chaotic behavior parameters b c align equations framework starting equation letting x ux uy uzt deﬁning f†x vector ﬁeld appearing righthandside deﬁne discrete solution operator ψ† numerical integration f† ﬁxed time window ∆t corresponding uniform data sampling rate true system given continuoustime discretetime simulate scenarios available physics good imperfect assume exists additive unknown model error form m†x  mx function m determining structure model error scalar coeﬃcient  determining magnitude recall f† f m† assume f known us task learn f† learning m† adding f discrete solution operator ψ obtained b numerical integration f ﬁxed time window ∆t simplify exposition explicitly deﬁne m† let f f† − m† ﬁrst consider setting mx   bux modulate  control magnitude error term case f can viewed l equations perturbed parameter ˜b b −  b artiﬁcially decreased  consider general case heterogeneous multidimensional residual error drawing m zeromean gaussian process gp radial basis kernel lengthscale form map r constructing three independent draws scalarvalued gp r resulting function visualized twodimensional projections figure observe continuoustime framing changes  impact complexity learned error term however grow magnitude error term discretetime framing larger values  can magnify complexity discrepancy ψx − ψ†x results perform series experiments l system order illustrate key points using data learn model errors dynamical systems first demonstrate hybrid modeling tends outperform dataonly physicsonly methods terms prediction quality ﬁrst consider model error see figure study performance validity time versus model error amplitude  using random feature maps d single trajectory length t sampled timestep ∆t unless otherwise speciﬁed also conﬁguration used subsequent experiments matthew e levine andrew m stuart figure visualize example function m obtained single random draw zeromean gaussian process mapping r → r plotted output surface three scalar functions left right ﬁrst two inputs plot axes third input component ﬁxed see identical trends figure b general case non parametric model error term constructed gaussian processes interestingly see small moderate amounts model error  hybrid methods substantially outperform dataonly physicsonly methods eventually large enough model discrepancy hybridmethods dataonly methods similar performance indeed hybridmethod may outperformed dataonly method large discrepancies simple model error appears occur discrepancy term larger magnitude f eg b  model error term bux can take values larger f† figure b also shows continuoustime approach favored discretetime using dataonly methods suggests converse hybrid modeling context suspect artifact diﬀerent integration schemes used data generation training testing phases data generated higherﬁdelity integrator one available training testing continuoustime method presents fundamental limitation forecast quality chose avoid artiﬁcially high forecast validity times however discretetime method can overcome learning mechanistic model discrepancy also discrepancy term associated mismatched integrator typically happens closure perfectly learnable deterministic ie lorenz ’ example case combination physicsbased integratorsourced closures can learned nearly perfectly later experiments multiscale system closures considered approximate model mean noisy underlying process discrete continuoustime methods perform similarly inevitable imperfections learned closure term dominate error rather misspeciﬁed integrator note approximate closures driven scaleseparation much realistic thus expect hybrid discretetime method dramatically outperform machine learning model error b figure plots shows temporal length forecast validity learnt models l function model error parameterized  d t ∆t continuoustime methods shown blue discretetime approaches orange dotted lines indicate purely datadriven methods learn entire vector ﬁeld deﬁning dynamics solid lines indicate methods learn perturbations imperfect mechanistic models f ψ integration using imperfect mechanistic model without recourse data shown green figure employ linear form model error m deﬁned figure b let m single draw gaussian process whose structure shown figure plot means error bars standard deviation hybrid continuoustime methods unless limitations present eg slow sampling rate importantly parameter regime hybrid methods sustain advantage imperfect physicsonly method substantial latter trajectory predictive performance drops oﬀ rapidly small  suggests apparently uninformative model can eﬃciently modiﬁed machine learning techniques obtain useful model outperforms de novo learning approach next show hybrid methods simplify machine learning task terms complexity learned function consequently amount data needed learning figure examines prediction performance validity time function training data quantity using random feature maps d ﬁxed parametric model error  sampling rate ∆t see hybrid methods substantially outperform dataonly approaches regimes limited training data continuoustime example see expected trend dataonly methods able catch hybrid methods acquisition data discretetime models exhibit behavior expect dataonly discretetime model eventually catch albeit additional training data number parameters note greater expressivity also required dataonly methods— choice large d aims give methods ample expressivity thus test convergence respect training data quantity alone results demonstrate advantage matthew e levine andrew m stuart hybrid modeling magniﬁed training data limited fully inform de novo learning figure studies impact expressivity ﬁxing parametric model error  training length t sampling rate ∆t see methods improve larger number random features relative superiority hybrid methods maintained even d figure examine performance proposed methods function length interval training data provided ∆t  d held constant l example see description figure explanation legend observe methods improve increasing training lengths see continuoustime primary beneﬁt hybrid modeling training data limited finally study tradeoﬀs learning discrete versus continuoustime l example figure examines prediction performance validity time function data sampling rate ∆t using random feature maps d ﬁxed parametric model error  abundance training data t observe fast sampling rates ∆t continuous time discretetime hybrid methods similar performance ∆t derivatives become diﬃcult estimate data performance continuoustime methods rapidly decline however discretetime methods sustain predictive performance slower sampling rates ∆t ∈ point discretetime methods deteriorate well discrete map becomes complex learn longer terms sensitivity initial conditions hallmark chaotic systems discretetime methods begin fail around ∆t note can extended longer time intervals increasing d amount training data returns diminish quickly lorenz ’ multiscale lms system machine learning model error figure examine performance proposed methods function model complexity ∆t  t held constant l example see description figure explanation legend observe methods improve increasing number parameters hybrid methods especially beneﬁcial available complexity limited setting consider multiscale system form variable xk ∈ r coupled subgroup fast variables yk ∈ rj x ∈ rk y ∈ rk×j k k j j write ˙xk fkx hxy k rjxk yk ˙ykj y k ε j jcid j ykj b c d e fkx −xk−xk− − xk − xk f rjxk yk −ykjykj − ykj− − ykj hyxk ykjj ykj xkk xk ykkj ykj f ε scaleseparation parameter hx hy ∈ r govern couplings fast slow systems f provides constant forcing set k j hx − hy f leads chaotic dynamics ε small studying scaleseparation consider ε ∈ − − − − consider setting learn markovian random features models variable x alone x data generated coupled x y system large scaleseparation observed x unobserved y spaces can simplify problem accounting unobserved components particular suﬃcient scaleseparation expect markovian term recover large majority matthew e levine andrew m stuart figure shows temporal forecast validity function step size training data tested methods l example hold ﬁxed d  t see description figure explanation legend see purely datadriven discretetime methods struggle short time steps hybrid version thrives scenario approaches course eventually decay large time steps create complex forward maps due sensitivity initial conditions also see continuoustime methods work well small time steps deteriorate tandem quality estimated derivatives residual errors fact simplify problem learning scalarvalued model error m applied xk identically slow system ˙xk ≈ fkx m xk choice stems observations statistical interchangeability amongst slow variables system properties lms model scale separated regime discussed can directly align reduction markovian hybrid learning framework follows ˙x ≈ fx mx fx fx ··· fkxt mx m x ··· m xkt results plot performance gains hybrid learning approaches figure considering validity times trajectory forecasts estimation invariant measure acf estimation three metrics scale separations ε de novo learning discrete ψ† ≈ m continuoustime f† ≈ m inferior using nominal mechanistic model f found amount data used experiments insuﬃcient learn full system scratch hand hybrid models discrete ψ† ≈ ψ m continuoustime f† ≈ f m noticeably outperformed nominal physics machine learning model error surprisingly figure shows markovian closure methods still qualitatively reproduce invariant statistics even large ε settings expect substantial memory eﬀects figure also demonstrates quantitatively using kldivergence invariant measures meansquarederror acfs seems dissipative system memory eﬀects tend average invariant statistics however improvements validity time trajectorybased forecasting deteriorate ε − visualize nonmarkovian structure might exploited examine residuals f figure observe discernible trajectories walking around markovian closure term small ε trajecto ries oscillate rapidly around closure term large ε eg − however observe slowmoving residual trajectory around markovian closure term indicates presence stronger memory component thus beneﬁt nonmarkovian approach closure modeling jiang harlim show memory component setting ε − can described using closure term simple delay embedding previous state lag learn closure using kernel method cast rkhs framework random feature methods provide approximation learning partial noisy observations section focus nonmarkovian setting outlined section attempt model dynamics observable using f f given twolayer fully connected neural networks gelu activations perform learning minimizing section using dvar data assimilation adam optimizer learning rate initialized tuned automatically using scheduler halved learning rate training error decreased minibatched epochs data sampled ∆t cases normalized mean zero unit variance numerical integration performed torchdiffeq implementation dormand prince adaptive ﬁfthorder rungekutta method l example simple backpropagated autodiﬀerentiation performed solver lms example used adjoint method provided lorenz ’ ﬁrst consider modeling dynamics ﬁrstcomponent l system noisily observe ﬁrstcomponent – observe noisy trajectory ux iid additive zeromean varianceone gaussian observe remaining components uy uz jointly trained trajectories length t randomly initialized box around attractor chose approach ensure data coverage oﬀ attractor although note similar success obtained single trajectory length t neural network width chose assimilation time τ forecast time τ optimization ran approximately epochs took roughly hrs single gpu adequate results obtained using ﬁxed dvar gain matrix k t however present results using algorithm k θda jointly learned along parameters θdyn described section demonstrates gain need known priori first present results using knowledge correct hidden dimension dr figure show example trained model assimilated matthew e levine andrew m stuart figure ﬁgure shows performance diﬀerent approaches modeling lms slow subsystem f† ≈ f use nominal physics f ψ† ≈ m f† ≈ m try learn entire righthandside using data discrete continuoustime settings respectively ψ† ≈ ψ m f† ≈ f m focus learning markovian residuals known physics discrete continuoustime settings respectively residualbased correctors substantially outperform nominal physics purely datadriven methods according presented metrics invariant measure shown qualitatively ﬁrst row quantitatively third row acf shown qualitatively second row quantitatively fourth row trajectory forecasts shown ﬁnal row boxplots show distributions quantitative metrics eg kl divergence squared errors validity time come diﬀerent models trained diﬀerent trajectory generated using independent random feature set notably markovian residualbased methods’ performance deteriorates small scaleseparation ε − markovian assumption breaks probabilityf trueffmffmf trueffmffmf trueffmffmacfautocorrelationf trueffmffmautocorrelationf trueffmffmautocorrelationf trueffmffmkldivergencekldivergencekldivergenceacf erroracf erroracf errorffmfmmffmmodelvalidity timeffmfmmffmmodelvalidity timeffmfmmffmmodelvalidity time machine learning model error figure ﬁgure shows observed estimated residuals nominal physics model f lms slow subsystem diﬀerent scaleseparation factors ﬁrst row shows density residuals yellow high density blue low well ﬁt closure terms continuous blue discrete orange time discrete model normalized dividing ∆t second row shows temporal structure errors residual ﬁt superimposing short t onedimensional trajectory represents ∼ training data using dvar learnt k ﬁrst time units predicting another time units recall training performed using τ forecasting horizon evaluate longer time horizon provide robust test metric observe learnt hidden dynamics gray synchronized data used perform forecast figures b c show solving system long time t able accurately reproduce invariant statistics invariant measure autocorrelation resp true system figure d show evolution learnt k next let dr exceeding true dimension hidden states thus able explore issues caused learning overly expressive terms dimension hidden states dynamical model figure shows dynamics learnt model setting found reproduction invariant statistics similar cases figures b c omit plots brevity success aligns approximation theory discussed remark provides empirical reassurance methodology can behave well situations dimension hidden variable unknown dimension dr used learning exceeds true dimension nevertheless construct example section speciﬁc embedding true dynamics system higher dimension can lead poor approximation caused instability model allows departure invariant manifold true dynamics accurately captured however emphasize phenomenon observed empirically experiment reported dr nonetheless also note expected decreases eﬃciency caused overestimating dimension hidden variable model training testing thus determining matthew e levine andrew m stuart smallest choice dr compatible good approximation important recent research addressed challenge discretetime setting applying manifold learning delayembedding space using learnt manifold inform initialization dimensionality lstm hidden states note early attempts achieving numerical results using optimization ideas sections yielded unstable models exhibited blowup shorter time scales eg t however incorporating data assimilation tuning optimization achieve lower training errors able obtain model empirically exhibit blowup even solved long time eg t also note unable achieve highﬁdelity results using methods neural networks nonlinear activation functions may explained noting ouala et al achieved results using linear identitybased activations resulting inference polynomial models containing true l model lorenz ’ multiscale ε − recall markovian closures fail capture autocorrelation statistics slow components model case ε − see top right panel evidenced slowmoving trajectory around markovian closure figure case ripe nonmarkovian modeling investigate applicability continuoustime ode formulation using neural network width applied described methodology minimizing data setting described section learn hidden dynamics similarly previous section jointly trained trajectories length t randomly initialized box around attractor chose assimilation time τ forecast time τ note longer times can become quite costly especially highdimensional systems nevertheless assimilation time τ appears intrinsically tied amount memory present system figures b plot comparisons true learnt via acf invariant measure observe substantial improvement markovian closure however learnt model exhibited instabilities solved longer t expect can remedied via training found l example however incorporation stability constraints model valuable order train larger model longer time studies eﬃcient optimization must also performed setting begun highly relevant investigation direction figure visualize learnt dvar gain encodes learnt model’s covariance structure row corresponds gain given component learnt model function observed components indexed columns trends elucidated via hierarchical clustering rowbased malization learnt matrix k clearly learns consistent diagonal covariance structure observables impressively illustrates crosscovariances tween observed hidden components mirror compartmentalized structure model note observed component distinct grouping hidden variables high correlation white primarily component low correlation black observables type analysis may provide greater interpretability learnt models hidden dynamics machine learning model error b c d figure ﬁgure concerns learning continuoustime rnn model l system based noisy observation ﬁrst component uses augmented state space dr figure shows trained model can used forecasting— ﬁrst synchronizing data using dvar forecasting future tophalf depicts dynamics observed component model solutions blue observations yellow bottomhalf depicts augmented state space hidden components shown gray observed validity time roughly model time units figures b c shows longtime solutions learnt model accurately mirror invariant statistics invariant measure autocorrelation resp true system figure d shows learning process estimating dvar gain k initializing stabilizing rnn mentioned remark rnn approximates enlarged system contains solutions original system trajectories conﬁned invariant manifold m m†x y see identity however invariant manifold may unstable either manifold within continuoustime model result numerical instability now demonstrate numerical experiments instability points need data assimilation used rnns prediction original system desired initialize system also stabilize dynamics remain near desired invariant manifold illustrate challenges consider problem modeling evolution single component l system consider variable x true state nnpredicted latent statetrue state noisy nnassimilated latent statennassimilated latent statennpredicted latent statedensityfirst coordinate kde alltimetrue systemnn systemtime lagacftrue systemnn system matthew e levine andrew m stuart figure ﬁgure concerns learning continuoustime rnn model l system based noisy observation ﬁrst component uses augmented state space dr tophalf depicts dynamics observed component modelsolutions blue observations yellow bottomhalf depicts augmented state space hidden components shown gray ﬁrst time units model assimilated sequence observed data using dvar subsequent time units forecast issued found model similar shortterm longterm ﬁdelity compared model presented figures d used correct hidden dimension dr b figure ﬁgure concerns learning continuoustime rnn model ﬁrst slow components lms system ε based noisy observations slow components uses augmented state space dr trained using noised observations standard deviation ﬁrst components true −dimensional system plots show model can accurately reproduce invariant measure figure acf figure b observed states statistics calculated running learnt model t model time units longer runs encountered instabilities caused trajectories leave attractor blowup true state nnpredicted latent statetrue state noisy nnassimilated latent statennassimilated latent statennpredicted latent statenonedensityfirst coordinate kde alltimetrue systemnn systemtime lagacftrue systemnn system machine learning model error figure visualize learnt dvar gain matrix k x θda section associated nonmarkovian learning lms ﬁrst compute entrywise absolute values apply rownormalization white indicates highest correlation black indicates lowest correlation top rows shown directly correspond ﬁrst rows k bottom rows reordered via hierarchical clustering illustrate associations observed components hidden variables exhibited model error may addressed setting learning representation contains hidden states y ie two unobserved components since dimension hidden states typically known priori dimensions latent variables rnn system approximates may greater y speciﬁc construction use prove existence approximating rnn introduce vector ﬁeld evolution error m well y now discuss implications embedding true dynamics higher dimensional system speciﬁc context embedded system however observations apply embedding desired dynamics  within higher dimensional system choose examples implies m − m† constant time cidm − m†x ycidt constant matthew e levine andrew m stuart constant time desired invariant manifold constant thus stable however stability holds neutral sense linearization manifold exhibits zero eigenvalue related translation m − m† constant now illustrate embedded invariant manifold can unstable case instability caused numerical integration breaks conservation m − m† time example consider equation write form setting x ux y uy uz let fux −aux yielding m†uy auy thus f† f m† deﬁned ﬁrst component righthand side function g†uy uz given second third components righthand side applying methodology leading results following four dimensional system b c d ˙ux fux m ˙uy bux − uy − uxuz ˙uz −cuz uxuy ˙m acidbux − uy − uxuz cid ux x uy y uz z m m†y omitted uydependence equation ux aim learn error term introduce variable m order system projected ux uy uz behaves identically m m†y thus −dimensional system embedded invariant manifold dynamics coincident −dimensional l system numerically integrate −dimensional system model time units initialized x y z m ay show figure resulting measure ux dashed red nearly identical invariant measure traditional −dimensional l system solid black however rerun simulation perturbed m m†y see figure dotted blue yields diﬀerent invariant measure ux result emphasizes importance correctly initializing rnn eﬃcient trajectory forecasting also accurate statistical representation longtime behavior example now consider write form setting x uz y ux uy let fuz −cuz m†ux uy uxuy f† f m† corresponds third component righthand side function g†ux uy deﬁned ﬁrst two components righthand side form −dimensional system corresponding using methodology leads b c d ˙ux auy − ux ˙uy bux − uy − uxuz ˙uz fuz m ˙m ux ˙uy uy ˙ux ux x uy y uz z m m†x y integrate model time units initialized x y z m xy show figure −dimensional lorenz attrac tor unstable respect perturbations numerical integration −dimensional system solutions ux uy uz eventually collapse ﬁxed point growing discrepancy mt m† becomes large machine learning model error figure show invariant density ﬁrst component l black can reproduced correctly initialized augmented −d system dashed red however incorrect initialization m dotted blue yields diﬀerent invariant density time collapse occurs may delayed using smaller tolerances within numerical integrator employ matlab rk demonstrating instability caused numerical integrator collapse undesirable prediction longtime statistics desirable goal hand figure shows shortterm accuracy −dimensional system model time units correctly initialized m m†x y dashed red accuracy model time units initialization m perturbed m m†x y dotted blue result demonstrates fundamental challenges representing chaotic attractors enlarged dimensions may help explain observations rnns yielding good shortterm accuracy inaccurate longterm statistical behavior empirical stability observed discretetime lstms general problem illustrated likely manifest problems dimension learned model exceeds true model issue address initialization models interaction data assimilation therefore merits study conclusions work evaluate utility blending mechanistic models dynamical systems datadriven methods demonstrating power hybrid approaches provide mathematical framework consistent across parametric nonparametric models encompasses continuous discretetime allows markovian memorydependent model error also provide basic theoretical results underpin adopted approaches uniﬁed framework elucidates commonalities seemingly disparate approaches across various applied theoretical disciplines desirable growing recognition need hybrid modeling motivate ﬂexible incorporation mechanistic models opensource software continuoustime markovian nonmarkovian modeling error work focused immutable mechanistic models f ψ models often tunable parameters principle one can jointly learn parameters mechanistic model closure term however lack matthew e levine andrew m stuart figure show embedded −dimensional manifold l within −dimensional system given unstable indeed correctly initialized −dimensional system dashed red solution decays ﬁxed point bottom ﬁgure shows divergence numerically integrated model error term mt statedependent term m† growing discrepancy likely responsible eventual collapse −dimensional system identiﬁability modifying closure modifying physics brings interesting question explainability future work might focus decoupling learning parameters closure terms maximal expressivity ﬁrst squeezed mechanistic model numerical results demonstrate superiority hybrid modeling learning entire system scratch even available mechanistic model large inﬁdelities hybrid modeling also showed surprisingly large performance gains using mechanistic models small inﬁdelities quantify improvements terms data hunger demands model complexity overall predictive performance ﬁnd three signiﬁcantly improved hybrid methods experiments √ establish bounds excess risk generalization error decay t learning model discrepancy trajectory length t ergodic continuoustime markovian setting make minimal assumptions nominal physics ie f ∈ c thus result equivalently holds learning entire vector ﬁeld f† ie f ≡ however upper bounds excess risk generalization error scale size function learned hence going way towards explaining superiority hybrid modeling observed numerical experiments future theoretical work aimed quantifying beneﬁts hybrid learning versus purely datadriven learning interest also note ergodic assumption underlying theory will satisﬁed many dynamical models alternate statistical learning theories need developed settings machine learning model error figure show shortterm accuracy −dimensional system predictions using correct initialization m dashed red remain accurate nearly twice long predictions use perturbed initialization m m†ux uy bottom ﬁgure shows mt diverges statedependent m† quickly poorly initialized model cases errors accumulate time illustrate tradeoﬀs discretetime continuoustime modeling approaches studying performance function training data sample rate ﬁnd hybrid discretetime approaches can alleviate instabilities seen purely datadriven discretetime models small timesteps likely due structure integrator ψ correct parametric dependence timestep continuoustime setting ﬁnd performance best derivatives can accurately reconstructed data deteriorates tandem diﬀerentiation inaccuracies caused large timesteps continuoustime hybrid methods appear oﬀer additional robustness inaccurate diﬀerentiation compared purely datadriven methods cases large timesteps poorly resolved derivatives ensemblebased data assimilation methods may still allow accurate learning residuals ﬂow ﬁeld continuoustime modeling finally study nonmarkovian memorydependent model error nu merical experiments theory using rnns prove universal approximation continuoustime hybrid rnns demonstrate successful deployment methodology future work focusing eﬀective training models complex problems great value ideas data assimilation likely play central role work theoretical properties reservoir computing rc variants rnns also value beneﬁt convex optimization may viewed random feature methods banach spaces matthew e levine andrew m stuart rnn rc methods will beneﬁt constraining learning ensure stability latent dynamical model issues illustrated via numerical experiments relate rnns question stability invariant manifolds representing embedded desired dynamics within higher dimensional system appendix proof excess riskgeneralization error theorem note ϕ· evaluated compact obviating need boundedness assumptions functions fcidp lemma let assumptions hold σ positive semi ∞ ⇒ deﬁnite symmetric rp×p θ∗ n σ respect x ∼ µ furthermore constant c ∈ ∞ almost surely wrt x ∼ µ cid m† follows ∞ almost surely t − θ∗ t θ∗ √ t → θ∗ cid t cid log log t proof rearranging equation θ∗ limsupt→∞ cidθ∗ t − θ∗ ∞cid ≤ c ∞ see θ∗ θ∗ t bt ∞ b∞ − ∞θ∗ ∞ thus subtracting t − θ∗ θ∗ preceding arecidfi· fj·cid andcidm†· fj·cid thus entry matrix fcid· m†· h¨older assumption discussion immediately t bt − b∞ − − t − ∞θ∗ ∞ ∞ − resp vector bt converges almost surely corresponding entry ∞ resp b∞ ergodicity implied assumption pointwise ergodic theorem almost sure convergence θ∗ ∞ follows noting ∞ invertible furthermore also assumption constants σijσj t θ∗ cid cid √ √ t t cid ⇒ n σ cid ⇒ n σ j ij ij − ∞ij bt j − b∞j √ since arbitrary linear combinations ijbt j timeaverages tat − ∞ bt − b∞ converges distribution h¨older functions follows ∞ gaussian cram´erwold theorem weak convergence gaussian follows use slutsky lemma since converges almost surely invertible ∞ matrix σ identiﬁed explicitly terms σijσi correlations bt almost sure ∞cid follows multiplying t log log t bound cidθ∗ noting → ∞ almost surely almost sure bounds t log log t cidat − ∞cidcidbt − b∞cid using assumption cid t − θ∗ t −θ∗ t θ∗ √ follows helpful deﬁne t − θ∗ t θ∗ r t m∗ g tcid cidθ∗ ∞ ∞ − ∞m∗ ∞cidcidθ∗ ∞cid cid machine learning model error lemma let assumption hold assuming x ∼ µ constant c excess risk rt satisﬁes rt ≤ ccidr t cid furthermore generalization error satisﬁes gt ≤ ccidr rdx cid ≤cidcid ∞ m† ∞x m∗ cid t m† − lµm∗ m∗ cidcidm∗ t − m∗ t − m∗ t cid g t proof bound excess risk note cid rt lµm∗ cidcid ∞ − m†x t m∗ cid cid cidcidm∗ t m∗ cid m† since ﬁrst term ∞cid second ﬁrst follows boundedness fcidp product bounded constant multiple cidθ∗ term constant multiple cidθ∗ ∞cid supa cidm†cid bound generalization error note ∞ − m†xcidcid ∞xcidcid t − θ∗ µdx µdx µdx rdx rdx tcid cidθ∗ t − ∞m∗ t t − m∗ ∞ m∗ ∞ − ∞m∗ ∞ ∞ − ∞m∗ ∞m∗ t ∞cid g t − rt t − m∗ gt m∗ m∗ cidit m∗ third term ﬁnal identity excess risk just bounded ﬁrst term may bounded manner bounded excess risk noting integration respect µ simply replaced integration respect empirical measure generated trajectory data assumption conﬁned attractor second term simply g t thus cid result follows ∞xcid proof theorem assumption choice ϕx cidm†x−m∗ √ t g t converges distribution scalarvalued centred gaussian lemma t converges distribution centred gaussian t converges distribution slutsky lemma rp cramerwold theorem centred gaussian rp convergence distribution results excess risk rt generalization error gt follow lemma assumption furthermore lemma constant c t g t r t r √ √ cid cid cid cid limsupt→∞ t log log t cidr t cid ≤ c similarly possibly enlarging c assumption gives t ≤ c limsupt→∞ g t log log t desired almost sure bound rt gt follows lemma cid matthew e levine andrew m stuart proof continuoustime ode approximation theorem general case nm parameterizations θg ∈ rng θm ∈ rnm cidx ycid ∈ proof recall equations δo exist dimensions ng b ρt maximum norm cidg†x y − fx y θgcid ≤ δo cidm†x y − fx y θmcid ≤ δo using can rewrite uniformly forcidx ycid ∈ b ρ ˙x fx fx y θm ext ˙y fx y θg eyt cideytcid ≤ δo cidextcid ≤ δo sup t∈t sup t∈t removing bounded error terms obtain approximate system ˙xδ fxδ fxδ yδ θm ˙yδ fxδ yδ θg next obtain stability bound discrepancy approximate system true system originally written reformulated first let w x y wδ xδ yδ deﬁne f concatenated righthandside note f l−lipschitz maximum norm b ρt l related lipschitz continuity f f f can write true approximate systems respectively using maximum norm b ˙w f w ewt ˙wδ f wδ sup t∈t cidewtcid ≤ sup t∈t cideytcid sup t∈t cidextcid ≤ δo cid t cid t let p w x y t ∈ t p w p wδ ∈ b ρ follows writing integrated form subtracting taking norms cidwt − wδtcid ≤cidcidw − wδcidcid cidcidfcidwscid − f wδscidcidcidds using facts thatcidcidewscidcid ≤ δo f l−lipschitz obtain t ∈ t cidcidws − wδscidcidds cidcidewscidcidds cid t cidcidwt − wδtcidcid ≤cidcidw − wδcidcid δot l cidcidwt − wδtcidcid ≤cidcidw − wδcid δotcid explt integral form gronwall lemma follows t ∈ t machine learning model error cidcidwt − wδtcidcid ≤cidcidw − wδcid δotcid explt thus sup t∈t choice initial conditions δo suﬃciently small can achieve δ approximation finally note approximate system function parameter θδ θm θg ∈ rnδ nδ ng nm cid proof continuoustime rnn approximation theorem linear observation proof recall equations approximation theory means twolayer feedforward neural networks δo exist embedding dimensions ng nh parameterizations θg cg ∈ rdy×ng bg ∈ rng×dx ag ∈ rng×dy cg ∈ rng θh ch ∈ rdx×nh bh ∈ rnh×dx ah ∈ rnh×dy ch ∈ rnh anycidx ycid ∈ b ρt maximum norm cidg†x y − cgσbgx agy cgcid ≤ δo cidh†x y − chσbhx ahy chcid ≤ δo without loss generality may assume cg ch full rank since arbitrarily small changes can made restore full rank using parameterizations embedding dimensions can rewrite ˙x fx m ˙y cgσbgx agy cg eyt ˙m chσbhx ahy ch em†t uniformly forcidx ycid ∈ b ρ cideytcid ≤ δo cidem†tcid ≤ δo sup t∈t sup t∈t removing bounded error terms obtain approximate system ˙xδ fxδ mδ ˙yδ cgσbgxδ agyδ cg ˙mδ chσbhxδ ahyδ ch mδt initialized m†x y next obtain stability bound discrepancy approximate system true system originally written reformulated first let w x y m wδ xδ yδ mδ deﬁne f concatenated righthandside note f l−lipschitz maximum norm l related lipschitz continuity f approximation parameterization θδ regularity nonlinear activation function σ can write true approximate systems respectively b ˙w f w ewt ˙wδ f wδ matthew e levine andrew m stuart sup t∈t cidewtcid ≤ sup t∈t cideytcid sup t∈t cidem† tcid ≤ δo let p w x y p ⊥w m recall p ⊥w deﬁned terms p w t ∈ t p w p wδ ∈ b ρ cidwt − wδtcid ≤cidcidw − wδcidcid cid t cidcidewscidcidds cid t cidcidfcidwscid − f wδscidcidcidds following logic section cidcidwt − wδtcidcid ≤cidcidw − wδcid δotcid explt sup t∈t choice initial conditions δo suﬃciently small can achieve δ approximation finally note approximate system may written recurrent neural network form follows consider equations ˙xδ fxδ chnδ ˙zδ σbgxδ agcgzδ cg ˙nδ σbhxδ ahcgzδ ch deﬁned zδ nδ terms yδ mδ yδ cgzδ mδ chnδ now note equivalent recurrent state rδ parameters θδ given • rδ cid nδ cid cidzδ • cδ cid ch cidbg cid cidagcg cidcg cid • bδ • aδ • cδ ahcg bh ch cid initial condition yδ mδ may achieved choice initializations zδ nδ since cg ch full rank cid random feature approximation random feature methods lead func tion approximation mappings hilbert spaces x → y oper ate constructing probability space θ νf θ ⊆ rp feature map ϕ x × θ → y kx xcid eϑϕx ϑ ⊗ ϕxcid ϑ ∈ ly y forms reproducing kernel associated reproducing kernel hilbert space rkhs k l ϑl picked iid solutions sought within spanϕ· ϑlm random theory supporting approach established ﬁnite dimensions rahimi recht method recently applied inﬁnite dimensional setting now explain precise random features setting adopted section hypothesis classes given start random feature functions machine learning model error ϕ· ϑ rdx → r ϑ w b w ∈ rdx ∼ u−ω ω b ∈ r ∼ u−β β ϕx w b tanhwt x b ω β choose d iid draws w b stack resulting random feature functions form map φx rdx → rd given φx cidϕx w bw ϕx wd bdcidt deﬁne hypothesis class introducing matrix c rd → rdx seeking approximation model error form mx cφx optimizing least squares function matrix c quite correspond random features model x y rdx written linear span vector ﬁelds mapping rdx vector ﬁelds independent nonetheless found approach convenient practice employ numerics align random features model x y rdx choose d dx draw p functions φ· labelled fcid· iid random preceding construction leading hypothesis class seek approximation model cid θcidfcidx ﬁnd form random features model error form mx cidp convenient explain learning theory perspective model error derivation tikhonovregularized linear inverse problem show optimization cid t cidcid ˙xt − fx − cφcidxtcidcidcid dt cidccid λ jt c t reduces tikhonovregularized linear inverse problem since quadratic ∂c c∗ c exists unique global minimizer c∗ ∂jt minimizer c∗ satisﬁes z λic t y z φ ⊗ φt y φ ⊗ ˙x − ft ⊗ bt atbt t cid t atdt t ∈ rm×n bt ∈ rm×l see observe matthew e levine andrew m stuart cidccid λ cid ˙xt − f cid ˙xt − f cid t cidxtcid − cφxtciddt cid t cidxtcidcid cidcφcidxtcid cφcidxtcidcid − cid ˙xt − f cid t ccidφcidxtcid ⊗ φcidxtcidcid − cid ˙xt − f cidcid ˙xt − f cidφcidxtcid ⊗ φcidxtcidciddt λi cid t t cid λ cidc ccid cidxtcid cφcidxtcidciddt cidxtcid ⊗ φcidxtcidciddt λc cidxtcidcid ⊗ φcidxtcidciddt t setting gradient zero see jt c t t ∂jt c ∂c cid cid t t c finally can take transpose sides apply deﬁnitions y z use symmetry z get z λic t y references romeo alexander dimitrios giannakis operatortheoretic framework forecasting nonlinear time series kernel analog techniques physica d nonlinear phenomena august issn doi jphysd url httpwwwsciencedirectcomscience articlepiisx ranjan anantharaman yingbo ma shashi gowda chris laughman viral shah alan edelman chris rackauckas accelerating simulation stiﬀ nonlinear systems using continuoustime echo state networks october url httpsarxivorgabsv mark asch marc bocquet ma¨elle nodet data assimilation methods algorithms applications siam ibrahim ayed emmanuel de b´ezenac arthur pajot julien brajard patrick gallinari learning dynamical systems partial observations february yunhao ba guangyuan zhao achuta kadambi blending diverse physical priors neural networks arxiv cs stat october url httparxivorgabs arxiv dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv cs stat may url httparxivorgabs arxiv wael bahsoun ian melbourne marks ruziboev variance continuity lorenz ﬂows annales henri poincare volume pages – springer issue machine learning model error randall d beer dynamics small continuoustime recurrent neural networks adaptive behavior – march issn doi url httpjournals sagepubcomdoi alain bensoussan jacqueslouis lions george papanicolaou asymptotic analysis periodic structures volume american mathematical soc jos´e bento morteza ibrahimi andrea montanari information theoretic limits learning stochastic diﬀerential equations ieee international symposium information theory proceedings pages – ieee tom beucler michael pritchard stephan rasp jordan ott pierre baldi pierre gentine enforcing analytic constraints neural networks emulating physical systems physical review letters publisher aps kaushik bhattacharya bamdad hosseini nikola b kovachki drew m stuart model reduction neural networks paramet ric pdes arxiv cs math stat may url http arxivorgabs arxiv marc bocquet julien brajard alberto carrassi laurent bertino bayesian inference chaotic dynamics merging data assimilation ma chine learning expectationmaximization foundations data science doi fods url httpswwwaimsciences orgarticledoifods company foundations data science distributor foundations data science institution foundations data science label foundations data science publisher american institute mathematical sciences francesco borra angelo vulpiani massimo cencini eﬀective models predictability chaotic multiscale systems via machine learning physical review e november doi physreve url httpslinkapsorgdoiphysreve pub lisher american physical society julien brajard alberto carassi marc bocquet laurent bertino com bining data assimilation machine learning emulate dynamical model sparse noisy observations case study lorenz model journal computational science july issn doi jjocs url httparxivorgabs arxiv julien brajard alberto carrassi marc bocquet laurent bertino com bining data assimilation machine learning infer unresolved scale parametrization philosophical transactions royal society mathe matical physical engineering sciences april doi rsta url httpsroyalsocietypublishingorg doifullrsta publisher royal society leo breiman bagging predictors machine learning – august issn doi bf n d brenowitz c s bretherton neural network uniﬁed physics parameterization research letters – prognostic validation geophysical doi issn matthew e levine andrew m stuart httpsdoiorggl onlinelibrarywileycomdoiabsgl httpsagupubsonlinelibrarywileycomdoipdfgl httpsagupubs eprint url steven l brunton joshua l proctor j nathan kutz discovering governing equations data sparse identiﬁcation nonlinear dynamical systems proceedings national academy sciences – april issn doi pnas url httpswwwpnasorgcontent publisher national academy sciences section physical sciences dmitry burov dimitrios giannakis krithika manohar andrew stuart kernel analog forecasting multiscale test problems arxiv physics stat may url httparxivorgabs arxiv kathleen champion bethany lusch j nathan kutz steven l brunton datadriven discovery coordinates governing equations proceedings national academy sciences – november issn doi pnas url https wwwpnasorgcontent publisher national academy sciences section physical sciences bo chang minmin chen eldad haber ed h chi antisymmetricrnn dynamical system view recurrent neural networks arxiv cs stat february url httparxivorgabs arxiv ashesh chattopadhyay pedram hassanzadeh devika subramanian datadriven predictions multiscale lorenz chaotic system using machinelearning methods reservoir computing artiﬁcial neural network long shortterm memory network nonlinear processes geophysics – july issn doi httpsdoiorg npg url httpsnpgcopernicusorgarticles publisher copernicus gmbh ashesh chattopadhyay adam subel pedram hassanzadeh datadriven superparameterization using deep learning experimentation multiscale lorenz systems transfer learning journal advances modeling earth systems ems issn doi url httpsagupubs onlinelibrarywileycomdoiabsms eprint httpsagupubsonlinelibrarywileycomdoipdfms httpsdoiorgms yifan chen bamdad hosseini houman owhadi andrew m stuart solv ing learning nonlinear pdes gaussian processes arxiv cs math stat march url httparxivorgabs arxiv yuming chen daniel sanzalonso rebecca willett autodiﬀerentiable ensemble kalman filters arxiv cs stat july oksana chkrebtii david campbell ben calderhead mark girolami others bayesian solution uncertainty quantiﬁcation diﬀerential equa tions bayesian analysis – publisher international society bayesian analysis machine learning model error kyunghyun cho bart van merrienboer dzmitry bahdanau yoshua bengio properties neural machine translation encoderdecoder approaches arxiv cs stat october url httparxiv orgabs arxiv kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bah danau fethi bougares holger schwenk yoshua bengio learning phrase representations using rnn encoderdecoder statistical ma chine translation arxiv cs stat september url httparxivorgabs arxiv alexandre j chorin fei lu discrete approach stochastic parametriza tion dimension reduction nonlinear dynamics proceedings national academy sciences – august issn doi pnas url httpswwwpnas orgcontent publisher national academy sciences section physical sciences alexandre j chorin ole h hald raz kupferman optimal prediction mori–zwanzig representation irreversible processes proceedings national academy sciences – publisher national acad sciences david colton rainer kress inverse acoustic electromagnetic scat tering theory applied mathematical sciences springerverlag new york edition isbn doi url httpswwwspringercomgpbook peter craven grace wahba smoothing noisy data spline functions numerische mathematik – g cybenko approximation superpositions sigmoidal function math ematics control signals systems – december issn x doi bf url httpsdoiorg bf eric darve jose solomon amirali kia computing generalized langevin equations generalized fokker–planck equations proceedings na tional academy sciences – publisher national acad sciences ronald devore george g lorentz constructive approximation grundlehren der mathematischen wissenschaften springerverlag berlin heidelberg isbn url httpswwwspringer comgpbook jonathan dong ruben ohana mushegh rafayelyan florent krzakala reservoir computing meets recurrent kernels structured transforms arxiv cs eess stat october url httparxivorg abs arxiv j r dormand p j prince family embedded rungekutta formulae journal computational applied mathematics – march issn doi x url httpswww sciencedirectcomsciencearticlepiix qiang du yiqi gu haizhao yang chao zhou discovery dy namics via linear multistep methods deep learning error estimation arxiv cs math march url httparxivorgabs matthew e levine andrew m stuart arxiv karthik duraisamy gianluca iaccarino heng xiao turbulence modeling age data annual review fluid mechanics – publisher annual reviews weinan e chao ma lei wu priori estimates population risk twolayer neural networks communications mathematical sciences – issn doi cms vna url httparxivorgabs arxiv n benjamin erichson omri azencot alejandro queiruga liam hodgkin son michael w mahoney lipschitz recurrent neural networks arxiv cs math stat october url httparxivorg abs arxiv alban farchi patrick laloyaux massimo bonavita marc bocquet using machine learning correct model error data assimilation forecast applications arxiv physics stat may url http arxivorgabs arxiv ibrahim fatkullin eric vandeneijnden computational strategy multiscale systems applications lorenz model journal compu tational physics – publisher elsevier brian freno kevin t carlberg machinelearning error models approximate solutions parameterized systems nonlinear equations com puter methods applied mechanics engineering – may doi jcma url https wwwsciencedirectcomsciencearticlepiis issn roger frigola yutian chen carl edward rasmussen variational gauss ian process statespace models advances neural information processing systems url httpsproceedingsneuripsccpaper hashffdedebcacfabstracthtml kenichi funahashi yuichi nakamura approximation dynamical systems continuous time recurrent neural networks neural networks – january issn doi s x url httpwwwsciencedirectcomsciencearticlepii sx daniel j gauthier erik bollt aaron griﬃth wendson s barbosa next generation reservoir computing nature communications september issn doi s faheem gilani dimitrios giannakis john harlim kernelbased prediction nonmarkovian time series physica d nonlinear phenomena april issn doi jphysd url https wwwsciencedirectcomsciencearticlepiis r gonz´alezgarc´ıa r ricomart´ınez ig kevrekidis identiﬁcation distributed parameter systems neural net based approach computers chemical engineering s–s march issn doi s url httpslinkinghubelseviercom retrievepiis ian goodfellow yoshua bengio aaron courville deep learning mit press machine learning model error georg gottwald sebastian reich combining machine learning data assimilation forecast dynamical systems noisy partial observations chaos interdisciplinary journal nonlinear science october issn doi georg gottwald sebastian reich supervised learning noisy observations combining machinelearning techniques data assimilation physica d nonlinear phenomena september issn doi jphysd url httpswwwsciencedirect comsciencearticlepiis ayoub gouasmi eric j parish karthik duraisamy priori estimation memory eﬀects reducedorder models nonlinear systems using mori–zwanzig formalism proceedings royal society mathematical physical engineering sciences september doi rspa url httpsroyalsocietypublishingorgdoi rspa publisher royal society wojciech w grabowski coupling cloud processes largescale dy namics using cloudresolving convection parameterization crcp journal atmospheric sciences – may issn doi cidccpwtlcid co url httpsjournalsametsocorgviewjournalsatsc ccpwtlcoxml publisher american meteorological society section journal atmospheric sciences lyudmila grigoryeva juanpablo ortega echo state networks univer sal arxiv cs august url httparxivorgabs arxiv geoﬀrey grimmett david stirzaker probability random processes oxford university press abhinav gupta pierre f j lermusiaux neural closure models dynamical systems arxiv physics may url http arxivorgabs arxiv eldad haber lars ruthotto stable architectures deep neural net works inverse problems december issn doi aaa url httpsdoiorg aaa publisher iop publishing franz hamilton alun l lloyd kevin b flores hybrid model ing prediction dynamical systems plos computational biology e july issn doi journalpcbi url httpsjournalsplosorgploscompbiolarticleid journalpcbi boumediene hamzi houman owhadi learning dynamical systems data simple crossvalidation perspective part parametric ker nel ﬂows physica d nonlinear phenomena july issn doi jphysd url httpslinkinghub elseviercomretrievepiis john harlim shixiao w jiang senwei liang haizhao yang ma chine learning prediction missing dynamics journal computa tional physics march issn doi jjcp url httpswwwsciencedirectcomsciencearticle matthew e levine andrew m stuart piis simon haykin jose c principe terrence j sejnowski john mcwhirter modeling large dynamical systems dynamical consistent neural new directions statistical signal processing sys networks tems brains pages – mit press isbn url httpsieeexploreieeeorgdocument conference name new directions statistical signal processing systems brains dan hendrycks kevin gimpel gaussian error linear units gelus doi arxiv url httpsarxivorgabs carmen hij´ pep espa˜nol eric vandeneijnden rafael delgado buscalioni mori–zwanzig formalism practical computational tool faraday discussions – publisher royal society chemistry sepp hochreiter j¨urgen schmidhuber long shortterm memory neural computation – december doi neco mark holland ian melbourne central limit theorems invariance principles lorenz attractors journal london mathematical society – publisher oxford university press fabr´ıcio p h¨arter haroldo fraga de campos velho data assimila tion procedure recurrent neural network engineering applications computational fluid mechanics – january issn doi url httpsdoiorg publisher taylor francis eprint httpsdoiorg herbert jaeger ” echo state” approach analysing training recurrent neural networkswith erratum note’ bonn germany german national research center information technology gmd technical report january xiaowei jia jared willard anuj karpatne jordan s read jacob zwart michael steinbach vipin kumar physicsguided machine learning scientiﬁc discovery application simulating lake temperature proﬁles acmims transactions data science – may issn doi url httpsdoiorg shixiao w jiang john harlim modeling missing dynamical systems deriving parametric models using nonparametric framework research mathematical sciences september issn doi s url httparxivorgabs arxiv kadierdan kaheman eurika kaiser benjamin strom j nathan kutz steven l brunton learning discrepancy models experimental data arxiv cs eess stat september url httparxivorg abs arxiv kadierdan kaheman steven l brunton j nathan kutz automatic diﬀerentiation simultaneously identify nonlinear dynamics extract noise probability distributions data machine learning science technology march issn doi aca url httpsdoiorgaca publisher machine learning model error iop publishing jari kaipio e somersalo statistical computational inverse problems applied mathematical sciences springerverlag new york isbn doi b url httpswwwspringercomgp book j nagoor kani ahmed h elsheikh drrnn deep residual recurrent neural network model reduction arxiv cs september url httparxivorgabs arxiv k kashinath m mustafa albert jl wu c jiang s esmaeilzadeh k azizzadenesheli r wang chattopadhyay singh manepalli d chirila r yu r walters b white h xiao h tchelepi p mar cus anandkumar p hassanzadeh null prabhat physicsinformed machine learning case studies weather climate modelling philo sophical transactions royal society mathematical physical engineering sciences april doi rsta url httpsroyalsocietypublishingorgdoifull rsta publisher royal society rachael t keller qiang du discovery dynamics using lin ear multistep methods siam journal numerical analysis – january issn doi mx url httpsepubssiamorgdoimx publisher society industrial applied mathematics felix p kemeth tom bertalan nikolaos evangelou tianqi cui saurabh malani ioannis g kevrekidis initializing lstm internal states via manifold learning chaos interdisciplinary journal nonlinear science september issn doi marat f khairoutdinov david randall cloud resolv ing model cloud parameterization ncar commu geophysical nity climate system model research letters – doi httpsdoiorggl httpsagupubs onlinelibrarywileycomdoiabsgl eprint httpsagupubsonlinelibrarywileycomdoipdfgl issn url preliminary results diederik p kingma jimmy ba adam method stochastic opti mization rd international conference learning representations iclr san diego ca usa may conference track proceedings url httparxivorgabs juˇs kocijan modelling control dynamic systems using gaussian process models advances industrial control springer international pub lishing isbn doi url httpswwwspringercomgpbook milan korda mihai putinar igor mezi´c datadriven spectral analysis koopman operator applied computational harmonic analysis – publisher elsevier k krischer r ricomart´ınez g kevrekidis h h rotermund g ertl j l hudson model identiﬁcation spatiotemporally varying catalytic matthew e levine andrew m stuart reaction aiche journal – january issn doi aic url httpdoiwileycom aic s kullback r leibler information suﬃciency annals mathematical statistics – march issn doi url httpsprojecteuclidorg journalsannalsofmathematicalstatisticsvolumeissue oninformationandsufficiencyaomsfull publisher institute mathematical statistics aoms yury kutoyants statistical inference ergodic diﬀusion processes springer series statistics springerverlag london isbn doi url httpswwwspringer comgpbook e lagaris likas d fotiadis artiﬁcial neural networks solving ordinary partial diﬀerential equations ieee transactions neural networks – september issn doi url httparxivorgabsphysics arxiv physics kody law abhishek shukla andrew stuart analysis dvar ﬁlter partially observed lorenz’ model discrete continuous dynamical systems – august issn doi dcds url httpwwwaimsciencesorgjournals displayarticlesnewjsppaperid kody law andrew stuart kostas zygalakis data assimilation cham switzerland springer publisher springer youming lei jian hu jianpeng ding hybrid model based deep lstm predicting highdimensional chaotic systems arxiv cs eess january url httparxivorgabs arxiv zhen li hee sun lee eric darve george em karniadakis computing nonmarkovian coarsegrained interactions derived mori–zwanzig formalism molecular systems application polymer melts journal chemical physics publisher aip publishing llc zhong li jiequn han weinan e qianxiao li curse memory recurrent neural networks approximation optimization analysis arxiv cs math stat september url httparxivorg abs arxiv zongyi li nikola kovachki kamyar azizzadenesheli burigede liu kaushik bhattacharya andrew stuart anima anandkumar fourier neural operator parametric partial diﬀerential equations arxiv cs math march url httparxivorgabs arxiv zongyi li nikola kovachki kamyar azizzadenesheli burigede liu kaushik bhattacharya andrew stuart anima anandkumar markov neural operators learning chaotic systems arxiv cs math june url httparxivorgabs arxiv kevin k lin fei lu datadriven model reduction wiener projec tions koopmanmorizwanzig formalism journal computational machine learning model error physics january issn doi jjcp url httpswwwsciencedirectcomsciencearticle piis ori linial neta ravid danny eytan uri shalit generative ode modeling known unknowns proceedings conference health inference learning chil ’ pages – new york ny usa april association computing machinery isbn doi url httpsdoiorg e lorenz predictabilitya problem partly solved proc seminar pre dictability reading uk ecmwf url httpsciniiac jpnaiden edward n lorenz journal deterministic nonperiodic flow issn atmospheric sciences – march ciddnfcidco url httpsjournalsametsocorgviewjournalsatsc dnfcoxml publisher american teorological society section journal atmospheric sciences doi robert j lovelett jos´e l avalos ioannis g kevrekidis partial ob servations conservation laws graybox modeling biotechnology optogenetics industrial engineering chemistry research – february issn doi acsiecrb url httpsdoiorgacsiecrb publisher american chem ical society fei lu datadriven model reduction stochastic burgers equations entropy december doi e url httpswwwmdpicom number publisher multidisciplinary digital publishing institute fei lu kevin lin alexandre chorin comparison continuous discretetime databased modeling hypoelliptic systems communications applied mathematics computational science – december issn doi camcos url https msporgcamcospxhtml publisher mathematical sciences publishers fei lu kevin k lin alexandre j chorin databased stochastic model reduction kuramoto–sivashinsky equation physica d non linear phenomena – february issn doi jphysd url httpswwwsciencedirectcomscience articlepiis lu lu pengzhan jin george em karniadakis deeponet learning nonlinear operators identifying diﬀerential equations based universal approximation theorem operators arxiv cs stat april url httparxivorgabs arxiv zhixin lu brian r hunt edward ott attractor reconstruction machine learning chaos interdisciplinary journal nonlinear science june issn doi url httpsaipscitationorgdoi publisher ameri can institute physics matthew e levine andrew m stuart mantas lukoˇseviˇcius herbert jaeger reservoir computing approaches recurrent neural network training computer science review – august issn doi jcosrev url https wwwsciencedirectcomsciencearticlepiis chao ma jianchun wang weinan e model reduction mem ory machine learning dynamical systems communications computational physics issn doi cicpoa url httpwwwglobalscicomintroarticle detailcicphtml romit maulik bethany lusch prasanna balaprakash reducedorder modeling advectiondominated systems recurrent neural networks convolutional autoencoders physics fluids march issn doi url httpsaipscitationorgdoi abs publisher american institute physics kevin mcgoﬀ sayan mukherjee andrew nobel natesh pillai con sistency maximum likelihood estimation dynamical systems annals statistics – february issn doi aos url httpsprojecteuclid orgjournalsannalsofstatisticsvolumeissue consistencyofmaximumlikelihoodestimationforsomedynamicalsystems aosfull publisher institute mathematical statistics xiaoli meng david van dyk em algorithm— old folksong sung fast new tune journal royal statistical society series b statistical methodology – andrew c miller nicholas j foti emily fox learning insulinglucose dynamics wild arxiv cs stat august url httparxivorgabs arxiv andrew c miller nicholas j foti emily b fox breiman’s two cultures don’t choose sides arxiv cs stat april url httparxivorgabs arxiv jonas mockus bayesian approach global optimization theory applications mathematics applications springer netherlands isbn doi url https wwwspringercomgpbook kumpati s narendra kannan parthasarathy neural networks dynamical systems international journal approximate reasoning – february issn x doi x q url httpswwwsciencedirectcomsciencearticlepii xq nicholas h nelsen andrew m stuart random feature model inputoutput maps banach spaces arxiv physics stat may url httparxivorgabs arxiv duong nguyen said ouala lucas drumetz ronan fablet emlike learn ing chaotic dynamics noisy partial observations arxiv cs stat march url httparxivorgabs arxiv machine learning model error murphy yuezhen niu lior horesh isaac chuang recurrent neural networks eye diﬀerential equations arxiv quant ph stat april url httparxivorgabs arxiv fernando nogueira bayesian optimization open source constrained global optimization tool python url httpsgithubcomfmfn bayesianoptimization paul o’gorman john g dwyer parameterize moist convection potential mate climate change extreme events modeling earth systems – doi onlinelibrarywileycomdoiabsms httpsagupubsonlinelibrarywileycomdoipdfms using machine learning modeling cli journal advances issn url httpsagupubs eprint httpsdoiorgms s ouala d nguyen l drumetz b chapron pascual f collard l gaultier r fablet learning latent dynamics partially observed chaotic systems chaos interdisciplinary journal nonlinear science october issn doi url httpaipscitationorgdoi eric j parish karthik duraisamy dynamic subgrid scale model large eddy simulations based mori–zwanzig formalism journal computational physics – publisher elsevier jaideep pathak brian hunt michelle girvan zhixin lu edward ott modelfree prediction large spatiotemporally chaotic systems data reservoir computing approach physical review letters jan uary issn doi physrevlett url httpslinkapsorgdoiphysrevlett jaideep pathak alexander wikner rebeckah fussell sarthak chandra brian r hunt michelle girvan edward ott hybrid forecasting chaotic processes using machine learning conjunction knowledge based model chaos interdisciplinary journal nonlinear science april issn doi url httpsaipscitationorgdoi grigoris pavliotis andrew stuart multiscale methods averaging homogenization springer science business media matthew plumlee bayesian calibration inexact computer models journal american statistical association – july issn x doi url https wwwtandfonlinecomdoifull matthew plumlee v roshan joseph orthogonal gaussian pro cess models statistica sinica issn doi ss url httpwwwstatsinicaedutwstatisticajn jnjnhtml manuel pulido pierre tandeo marc bocquet alberto carrassi mag dalena lucini stochastic parameterization identiﬁcation using ensemble kalman ﬁltering combined maximum likelihood methods tellus dynamic meteorology oceanography – publisher taylor francis matthew e levine andrew m stuart ryan pyle nikola jovanovic devika subramanian krishna v palem ankit b patel domaindriven models yield better predictions lower cost reservoir computers lorenz systems philosophical transac tions royal society mathematical physical engineering sci ences april issn x doi rsta url httpsroyalsocietypublishingorgdoi rsta zhaozhi qian william r zame lucas m fleuren paul elbers mihaela van der schaar integrating expert odes neural odes pharmacology disease progression arxiv cs stat june url http arxivorgabs arxiv alejandro f queiruga n benjamin erichson dane taylor michael w mahoney continuousindepth neural networks arxiv cs math stat august url httparxivorgabs arxiv christopher rackauckas yingbo ma julius martensen collin warner kirill zubov rohit supekar dominic skinner ali ramadhan alan edelman universal diﬀerential equations scientiﬁc machine learn ing arxiv cs math qbio stat august url http arxivorgabs arxiv christopher rackauckas roshan sharma bernt lie hybrid mechanistic neural model laboratory helicopter pages – march doi ecp url httpsepliuseenconferencearticle aspxseriesecpissuearticleno ali rahimi benjamin recht random features largescale ker nel machines j platt d koller y singer s roweis editors advances neural information processing systems volume curran sociates inc url httpsproceedingsneuripsccpaper fileafdbceffebffdapaperpdf ali rahimi benjamin recht uniform approximation functions random bases th annual allerton conference communication control computing pages – ieee ali rahimi benjamin recht weighted sums random kitchen sinks replacing minimization randomization learning nips pages – citeseer m raissi p perdikaris g e karniadakis physicsinformed neu ral networks deep learning framework solving forward inverse problems involving nonlinear partial diﬀerential equations journal com putational physics – february issn doi jjcp url httpswwwsciencedirectcomscience articlepiis maziar raissi paris perdikaris george em karniadakis multistep neural networks datadriven discovery nonlinear dynamical systems arxiv nlin physicsphysics stat january url http arxivorgabs arxiv carl edward rasmussen christopher k williams gaussian processes machine learning adaptive computation machine learning mit press cambridge mass isbn oclc ocm machine learning model error stephan rasp michael s pritchard pierre gentine deep learning represent subgrid processes climate models proceedings national academy sciences – september issn doi pnas url httpswwwpnasorg content isbn publisher national academy sciences section physical sciences sebastian reich colin cotter probabilistic forecasting bayesian data assimilation cambridge university press r ricomartines g kevrekidis m c kube j l hudson discrete vs continuoustime nonlinear signal processing attractors transitions parallel implementation issues american control conference pages – san francisco ca usa june ieee isbn doi acc url httpsieeexploreieeeorg document r ricomartinez js anderson ig kevrekidis continuoustime nonlinear signal processing neural network based approach gray box identiﬁcation proceedings ieee workshop neural networks signal processing pages – ermioni greece ieee isbn doi nnsp url httpieeexplore ieeeorgdocument r ricomart´ınez k krischer ig kevrekidis mc kube jl hud son discrete vs continuoustime nonlinear signal pro cessing cu electrodissolution data chemical engineer ing communications – november issn doi url httpswwwtandfonline comdoifull yulia rubanova ricky t q chen david duvenaud latent odes irregularlysampled time series arxiv cs stat july url httparxivorgabs arxiv david e rumelhart geoﬀrey e hinton ronald j williams learning representations backpropagating errors nature – publisher nature publishing group matteo saveriano yuchao yin pietro falco dongheui lee dataeﬃcient control policy search using residual dynamics learning ieeersj international conference intelligent robots systems iros pages – september doi iros issn hayden schaeﬀer giang tran rachel ward learning dynamical systems bifurcation via group sparsity arxiv preprint arxiv hayden schaeﬀer giang tran rachel ward extracting sparse high dimensional dynamics limited data siam journal applied mathe matics – publisher siam hayden schaeﬀer giang tran rachel ward linan zhang extracting structured dynamical systems using sparse optimization samples multiscale modeling simulation – publisher siam robert e schapire strength weak learnability machine learning – june issn doi bf matthew e levine andrew m stuart tapio schneider andrew m stuart jinlong wu learning stochastic closures using ensemble kalman inversion transactions mathematics applications tnab anton maximilian sch¨afer hansgeorg zimmermann recurrent neu ral networks universal approximators international journal neu ral systems – august issn doi s skipper seabold josef perktold statsmodels econometric statistical modeling python th python science conference alex sherstinsky fundamentals recurrent neural network rnn long shortterm memory lstm network physica d nonlin ear phenomena march doi jphysd url httpwwwsciencedirectcomscience articlepiis issn guanya shi xichen shi michael o’connell rose yu kamyar azizzadenesheli animashree anandkumar yisong yue soonjo chung neural lander stable drone landing control using learned dynamics arxiv cs november url httparxivorgabs arxiv jonathan d smith kamyar azizzadenesheli zachary e ross eikonet solving eikonal equation deep neural networks ieee transac tions geoscience remote sensing pages – issn doi tgrs conference name ieee transactions geoscience remote sensing peter d sottile david albers peter e dewitt seth russell j n stroh david p kao bonnie adrian matthew e levine ryan mooney lenny larchick jean s kutner matthew k wynia jeﬀrey j glasheen tellen d bennett realtime electronic health record mortality pre diction covid pandemic prospective cohort study medrxiv page january doi url httpswwwmedrxivorgcontent v publisher cold spring harbor laboratory press langxuan su sayan mukherjee large deviation approach posterior consistency dynamical systems arxiv math stat june url httparxivorgabs arxiv floris takens detecting strange attractors turbulence david rand laisang young editors dynamical systems turbulence warwick lecture notes mathematics pages – berlin heidelberg springer isbn doi bfb zhihong tan colleen m kaul kyle g pressel yair cohen tapio schneider jo˜ao teixeira extended eddydiﬀusivity massﬂux scheme uniﬁed representation subgridscale turbulence convection journal advances modeling earth systems – publisher wiley online library giang tran rachel ward exact recovery chaotic systems highly corrupted data multiscale modeling simulation – publisher siam machine learning model error jonathan h tu clarence w rowley dirk m luchtenburg steven l brun ton j nathan kutz dynamic mode decomposition theory applications journal computational dynamics doi jcd url httpswwwaimsciencesorgarticledoi jcd company journal computational dynamics distributor journal computational dynamics institution journal com putational dynamics label journal computational dynamics publisher american institute mathematical sciences eric vandeneijnden others fast communications numerical techniques multiscale dynamical systems stochastic eﬀects communications mathematical sciences – publisher international press boston vladimir vapnik nature statistical learning theory springer science business media pauli virtanen ralf gommers travis e oliphant matt haberland tyler reddy david cournapeau evgeni burovski pearu peterson warren weckesser jonathan bright st´efan j van der walt matthew brett joshua wilson k jarrod millman nikolay mayorov andrew r j nelson eric jones robert kern eric larson c j carey ilhan polat yu feng eric w moore jake vanderplas denis laxalde josef perktold robert cimrman ian henriksen e quintero charles r harris anne m archibald antˆonio h ribeiro fabian pedregosa paul van mulbregt scipy fundamental algorithms scientiﬁc computing python nature methods – march issn doi s url httpswwwnaturecomarticless number pub lisher nature publishing group pr vlachas j pathak br hunt tp sapsis m girvan e ott p koumoutsakos backpropagation algorithms reservoir computing recurrent neural networks forecasting complex spatiotempo ral dynamics neural networks – june issn doi jneunet url httpslinkinghubelsevier comretrievepiis jack wang aaron hertzmann david j fleet gaussian pro cess dynamical models advances neural information processing systems url httpspapersnipsccpaperhash ccddfddffeeafabstracthtml qian wang nicolo ripamonti jan s hesthaven recurrent neural network closure parametric podgalerkin reducedorder models based morizwanzig formalism journal computational physics publisher elsevier peter g watson applying machine learning improve simulations chaotic dynamical system using empirical error correction arxiv nlin physicsphysics stat april url httparxivorgabs arxiv matthew e levine andrew m stuart alexander wikner jaideep pathak brian hunt michelle girvan troy ar comano istvan szunyogh andrew pomerance edward ott com bining machine learning knowledgebased modeling scalable fore casting subgridscale closure large complex spatiotemporal sys tems chaos interdisciplinary journal nonlinear science may doi url httpsaipscitationorgdoi publisher amer ican institute physics issn jared willard xiaowei jia shaoming xu michael steinbach vipin ku mar integrating scientiﬁc knowledge machine learning engineering environmental systems arxiv physics stat july url httparxivorgabs arxiv ja wilson lfm zorzetto generalised approach process state estimation using hybrid artiﬁcial neural networkmechanistic models com puters chemical engineering – june issn doi s url httplinkinghubelsevier comretrievepiis armand wirgin inverse crime arxivmathph january url httparxivorgabsmathph arxiv mathph david h wolpert stacked generalization neural networks – jan uary issn doi s url https wwwsciencedirectcomsciencearticlepiis zhong yi wan petr karnakov petros koumoutsakos themistoklis p sapsis bubbles turbulent ﬂows datadriven kinematic models history terms international journal multiphase flow august issn doi jijmultiphaseﬂow url http wwwsciencedirectcomsciencearticlepiisx yuan yin vincent le guen j´er´emie dona emmanuel de b´ezenac ibrahim ayed nicolas thome patrick gallinari augmenting physical models deep networks complex dynamics forecasting journal statistical mechanics theory experiment december issn doi acae zhang john harlim xiantao li department mathematics penn sylvania state university university park pa usa department mathematics department meteorology atmospheric science insti tute computational data sciences pennsylvania state university university park pa usa estimating linear response statistics using orthogonal polynomials rkhs formulation foundations data sci ence – issn doi fods url httpaimsciencesorgarticledoifods zhang john harlim xiantao li error bounds invariant statistics machine learning ergodic ´o diﬀusions arxiv cs math may url httparxivorgabs arxiv jian zhu masafumi kamachi adaptive variational method data assimilation imperfect models tellus dynamic meteorology oceanography – january issn null doi tellusa machine learning model error vi url httpsdoiorgtellusavi pub lisher taylor francis eprint httpsdoiorgtellusavi yuanran zhu jason m dominy daniele venturi estimation morizwanzig memory integral journal mathematical physics publisher aip publishing llc dept computing mathematical sciences california institute technology pasadena ca usa email address mlevinecaltechedu dept computing mathematical sciences california institute technology pasadena ca usa email address astuartcaltechedu ",86.1,"1"
1,"Geometric Deep Learning on Molecular Representations","geometric deep learning molecular representations kenneth atz† francesca grisoni†∗ gisbert schneider∗ eth zurich dept chemistry applied biosciences rethink vladimirprelogweg zurich switzerland eindhoven university technology dept biomedical engineering groene loper az eindhoven netherlands eth singapore sec ltd create way create tower singapore singapore † authors contributed equally work fgrisonituenl gisbertethzch c e d h p m e h c s c s y h p v v x r abstract geometric deep learning gdl based neural network architectures incorporate process symmetry information emerged recent paradigm artiﬁcial intelligence gdl bears particular promise molecular modeling applications various molecular representations diﬀerent symme try properties levels abstraction exist review provides structured harmonized overview molecular gdl highlighting applications drug discovery chemical synthesis prediction quantum chemistry emphasis placed relevance learned molecular features complementar ity wellestablished molecular descriptors review provides overview current challenges opportunities presents forecast future gdl molecular sciences introduction recent advances deep learning instance artiﬁcial intelligence ai based neural networks led numerous applications molec ular sciences eg drug discovery quantum chemistry structural biology two charac teristics deep learning render particularly promis ing applied molecules first deep learning methods can cope unstructured data represen tations text sequences speech signals images – graphs ability particularly useful molecular systems chemists developed many models ie molecu lar representations capture molecular properties varying levels abstraction figure sec ond key characteristic deep learning can per form feature extraction feature learning input data produce datadriven features input data without need manual interven tion two characteristics promising deep learning complement “classical” machine learning applications eg quantitative structureactivity re lationship qsar molecular features ie molecular descriptors encoded priori rulebased algorithms capability learn un structured data obtain datadriven molecular fea tures led unprecedented applications ai molecular sciences one promising advances deep learn ing geometric deep learning gdl geometric deep learning umbrella term encompassing emerging techniques generalize neural networks eu clidean noneuclidean domains graphs manifolds meshes string representations general gdl encompasses approaches incorpo rate geometric prior ie information structure space symmetry properties input variables geometric prior leveraged improve qual ity information captured model although gdl increasingly applied molecular mod eling full potential ﬁeld still untapped figure exemplary molecular representations selected molecule ie penam substructure peni cillin twodimensional d depiction kekulé structure b molecular graph d composed vertices atoms edges bonds c smiles string atom type bond type connectivity speciﬁed alphanumerical char acters d threedimensional d graph composed vertices atoms position x y z coordinates d space edges bonds e molecular surface represented mesh colored ac cording respective atom types aim review provide structured harmonized overview applications gdl molecular systems ii delineate main research directions ﬁeld iii provide forecast future impact gdl three ﬁelds application highlighted namely drug discovery quantum chem istry computeraided synthesis planning casp nsohocccchcoonchccsabced particular attention datadriven molecular features learned gdl methods glossary se lected terms can found box principles geometric deep learning term geometric deep learning coined although gdl originally used methods applied noneuclidean data now extends deep learning methods incorporate geometric priors information structure symmetry system interest symmetry cru cial concept gdl encompasses properties system respect manipulations transforma tions translation reﬂection rotation scaling permutation box symmetry often recast terms invariance equivariance express behavior mathemati cal function respect transformation t eg ro tation translation reﬂection permutation act ing symmetry group mathematical func tion neural network f applied given molecular input x fx can therein transform equivariantly invariantly neither respect t described • equivariance neural network f applied input x equivariant transformation t transformation input x commutes transformation fx via trans formation t cid symmetry group ft x t cidfx neural networks therefore equivariant actions symmetry group inputs layer network “equivalently transforms transformation group • invariance invariance special case equiv ariance fx invariant t t cid trivial group action ie identity ft x t cidfx fx • fx neither equivariant invariant t transformation input x commute transformation fx ft x cid t cidfx symmetry properties neural network archi tecture vary depending network type symmetry group interest individually dis cussed following sections readers can ﬁnd indepth treatment equivariance group equivari ant layers neural networks elsewhere – concept equivariance invariance can also used reference molecular features obtained given molecular representation x depending behaviour transformation applied x instance many molecular descriptors invari ant rotation translation molecular rep resentation design eg moriguchi octanol water partitioning coeﬃcient relies occurrence speciﬁc molecular substructures calculation symmetry properties molecular fea tures extracted neural network depend symmetry properties input molecular representa tion utilized neural network many relevant molecular properties eg equilib rium energies atomic charges physicochemical prop erties permeability lipophilicity solubility invariant certain symmetry operations box many tasks chemistry thus desirable de sign neural networks transform equivariantly actions predeﬁned symmetry groups exceptions occur targeted property changes upon symme try transformation molecules eg chiral prop erties change inversion molecule vector properties change rotation molecule cases inductive bias learning bias equivariant neural networks allow diﬀerentiation symmetrytransformed molecules neural networks can considered uni versal function approximators incorporating prior knowledge reasonable geometric information geometric priors evolved core design principle neural network modeling incorporating geo metric priors gdl allows increase quality model bypasses several bottlenecks related need force data euclidean geometries eg feature engineering moreover gdl provides novel modeling opportunities data augmentation low data regimes molecular gdl application gdl molecular systems chal lenging part multiple valid ways representing molecular entity molecular rep resentations can categorized based diﬀerent levels abstraction physicochemical geo metrical aspects capture importantly representations models reality thus suitable purposes others gdl provides opportunity experiment dif ferent representations molecule lever ages intrinsic geometrical features increase quality model moreover gdl repeatedly proven useful providing insights relevant molecu lar properties task hand thanks feature extraction feature learning capabilities follow ing sections delineate prevalent molecular gdl approaches applications chemistry grouped according respective molecular represen tations used deep learning molecular graphs grids strings surfaces note review term representation used solely denote humanmade models molecules eg molecular graphs d conformers smiles strings avoid confusion usages word representation deep learning will use term feature whenever referring numerical description molecules obtained either rulebased algorithms molecular descriptors learned extracted neural networks box glossary selected terms comfa comsia comparative molecular field analysis comfa comparative molecu lar similarity indices analysis comsia popular d qsar methods developed s s threedimensional grids used capture distributions molecular features eg steric hydrophobic electrostatic properties obtained molecular descriptors serve inputs regression model quantitative bioactivity prediction convolution operation within neural network transforms feature space new feature space thereby captures local information found data convolutions ﬁrst introduced pixels images term convolution now used neural network architectures covering variety data structures graphs point clouds spheres grids manifolds density functional theory dft quantum mechanical modeling approach used investigate electronic structure molecules data augmentation artiﬁcial increase data volume available model training often achieved leveraging symmetrical properties input data captured model eg rotation permutation feature individually measurable computationally obtainable characteristic given sample eg molecule form scalar review term refers numeric value characterizing molecule molecular features can computed rulebased algorithms molecular descrip tors generated automatically deep learning molecular representation hidden learned features geometric prior inductive bias incorporating information symmetric nature system interest neural network architecture also known symmetry prior inductive bias set assumptions learning algorithm eg neural network uses learn target function make predictions previously unseen data points onehot encoding method representing categorical variables numerical arrays obtaining binary variable category often used convert sequences eg smiles strings numerical matrices suitable inputs andor outputs deep learning models eg chemical language models quantitative structureactivity relationship qsar machine learning techniques aimed ﬁnd ing empirical relationship molecular structure usually encoded molecular descriptors experimentally determined molecular properties pharmacological activity toxicity reinforcement learning technique used steer output machine learning algorithm toward userdeﬁned regions optimality via predeﬁned reward function transfer learning transfer knowledge existing deep learning model related task fewer training samples available unstructured data data arranged vectors typically handcrafted features examples unstructured data include graphs images meshes molecular representations typically unstruc tured whereas numerical molecular descriptors eg molecular properties molecular ﬁngerprints examples structured data voxel element regularly spaced d grid equivalent pixel d space learning molecular graphs molecular graphs graphs among intuitive ways represent molecular structures molecule can thought mathematical graph g ve whose vertices vi ∈ v represent atoms whose edges eij ∈ e constitute connection figure many deep learning applications molecular graphs can characterized set vertex edge features graph neural networks deep learning methods devoted handling graphs input commonly referred graph neural net works gnns applied molecules gnns al low feature extraction progressively aggregating information atoms molecular environ cid → vl ments figure diﬀerent architectures gnns introduced popular fall umbrella term message passing neural networks networks iteratively update vertex features lth network layer vl via graph convolutional operations em ploying least two learnable functions ψ φ local permutationinvariant aggregation operator eg sum vl j∈n ψcidvl icid since introduction means predict quan tum chemical properties small molecules den sity functional theory dft level gnns found many applications quantum chemistry – drug discovery casp molecular prop erty prediction applied quantum chemistry tasks gnns often use einvariant d formation including radial angular information cidcid φ vl vl j table summary selected geometric deep learning gdl approaches molecular modeling ap proach utilized molecular representations selected applications reported d onedimensional d twodimensional d threedimensional gdl approach graph neural networks gnns molecular representations applications d d molecular graph d point cloud d grid convolutional d d neural networks cnns convolutional mesh networks neural geodesic cnns d gnns recurrent neural net works rnns string notation d grid transformers string notation encoded graph surface mesh encoded d grid d graph proteinprotein interaction prediction ligandpocket ﬁngerprinting molecular property prediction drug dis covery quantum chemistry energies – forces – wave functions casp generative molecular design structurebased drug design property pre diction generative molecular design synthe sis planning protein structure prediction prediction properties drug dis covery synthesis planning prediction reaction yields generative molecular design prediction properties drug discovery protein structure prediction edge features graph thereby improving prediction accuracy quantum chemical forces energies equilibrium non equilibrium molecular conformations case schnet painn schnetlike architec tures used predict quantum mechanical wave functions form hartreefock dft density matrices diﬀerences quantum properties ob tained dft coupled cluster leveloftheory cal culations gnns molecular property prediction shown outperform humanengineered molecular de scriptors several biologically relevant properties although including d information molec ular graphs generally improved prediction drug relevant properties marked diﬀerence observed using single multiple molecular conform ers network training natural connection molecular representations gnns seem particularly suitable context explainable ai xai used interpret mod els predicting molecular properties preclinical rele vance quantum chemical properties gnns used de novo molecule genera tion – example performing vertex edge addition initial vertex figure b gnns also combined variational au toencoders – reinforcement learning finally gnns applied casp however current approaches limited reactions one bond removed products reactants equivariant message passing recent area development graphbased meth ods se eequivariant gnns equivariant message passing networks deal absolute coordinate systems d graphs figure b thus networks may particularly wellsuited applied d molecular representations net works exploit euclidean symmetries system box d molecular graphs gd ver addition vertex edge features vi ∈ v eij ∈ e re spectively also encode information vertex posi tion d coordinate system ri ∈ r employing e seequivariant convolutions networks shown high accuracy predicting sev eral quantum chemical properties energies – interatomic potentials molecular dynamics simulations wavefunctions se equivariant neural networks com mute reﬂections input ie nonequivariant reﬂections thereby enable se equivariant models distinguish stereoisomers chiral molecules including enantiomers e equivariant neural networks side transform equivari antly refelctions allows e equivariant models distinguish diastereomers eneantiomers se neural networks compu tationally expensive due use spherical har monics wigner dfunctions com pute learnable weight kernels eequivariant neu ral networks computationally eﬃcient shown perform equal better se equivariant networks eg modeling quantum box euclidean symmetries molecular systems molecular systems threedimensional representations thereof can considered objects eu clidean space space one can apply several symmetry operations transformations performed respect three symmetry elements ie line plane point ii rigid preserve euclidean distance pairs atoms ie isometry euclidean transformations follows • rotation movement object respect radial orientation given point • translation movement every point object distance given direction • reﬂection mapping object point inversion line plane mirroring three transformations arbitrary ﬁnite combinations included euclidean group e special euclidean group se comprises translations rotations molecules always symmetric se group ie intrinsic properties eg biological physicochemical properties equilibrium energy invariant coordinate rotation translation combinations thereof several molecules chiral chiral properties depend absolute conﬁguration stereogenic centers thus noninvariant molecule reﬂection chirality plays key role chemical biology relevant examples chiral molecules dna several drugs whose enantiomers exhibit markedly diﬀerent pharmacological toxicological properties chemical properties dynamic systems equiv ariant message passing networks applied predict quantum mechanical wavefunction nu clei electronbased representations endto end fashion – however networks cur rently limited small molecular systems large size learned matrices scale quadrat ically number electrons system learning grids grids capture properties system regularly spaced intervals based number dimensions cluded system grids can d eg sequences d eg rgb images d eg cubic lattices higherdimensional grids deﬁned euclidean geometry can considered graph spe cial adjacency vertices ﬁxed dering deﬁned spatial dimensions grid ii vertex identical number ad jacent edges therefore indistinguishable vertices structurewise two properties render local convolutions applied grid inherently permutation invariant provide strong geometric prior translation invariance eg weight sharing convolutions grid properties critically determined success convolutional neural networks cnns eg computer vision natural lan guage processing speech recognition originalrotationtranslationreflection mirroringreflection inversion figure deep learning molecular graphs message passing graph neural networks applied twodimensional d molecular graphs d molecular graph g ve labeled vertex atom features vi ∈ rdv edge bond features eij ∈ rde vertex features updated iterative message passing deﬁned number time steps t across pair vertices vi vj connected via edge eji last message passing convolution ﬁnal vertex vt can mapped bond yij atom yi property ii aggregated form molecular features can mapped molecular property y b eequivariant message passing graph neural networks applied threedimensional d molecular graphs d graphs g ver labeled atom features vi ∈ rdv absolute coordinates d space ri ∈ r edge features eij ∈ rde iterative spherical convolutions used obtain data driven atomic features vt can mapped atomic properties aggregated mapped molecular properties yi y respectively molecular grids molecules can represented grids diﬀerent ways d grids eg molecular structure drawings gener ally useful visualization rather prediction exceptions analogous popu lar predeep learning approaches example compar ative molecular ﬁeld analysis comfa com parative molecular similarity indices analysis com sia d grids often used capture spa tial distribution properties within one molecular conformer representations used inputs d cnns d cnns char acterized greater resource eﬃciency equiv ariant gnns now mainly ap plied molecules fewer approximately atoms thus d cnns often method choice protein structure considered eg proteinligand binding aﬃnity prediction – active site recognition learning molecular surfaces molecular surfaces can deﬁned surface en closing d structure molecule certain distance atom center point continuous surface can characterized chemical eg hydrophobic electrostatic ge ometric features eg local shape curvature geometrical perspective molecular surfaces con sidered d meshes ie set polygons faces describe mesh coordinates exist d space vertices can represented d grid structure four vertices mesh deﬁne pixel d graph structure grid graphbased structures meshes enable applications d cnns geodesic cnns gnns learn meshbased molecular surfaces recently geodesic d cnns applied learn meshbased repre sentations protein surfaces predict proteinprotein interactions recognize corresponding binding sites approach generated datadriven ﬁngerprints relevant speciﬁc biomolecular interactions approaches like d cnns applied meshes come feature labelingfeature updatesaggregationmolecular propertyatomic propertybond propertymessage passingabmolecular propertyatomic propertyequivariant message passingfeature labelingfeature updatesaggregation certain limitations need rotational data augmentation due nonequivariance rota tions enforcing homogeneous mesh resolution ie uniform spacing points mesh recently introduced gnns meshbased representa tions shown incorporate rotational equiv ariance network architecture allow heterogeneous mesh resolution gnns computationally eﬃcient potential mod eling macromolecular structures however yet found applications molecular systems studies used d voxelbased surface representa tions macromolecules inputs d cnns eg proteinligand aﬃnity protein bindingsite prediction learning string representations molecular strings molecules can represented molecular strings ie linear sequences alphanumeric symbols molecu lar strings originally developed manual cipher ing tools complement systematic chemical nomen clature later became suitable data storage retrieval popular string based representations wiswesser line notation sybyl line notation international chemical identiﬁer inchi hierarchical editing language macromolecules simpliﬁed molecular input line entry system smiles type linear representation can considered chemical language fact notations pos sess deﬁned syntax ie possible combinations alphanumerical characters will lead “chemically valid” molecule furthermore notations possess semantic properties depending elements string combined corresponding molecule will diﬀerent physicochemical biological prop erties characteristics make possible extend deep learning methods developed language sequence modeling analysis molecular strings chemical language modeling smiles strings – letters used represent atoms symbols numbers used encode bond types connectivity branching stereochemistry figure – become frequently employed data representation method sequencebased deep learning whereas sev eral string representations tested combination deep learning eg inchi deepsmiles selfreferencing embedded strings selfies smiles remains de facto representation choice chemical language model ing following text introduces promi nent chemical language modeling methods along selected examples application chemistry chemical language models chemical language models machine learning meth ods can handle molecular sequences inputs andor outputs common algorithms chemical language modeling recurrent neural net works rnns transformers • rnns figure b neural networks process sequence data euclidean structures usually via onehotencoding rnns model dy namic system hidden state ht network tth time point ie t th position sequence depends current observation st previous hidden state ht− rnns can process sequence inputs arbitrary lengths provide outputs arbi trary lengths rnns often used auto regressive fashion ie predict probability distribution next possible elements kens time step t given current hid den state ht preceding portions sequence several rnn architectures proposed solve gradient vanishing ex ploding problems vanilla rnns long shortterm memory gated recurrent units • transformers figure c process sequence data noneuclidean structures encoding se quences either fully connected graph ii sequentially connected graph token connected previous tokens sequence former approach ten used feature extraction general eg transformerencoder whereas latter employed nexttoken prediction eg transformerdecoder positional informa tion tokens usually encoded positional embedding sinusoidal positional encoding transformers combine graphlike processing socalled attention layers attention layers allow transformers focus pay attention perceived relevant tokens pre diction transformers particularly suc cessful sequencetosequence tasks lan guage translation extending early studies rnns nexttoken prediction routinely applied de novo generation molecules desired bi ological physicochemical properties combination transfer – reinforcement learning context rnns shown re markable capability learn smiles syntax capture highlevel molecular features se mantics physicochemical bio logical properties context data augmentation based smiles randomization bidirectional learning proven eﬃcient improving quality chemi cal language learned rnns published studies used smiles strings derivative representations studies oneletter amino acid sequences employed peptide design – rnns also applied predict ligand–protein interactions pharmacokinetic properties drugs figure chemical language modeling smiles strings atom types represented element symbols bond types branching indicated predeﬁned alphanumeric symbols molecule via smiles algorithm string t symbols tokens obtained s s s st encodes molecular connectivity herein illustrated via color indicates corresponding atomic position graph left string right molecule can encoded via diﬀerent smiles strings depending chosen starting atom three random permutations incorporating identical molecular information presented b recurrent neural networks sequence position t learn predict next token st sequence s given current sequence s s st hidden state ht c transformerbased language models input sequence structured graph vertices featur ized according token identity eg via token embedding vi ∈ rdv position sequence eg via sinusoidal positional encoding pi ∈ rdv transformer learning vertices updated via token residual attention blocks passing t attention layers individual feature representation st obtained t protein secondary structure tempo ral evolution molecular trajectories rnns applied molecular feature extraction showing learned features outperformed traditional molecular descriptors graphconvolution methods virtual screening property prediction fréchet chemnet distance based physicochemical biological features learned rnn model become de facto reference method capture molecular similarity context molecular transformers applied casp can cast sequencetosequence translation task string representations reactants mapped corresponding product vice versa since initial applications trans formers employed predict multistep syn theses regio stereoselective reactions enzymatic reaction outcomes reaction yields classes recently transformers applied molecular property prediction optimization transformers also used de novo molecule design learning translate target protein sequence smiles strings cor responding ligands representations learned smiles strings transformers shown promise property prediction lowdata regimes fur thermore transformers recently combined e se equivariant layers learn d structures proteins aminoacid sequence equivariant transformers achieve stateof theart performance protein structure prediction deep learning approaches relied stringbased representations de novo design eg conditional generative adversarial networks – variational autoencoders models however limited equivalent ability automatically learn smiles syntax compared rnns d cnns selfattention networks – used smiles property prediction recently deep learning amino acid se quences property prediction shown perform par approaches based humanengineered fea tures conclusions outlook geometric deep learning chemistry allowed re searchers leverage symmetries diﬀerent un structured molecular representations resulting greater ﬂexibility versatility available com putational models molecular structure generation abccccchcoonchccsnsohonchccscccchcooococchnchccscccsgraphresidual attention blocksfeature labelingssequencefeature updatessssssssssssssssssss box structureactivity landscape modeling geometric deep learning worked example shows geometric deep learning gdl can used interpret structure activity landscape learned trained model starting publicly available molecular dataset containing estrogen receptor binding information trained eequivariant graph neural network six hidden layers hidden neurons per layer analyzed learned features relationship ligand binding estrogen receptor ﬁgure shows analysis learned molecular features third hidden layer analyzed via principal component analysis ﬁrst two principal components shown features relate density active inactive molecules chemical space network successfully separated molecules based experimental bioactivity structural features eg atom scaﬀolds might oﬀer novel opportunities explainable ai gdl property prediction approaches represent valid alternative classical chemoinformatics ap proaches based molecular descriptors humanengineered features modeling tasks usually characterized need highly engineered rules eg chemical transformations de novo design reactive site speciﬁcation casp beneﬁts gdl consistently shown published applications gdl molecular repre sentation shown characteristic strengths weak nesses molecular strings like smiles proven partic ularly suited generative deep learning tasks de novo design casp success may due relatively easy syntax chemical language facilitates nexttoken sequencetosequence prediction molecular property prediction smiles strings limited due nonunivocity molecular graphs shown particular usefulness property prediction partly human interpretability ease inclusion desired edge node features incorporation d informa tion eg equivariant message passing useful quantum chemistry related modeling whereas drug discovery applications approach often failed clearly outbalance increased complexity model eequivariant graph neural networks also applied conformationaware de novo design prospective experimental validation studies yet published molecular grids become de facto standard d representations large molecular systems due ability capture information user deﬁned resolution voxel density ii euclidean structure input grid finally molecular surfaces currently fore front gdl expect many interesting applications gdl molecular surfaces near future application impact gdl chemistry evaluation optimal tradeoﬀ tween algorithmic complexity performance model interpretability will required aspects cru cial reconciling “two qsars” connect computer science chemistry communities en courage gdl practitioners include aspects inter pretability models eg via xai whenever possible transparently communicate domain experts feedback domain experts will also crucial develop new chemistryaware architec tures potential molecular gdl concrete prospective applications potential gdl molecular feature extrac tion yet fully explored several studies shown beneﬁts learned representations com pared classical molecular descriptors cases gdl failed live promise terms superior learned features although several benchmarks evaluating machine learning models property prediction molecule generation present framework en able systematic evaluation usefulness data driven features learned ai benchmarks ohoohshoohoonohhofosonohhohigh density activeslow density activesatom scaffold systematic studies key obtaining unvarnished assessment deep representation learning moreover investigating relationships learned fea tures physicochemical biological properties input molecules will augment interpretability applicability gdl eg modeling structure function relationships like structureactivity landscapes box compared conventional qsar approaches assessment applicability domain ie region chemical space model predic tions considered reliable routinely per formed contemporary gdl studies lack sessment systematic gap might constitute one limiting factors widespread use gdl approaches prospective studies lead unreliable predictions eg molecules diﬀerent mechanisms action functional groups physico chemical properties training data fu ture will necessary devise “geometryaware” approaches applicability domain assessment another opportunity will leverage less ex plored molecular representations gdl instance electronic structure molecules vast poten tial tasks casp molecular property pre diction prediction macromolecular interactions eg proteinprotein interactions although accu rate statistical quantum mechanical simulations computationally expensive modern quantum machine learning models trained large quantum data collections – allow quantum information accessed much faster high accuracy aspect enable quantum electronic featuriza tion extensive molecular datasets used input molecular representations task interest deep learning can applied multitude bio logical chemical representations correspond ing deep neural network models potential augment human creativity paving way new sci entiﬁc studies previously unfeasible ever research explored tip iceberg one signiﬁcant catalysts integra tion deep learning molecular sciences may responsibility academic institutions foster interdis ciplinary collaboration communication education picking high hanging fruits will possible deep understanding chemistry com puter science along outofthebox thinking collaborative creativity setting expect molecular gdl increase understanding molec ular systems biological phenomena acknowledgements research supported swiss national sci ence foundation snsf grant eth rethink initiative competing interest gs declares potential ﬁnancial conﬂict interest cofounder insilicom llc zurich role scientiﬁc consultant pharmaceutical industry list abbreviations ai artiﬁcial intelligence casp computeraided synthesis planning cnn convolutional neural network dft density functional theory e euclidean symmetry group gdl geometric deep learning gnn graph neural network qsar quantitative structureactivity relationship rnn recurrent neural network se special euclidean symmetry group smiles simpliﬁed molecular input line entry systems xai explainable artiﬁcial intelligence d onedimensional d twodimensional d threedimensional references lecun y bengio y hinton g deep learn ing nature – schmidhuber j deep learning neural net works overview neural networks – gawehn e hiss j schneider g deep learning drug discovery molecular informat ics – jiménezluna j grisoni f weskamp n schneider g artiﬁcial intelligence drug dis covery recent advances future perspectives expert opinion drug discovery – gilmer j schoenholz s s riley p f vinyals o dahl g e neural message passing quantum chemistry international confer ence machine learning – jumper j et al highly accurate protein struc ture prediction alphafold nature – baek m et al accurate prediction protein structures interactions using threetrack neural network science vaswani et al attention need advances neural information processing sys tems – brown t b et al language models fewshot learners arxiv hinton g et al deep neural networks acous tic modeling speech recognition shared views four research groups ieee signal pro cessing magazine – mikolov t deoras povey d burget l černocky j strategies training large scale neural network language models ieee workshop automatic speech recognition understanding – krizhevsky sutskever hinton g e im agenet classiﬁcation deep convolutional neu ral networks communications acm – farabet c couprie c najman l lecun y learning hierarchical features scene label ing ieee transactions pattern analysis machine intelligence – tompson j j jain lecun y bregler c joint training convolutional network graphical model human pose estimation advances neural information processing sys tems – bronstein m m bruna j lecun y szlam vandergheynst p geometric deep learn ing going beyond euclidean data ieee signal processing magazine – monti f frasca f eynard d mannion d bronstein m m fake news detection social media using geometric deep learning arxiv todeschini r consonni v molecular descrip tors chemoinformatics volume alphabetical listingvolume ii appendices references john wiley sons gainza p et al deciphering interaction ﬁnger prints protein molecular surfaces using ge ometric deep learning nature methods – segler m h kogej t tyrchan c waller m p generating focused molecule libraries drug discovery recurrent neural networks acs central science – weininger d smiles chemical language information system introduction method ology encoding rules journal chemical information computer sciences – bronstein m m bruna j cohen t veličković p geometric deep learning grids groups graphs geodesics gauges arxiv marsden j weinstein reduction sym plectic manifolds symmetry reports mathematical physics – cohen t s welling m group equivariant convolutional networks proceedings rd international conference international con ference machine learningvolume – cohen t s welling m steerable cnns arxiv cohen t s geiger m köhler j welling m spherical cnns international conference learning representations kondor r trivedi s generalization equivariance convolution neural net works action compact groups interna tional conference machine learning – moriguchi hirono s liu q nakagome matsushita y simple method calculating octanolwater partition coeﬃcient chemical pharmaceutical bulletin – cybenko g approximation superpositions sigmoidal function mathematics control sig nals systems – tetko v karpov p van deursen r godin g stateoftheart augmented nlp transformer models direct singlestep retrosynthesis nature communications – skinnider m stacey r g wishart d s foster l j chemical language models enable navigation sparsely populated chemical space nature machine intelligence – cramer r d patterson d e bunce j d comparative molecular ﬁeld analysis comfa eﬀect shape binding steroids car rier proteins journal american chemical society – klebe g comparative molecular similarity dices analysis comsia d qsar drug de sign – lecun y bengio y et al convolutional net works images speech time series handbook brain theory neural networks lecun y bottou l bengio y haﬀner p gradientbased learning applied document recognition proceedings ieee – sutton r s barto g reinforcement learning introduction mit press pan s j yang q survey transfer learn ing ieee transactions knowledge data engineering – feinberg e n et al potentialnet molecu lar property prediction acs central science – jiménezluna j skalic m weskamp n schneider g coloring molecules explain able artiﬁcial intelligence preclinical relevance assessment journal chemical information modeling – miller b k geiger m smidt t e noé f relevance rotationally equivariant con volutions predicting molecular properties arxiv anderson b hy t s kondor r cormorant covariant molecular neural networks advances neural information processing systems – satorras v g hoogeboom e welling m e n equivariant graph neural networks arxiv fuchs f worrall d fischer v welling m se transformers d rototranslation equivariant attention networks advances information processing systems neural schütt k t unke o t gastegger m equivariant message passing prediction tensorial properties molecular spectra arxiv unke o t et al se equivariant prediction molecular wavefunctions electronic densi ties arxiv coley c w et al graphconvolutional neu ral network model prediction chemical reactivity chemical science – jin w coley c barzilay r jaakkola t predicting organic reaction outcomes weisfeilerlehman network advances neu ral information processing systems – zhou z kearnes s li l zare r n riley p optimization molecules via deep reinforce ment learning scientiﬁc reports – jin w barzilay r jaakkola t junction tree variational autoencoder molecular graph gen eration international conference machine learning – jiménez j skalic m martinezrosell g de fabritiis g k deep protein–ligand absolute binding aﬃnity prediction via dconvolutional neural networks journal chemical informa tion modeling – ragoza m hochuli j idrobo e sunseri j koes d r protein–ligand scoring con volutional neural networks journal chemical information modeling – grisoni f et al designing anticancer peptides constructive machine learning chemmed chem – schwaller p gaudin t lanyi d bekas c laino t found translation predicting comes complex organic chemistry reactions us ing neural sequencetosequence models chemi cal science – senior w et al protein structure prediction using multiple deep neural networks th critical assessment protein structure predic tion casp proteins structure function bioinformatics – wang x et al optimizing pharmacoki netic property prediction based integrated datasets deep learning approach jour nal chemical information modeling – zheng s li y chen s xu j yang y predicting drug–protein interaction using quasi visual question answering system nature ma chine intelligence – schwaller p et al molecular transformer model uncertaintycalibrated chemical reac tion prediction acs central science – schwaller p vaucher c laino t rey mond jl prediction chemical reaction yields using deep learning machine learning science technology grechishnikova d transformer neural network proteinspeciﬁc de novo drug generation machine translation problem scientiﬁc reports – morris p st clair r hahn w e baren holtz e predicting binding screening says transformer network embeddings journal chemical information modeling – hoﬀmann r laszlo p representation chemistry angewandte chemie international edition english – nguyen l h phamhuy c chi ral drugs overview international journal biomedical science ijbs kipf t n welling m semisupervised clas siﬁcation graph convolutional networks arxiv battaglia p pascanu r lai m rezende d j et al interaction networks learn ing objects relations physics ad vances neural information processing systems – battaglia p w et al relational inductive biases deep learning graph networks arxiv zhou j et al graph neural networks review methods applications ai open – geerts f mazowiecki f pérez g let’s agree degree comparing graph convolu tional networks messagepassing frame work arxiv duvenaud d et al convolutional networks graphs learning molecular ﬁngerprints pro ceedings th international conference neural information processing systemsvolume – klicpera j groß j günnemann s direc tional message passing molecular graphs international conference learning represen tations zhang s liu y xie l molecular mechanicsdriven graph neural network multiplex graph molecular structures arxiv withnall m lindelöf e engkvist o chen h building attention edge message pass ing neural networks bioactivity physical– chemical property prediction journal chem informatics tang b et al selfattention based message passing neural network predicting molecular lipophilicity aqueous solubility journal cheminformatics – liu y et al spherical message passing d graph networks arxiv stokes j m et al deep learning approach antibiotic discovery cell – torng w altman r b graph convolutional neural networks predicting drugtarget teractions journal chemical information modeling – somnath v r bunne c coley c w krause barzilay learning graph models retrosynthesis prediction arxiv r li j cai d x learning graphlevel rep resentation drug discovery arxiv liu k et al cheminet molecular graph con volutional network accurate drug property prediction international journal molecular sciences unke o t meuwly m physnet neural net work predicting energies forces dipole mo ments partial charges journal chemical theory computation – schütt k t sauceda h e kindermans pj tkatchenko müller kr schnet– deep learning architecture molecules materials journal chemical physics schütt k t arbabzadah f chmiela s müller k r tkatchenko quantum chemical insights deep tensor neural net works nature communications – schütt k gastegger m tkatchenko müller kr maurer r j unifying machine learning quantum chemistry deep neu ral network molecular wavefunctions nature communications – bogojeski m vogtmaranto l tuckerman m e müller kr burke k quantum chemical accuracy density functional ap proximations via machine learning nature com munications – yang k et al analyzing learned molecular rep resentations property prediction journal chemical information modeling – axelrod s gomezbombarelli r molecu lar machine learning conformer ensembles arxiv jiménezluna j grisoni f schneider g drug discovery explainable artiﬁcial intel ligence nature machine intelligence – schnake t et al xai graphs explaining graph neural network predictions identify ing relevant walks arxiv li y vinyals o dyer c pascanu r battaglia p learning deep generative models graphs arxiv simonovsky m komodakis n graphvae wards generation small graphs using varia tional autoencoders international conference artiﬁcial neural networks – de cao n kipf t molgan implicit small molecular graphs generative model arxiv flamshepherd d wu t c aspuruguzik mpgvae improved generation small ganic molecules using message passing neural nets machine learning science technology j liu b ying z pande v leskovec j graph convolutional policy network goal directed molecular graph generation advances neural information processing systems – jin w barzilay r jaakkola t multi objective molecule generation using interpretable substructures international conference ma chine learning – lei t jin w barzilay r jaakkola t deriving neural architectures sequence graph kernels arxiv thomas n et al tensor ﬁeld networks rotationand translationequivariant neural net works d point clouds arxiv smidt t e geiger m miller b k find ing symmetry breaking order parameters euclidean neural networks physical review re search l smidt t e euclidean symmetry equivari ance machine learning trends chemistry hutchinson m et lietransformer selfattention lie groups al equivariant arxiv unke o t et al spookynet learning force ﬁelds electronic degrees freedom non local eﬀects arxiv batzner s et al se equivariant graph neu ral networks dataeﬃcient accurate teratomic potentials arxiv müller c spherical harmonics springer dray t uniﬁed treatment wigner d func tions spinweighted spherical harmonics monopole harmonics journal mathematical physics – hermann j schätzle z noé f deepneural network solution electronic schrödinger equation nature chemistry – pfau d spencer j s matthews g foulkes w m c ab initio solution many electron schrödinger equation deep neural networks physical review research choo k mezzacapo carleo g fermionic neuralnetwork states abinitio electronic structure nature communications – hochreiter s schmidhuber j long short term memory neural computation – rajan k zielesny steinbeck c dec imer towards deep learning chemical image recognition journal cheminformatics – li y rezaei m li c li x deepatom framework proteinligand binding aﬃnity prediction ieee international conference bioinformatics biomedicine bibm – karimi m wu d wang z shen y deepaﬃnity interpretable deep learning compound–protein aﬃnity uniﬁed recur rent convolutional neural networks bioinfor matics – jiménez j et al deltadelta neural networks lead optimization small molecule potency chemical science – jiménez j doerr s martinezrosell g rose s de fabritiis g deepsite protein binding site predictor using dconvolutional neural networks bioinformatics – ahmed e et al survey deep learning advances diﬀerent d data representations arxiv pfaﬀ t fortunato m sanchezgonzalez battaglia p learning meshbased simulation graph networks international conference learning representations liu q et al octsurf eﬃcient hierarchical voxelbased molecular surface representation proteinligand aﬃnity prediction journal molecular graphics modelling mylonas s k axenopoulos daras p deepsurf surfacebased deep learning ap proach prediction ligand binding sites proteins arxiv barnard j m representation molecular structuresoverview handbook chemoinfor matics data knowledge volumes – wiswesser w j historic development chem ical notations journal chemical information computer sciences – wiswesser w j wiswesser line formula notation chemical engineering news archive – ash s cline m homer r w hurst t smith g b sybyl line notation sln versatile language chemical structure repre sentation journal chemical information computer sciences – heller s mcnaught stein s tchekhovskoi d pletnev inchi worldwide chemical structure identiﬁer standard journal chem informatics – zhang t li h xi h stanton r v rot stein s h helm hierarchical notation lan guage complex biomolecule structure repre sentation journal chemical information modeling – öztürk h özgür schwaller p laino t ozkirimli e exploring chemical space using natural language processing methodologies drug discovery drug discovery today – cadeddu wylie e k jurczak j wampler doty m grzybowski b organic chem istry language implications chemical linguistics structural ret rosynthetic analyses angewandte chemie inter national edition – gómezbombarelli r et al automatic chemical design using datadriven continuous represen tation molecules acs central science – o’boyle n dalke deepsmiles adap tation smiles use machinelearning chemical structures chemrxivv krenn m häse f nigam friederich p aspuruguzik selfreferencing embedded strings selfies robust molecular string representation machine learning science technology rumelhart d e hinton g e williams r j learning internal representations error propagation tech rep california univ san diego la jolla inst cognitive science hochreiter s vanishing gradient problem learning recurrent neural nets prob lem solutions international journal uncer tainty fuzziness knowledgebased systems – pascanu r mikolov t bengio y diﬃculty training recurrent neural networks international conference machine learning – chung j gulcehre c cho k bengio y empirical evaluation gated recurrent neural networks sequence modeling arxiv valsecchi c grisoni f motta s bonati l ballabio d nura curated dataset nu clear receptor modulators toxicology ap plied pharmacology bemis g w murcko m properties known drugs molecular frameworks journal medicinal chemistry – yuan w et al chemical space mimicry drug discovery journal chemical information modeling pmid – bjerrum e j threlfall r molecular gen eration recurrent neural networks rnns arxiv gupta et al generative recurrent networks de novo drug design molecular informatics merk d friedrich l grisoni f schneider g de novo design bioactive small molecules artiﬁcial intelligence molecular informatics merk d grisoni f friedrich l schneider g tuning artiﬁcial intelligence de novo design naturalproductinspired retinoid x re ceptor modulators communications chemistry – olivecrona m blaschke t engkvist o chen h molecular denovo design deep reinforcement learning journal cheminfor matics – popova m isayev o tropsha deep rein forcement learning de novo drug design sci ence advances eaap grisoni f et al combining generative artiﬁ cial intelligence onchip synthesis de novo drug design science advances arúspous j et al randomized smiles strings improve quality molecular generative mod els journal cheminformatics – grisoni f moret m lingwood r schnei der g bidirectional molecule generation recurrent neural networks journal chemical information modeling müller t hiss j schneider g recur rent neural network model constructive pep tide design journal chemical information modeling – nagarajan d et al computational antimi crobial peptide design evaluation multidrugresistant clinical isolates bacteria journal biological chemistry – hamid mn friedberg identifying antimi crobial peptides using word embedding deep recurrent neural networks bioinformatics – issn nov das p et al accelerated antimicrobial discov ery via deep generative models molecular dynamics simulations nature biomedical engi neering – zhou s zou h liu c zang m liu t combining deep neural networks protein secondary structure prediction ieee access – tsai st kuo ej tiwary p learning molecular dynamics simple language model built upon long shortterm memory neural net work nature communications – gomezbombarelli r et al automatic chemical design using datadriven continuous repre sentation molecules acs central science – lin x quan z wang zj huang h zeng x novel molecular representation bigru neural networks learning atom brief ings bioinformatics – preuer k renz p unterthiner t hochreiter s klambauer g fréchet chemnet distance metric generative models molecules drug discovery journal chemical information modeling – schwaller p et al predicting retrosynthetic pathways using transformerbased models hypergraph exploration strategy chemical sci ence – pesciullesi g schwaller p laino t rey mond jl transfer learning enables molec ular transformer predict regioand stereoselec tive reactions carbohydrates nature commu nications – kreutter d schwaller p reymond jl predicting enzymatic reactions molec ular transformer chemical science schwaller p et al mapping space chemical reactions using attentionbased neural networks nature machine intelligence – chithrananda s grand g ramsundar b chemberta largescale selfsupervised pretraining molecular property prediction arxiv j et al molecular optimization captur ing chemist’s intuition using deep neural net works honda s shi s ueda h r smiles trans former pretrained molecular ﬁngerprint low data drug discovery arxiv mirza m osindero s conditional generative adversarial nets arxiv arjovsky m chintala s bottou l wasser stein generative adversarial networks interna tional conference machine learning – méndezlucio o baillif b clevert da rouquié d wichard j de novo generation hitlike molecules gene expression signa tures using artiﬁcial intelligence nature commu nications – griﬃths rr hernándezlobato j m con strained bayesian optimization automatic chemical design using variational autoencoders chemical science – alperstein j t arxiv z cherkasov rolfe smiles variational autoencoder hirohara m saito y koda y sato k sakakibara y convolutional neural network based smiles representation compounds detecting chemical motif bmc bioinformat ics – kimber t b engelke s tetko v bruno e godin g synergy eﬀect convo lutional neural networks multiplicity smiles improvement molecular predic tion arxiv zheng s yan x yang y xu j iden tifying structure–property relationships smiles syntax analysis selfattention mechanism journal chemical information modeling – lim s lee y o predicting chemi cal properties using selfattention multitask learning based smiles representation arxiv shin b park s kang k ho j c self attention based molecule representation pre dicting drugtarget interaction machine learn ing healthcare conference – elabd h et al amino acid encoding deep learning applications bmc bioinformatics – satorras v g hoogeboom e fuchs f b posner welling m e n equivariant malizing flows molecule generation d arxiv fujita t winkler d understanding roles “two qsars” journal chemical information modeling – al open graph benchmark learning graphs hu w et datasets machine arxiv wu z et al moleculenet benchmark molecular machine learning chemical science – polykovskiy d et al molecular sets moses benchmarking platform molecular generation models frontiers pharmacology brown n fiscato m segler m h vaucher c guacamol benchmarking models de novo molecular design journal chemical formation modeling – von lilienfeld o müller kr tkatchenko exploring chemical compound space quantumbased machine learning na ture reviews chemistry – unke o t et al machine learning force ﬁelds chemical reviews – ramakrishnan r dral p o rupp m von lilienfeld o quantum chemistry structures properties kilo molecules scientiﬁc data – isert c atz k jiménezluna j schneider g qmugs quantum mechanical properties druglike molecules arxiv von rudorﬀ g f heinen s n bragato m von lilienfeld o thousands reactants transition states competing e s reac tions machine learning science technology ",64.2,"1"
2,"Dilemma of the Artificial Intelligence Regulatory Landscape","dilemma artificial intelligence regulatory landscape weiyue wu shaoshan liu perceptin abstract autonomous driving startup company struggled broad spectrum regulatory requirements four years large amount time effort budget poured varying compliance procedures instead rd way complex varying regulatory processes subtly give advantage wellestablished resourceful technology firms resourceconstrained ai startups however situation alone extent reflects dilemma artificial intelligence ai regulatory landscape article introduces firsthand experiences dealing varying ai regulatory frameworks provides practical recommendations ai startups work regulators efficiently effectively introduction legal regulations get ahead technological developments progress human society may constrained conversely technological developments run ahead legal regulations unregulated new technologies may harm human society defying technological developments fundamental purpose exactly happened world past decade technological developments far outpaced legal regulations worse traditional legal frameworks focus relationship people whereas must develop legal framework regulate relations people intelligent machines current era integrating ai technologies human society imposes unique legal challenges without precedence first time history ai potential generate solutions superior human expect however today ai bound ethics decisions made ai may considered line standards ethics generally accepted human society unforeseeable superiority problem ethical control may create chaos ai regulatory landscape ai adequately regulated problems will exacerbate unforeseen social issues current legal framework unprepared autonomous driving startup founded launched commercial autonomous driving operations us europe japan china find promising market treacherous journey filled regulatory roadblocks land mines problem unique autonomous driving universal ai applications article share firsthand experiences tackling regulatory requirements attempting reduce friction innovative technologies regulatory requirements advance human society properly regulated ai technologies global commercial deployment experiences autonomous driving startup business model provide turnkey solutions customers solution includes hardware drivebywire chassis computing units full stack autonomous driving software perception localization user interface past four years performed commercial deployments different regions worldwide bumped broad spectrum regulatory rules across countries regions within country lack regulation standardization become unhealthy companies ai sector mostly startups spend significant amount budget time effort dealing different regulatory measures section summarizes first hand experiences working authorities worldwide deployments china autonomous driving test permits china virtually nonexistent governmental documents stated highlevel goals principles pilot cities like shanghai absence test permit certification pushed startups field test vehicles wildly highways traffic predictable less complicated rural areas population scarce posing fatal risks general public minimize compliance safety risks chose carry autonomous driving projects restricted areas factories school campuses since areas private test pod go roads approved property management addition set speed limit mileshour cope safety risks designed several redundant perception systems avoid accidents accumulated months testing data different driving scenarios company invited join autonomous driving pilot program shenzhen china’s silicon valley selected present latest autonomous driving technology global trend field seminar hosted local government seized opportunity emphasize urgency importance regulated test drives six months voice heard file first test permit application case whole regulatory process lagged behind actual technological development pushing companies play wild first file applications later furthermore company goes another city new autonomous driving project whole process will start zero deployments europe union late invited carry autonomous driving operations european city took us seven months complete application process launch commercial deployment first three months focused technical specifications communication bandwidth requirements visual perception response time latter four months focused functional specifications scenarios recognizing unexpected object test field preparing backup test plan rainy days dry city filing requested reports granted restrictive operation permit lasted month europe strictly allowed perform experiments roads whether public private without valid pemit addition room negotiation local authorities us regardless actual operation environment spend extended amount time effort prepare scenarios go regulatory process imposes significant financial burden ai startups already constrained budget race time growth way complex varying regulatory processes across different regions subtly give advantage wellestablished resourceful technology firm resourceconstrained startups deployments japan autonomous driving cases launched china europe entered japan market directly worked japan’s ministry land infrastructure transport tourism mlit carry driverless vehicle project experience japan combination china europe negotiate authorities start straightforward scenarios reduce paperwork meanwhile authority closely reviewed technical specifications riskmitigation plans scenarios application endorsement process topdown work national government gain approval trickled local governments carry operation first ran test operation park fukuoka supervision mlit operation data used go basic safety check assess risk autonomous driving operations procedure completed allowed continue operation new site historic park nara whole operation scaleup process took months worthwhile since repeat initial certification process received endorsements mlit deployments us contrast japan take bottomup approach deployments us local governments responsible innercity transportation local governments authority grant autonomous driving operation permits since state federal governments limited guidance deployment autonomous driving services deal city individually expand operation inefficient costly process given regulatory differences among cities cases us test data safety assurance plans california give us privileges applied test permit indiana although arrangement may adequate mitigate people’s safety risk different states different traffic densities room improve process standardization transparency save ai startups’ resources turn help advance technology dilemma figure summarizes basic process autonomous driving deployment four areas discussed deployment process consisted four essential parts application filing initiate conversations nationallevel approval locallevel approval commercial deployment explicit instruction follow took least eight months reach commercial deployment technical readiness addition broad spectrum opacity regulatory requirements consumed significant amount company’s resources costing annual budget deal regulatory issues compared average budget software companies figure process getting test permit autonomous driving different regions traditionally software companies may take incremental approach comply regulations first launch minimum viable product collect feedback pilot users reiterate several times product rolls general public compliance issues may arise yet get addressed fly software updates conversely autonomous driving startups deal various compliance issues day one imposes significant financial burden autonomous driving startups dealing regulatory uncertainty ubiquitous problem among many ai companies exemplified microsofts retracting facial recognition products believe dilemma caused lack standardization ai product deployment process lack understanding ai technologies regulatory bodies without properly regulated environment ai companies thrive transform latest technology successful products regulatory requirements absent players field may go wild put company’s future public safety stake jeopardizing technological advancement long run requirements tight ai companies especially startups sufficient budget compliance exhausted filing repetitive paperwork different offices recommendations ai startups lessons can help ai companies especially startups limited resources navigate various regulatory barriers specific questions asked regulators varied time time found key settling concerns clearly convey message prospectus benefits outpasses relevant risks recommendations proven effective deployment projects across multiple countries first crossfunction communication information sharing critical unifying team deal external complexities complex regulatory requirements often cause chaos within organization instance carried deployment projects field engineers often needed work directly regulators demonstrate technical progress fix deployment problems address concerns engineers crossfunctional experiences tend outperform without former ones capable communicating effectively regulators limited technical background second show strong evidence support benefits project regulators welcome innovations eager integrate innovative technologies economies look strong evidence technology indeed works look evidence credible sources publications leading journals field reports released leading research institutes hence suggest ai startup companies collaborate academic partners perform scientific research verify project’s benefits industryacademia collaboration essential foster ai industry ever third present concrete risk mitigation plan regulators regulatory actions designed mitigate risks local authority will often carry blame innovative project goes wrong therefore regulators try best mitigate risks presenting clear risk mitigation plan success stories deployment case studies elsewhere will undoubtedly improve regulator’s confidence project experiences presenting cases china europe helped us obtain approval operation japan instance development awareness technical standards safety intended functionality sotif autonomous driving effective method improve product creditability boost regulators’ confidence summary summarize firsthand experiences dealing broad spectrum regulatory requirements pinpoint dilemma ai regulatory landscape today provide practical recommendations ai startups work efficiently effectively within regulatory framework address problem however recommendations make ai startups better adapt regulatory landscape hoping standardization ai regulatory process will make ai industry much efficient effective references scherer matthew u regulating artificial intelligence systems risks challenges competencies strategies may harvard journal law technology vol spring available ssrn httpsssrncomabstract awad e dsouza s kim r schulz j henrich j shariff bonnefon jf rahwan moral machine experiment nature pp liu s critical business decision making technology startups perceptin case study ieee engineering management review pp yu b hu w xu l tang j liu s zhu y october building computing system autonomous micromobility vehicles design constraints architectural optimizations rd annual ieeeacm international symposium microarchitecture micro pp ieee liu s gaudiot jl autonomous vehicles lite selfdriving technologies start small go slow ieee spectrum pp japan smart island pilot project accessed httpswwwmlitgojpkokudoseisakuchiritkokudoseisakuchirittkhtml widen william h koopman philip autonomous vehicle regulation trust revised january ucla journal law technology spring forthcoming university miami legal studies research paper available ssrn httpsssrncomabstract true cost compliance data protection regulations accessed httpsstatichelpsystemscomglobalscapepdfsguidesgstruecostofcompliancedataprotection regulationsgdpdf microsoft plans eliminate face analysis tools push ‘responsible ai’ accessed httpswwwnytimescomtechnologymicrosoftfacialrecognitionhtml safety intended functionality sotif autonomous driving accessed httpswwwforbescomsitesforbestechcouncilsafetyoftheintendedfunctionality sotifforautonomousdriving ",41.1,"0.999"
3,"Classification Performance Metric Elicitation and its Applications"," g u l m t t s v v x r © gaurush hiranandani classification performance metric elicitation applications gaurush hiranandani dissertation submitted partial fulﬁllment requirements degree doctor philosophy computer science graduate college university illinois urbanachampaign urbana illinois doctoral committee associate professor oluwasanmi koyejo chair professor srikant rayadurgam professor paris smaragdis associate professor shivani agarwal university pennsylvania abstract given learning problem realworld tradeoﬀs cost function model trained optimize metric selection problem machine learning despite practical interest limited formal guidance select metrics machine learning applications thesis outlines metric elicitation principled framework selecting performance metric best reﬂects implicit user preferences speciﬁed evaluation metric can used compare train models manuscript formalize problem metric elicitation devise novel strate gies eliciting classiﬁcation performance metrics using pairwise preference feedback classiﬁers speciﬁcally provide novel strategies eliciting linear linearfractional metrics binary multiclass classiﬁcation problems extended framework elicits groupfair performance metrics presence multiple sensitive groups elicitation strategies discuss robust ﬁnite sample feedback noise thus useful practice realworld applications using tools geometric characterizations feasible confusion statistics space binary multiclass multiclassmultigroup classiﬁcation setups provide strategies elicit wider range complex modern multiclass metrics deﬁned quadratic functions predictive rates exploiting local linear structure strategy can easily extended eliciting metrics higher order polynomials application perspective also propose use metric elicitation framework optimizing complex black box metrics amenable deep network training particular linear elicitation strategies can used elicit locallinear approximation blackbox metrics exploited existing iterative optimization routines lastly bring theory closer practice conduct preliminary realuser study shows eﬃcacy metric elicitation framework recovering users’ preferred performance metric binary classiﬁcation setup ii “ parents brother sisterinlaw love support” iii acknowledgments goal pursue phd bridge knowledge gap perspective asked better advising guidance advisor professor oluwasanmi koyejo provided writing thesis came believe successful achieving goal great extent owe every success phd esteemed advsisor professor oluwasanmi koyejo expertise guidance invaluable research will always cherish alignment thinking around research work kind freedom working support constructive feedback every idea came immensely encouraging always much learn especially context switching multiple projects high standard workethics like thank doctoral committee members professor oluwasanmi koyejo professor srikant rayadurgam professor paris smaragdis professor shivani agarwal always helpful given extremely thoughtful feedback thesis research incredibly honored able phd committee feel utmost gratitude help support like acknowledge colleagues machinelearning group enjoyed sharing oﬃces good friends like thank professor matus tel garsky professor pierre moulin professor ruoyu sun professor jiawei han professor nan jiang lectured outstanding courses taken university illinois urbanachampaign uiuc also like express gratitude computer science department giving nice friendly environment work without support mentors goal bridging knowledge gap possible like thank harikrishna narasimhan mahdi milani fard nikhil rao sumeet katariya karthik subbian prateek jain branislav kveton atanu sinha shiv kumar saini sunav choudhary sumit shekhar guidance sharing knowledge describe words much learnt colleaguesco authors various universities adobe research used work joining phd program thankful everyone like thank former advisors professor harish karnick professor jeanmarc schlenker inspired beginning research career encouraged pursue doctorate degree also like thank previous colleagues indian institute technology kanpur iv words express gratitude towards parents jayshree hiranandani naren dra hiranandani always everything grateful parents sacriﬁces made order make reach today also like thank brother dharmendra hiranandani inspiration idol since childhood guidance immensely helpful throughout learning discussions several aspects life career deeply engraved inside force behind achievements also grateful sisterinlaw ruchira bhelekar hiranandani learn something every day regarding positive cheerful attitude towards life writing thesis going acl surgery rehabilitation timely deposit thesis possible without support friends monika salkar ishita jain siddhansh agarwal amber srivastava lastly like thank computer science department uiuc awarding cl jane ws liu award acted catalyst research metric elicitation also like thank intel microsoft azure google cloud platform providing computational resources support research v table contents chapter introduction contributions thesis organization chapter metric elicitation preliminaries metric elicitation problem setup chapter binary classification performance metric elici tation background confusion matrices algorithms metric elicitation guarantees experiments related work concluding remarks chapter multiclass classification performance metric elicitation preliminaries geometry parametrizations query spaces metric elicitation extensions guarantees experiments discussion future work related work concluding remarks chapter fair performance metric elicitation background geometry product set rm metric elicitation guarantees experiments related work concluding remarks future work vi chapter quadratic metric elicitation fairness beyond background quadratic metric elicitation eliciting quadratic fairness metrics guarantees experiments extension higher order polynomials related work discussion limitations future work chapter optimizing blackbox metrics metric elic itation introduction problem setup example weighting linear metrics plugin based algorithms theoretical guarantees related work experiments concluding remarks chapter practical metric elicitation dataset visualization choice user interface user study results concluding remarks chapter conclusion future work appendix binary classification performance metric elic itation visualizing set confusion matrices proofs extended experiments monotonically decreasing case appendix b multiclass classification performance metric elicitation b shrinkinterval shrinkinterval subroutines b proofs section b proofs section b proofs section b proofs section b extended experiments vii appendix c fair performance metric elicitation c proofs details section c derivations section c proof section appendix d quadratic performance metric elicitation d geometry feasible space proofs section d quadratic performance metric elicitation procedure d fair quadratic performance metric elicitation procedure d elicitation guarantee qpme procedure appendix e optimizing blackbox metrics metric elicitation e extension general linear metrics e proofs e error bound weight elicitation fixed probing classiﬁers e error bound fweg unknown ψ e running time algorithm e plugin coordinatewise search baseline e solving constrained satisfaction problem e additional experimental details references viii chapter introduction given class prediction problem performance metric classiﬁer optimize machine learning practitioners often encounter question diﬀerent forms example natural language processing practitioners face question “ good summary given article ” similarly computer vision folks “ good caption given image” poses identical challenge ﬁeld musicaudio research question “ one piece music similar another” may get similar treatment medical predictions another application ignoring cost sensitive tradeoﬀs can directly impact lives even companies industry struggle ﬁnd answer similar questions specialized teams statisticianseconomists routinely hired monitor many metrics – since optimizing wrong metric directly translates lost revenue unfortunately scant formal guidance within machine learning literature practitioner might choose appropriate metric beyond default choices even less guidance selecting metric reﬂects preferences practitioners address issue propose framework metric elicitation goal estimate performance metric best reﬂect implicit user preferences framework enables practitioner adjust performance metrics based applica tion context population hand motivation employing metrics reﬂect user’s innate tradeoﬀs one can learn models best capture user prefer ences face simply requires querying user oracle determine quality assigns classiﬁers learned using standard classiﬁcation data however humans often inaccurate asked provide absolute preferences therefore propose gathering feedback form pairwise classiﬁer comparison queries user asked compare two classiﬁers provide indicator relative preference using queries aims elicit innate performance metric user see figure visual intuition framework focus eliciting common performance metrics functions either confusion matrix predictive rates elements commonly referred measurements classiﬁer statistics manuscript thus classiﬁer comparison query can con ceptually represented classiﬁer statistics comparison query despite apparent sim pliﬁcation problem remains challenging one can query feasible classiﬁer statistics ie classiﬁer statistics exists classiﬁer solve problem introduce new characterizations space feasible classiﬁer statistics associated metrics depending factors model complexity interpretability beyond scope manuscript figure illustration metric elicitation framework goal eﬃciently estimate oracle’s performance metric assume models summarized via measurements classiﬁer statistics metric function measurements elicitation procedure poses pairwise comparisons queries type classiﬁer vs classiﬁer b equiv classiﬁer statistics vs classiﬁer statistics b based relative preference feedback oracle framework elicits oracle’s metric queries possible binary multiclass multiclassmultigroup classiﬁcation problems enabling design binarysearch type procedures identify innate performance metric oracle furthermore proposed procedures remain robust noise classiﬁer esti mation noise pairwise comparison thus work directly results practical algorithms utility illustrated via following real life applications motivating application medical decisionmaking using costsensitive clas siﬁcation automated medical decisionmaking important application ignoring cost tradeoﬀs can directly impact lives consider case cancer diagnosis treat ment support binary classiﬁcation setting doctor’s unknown innate performance metric may approximated linear function confusion matrix el ements ie innate reward values true positives true negatives – equivalently costs false positives false negatives – based known consequences misdiagnosis ie sideeﬀects treating healthy patient vs mortality rate treating sick patient doctor takes role oracle proposed approach exploits space confusion matrices associated possible classiﬁers can learned standard classiﬁcation data determine underlying rewards equivalently costs provably using least possible number pairwise comparison queries posed doc tor metric elicited can used evaluate classiﬁers andor train future classiﬁers motivating application fair machine learning machine learning models increasingly applied important decisionmaking tasks hiring sentencing yet increasingly clear automated decisionmaking susceptible bias whereby decisions made algorithm unfair certain subgroups end several fairness metrics proposed – goal reducing discrimination bias automated decisionmaking one diﬃcult steps involved practical deployment decision fairness metric employ exacerbated observation common metrics often lead contradictory outcomes approach metric elicitation can directly used solve fairness metric selection problem perhaps groups ethicists relevant decision makers take role oracle groupspeciﬁc predictive rates correspond query space interest – easily approximated classiﬁer metric elicitation can used formally quantify intuitions – specifying quantitative metric best applied measuring optimizing fairness given machine learning task quantify tradeoﬀ predictive performance fairness applications proposed metric elicitation framework goes beyond just specifying user preferred performance metrics can also used learn classiﬁers optimize complex performance metrics – aspect often crucial practical applications several existing optimization algorithms iterative nature iteration locallinear objective optimized iterates optimization routine combined get ﬁnal classiﬁer form metric known obtaining local linear objective metric boils eliciting linear performance metric local neighborhood thus tools metric elicitation framework can readily applied optimizing blackbox metrics discuss one procedure optimizes black box metrics presence machine oracle queried classiﬁer returns absolute quality feedback classiﬁer brieﬂy discuss proposed procedure can extended human oracle provides pairwise preference feedback along challenges associated lastly conduct preliminary user study build upon existing visualiza tions confusion matrices ask pairwise preferences b try elicit linear performance metric using proposed procedure binary classiﬁcation setup associated cancer diagnosis goal preliminary study test certain assumptions check workﬂow implementation provide future guidance visualizing confusion matrices pairwise comparisons ﬁnally eliciting actual performance metrics reallife scenarios contributions thesis organization ﬁrst brieﬂy summarize contributions thesis dig deep contribution later chapters metric elicitation framework chapter formalize metric elicitation – principled framework determining supervised classiﬁcation metrics user feedback case pairwise feedback show certain conditions met ric elicitation equivalent learning preferences pairs classiﬁer statistics confusion matrices predictive rates b binary classiﬁcation performance metric elicitation chapter underlying metric linear binary classiﬁcation setup propose elicitation strategy recover oracle’s metric whose query complexity decays logarithmically desired resolution also show querycomplexity rates match lower bound extend linear metric elicitation algorithm elicit complex yet prevalent linearfractional binary classiﬁcation performance metrics c multiclass classiﬁcation performance metric elicitation chapter extend work binary classiﬁcation setup proposing strategies complicated multiclass classiﬁcation setting – thus signiﬁcantly increasing use cases propose two algorithms multiclass classiﬁcation metric elicitation use multiple binarysearch subroutines recover oracle’s linear metric one proposed algorithms assumes sparsity condition metric thus useful number classes large similar binary case provide algorithms eliciting linearfractional multiclass classiﬁcation performance metrics d fair performance metric elicitation chapter respect applications fairness devise novel strategy elicit groupfair performance metrics multi class classiﬁcation problems multiple sensitive groups also includes selecting tradeoﬀ predictive performance fairness violation procedure exploits piecewise linearity metric groupspeciﬁc predictive rates uses binarysearch based subroutines recovers metric linear query complexity e extension quadratic metric elicitation beyond chapter previous strategies can handle metrics linear quasilinear functions classiﬁer statistics can restrictive domains metrics thus propose novel strategies eliciting complex nuanced eg metrics deﬁned quadratic functions classiﬁer statistics can easily applied fair metric elicitation setups well thus able handle general family metrics can better capture practitioner’s innate preferences generalize quadratic elicitation strategy higherorder polynomial functions idea approximate dth order polynomial locally d − th order polynomials recursively apply procedure lowerorder polynomials f optimizing blackbox metrics metric elicitation chapter consider learning optimize classiﬁcation metric deﬁned blackbox function confusion matrix blackbox learning settings ubiquitous example learner query access metric interest noisylabel domain adaptation applications learner must evaluate metric via performance evaluation using small validation sample approach adaptively learn example weights training dataset resulting weighted objective best approximates metric validation sample use fact example weights can seen gradient metric estimated metric elicitation procedure machine oracle responds absolute quality value classiﬁer clean validation dataset show model estimate example weights use iteratively postshift pretrained class probability estimator construct classiﬁer also analyze resulting procedure’s statistical properties experiments various label noise domain shift fair classiﬁcation setups conﬁrm proposal compares favorably stateoftheart baselines application g eliciting realuser metric preferences chapter beyond technical contribu tions research raises novel questions regards classiﬁer classiﬁer statistics visualization interpretability eliciting human preferences explore existing humancomputer interface techniques task including work visualizing con fusion matrices nonexpert users create web userinterface conduct preliminary userstudy binary classiﬁcation setup order elicit realusers’ performance metrics devise procedures evaluate ﬁdelity metrics recovered proposed metric elicitation framework metric elicitation procedures contributions ae shown robust ﬁnite sample oracle feedback noise thus useful practice methods can applied either querying preferences classiﬁers classiﬁers statistics equiva lence crucial practical applications provide statistical consistency guarantees blackbox optimization algorithm contribution f uses metric elicitation techniques presence machine oracles brieﬂy discuss algorithm can extended presence human oracles provide pairwise feedback including feedback ab tests challenges associated related literature corresponding subtopic provided respective chapter draw conclusions future work chapter lastly proofs provided corresponding chapters’ appendices chapter metric elicitation section formally describe problem metric elicitation ﬁrst lay preliminaries standard notations corresponding classiﬁcation problems common entire manuscript notation k ∈ z denote index set k ··· k use ∆k denote k − dimensional simplex denote inner product vectors cid··cid hadamard product cid matrix oﬀ diaga returns vector oﬀdiagonal elements rowmajor form diaga returns vector diagonal elements denote cidnorm cid∞norm vector cid · cid cid · cid∞ respectively preliminaries consider standard kclass classiﬁcation setting x ∈ x y ∈ k repre senting input output random variables respectively assume access sample n examples generated iid distribution px y work ran x yin domized classiﬁers takes feature vector x input outputs prediction form probability distribution kclasses use h x → ∆k h h x → ∆k denote set classiﬁers measurements classiﬁer statistics assume q measurements classiﬁer statistics model h measurement functions gi h × p → rq denote measure ments classiﬁer statistics classiﬁer h vector csh p gh p gqh p ex amples statistics classiﬁer include confusion matrix cijh py h j j ∈ k predictive rate matrix rijh ph jy j ∈ k etc q → r classiﬁer statistics cs metrics consider performance metrics deﬁned general function φ φcsh p since scale metric aﬀect learning problem allow φ bounded observe purposes metric invariant positive multiplicative scaling additive bias one common example metrics linear metric given coeﬃcient vector ∈ rq cidacid without loss generality due scaleinvariance given φlin cida csh pcid feasible classiﬁer statistics will restrict attention classiﬁer statistics feasible ie can achieved classiﬁer allows us build elicitation methods can applied either querying preferences classiﬁers classiﬁers statistics set feasible classiﬁer statistics given cs csh p h ∈ h simplicity will suppress dependence p h clear context metric elicitation problem setup now describe problem metric elicitation ’s unknown metric φ seek elicit form posing queries oracle asking two classiﬁers preferred oracle access underlying metric φ provides answers comparing value two classiﬁers deﬁnition oracle query given two classiﬁers h h equiv classiﬁer statistics cs cs respectively query oracle metric φ represented γh h φ ωcs cs φ φcs φcs γ h×h → ω cs ×cs → query asks whether h preferred h equiv cs preferred cs measured φ practice oracle can expert group experts entire user population framework can applied posing classiﬁer comparisons directly via interpretable learning techniques via ab testing example internetbased applications one may perform ab testing deploying two classiﬁers b two diﬀerent subpopulations users use level engagement decide two classiﬁers preferred applications may present user visualizations measurements predictive rates two diﬀerent classiﬁers eg user provide pairwise feedback since metrics consider functions classiﬁer statistics queries compar ing classiﬁers queries associated classiﬁer statistics convenience will algorithms pose queries comparing two feasible classiﬁer statistics can equivalently seen comparing two classiﬁers next formally state problem deﬁnition metric elicitation pairwise queries given p suppose oracle’s unknown performance metric φ using oracle queries form ωcs cs φ recover metric ˆφ cidφ − ˆφcid κ suitable norm cid · cid suﬃciently small error tolerance κ notice deﬁnition involves true population quantities cs cs however prac tice given ﬁnite samples leads practical deﬁnition metric elicitation problem deﬁnition metric elicitation pairwise queries given xi yin problem stated deﬁnition except queries form ω ˆcs ˆcs ˆcs ˆcs estimated classiﬁer statistics given samples performance evaluated query complexity quality elicited metric standard decision theory literature present approach ﬁrst assuming access population quantities population classiﬁer statistics csh p deﬁnition examine estimation error ﬁnite samples ie empirical rates ˆcshx yin deﬁnition lastly proposed metric elicitation strategies work following noise model deﬁnition oracle feedback noise ω ≥ oracle may provide wrong answers whenever φcs − φcscid ω otherwise provides correct answers simply put classiﬁer statistics cs cscid close measured φ oracle responses may incorrect show robustness approaches noise model next discuss elicitation strategies diﬀerent classiﬁcation scenarios starting binary classiﬁcation problem setup chapter binary classification performance metric elicitation chapter focus eliciting binary classiﬁcation performance metrics pair wise feedback practitioner queried provide relative preference two classiﬁers choose measurement space space feasible confusion matrices associated classiﬁers binary classiﬁcation exploiting key geometric properties space confusion matrices obtain provably query eﬃcient algorithms eliciting performance metrics emphasize notion pairwise classiﬁer com parison new already prevalent industry example ab testing whole population users acts oracle similarly classiﬁer comparison single expert becoming commonplace due advances ﬁeld interpretable machine learning ﬁrst edition metric elicitation strategies focus common perfor mance metrics functions confusion matrix particularly linear ratiooflinear functions includes almost modern metrics accuracy fβmeasure jaccard similarity coeﬃcient etc construction pairwise classiﬁer com parisons may conceptually represented associated pairwise confusion matrix comparisons despite apparent simpliﬁcation problem remains challenging one can query feasible confusion matrices ie confusion matrices exists classiﬁer show characterization space confusion matrices enables design eﬃcient binarysearch type procedures identify innate performance met ric oracle classiﬁer confusion matrix comparisons may introduce additional noise approach remains robust noise classiﬁer confusion matrix estima tion noise comparison thus work directly results practical algorithm example consider case cancer diagnosis doctor’s unknown innate per formance metric linear function confusion matrix ie innate reward values true positives true negatives – equivalently equiv costs false positives false negatives – based known consequences misdiagnosis doctor takes role oracle proposed approach exploit space confusion matrices asso ciated possible classiﬁers can learned standard classiﬁcation data determine underlying rewards equiv costs provably using least possible number ab testing subpopulations users shown classiﬁer vs classiﬁer b responses determine overall preference interestingly person shown sample output one classiﬁers entire user population acts oracle comparing classiﬁers pairwise comparison queries posed doctor contributions chapter summarized follows • underlying metric linear propose binary search algorithm can recover metric query complexity decays logarithmically desired resolution show querycomplexity rates match lower bound • extend elicitation algorithm complex linearfractional performance metrics • prove robustness proposed approach feedback classiﬁer estimation noise proofs chapter provided appendix background let x ∈ x y ∈ represent input output random variables respectively negative class positive class assume dataset size n xi yin generated iid data generating distribution p iid∼ x y let fx marginal distribution x let ηx py x x ζ py represent conditional unconditional probability positive class respectively note earlier term function input x whereas latter constant denote classiﬁer h let h h x → set classiﬁers confusion matrix classiﬁer h denoted ch p ∈ r× comprising true positives tp false positives fp false negatives fn true negatives tn given c t p h p py h c f p h p py h c f n h p py h c t n h p py h clearly cid ij cij denote set confusion matrices c ch p h ∈ h population law p components confusion matrix can decomposed f n h p ζ − t p h p f p h p − ζ − t n h p decomposition reduces four dimensional space two dimensional space therefore set confusion matrices can deﬁned c t p h p t n h p h ∈ h clarity will suppress dependence p notation addition will subsume notation h implicit context denote confusion matrix c t p t n represent boundary set c ∂c hyperplane line cid tp tn coordinate system given cid · tp b · tn c b c ∈ r let φ × → r performance metric classiﬁer h determined confusion matrix ch without loss generality wlog assume φ utility larger values better types performance metrics consider two common families binary classiﬁcation metrics namely linear linearfractional functions confusion matrix deﬁnition linear performance metric lpm denote family ϕlp m given constants representing weights ∈ r deﬁne metric φc p af p af n n mt p mt n m m − m − m aζ − ζ example weighted accuracy wa w wt p wt n w w ∈ w w can shifted scaled without changing learning problem deﬁnition linearfractional performance metric lfpm denote family ϕlf p m given constants b b b b ∈ r deﬁne metric φc p af p af n n bt p bf p bf n bt n pt p pt n p qt p qt n q p − p − q b − b q b − b p aζ − ζ q bζ b − ζ example fβ measure jaccard similarity coeﬃcient jac fβ t p β βζ−ζ β − t n β t p jac t p − t n bayes optimal inverse bayes optimal classiﬁers given performance metric φ bayes utility τ optimal value performance metric classiﬁers ie τ sup h∈h φch sup c∈c φc bayes classiﬁer h exists classiﬁer optimizes performance metric similarly bayes confusion matrix given h argmax h∈h φch deﬁne inverse bayes utility c argmax c∈c φc τ inf h∈h φch inf c∈c φc inverse bayes classiﬁer given h argmin h∈h φch similarly inverse bayes confusion matrix given c argmin c∈c φc notice φ ∈ ϕlp m bayes classiﬁer predicts label maximizes expected utility conditioned instance discussed proposition let φ ∈ ϕlp m cid hx ηx ≥ m m mm ≥ ηx mm m m ≥ ow cid bayes optimal classiﬁer wrt φ inverse bayes classiﬁer given h −h problem setup borrow problem setup chapter particularly deﬁnitions oracle query deﬁnition metric elicitation ﬁnite samples deﬁnition since choice measurements confusion matrix entries ease understanding restate deﬁnitions replacing classiﬁer statistics confusion matrices binary classiﬁcation ﬁrst formalize oracle query recall deﬁnition confusion matrices exists surjective mapping h → c oracle queried determine relative preference two classiﬁers however since consider metrics func tions confusion matrix comparison query classiﬁers becomes equivalent comparison query confusion matrices setting deﬁnition oracle query given two classiﬁers h hcid equiv c ccid respectively query oracle metric φ represented γh hcid φ ωc ccid φ φc φccid c cid ccid confusion matrices γ h×h → ω c ×c → query denotes whether h preferred hcid equiv c preferred ccid measured according φ emphasize depending practical convenience oracle may asked com pare either confusion matrices classiﬁers achieving corresponding confusion matrices via approaches discussed beginning chapter henceforth simplicity nota tion will treat comparison query confusion matrix comparison query next state metric elicitation problem t n − ζ ∂c− ∂c −ζ ζ φ t p ζ θa θc θd θe m θb t n ¯cid∗ f ¯cid cid∗ f cid c∗ ¯c∗ ∇φ∗ t p figure supporting hyperplanes normal vectors resulting geometry c minimizer c∗ along supporting b sketch algorithm c maximizer c hyperplanes lfpms ∗ deﬁnition metric elicitation given xi yin suppose oracle’s true unknown performance metric φ recover metric ˆφ querying oracle pairwise comparisons form ω ˆc ˆccid ˆc ˆccid estimated confusion matrices samples cidφ− ˆφcid κ suﬃciently small r cid κ suitable norm cid · cid ultimately want perform described deﬁnition good approach ﬁrst solve assuming access appropriate population quantities population confusion matrices ch p consider practical implementation using estimated confusion matrices ﬁnite data ie chxi yin standard approach decision theory see eg estimation error ﬁnite samples adjudged noise source handled accordingly confusion matrices will require confusion matrices achieved possible classiﬁers thus necessary characterize set c way useful task assumption assume gt pηx ≥ t continuous strictly decreasing t ∈ equivalent standard assumptions event ηx t positive density zero probability note requires x point mass proposition properties c — figure set confusion matrices c convex closed contained rectangle ζ × − ζ bounded degree rotationally symmetric around centerpoint ζ assumption − ζ ζ vertices c c strictly convex thus supporting hyperplane c tangent one point −ζ additional visual intuition geometry c via example given appendix lpm parametrization connection supporting hyperplanes c lpm φ proposition guarantees existence unique bayes confusion matrix boundary ∂c optimum linear function strictly convex set unique lies boundary note linear function tradeoﬀs tp tn ie m m maximized boundary point regardless bias term m thus diﬀerent lpms can generated varying tradeoﬀs m m m thatcidmcid m conditioncidmcid aﬀect learning problem discussed example words performance metric scale invariant allows us represent family linear metrics ϕlp m single parameter θ ∈ π ϕlp m m cos θ sin θ θ ∈ π given m equiv θ can recover bayes classiﬁer using proposition bayes confusion matrix c θ c m t p m t n m using assumption due strict convexity c bayes confusion matrix c m unique therefore cidm ccid cidm c mcid ∀ c ∈ c c cid c m notice connection linear performance metrics supporting hyper planes set c see figure given m exists supporting hyperplane tangent c c m deﬁned follows cidm m · tp m · tn mt p m mt n m clearly m m opposite sign ie θ ∈ π π ∪ π π hm trivial classiﬁer predicting either everywhere words slope hyperplane positive touches set c either ζ − ζ m m cid sign ie θ ∈ π ∪ π π bayes confusion matrix away two vertices now may split boundary ∂c follows deﬁnition bayes confusion matrices lpms m m ≥ θ ∈ π form upper boundary denoted ∂c bayes confusion matrices lpms m m θ ∈ π π form lower boundary denoted ∂c− proposition follows confusion matrices ∂c ∂c− correspond classiﬁers form ηx ≥ δ δ ≥ ηx respectively δ ∈ algorithms section propose binarysearch type algorithms exploit geometry set c section ﬁnd maximizer minimizer associated supporting hyperplanes quasiconcave quasiconvex metrics algorithms used elicit lpms lfpms belong quasiconcave quasiconvex function families allow noisy oracles however simplicity will ﬁrst discuss algorithms elicita tion nonoise show robust noisy feedback section moreover one typically prefers metrics reward correct classiﬁcation ﬁrst discuss metrics monotonically increasing tp tn monotonically decreasing case discussed appendix natural extension following lemma quasiconcave quasiconvex metrics forms basis proposed algorithms lemma let ρ → ∂c ρ− → ∂c− continuous bijective parametriza tions upper lower boundary respectively let φ c → r quasiconcave function ψ c → r quasiconvex function monotone increasing t p t n composition φ ◦ ρ → r quasiconcave therefore uni modal interval ψ ◦ ρ− → r quasiconvex therefore unimodal interval unimodality quasiconcave quasiconvex metrics upper lower boundary set c along onedimensional parametrization m using θ ∈ π section allows us devise binarysearchtype methods ﬁnd maximizer c minimizer c ﬁrst order approximation φ points ie supporting hyperplanes c c algorithm maximizing quasiconcave metrics ﬁnding supporting hyperplanes optimum since φ monotonically increasing tp tn c convex maximizer must upper boundary hence start interval θa θb π deﬁnition divide four equal parts set slopes using line see figure b visual intuition compute bayes classiﬁers using proposition associated bayes confusion matrices line pose four pairwise queries oracle line line gives default direction binary search case outoforder responses lines shrink search interval half based oracle responses due ﬁnite samples c’s boundary may staircasetype bumps practice may lead outoforder responses even metric unimodal wrt θ algorithm quasiconcave metric maximization input  oracle ω initialize θa θb π whileθb − θa  set θc θaθb θd θaθb θe θaθb set corresponding slopes m’s using obtain hθahθchθd hθe hθb using proposition compute c θac θcc θdc θe c θb us ing query ωc θc c θa ωc θd c θc ωc θe c θd ωc θb c θe c θ cid c θcid ≺ c θcidcid consecutive θ θcid θcidcid assume default order c θ ≺ c θcid ≺ c θcidcid c θa cid c θc set θb θd elseif c θa ≺ c θc cid c θd set θb θd elseif c θc ≺ c θd cid c θe set θa θc θb θe elseif c θd ≺ c θe cid c θb set θa θd else set θa θd end output m c cid m md θd c c θd cid cidm tp tncid cidm ccid algorithm quasiconcave metric minimization follow algorithm except initialize θa π θb π invert responses replace oracle responses c ≺ ccid c cid ccid vice versa stop search interval becomes smaller given  tolerance lastly output slope m bayes confusion c supporting hyperplane cid point algorithm minimizing quasiconvex metrics ﬁnding supporting hyperplane optimum algorithm can used quasiconvex minimization two changes first start θ ∈ π π optimum will lie lower boundary ∂c− second check c ≺ ccid whenever algorithm checks c cid ccid vice versa output counterparts ie slope m inverse bayes confusion matrix c supporting hyperplane cid metric elicitation section discuss algorithms discussed later used subroutines elicit lpms lfpms see figure brief summary eliciting lpms suppose oracle’s metric ϕlp m cid φ∗ m∗ wlog cidm∗ cid m∗ section application algorithm oracle responds according ∗ hyperplane cid lpm elicitation true metric φ∗ m∗ run algorithm get c set elicited metric slope cid lfpm elicitation true metric φ∗ run algorithm get c run algorithm get c run oraclequery independent algorithm get elicited metric satisﬁes hyperplane cid soe hyperplane cid soe ∗ ∗ soes figure lpm lfpm elicitation procedures m∗ returns maximizer supporting hyperplane point since true performance metric linear take elicited metric ˆm slope resulting supporting hyperplane eliciting lfpms lfpm given p p q q simultaneously zero also bounded c scaling shifting change linearfractional form wlog may take φc ∈ ∀c ∈ c positive numerator denominator assumption let φ ∈ ϕlf p m assume p p ≥ p ≥ q p ≥ q p q p − qζ p − q − ζ p p proposition conditions assumption suﬃcient φ ∈ ϕlf p m bounded simultaneously monotonically increasing tp tn conditions assumption reasonable want elicit unknown bounded monotonically increasing lfpm surprise examples outlined koyejo et al satisfy conditions ﬁrst provide intuition eliciting lfpms figure obtain two hyperplanes one maximizer upper boundary minimizer lower boundary results two nonlinear systems equations soes one degree freedom satisﬁed true unknown metric thus elicited metric one solutions two systems match pointwise confusion matrices formally suppose oracle’s metric φ∗c p∗ t p p∗ q∗ t p q∗ t n t n q∗ let τ∗ τ ∗ maximum minimum value φ∗ c respectively ie τ∗ ≤ φ∗c ≤ τ∗ ∀ c ∈ c assumption hyperplane ∗ f p∗ cid − τ∗q∗ tp p∗ − τ∗q∗ tn τ∗q∗ touching set c t p hyperplane cid ∗ f p∗ upper boundary ∂c similarly tp p∗ tn τ ∗q∗ − τ ∗q∗ ∗ t n ∗ − τ ∗q∗ touches set c t p ∗ t n∗ lower boundary ∂c− help intuition see figure c since lfpm quasiconcave algorithm returns hyperplane ∗ cid mtp mtn c c mt p f equivalent cid constant multiple therefore true metric solution following nonlinear soe mt n ∗ ∗ p∗ − τ∗q∗ αm p∗ − τ∗q∗ αm τ∗q∗ αc α ≥ lhs m’s nonnegative additionally ignore case α since imply constant φ next may divide equations α sides coeﬃcients p∗’s q∗’s factored α change φ∗ thus soe becomes pcid − τ∗qcid m pcid − τ∗qcid m τ∗qcid c notice none conditions assumption changed except pcid however may still use condition learn constant α times true metric harm elicitation problem pcid lfpm also quasiconvex algorithm gives hyperplane cid mtp mtn c f constant multiple thus c mt p ∗ mt n∗ equivalent cid ∗ true metric also solution following soe p∗ − τ ∗q∗ γm p∗ − τ ∗q∗ γm τ ∗q∗ γc γ ≤ since lhs positive m’s negative may assume γ dividing equations −γ sides coeﬃcients p∗’s q∗’s factored −γ change φ∗ thus system equations becomes following pcidcid − τ ∗qcidcid m pcidcid − τ ∗qcidcid m τ ∗qcidcid c proposition assumption knowing pcid solves system equations algorithm grid search best ratio input k ∆ initialize σopt ∞ pcid generate c ck ∂c ∂c− section ≤ pcid pcid opt pcid pcid ∆ compute φcid φcidcid using proposition compute array r φcidc σ stdr σ σopt set σopt σ pcid opt pcid φcidcidc φcidck φcidcidck set end output pcid opt follows qcid − pcid pcid qcid pcid p cid c qcid p cid qcid qcid pcid − m p cid qcid − ζ qcid p cid c − mζ − m − ζ − m p cid pcid ζ pcid now assume know pcid p∗ pcid pcid pcidcid pcidcid using proposition may solve system obtain metric say φcid system can solved analogously provided know pcidcid φ∗c φcidcα get metric say φcidcid notice p∗ −φcidcidcγ means true ratios p’s known φcid φcidcid constant multiples know true pcid search grid select one ratios φcid φcidcid constant number confusion matrices since can generate many confusion matrices ∂c ∂c− vary δ deﬁnition can estimate ratio pcid using grid search based algorithm may use proposition output algorithm set elicited metric ˆφ φcid note algorithm independent oracle queries easy implement thus suitable purpose pcidcid pcid guarantees section discuss guarantees elicitation procedures section presence confusion matrices’ estimation noise ﬁnite samples b oracle feed back noise following notion borrowed deﬁnition deﬁnition oracle feedback noise ω ≥ oracle may provide wrong answers whenever φc − φccid ω otherwise provides correct answers simply put confusion matrices close measured φ oracle responses can wrong moving forward guarantees make two assumptions hold common settings assumption let ˆηixn size assume cidη − ˆηicid∞ p→ sequence estimates ηx depending sample assumption quasiconcave φ recall bayes classiﬁer form h ηx ≥ δ let δ threshold maximizes φ assume probability ηx lies near δ bounded formally kν ≤ pcid pcid cid √kω k ≥ k δ − ηx ∈ ν ν ≤ k cid ηx − δ ∈ ν ≤ kν assumption arguably natural estimation parametric function classes suﬃciently well behaved assumption ensures near optimal threshold δ values ηx bounded density words x point mass slope ηx attains optimal threshold δ neither vertical horizontal start guarantees algorithms respective tasks theorem given  ω ≥ lipschitz metric φ monotonically increasing tp tn quasiconcave quasiconvex algorithm algorithm ﬁnds approximate maximizer c minimizer c furthemore algorithm returns supporting hyperplane point ii value φ point within o√ω  optimum iii number queries olog  lemma model algorithm can ﬁnd maximizer minimizer fewer olog  queries theorem lemma guarantee algorithm algorithm quasi concave quasiconvex metric ﬁnds confusion matrix hypeplane close true maximizer minimizer associated supporting hyperplane using just optimal number queries since binary search always tends towards optimal whenever responses correct algorithms necessarily terminate within conﬁdence interval true maximizer thus can take  suﬃciently small error arises due feedback noise ω now present main result guarantees eﬀective lpm elicitation guarantees lfpm elicitation follow naturally discussed proof theorem appendix table lpm elicitation tolerance  radians φ∗ m∗ ˆφ ˆm φ∗ m∗ ˆφ ˆm theorem let ϕlp m cid φ∗ m∗ true performance metric assump tion given  lpm elicitation section outputs performance metric ˆφ ˆm thatcidm∗ − ˆmcid∞ ≤ √ √kω k far assumed access confusion matrices however practice need estimate using samples xi yin now discuss robustness algorithms working samples recall standard consequence chernoﬀtype bounds sample estimates truepositive truenegative consistent estimators therefore high probability can estimate confusion matrix within desired tolerance provided suﬃcient samples implies can also estimate φ values within tolerance since lpm lfpm lipschitz due assumption respectively thus high probability elicitation procedures gather correct oracle’s preferences within feedback noise ω may prove following lemma allow us control error optimal classiﬁers using estimated ˆηx rather true ηx lemma let hθ ˆhθ two classiﬁers estimated using η ˆη respectively let θ hθ argmaxθ φhθ cidcˆhθ − chθcid∞ ocidˆηn − ηcid∞ errors due using ˆη instead true η may propel results discussed earlier however bounded sense shows elicitation approach robust feedback ﬁnite sample noise experiments section empirically validate theory investigate sensitivity due sample estimates synthetic data experiments assume joint probability x − y given fx u− eax u− uniform distribution − parameter ηx subset results shown please refer appendix extended set results table lfpm elicitation synthetic distribution section magic m dataset section α σ mean standard deviation ˆφφ∗ evaluated subset confusion matrices used algorithm p∗ p∗ true metric q∗ q∗ q∗ ˆp ˆp ˆq ˆq ˆq results synthetic distribution section results real world dataset m section α σ ˆp ˆp ˆq ˆq ˆq α σ table line col b table line col c table line col d table line col figure true solid green elicited dashed blue lfpms synthetic distribution dataset m table solid red coinciding dashed black vertical lines argmax true elicited metric respectively controlling degree noise labels ﬁx experiments verify lpm elicitation ﬁrst deﬁne true metric φ∗ speciﬁes query outputs line algorithm algorithm run lpm elicitation procedure section check whether compute metric results shown table elicit true metrics even  radians next elicit lfpm deﬁne true metric φ∗ p∗ follow lfpm elicitation procedure section algorithms run  algorithm run k ∆ elicited metric q∗ q∗ q∗ p∗ ˆφ denoted ˆp ˆp ˆq ˆq ˆq presented table column also present mean α standard deviation σ ratio elicited metric ˆφ true metric φ∗ subset confusion matrices columns improved comparisons figure shows true elicited metrics evaluated selected pairs t p t n ∈ ∂c metrics plotted together sorting slope parameter θ clearly elicited metric constant multiple true metric also see argmax true elicited metric coincide thus validating theorem realworld data experiments now validate elicitation procedures two realworld datasets datasets breast cancer bc wisconsin diagnostic dataset containing instances b magic m dataset containing instances datasets standardize features split data two parts s s s learn estimator ˆη using regularized logistic regression model use s making predictions computing sample confusion matrices randomly selected twentyeight lpms choosing θ∗ m∗ used algo rithm algortihm diﬀerent tolerance  diﬀerent datasets recovered estimate ˆm using lpm elicitation table appendix report pro portion number times procedure failed recover true m∗ see improved elicitation dataset m suggesting improves larger datasets particular dataset m elicit metrics within threshold  radians also observe  overly tight tolerance datasets leading many failures elicitation routine gets stuck closest achievable confusion matrix ﬁnite samples need optimal within given small tolerance next evaluate lfpm elicitation using dataset m deﬁne true metrics follow lfpm elicitation process deﬁned section table columns present elicitation results along mean α standard deviation σ ratio elicited metric true metric also show true elicited metrics evaluated selected pairs t p t n ∈ ∂c figure ordered parameter θ see elicited metrics equivalent true metrics constant related work work may compared ranking pairwise comparisons however note results depend novel geometric ideas space confusion matrices thus instead ranking problem show standard models can reduced just ﬁnding maximizer minimizer unknown function turn yields true metric – resulting low query complexity direct ranking approach adds unnecessary complexity achieve task contrast approach large margin ordinal regression based ranking fail control samples queried another line work actively controls query samples ranking eg however knowledge requires number objects ﬁnite ﬁnite dimensional – thus directly applied without signiﬁcant modiﬁcations eg exploiting confusion matrix properties learning performance metric correlates human preferences studied however studies learn regression function predeﬁned features fundamentally diﬀerent problem lastly address one might qualitatively choose metrics none addresses central contribution – principled approach eliciting ideal metric user feedback concluding remarks conceptualize metric elicitation binary classiﬁcation setup elicit linear linearfractional metrics using preference feedback pairs classiﬁers propose provably query eﬃcient robust algorithms elicit metrics exploit key geometric properties set confusion matrices associated binary classiﬁcation tasks chapter multiclass classification performance metric elicitation conceptually metric elicitation applicable learning setting however proposed methods previous chapter limited eliciting binary classiﬁcation performance metrics chapter extends previous work proposing strategies complicated multiclass classiﬁcation setting – thus signiﬁcantly increasing use cases similar binary case consider common families performance metrics functions confusion matrix choice measurement space chapter however case elements confusion matrix summarize multiclass error statistics order perform eﬃcient multilcass performance metric elicitation study novel geometric properties space multiclass confusion matrices analysis reveals due structural diﬀerences space binary multiclass confusions can trivially extend elicitation procedure used binary multiclass case instead provide novel strategies eliciting linear functions multiclass confusion matrix extend elicitation complicated yet popular functional forms linearfractional functions confusion matrix elements speciﬁcally elicitation procedures involve binarysearch type algorithms robust ﬁnite sample oracle feedback noise addition proposed methods can applied either querying pairwise classiﬁer preferences pairwise confusion matrix preferences summary main contributions novel query eﬃcient metric elicitation algorithms multiclass classiﬁcation ﬁrst study linear functions confusion matrix discuss extensions complicated functional forms linearfractional arbitrary monotonic functions confusion matrix lastly show pro posed procedures robust ﬁnite sample feedback noise thus useful practice proofs chapter provided appendix b notation matrices vectors denoted bold upper case bold lower case letters respectively recall given matrix oﬀ diaga returns vector oﬀ diagonal elements rowmajor form diaga returns vector diagonal elements cid·cidcid·cid andcid·cid∞ denote cidnorm cidnorm cid∞norm respectively preliminaries standard multiclass classiﬁcation setting comprises k classes x ∈ x y ∈ k representing input output random variables respectively access table bayes optimal bo restrictedbayes optimal rbo entities name bo confusion c subset s ⊆ c rbo classiﬁer hkk rbo diagonal confusion dkk deﬁnition argmax φc c∈s⊆c argmax h∈hkk argmax d∈dkk ψd ψdh generated iid distribution px y let dataset size n denoted x yin ηix py ix x ζi py ∈ k conditional unconditional probability k classes respectively let h h x → ∆k set classiﬁers confusion matrix classiﬁer h denoted ch p ∈ rk×k elements given cijh p py h j j ∈ k population law p useful keep following decomposition mind kcid jjcidi py h ζi − py h cid ⇒ ciih p ζi − cijh p using decomposition confusion matrix uniquely represented q k−k oﬀdiagonal elements hence will represent confusion matrix ch p vector ch p oﬀ diagch p interchangeably refer confusion matrix vector ‘oﬀdiagonal confusions’ space oﬀdiagonal confusions denoted c ch p oﬀ diagch p h ∈ h clarity will suppress dependence p h clear context performance classiﬁer often determined just misclassiﬁcation type misclassiﬁcation especially number classes large therefore will also consider metrics depend correct incorrect predictions namely py h py h cid following decomposition metrics require diagonal elements original confusion matrices given confusion matrix c will denote diagonal d diagc refer vector ‘diagonal confusions’ space diagonal confusions represented d d diagch h ∈ h let φ q → r ψ k → r performance metrics classiﬁer h determined corresponding oﬀdiagonal diagonal confusion entries ch dh respectively without loss generality wlog assume metrics φ ψ utilities larger values preferred furthermore metrics scale invariant global scale aﬀect learning problem chapter assume following regularity assumption data distribution assumption assume functions gijr pcid ηix cid ηj x ≥ r ∀ j ∈ k continuous strictly decreasing r ∈ ∞ intuitively weak assumption ensures cost reward tradeoﬀs classes change preferred confusions tradeoﬀs also change viceversa bayes optimal restricted bayes optimal confusions classiﬁers illustrated table bayes optimal bo confusion c represents optimal value oﬀdiagonal confusions according metric φ subset s ⊆ c analogously deﬁned ψ d restricted bayes optimal rbo entities interest diagonal metrics ψ indicate case classiﬁers ‘restricted’ predict classes k k ∈ k thus hkk dkk denote space classiﬁers exclusively predict either k k associated space diagonal confusions respectively note restricted classiﬁers h ciih dih evaluates zero every index cid k k performance metrics ﬁrst discuss elicitation following two major types metrics deﬁnition diagonal linear performance metric dlpm denote family ϕdlp m given ∈ rk cidacid wlog due scale invariance metric deﬁned also called weighted accuracy focuses correct classiﬁcation ψd cida dcid deﬁnition linear performance metric lpm denote family ϕlp m given ∈ rq thatcidacid wlog due scale invariance metric deﬁned φc cida ccid costsensitive linear metrics belong ϕlp m focus types misclassiﬁcations diﬀerence norms deﬁnitions simplicity exposition chosen best complement underlying metric elicitation algorithm viceversa moreover notice elements diagonal confusions d’s oﬀdiagonal confusions c’s re ﬂect correct incorrect classiﬁcation respectively thus according standard practice wlog focus eliciting monotonically increasing dlpms monotonically decreasing lpms respective arguments metric elicitation problem setup section describes problem metric elicitation associated oracle query deﬁnitions follow chapter extended confusion elements perfor mance metrics correspond multiclass classiﬁcation setting following deﬁnitions hold analogously diagonal case replacing φ c c ψ d d respectively deﬁnition oracle query given two classiﬁers h hcid equivalent oﬀdiagonal con fusions c ccid respectively query oracle metric φ represented γh hcid φ ωc ccid φ φc φccid c cid ccid γ h × h → ω c × c → query asks whether h preferred hcid equivalent c preferred ccid measured φ elicit metrics functions confusion matrix thus comparison queries using classiﬁers indistinguishable comparison queries using confusions henceforth simplicity notation denote query confusions based query next formally state problem deﬁnition metric elicitation pairwise queries given x yin suppose oracle’s unknown performance metric φ using oracle queries form ωˆc ˆccid ˆc ˆccid estimated oﬀdiagonal confusions samples recover metric ˆφ cidφ − ˆφcid κ suitable norm cid · cid suﬃciently small error tolerance κ performance evaluated ﬁdelity recovered metric query complexity given formal deﬁnitions can now proceed standard decision theory literature present solution ﬁrst assuming access population quantities population confusions ch p examine practical implementation considering estimation error ﬁnite samples eg empirical confusions ˆchx yin d v ζ d v ζ dk ζk d v ζ ∂d kk ζk ζk ∂d− k k ∗ cid u ζk dk b cid∗ u c∗ −∇φ∗ sλ λ uk c f∗ c c o c∗ c figure geometry space diagonal confusions d k strictly convex space notice three axisaligned faces equivalent geometry following ﬁgure b b geometry diagonal confusions restricted classiﬁers predicting classes k k ie dkk c sphere sλ centered o radius λ contained convex space oﬀdiagonal confusions c f∗c denotes distance c ∗ hyperplane cid tangent c∗ geometry parametrizations query spaces query based approach important understand structure query space thus ﬁrst study properties query spaces develop parametriza tions required eﬃcient elicitation readers may ﬁnd properties independently useful applications well geometry space diagonal confusions d parametrization boundary let vi ∈ rk ∈ k vectors ζi ith index zero everywhere else notice vi’s diagonal confusions trivial classiﬁers predicting class entire space x proposition geometry d – figure assumption space diagonal confusions d strictly convex closed contained box ζ×···× ζk diagonal confusions vi ∀ ∈ k vertices d moreover k k ∈ k dimensional k k axesaligned face d dkk figure b equivalent space binary classiﬁcation confusion matrices conﬁned classes k k particular dkk strictly convex proposition characterizes geometry space diagonal confusions d fig ure illustrates geometry k interestingly dimensional axesaligned faces d figure b exactly geometry space binary classiﬁcation confusion matrices compare figure recall binary classiﬁcation confusion matrix uniquely determined two diagonal elements due will exploit set dkk speciﬁcally boundary elicitation task now notice ψ ∈ ϕdlp m rbo classiﬁer restricted predict classes k k predicts label two possible choices maximizes expected utility conditioned instance discussed proposition let ψ ∈ ϕdlp m parametrized cidacid let k k ∈ k cid cid hkkx k k akηkx ≥ akηkx ow restricted bayes optimal classiﬁer restricted classes k k respect ψ metric ψ ∈ ϕdlp m proposition provides rbo classiﬁers hkk gives us rbo diagonal confusions dkk using know dkk unique since linear metric strictly convex domain dkk maximized unique point boundary given dlpm access unique point query space allows us deﬁne parametrize subset query space speciﬁcally upper boundary dkk dlpms deﬁnition upper boundary dkk denoted ∂d constitutes rbo diagonal confusions conﬁned classes k k ∈ k monotonically increasing dlpms ai ≥ ∀ ∈ k least one ak ak nonzero ie ak ak kk parameterizing upper boundary ∂d let m ∈ construct dlpm setting ak m ak − m ai cid k k using proposition obtain rbo diagonal confusions deﬁnition lies upper bound ary thus varying m process parametrizes upper boundary ∂d denote parametrization νm k k ν k k → ∂d kk kk kk geometry space c parametrization enclosed sphere recall unlike diagonal case focus eliciting lpms monotonically decreasing elements oﬀdiagonal confusions section end let ui ∈ c ∈ k oﬀdiagonal confusions achieved trivial classiﬁers predicting class entire space x proposition geometry c – figure c space oﬀdiagonal confusions c convex contained box ζk− × ··· × ζkk− uik belong set vertices c c always contains point o ui corresponds oﬀdiagonal confusions trivial classiﬁer randomly predicts class equal probability entire space x k cidk ﬁnd space oﬀdiagonal confusions c quite diﬀerent geometry diagonal case instance c strictly convex nevertheless since c convex always contains point o may make following assumption please see figure c illustration assumption exists qdimensional sphere sλ ⊂ c radius λ centered o sphere always exists long classconditional distributions completely overlapping ie signal nontrivial classiﬁcation method obtain sλ discussed section now recall optimum linear function optimized sphere given slope function scaled radius sphere formalized trivial lemma lemma let φ ∈ ϕlp m parametrized cidacid unique optimal oﬀdiagonal confusion c sphere sλ point boundary sλ given c λa o given lpm lemma provides unique point query space sλ ⊂ c gives us opportunity characterize parametrize subset query space lpms since focus eliciting monotonically decreasing lpms parametrize lower boundary sλ − deﬁnition lower boundary sλ denoted ∂s λ constitutes set optimal oﬀdiagonal confusions sphere sλ lpms ai ≤ ∀ ∈ q monotonically decreasing condition parameterizing lower boundary enclosed sphere ∂s − λ follow standard method parametrizing points surface sphere via angles let θ q − dimensional vector angles angles except primary angle second quadrant ie θi ∈ π πq− primary angle third quadrant ie θq− ∈ π π construct lpm cidacid setting ai πi− j sin θj cos θi ∈ q − aq πq− j sin θj choice quadrants ensures monontonically decreasing condition ie ai ≤ q using lemma obtain bo oﬀdiagonal confusions sphere sλ clearly lies lower boundary thus varying θ − procedure parametrizes lower boundary ∂s λ denote parametrization − µθ µ π πq− × π π → ∂s λ metric elicitation using outlined parametrizations ν µ propose eﬃcient binarysearch type algo rithms elicit oracle’s implicit performance metric will ﬁrst discuss elicitation feedback noise oracle will later show robustness noisy feedback section dlpm elicitation following lemma concerning broader family metrics route elicitation procedures since linear linearfractional functions quasiconcave lemma applies lemma let ψ d → r quasiconcave metric monotone increasing dik upper boundary composition ψ ◦ ρ → r quasiconcave thus k k ∈ k let ρ → ∂d continuous bijective parametrization kk unimodal remark assumption every supporting hyperplane dkk supports unique point boundary ∂d viceversa proposition therefore com position ψ ◦ ρ ﬂat regions words function ψ ◦ ρ concave kk kk proof lemma ﬁrst shows quasiconcave metric ψ deﬁned space d also quasiconcave restricted space dkk shows quasiconcavity thus unimodality due onedimensional parametrization ∂d ψ restricted space ∂d furthermore remark reveals function ψ ◦ ρ concave allowing us devise following binarysearch type method elicitation ∈ ϕdlp m parametrized ∗ cida∗ suppose oracle’s metric ψ∗ cid ∗ ik ≥ section using parametrization ν algorithm returns estimate ˆa ∗ takes two classes time class class since metric unimodal ∂d lemma algorithm applies binarysearch inner whileloop estimate ratio ∗ shrinkinterval subroutine shrinks interval ma mb half based oracle responses usual binarysearch way searching optimum figure b appendix b algorithm repeats k − times estimate ratios ∗ ∗ finally outputs normalized metric estimate ˆa ∗ kk ∗ ka∗ lpm elicitation now discuss lpm elicitation metrics assumed monotonically decreasing oﬀdiagonal confusions unfortunately ∂c may ﬂat regions due algorithm dlpm elicitation input  oracle ω ˆa ··· k whilecidcidmb − macidcid  initialize ma mb d md mamb mamb set mc mamb νma ie parametrization ∂d set d section similarly set c d d d e query ωd d ma mb ← shrinkinterval responses b ωd d ωd e d c d c ωd d d b d e end output ˆa end set md mamb cid ˆacidˆacid set ˆai −md md ˆa cid ··· ˆakcidˆacid lack strict convexity algorithm diagonal case apply instead consider query space given sphere sλ ⊂ c propose coordinatewise binary search style algorithm outcome novel geometric characterization approach derivativefree optimization dfo ∈ ϕlp m parametrized ∗ cida∗ suppose oracle’s metric φ∗ cid − ∗ iq ≤ section using parametrization µθ ∂s λ section algo rithm returns estimate ˆa ∗ iteration algorithm updates one angle θj keeping angles ﬁxed binarysearch procedure shrinkinterval subroutine shrinks interval θa j half based oracle responses figure b appendix b algorithm cyclically updates angle converges metric suﬃciently close true metric convergence assured intuitively algorithm via dual interpretation minimizes smooth strongly convex function f∗c ∗ whose slope given measuring distance boundary points hyperplane cid ∗ tangent bo confusion c∗ see figure c j θb extensions emphasize goal simply choose default popularly used metrics elicit novel metrics best match oracle preferences family human evaluation metrics believed large since already created strategies linear metrics can now certainly aim eﬃcient elicitation ﬂexible metric families therefore section discuss variety extensions family algorithm lpm elicitation input  oracle ω λ θ θ t ··· t set θa θc θd θe θb θt tq − set j tq − else set j q − j π θb j π initialize θa j π θb j π end end j q − else initialize θa cidcidcidθb cidcidcid  j − θa j j θa j θb j θa j θb j θa j θb j θd j set θc set ca µθa ie cc cd ce cb query ωcc ca ωcd cc ωce cd ωcb ce θa θe parametrization ∂s j j ← shrinkinterval responses j set θt θd j θb end j j θb set θd end output ˆai πi− θa j sin θt j cos θi t ∀ ∈ q − ˆaq πq− j sin θt j − λ section similarly set metrics purpose clarity section let us replace notation parametrization νm k k useful disambiguate useful νm k k upper boundary ∂d parametrization ν−m k k lower boundary ∂d − kk kk linearfractional elicitation addition entities deﬁned table deﬁne entities inverse bayes optimal ibo restricted inverse bayes optimal ribo classiﬁers diagonal confusions utility table six deﬁnitions left can analogously described diagonal metrics diagonal confusions six deﬁnitions right interest diagonal case useful elicitation linearfractional metrics lastly linearfractional elicitation need parametrize lower boundary ∂d − kk λ well parametrizations deﬁned upper boundary sphere ∂s deﬁnition rbo diagonal confusions dlpms parametrized ak ak form lower boundary dkk denoted ∂d − kk table bayes optimal bo inverse bayes optimal ibo restricted bayes optimal rbo restricted inverse bayes optimal ribo entities name bo classiﬁer h bo utility τ subset s ⊆ c bo confusion c subset s ⊆ c ibo classiﬁer h ibo utility τ subset s ⊆ c ibo confusion c subset s ⊆ c deﬁnition argmaxh∈h φch name rbo classiﬁer hkk deﬁnition argmaxh∈hkk ψdh maxc∈s⊆c φc rbo utility τ kk maxd∈dkk ψd c∈s⊆c φc argmax rbo confusion dkk argmax d∈dkk ψd argminh∈h φch ribo classiﬁer hkk argminh∈hkk ψdh minc∈s⊆c φc ribo utility τ kk mind∈dkk ψd c∈s⊆c φc argmin ribo confusion dkk argmin d∈dkk ψd − kk parametrization ∂d denote parametrization function ν−m k k take parameter − ≤ m ≤ create dlpm ψ setting ak m ak − − m ai cid k k ∈ k rbo diagonal confusions dlpms lie lower boundary ∂d deﬁnition optimal oﬀdiagonal confusions sphere sλ lpms parametrized ai ≥ ∀ ∈ k form upper boundary sλ denoted ∂s λ vary m move lower boundary ∂d − kk − kk parametrization ∂s lower boundary ∂s− ﬁrst quadrant ie θi ∈ πq− λ parametrization upper boundary ∂s λ λ section except now angles satisfy condition ai ≥ ∀ ∈ k diagonal linear fractional performance metric dlfpm elicitation start ﬁrst deﬁning diagonal linear fractional performance metric deﬁnition diagonal linearfractional performance metric dlfpm denote family ϕdlf p m given b ∈ rk b ∈ r metric deﬁned ψd cida dcid cidb dcid b ψ ∈ ϕdlf p m assume aik ibik wlog take ψd ∈ monotonically increasing dik following regularity assumption zero simultaneously also make ai ≥ ai ≥ bi ∈ k addition b cid assumption let ψ ∈ ϕdlf p m parametrized b deﬁnition assume iai − biζi andcid ai equivalent ﬁxing cidacid ai ≥ diagonal linear case section conditions assumption suﬃcient conditions dlfpms bounded monotonically increasing diagonal elements confusion matrices detailed following proposition consider b cid proposition conditions assumption suﬃcient ψ ∈ ϕdlf p m bounded simultaneously monotonically increasing dik iai − biζi instead derived condition b ≥ iai − biζi suﬃcient guarantee unique metric bounded elicitation purposes instead one equivalent alternatives note existing linearfractional metrics satisfy conditions cid ∈ ϕdlf p m let τ∗ τ ∗ maximum now suppose oracle’s metric ψ∗ minimum value ψ∗ respectively due strict convexity d hyperplane ∗ f cid − τ∗b∗ ∗ d∗ τ∗b kcid kcid tangent bo diagonal confusions d similarly hyperplane ∗ upper boundary d denoted ∂d cid ∗ f − τ ∗b∗ ∗ d∗ τ ∗b touches set d d∗ ibo diagonal confusions lower boundary denoted − see figure c visual intuition assume underlying space ∂d d instead sphere sλ since dlfpm quasiconcave algorithm returns slope hyperplane say s using slope can compute bayes optimal diagonal confusions d using proposition b general version proposition gives us hyperplane ∗ ∗ cid equivalent cid f constant multiple therefore true cid metric solution following nonlinear system equations soe cids dcid cids d ∗ ∗ ∗ − τ∗b∗ αsi ∀ ∈ k τ∗b∗ αcids d ∗ cid α ≥ lhs si’s nonnegative somehow know true ∗ algorithm diagonal quasiconcave metric minimization follow algorithm except initialize ma − mb step algorithm invert responses replace oracle responses d ≺ dcid d cid dcid vice versa using following proposition can elicit dlfpm upto constant multiple ie can get ˆψ ≈ αψ∗ suﬃcient elicitation task proposition knowing ∗ ie using ˆa ∗ solves soes λ cid cid cid ∗ ˆbi ˆai − si λ λ ˆaiζi λ cids d iˆai − siζi ˆb deﬁned assumption now question get true ∗ rescue also know dlfpm quasiconvex thus minimizing metric using restricted classiﬁers using − algorithm described next can get similar hyperplane lower boundary ∂d algorithm described algorithm minimizing diagonal quasiconvex metrics algorithm algorithm two changes first start m ∈ − optimum − second check d ≺ dcid whenever algorithm will lie lower boundary ∂d checks d cid dcid viceversa output counterpart ie slope s get slope s can obtain inverse bayes diagonal confusion d∗ using proposition b general version proposition will result supporting − f constant multiple thus true metric also solution cid hyperplane tangent lower boundary ∂d hyperplane cid ∗ cid s dcid cid s d∗ equivalent cid ∗ following soe ∗ − τ ∗b∗ γ s ∀ ∈ k τ ∗b∗ γcid s d∗ cid γ ≤ since lhs positive s ’s negative may assume γ dividing equations −γ sides coeﬃcients factored −γ change ψ∗ thus system equations becomes following acidcid − τ ∗bcidcid s ∀ ∈ k τ ∗bcidcid cid s d∗ cid now know acid b using proposition may solve system b obtain metric say ψcid system can solved analogously provided know algorithm dlfpm grid search best pairwise ratios input ncid δ j ··· k initialize σopt ∞ acid sample d dncid j ≤ acid acid j j acid j δ compute ψcid ψcidcid using proposition ψcidcidd ψciddncid compute array r ψcidd ψcidciddncid jopt acid σ σopt set σopt σ acid j set σ stdr j acid ∂dj bo ibo diagonal confusions random ncid dlpms acid −acid end set acid j end acid output acid jopt cid ··· acid kcidacidcid jopt cid acid cidacidcid iacid acidcid j acid j acidcid acidcid get metric say ψcidcid notice true ratio ie j j ∈ k ψ∗ ψcidα −ψcidcidγ means ∗ ∗ true ratios known ψcid ψcidcid constant multiples look ratios solution two systems just pointwise constant multiple one another idea used binary case see section however search entire grid k instead binary case computationally challenging task notice can randomly sample diagonal confusions boundary ∂d done ﬁrst randomly generating dlpms computing bo ibo diagonal ∗ cid ∗ run grid seacrh based confusions using proposition b obtaining cid algorithm ﬁnd estimates true ai’s although gridsearch based algorithm independent oracle queries computationally eﬃcient runs k − rounds round matches solution two soe’s closely possible number samples boundary ∂dk ﬁgures ratio aja j cid ∈ k ai access restricted diagonal confusions thanks propertycid saved searching entire grid k merely k − times gridsearch lfpm elicitation start deﬁning linearfractional performance metric oﬀdiagonal confusions deﬁnition linearfractional performance metric lfpm denote family ϕlf p m given constants b ∈ rq b ∈ r metric deﬁned φc cida ccid cidb ccid b φ ∈ ϕlf p m deﬁnition assume aiq zero simultaneously moroever wlog φc ∈ − ∀ c ∈ c monotonically decreasing ciq assumption let φ ∈ ϕlf p m deﬁnition assume ai ≤ ai ≤ −bi similar diagonal case make following regularity assumption ∈ q addition b cid −ai biζi andcid ai − ibiq equivalent ﬁxing cidacid ai ≥ diagonal linear case section conditions assumption suﬃcient conditions lfpms bounded mono tonically decreasing oﬀdiagonal elements confusion matrices detailed following proposition consider b cid proposition assumption suﬃcient φ ∈ ϕlf p m bounded − simultaneously monotonically decreasing ciq −ai biζi instead derived condition b ≥ −ai biζi suﬃcient guarantee unique metric bounded − elicitation purposes instead one equivalent alternatives note existing linearfractional metrics satisfy conditions ∈ ϕlf p m let τ∗ τ ∗ maximum now suppose oracle’s metric φ∗ minimum value φ∗ respectively due strict convexity sλ hyperplane cid qcid qcid ∗ f cid ¯c∗ − τ∗b∗ ∗ τ∗b touching set sλ bo confusions c∗ sphere sλ lower boundary − λ similarly hyperplane ∂s cid ∗ f ∗ − τ ∗b∗ c∗ τ ∗b touches set sλ inverse bayes optimal confusions c∗ sphere sλ upper boundary ∂s cids ccid cids c∗ use strict convexity sλ follow arguments dlfpm get cid using algortihm c∗ optimal best bo λ see figure c visual intuition ∗ hyerplane cid algorithm general quasiconcave metric minimization follow algorithm except initialize θa j θb invert responses replace oracle responses c ≺ ccid c cid ccid vice versa j π steps algorithm oﬀdiagonal confusion sphere diﬀerence bo confusions lie lower boundary ∂s − λ monotonically decreasing soe get − τ∗b∗ ∗ αsi ∀ ∈ q αcids c∗ τ∗b∗ cid α ≥ similar dlfpms knowing ∗ can elicit lfpm upto constant multiple proposition knowing ∗ ie using ˆa ∗ solves soes ˆbi ˆai − si λcid λcid cid cid cid λcid ˆaiζi λcid cids c∗ − iˆai − siζi ˆb deﬁned assumption now question get true ∗ rescue also know lfpm quasiconvex thus minimizing metric using algorithm described cid tangent upper boundary next can get similar hyperplane cid ∗ cid s ccid cid s c∗ ∂s λ algorithm minimizing quasiconvex metrics oﬀdiagonal confusions algo rithm algorithm two changes first start θ ∈ πq λ second check c ≺ ccid optimum will lie upper boundary ∂s whenever algorithm checks c cid ccid vice versa output counterpart ie slope s thus similar soe whose solution looks like proposition obtained ∗ cid ∗ run gridsearch algorithm ﬁnd estimates true obtaining cid ai’s algebra related lfpm elicitation dlfpm case however time need search q− grid easy access oﬀdiagonal confusions sphere ∂sλ corresponding bo ibo oﬀdiagonal confusions diﬀerent lpms lemma therefore can use following algorithm analogous algorithm algorithm lfpm gridsearch best pairwise ratios algorithm except following two changes first second line algorithm will loop algorithm lfpm gridsearch best pairwise ratios follow algorithm except run loop step algorithm q − generate samples ∂sλ running q − second line samples will generated surface sphere ∂sλ discussed instead ∂dk monotonic metrics diagonal confusions recall space d strictly convex suppose oracle’s metric ψ∗ let ∗ slope supporting hyperplane just monotonic increasing dik optimal diagonal confusions d∗ may use algorithm will return linear metric ˆa using pairwise comparisons notice may compute estimate bo diagonal confusions ˆd using proposition b corresponding output ˆa algorithm since space d strictly convex cidˆa dcid cidˆa ˆdcid becomes estimate unique supporting hyperplane ˆd ﬁrst order approximation ψ∗ ˆd can given ψ∗d ψ∗ˆd cidˆa d − ˆdcid since performance metrics aﬀected scale additive biases ﬁrst order approximation given cidˆa dcid suﬃces elicitation task notice high practical importance practitioners since estimate weighted accuracy estimate optimal diagonal confusions guarantees discuss robustness following feedback model useful practical scenarios borrowed deﬁnition deﬁnition oracle feedback noise ω ≥ oracle responds correctly long φc − φccid ω analogously ψd − ψdcid ω otherwise may provide incorrect answers words oracle may respond incorrectly confusions close measured metric φ analogously ψ next discuss elicitation guarantees dlpm lpm elicitation theorem given  ω ≥ lipschitz dlpm ψ∗ parametrized ∗ output ˆa algorithm ok − log − ˆacid∞ ≤ − ˆacid ≤ o√k√ω using standard norm bounds o√ω equivalent cida∗ cid queries theorem given  ω ≥ lipschitz lpm φ∗ parametrized ∗ suppose − ˆacid ≤ o√q cidωλ z z constants independent  q satisﬁes cida∗ λ cid ω output ˆa algorithm ocidz logzqq − log π next guarantee lpm elicitation sphere radius dominates oracle noise  queries oracle satisﬁes cida∗  see algorithms robust noise query complexity depends linearly unknown entities term z logzq may attribute number cycles algorithm due curvature sphere observe dominating factor query complexity instance ﬁnd  − two cycles ie t q− algorithm suﬃcient achieving elicitation error tolerance √q moreover query complexity theorem optimal show chapter quadratic elicitation case turn applies linear elicitation case well one remaining question lpm elicitation select suﬃciently large value λ algorithm b appendix b provides oﬄine procedure compute λ ≥ ˜rk ˜r radius largest ball contained set c finite samples ﬁnal step consider following questions working ﬁnite samples get correct feedback querying ωˆc ˆccid instead querying ωc ccid b eﬀect ˆηi’s used place true ηi’s answers straightforward since sample estimates confusion matrices consistent estimators metrics discussed lipschitz respect confusion matrices high probability gather correct oracle feedback long suﬃcient samples furthermore subject regularity assumptions lemma shows errors due using ˆη aﬀect binary confusion matrices boundary controlled manner since algorithm uses pairwise rbo binary classiﬁers inherits error guarantees multiclass case hand since algorithm use boundary results agnostic ﬁnite sample error long sphere contained within c experiments section empirically validate results theorems investigate sensitivity due ﬁnite sample estimates ease judgments show results k k classes subset results shown refer appendix b results table dlpm elicitation  synthetic data number queries used k k respectively classes k ψ∗ ∗ ˆψ ˆa classes k ψ∗ ∗ ˆψ ˆa table lpm elicitation  synthetic data number queries used k k respectively classes φ∗ ∗ ˆφ ˆa synthetic data experiments assume joint distribution x − y k given marginal distribution fx u− ηix epix ∈ k u− uniform distribution − pik parameters controlling degree noise labels ﬁx p p p p p p p experiments three four classes respectively verify elicitation ﬁrst deﬁne true metric ψ∗ φ∗ speciﬁes query outputs algorithm algorithm run algorithms check whether recover metric results shown table table results verify elicit true metrics even small  predicted requires k − cidlogcid tcidlogπcid queries dlpm lpm elicitation respectively cid·cid ceil function t q − realworld data experiments finite samples may aﬀect size sphere sλ lpm elicitation observe long λ greater ω lpms can elicited appendix b thus emprically validate dlpm elicitation ﬁnite samples consider two realworld datasets sensit acoustic dataset instances classes b vehicle dataset instances classes dataset create two datasets containing randomly chosen datapoints six datasets show proportion times estimates ˆa obtained k − cidlogcid queries figure dlpm elicitation real data  randomly chosen hundred ∗ satisfy cida∗ − ˆacid∞ ≤ ω total datasets standardize features split dataset two parts s s s learn ˆηixk using regularized softmax regression model use s making predictions computing sample confusions randomly selected dlpms ie ∗’s used algorithm  recover estimates ˆa’s figure show proportion times cida∗ − ˆacid∞ ≤ ω diﬀerent values ω see improved elicitation increase number datapoints datasets suggesting improves larger datasets particular full sensit acoustic dataset elicit metrics within ω also observe ω ∈ overly tight evaluation criterion can result failures elicitation routine gets stuck closest achievable sample confusions need optimal within small search tolerance  discussion future work • practical convenience procedures can also applied posing pairwise classiﬁer comparisons directly one way use ab testing user population acts oracle another way use comparisons single expert perhaps combined interpretable machine learning techniques suggest approach proposed narasimhan estimating classiﬁer associated given confusion matrix • advantage algorithm reason restrict metric search dlpm eg due prior knowledge algorithm preferred lower query complexity • future work plan extend procedures oracles probably correct can done easily applying majority voting repeated queries related work closest line work chapter simpler setting binary classiﬁcation chapter move multiclass performance ﬁnd form successful proportionme sensit acoustic dataset data data datasuccessful proportionme vehicle dataset data data data metrics complexity query space increases results stark diﬀerences elicitation algorithms algorithm closest binary approach works restricted bayes optimal classiﬁers algorithm requires coordinate wise binarysearch approach result novel methods also required provide query complexity guarantees lpm elicitation problem can posed derivativefree optimization certain extent exploiting geometry addition passively learning linear functions using pairwise comparisons studied approaches fail control sample ie query complexity end utilizing queries active approaches papers actively control query samples linear elicitation eg exploit query space like us order achieve lower query complexity however unlike us provide theoretical bounds also applied diﬀerent query space concluding remarks study space multiclass confusions propose eﬃcient algorithms elicit diagonallinear linear performance metrics theoretically show procedures robust feedback ﬁnite sample noise validate latter empirically via simulated oracles extend elicitation families eg linearfractional metrics thus covering wide range metrics encountered practice chapter fair performance metric elicitation machine learning models increasingly employed critical decisionmaking tasks hiring sentencing yet increasingly evident automated decisionmaking susceptible bias whereby decisions made algorithm unfair certain subgroups end wide variety group fairness metrics proposed – reduce discrimination bias automated decision making however dearth formal principles selecting appropriate metric highlighted confusion experts practitioners end users deciding group fairness metric employ exacerbated observation common metrics often lead contradictory outcomes problem selecting appropriate fairness metric gained prominence recent years perhaps best understood special case task choosing evaluation metrics machine learning instance costsensitive predictive model classiﬁes patients cancer categories even without considering fairness often unclear costtradeoﬀs chosen reﬂect expert’s decisionmaking ie replacing expert intuition quantiﬁable metrics proposed metric elicitation framework provides solution existing research suggests fundamental tradeoﬀ algorithmic fairness per formance addition appropriate metrics practitioner policymaker must choose tradeoﬀ operating point competing objectives end chapter extend framework eliciting multiclass classi ﬁcation metrics task eliciting fair performance metrics pairwise preference feedback presence multiple sensitive groups particular elicit metrics reﬂect jointly predictive performance evaluated weighting classiﬁer’s overall predictive rates ii fairness violation assessed discrepancy predictive rates among groups iii tradeoﬀ predictive performance fairness violation im portantly elicited metrics suﬃciently ﬂexible encapsulate generalize many existing predictive performance fairness violation measures eliciting groupfair performance metrics tackle three new challenges first preference query perspective predictive performance fairness violations corre lated thus increasing complexity joint elicitation second ﬁnd order measure positive negative violations fair metrics necessarily nonlinear functions predictive rates thus existing results linear previous chapters applied directly finally show number groups directly impacts query complexity overcome challenges proposing novel query eﬃcient procedure exploits geometric properties set predictive rates contributions consider metrics algorithmically groupfair classiﬁcation propose novel approach eliciting predictive performance fairness violations tradeoﬀ point expert pairwise feedback procedure uses binarysearch based subroutines recovers metric linear query complexity moreover procedure robust ﬁnite sample oracle feedback noise thus useful practice lastly method can applied either querying preferences classiﬁers predictive rates choice measurements classiﬁer statistics chapter proofs chapter provided appendix c notations matrices vectors denoted bold upper case bold lower case letters respectively group membership denoted superscripts coordinates vectors matrices tuples denoted subscripts background standard multiclass multigroup classiﬁcation setting comprises k classes m groups x ∈ x g ∈ m y ∈ k representing input group membership output random variables respectively groups assumed disjoint known apriori access dataset x g yin size n generated iid distribution px g y measurements classiﬁer statistics choose work chapter groupspeciﬁc rates overall rates described groupspeciﬁc rates consider separate randomized classiﬁers hg x → ∆k group g use denote set classiﬁers group g groupspeciﬁc rate matrix rghg p ∈ rk×k classiﬁer hg given hg hg x → ∆k ijhg p phg jy g g rg j ∈ k notice predictive rates satisfy following useful decomposition cidk iihg p − rg ijhg p rg jjcidi rate matrix uniquely represented q k − k oﬀdiagonal elements vector rghg p oﬀ diagrghg p will interchangeably refer rate matrix ‘vector rates’ feasible set rates associated group g denoted rg rghg p hg ∈ hg clarity will suppress dependence p hg clear context overall rates deﬁne overall classiﬁer h x m → ∆k hx g hgx denote tuple groupspeciﬁc rates rm r rm ∈ r × ··· × rm rm tuple allows us measure fairness violation across groups fairness violation believed tradeoﬀ predictive performance latter measured using overall rate matrix classiﬁer h rij ph jy rg tg ij pg gy prevalence group g within class overall tg classiﬁer h ‘vector rates’ r oﬀ diagr can conveniently written terms groupspeciﬁc tuple rates cidm g mcid τ g oﬀ diagtg tg tg g r τ g cid rg fairness violation measure approximate fairness classiﬁer often determined ‘discrepancy’ rates across diﬀerent groups eg equalized odds given two groups u v ∈ m deﬁne discrepancy rates cid since m groups number discrepancy vectors arecidm duv ru − rv fair performance metric aim elicit general class metrics recovers generalizes existing fairness measures based tradeoﬀ predictive performance fairness violation let φ q → r cost overall misclassiﬁcation aka predictive performance ϕ m×q → r fairness violation cost classiﬁer h determined overall rates rh group discrepancies duvhm uvvu respectively without loss generality wlog assume metrics φ ϕ costs moreover metrics scale invariant global scale aﬀect learning problem hence let φ q → ϕ m×q → deﬁnition fair performance metric let φ ϕ monotonically increasing linear functions overall rates group discrepancies respectively fair metric ψ trade oﬀ φ ϕ particular given ∈ rq ≥ misclassiﬁcation weights set vectors b buv ∈ rq buv ≥ m uvvu fairness violation weights scalar λ cidm uvvu cidbuvcid ≤ λ ≤ cidacid tradeoﬀ wlog due scale invariance deﬁne metric ψ cid cidcid cid ψrm b λ − λ tradeoﬀ cidcidcidcid cida rcid φr λ cidcidm cid cid cid uvvucidbuv duvcid cidcid ϕrm − r examples misclassiﬁcation cost φr include costsensitive linear metrics many existing fairness metrics two classes two groups equal opportunity balance negative class errorrate balance ie r weighted equalized odds ie br etc correspond fairness violations form ϕrm considered combination φr ϕrm deﬁned ψrm appears regularly prior work notice metric ﬂexible allow diﬀerent fairness violation costs diﬀerent pairs groups thus capable enabling reverse discrimination lastly metric linear respect wrt dis crepancies nonlinear wrt groupwise rates hence standard linear algorithm chapters trivially applied eliciting metric deﬁnition r br − r − r − r fair performance metric elicitation problem statement now state problem fair performance metric elicitation fpme deﬁne associated oracle query broad deﬁnitions follow chapter extended predictive rates classiﬁer statistics performance metrics correspond multiclass multigroupfair classiﬁcation setting deﬁnition oracle query given two classiﬁers h h equivalent tuple rates rm respectively query oracle metric ψ represented rm γh h ψ ωcidrm rm ψcid ψrm ψrm γ h × h → ω rm × rm → simple words query asks whether h preferred h equivalent whether rm measured ψ preferred rm practice oracle can expert group experts entire user population framework can applied posing classiﬁer comparisons directly via interpretable learning techniques via ab testing example one may perform ab testing internetbased application deploying two classiﬁers b use population’s level engagement decide preference two classiﬁers applications intuitive visualizations predictive rates two diﬀerent classiﬁers see eg can used ask preference feedback group domain experts emphasize metric ψ used oracle unknown us can accessed queries oracle since metrics consider functions rates comparing two classiﬁers metric equivalent comparing corresponding rates henceforth will denote query oracle pair rates rm also whenever refer oracles’s dimension referring dimension rate arguments instance will consider oracle deﬁnition dimension m × q next formally state fpme problem deﬁnition fair performance metric elicitation pairwise comparison queries given x g yin suppose oracle’s unknown performance metric ψ using oracle queries form ωˆrm estimated rates samples recover metric ˆψ cidψ − ˆψcid ω suitable norm cid · cid suﬃciently small ˆrm rm ˆrm ˆrm error tolerance ω similar standard metric elicitation problems chapters performance fpme evaluated ﬁdelity recovered metric query com plexity done decision theory literature present fpme solution ﬁrst assuming access population quantities population rates rmh p discuss elicitation can performed ﬁnite samples eg empirical rates ˆrmhx g yin linear performance metric elicitation – warmup revisit linear performance metric elicitation lpme procedure chapter will use subroutine elicit fair performance metrics lpme procedure assumes enclosed sphere s ⊂ z z qdimensional space classiﬁer statistics also assumes access q feasible ie can achieved classiﬁer dimensional oracle ωcid whose scale invariant linear metric form ξz cida zcid cidacid analogous misclassiﬁcation cost deﬁnition analogously oracle queries type ωcidz z ξz ξz number classes k lpme elicits coeﬃcients using simple one dimensional binary search k lpme performs binary search coordinate keeping others ﬁxed performs coordinatewise fashion conver gence restricting coordinatewise binary search procedure posing queries within sphere s lpme can equivalently seen minimizing stronglyconvex func tion shown converge solution ˆa close speciﬁcally algorithm takes query space s ⊂ z binarysearch tolerance  oracle ωcid input querying oq log queries recovers ˆa cidˆacid cida − ˆacid ≤ o√q theorem chapter please see details lpme procedure algorithm chapter completeness summarize discussion following remark remark given qdimensional space z enclosing sphere s ⊂ z oracle ωcid linear metric ξz cida zcid lpme algorithm algorithm chapter provides estimate ˆa cidˆacid estimated slope close true slope ie aiaj ≈ ˆaiˆaj ∀ j ∈ q note algorithm estimates direction coeﬃcient vector magnitude geometry product set rm lpme procedure described works rate queries dimension q like use procedure elicit fair metrics deﬁnition deﬁned tuples dimension m× q make use lpme restrict queries qdimensional sphere s common feasible rate region rg group g ie sphere intersection r ∩ ∩ rm show now sphere indeed exist mild assumption assumption groups conditionalclass distributions identical ie ∀ g ∈ m∀ cid j py ix g g cid py jx g g words nontrivial signal classiﬁcation group s  o e ek sρ rm r r e figure r × ··· × rm best seen colors ru ∀ u ∈ m convex sets common vertices ei ∀ ∈ k enclose sphere sρ let ei ∈ q rate proﬁle trivial classiﬁer predicts class inputs note trivial classiﬁers evaluate rates ei irrespective group apply proposition geometry rm figure group g ∈ m set confusion rates rg convex bounded q vertices eik intersection group rate sets r ∩ ··· ∩ rm convex always contains rate o ei interior associated uniform random classiﬁer predicts class equal probability cidk k since r ∩ ··· ∩ rm convex always contains point o interior can make following remark see figure illustration remark existence common sphere sρ exists qdimensional sphere sρ ⊂ r ∩ ··· ∩ rm nonzero radius ρ centered o thus rate s ∈ sρ feasible groups ie s achievable classiﬁer hg groups g ∈ m method obtain sρ suitable radius ρ chapter discussed appendix c remark observe tuple group rates rm s sm chosen sρ × × sρ achievable choice groupspeciﬁc classiﬁers h hm moreover two groups u v assigned rate proﬁle s ∈ sρ fairness discrepancy duv will exploit observations elicitation strategy discuss next metric elicitation access oracle whose unknown metric ψ given deﬁnition param eterized b λ proposed fpme framework eliciting oracle’s metric presented figure summarized algorithm figure workﬂow fpme procedure procedure three parts executed sequence eliciting misclassiﬁcation cost φr ie b eliciting fairness violation ϕrm ie b c eliciting tradeoﬀ misclassiﬁcation cost fairness violation ie λ simplicity will suppress coeﬃcients b λ notation ψ whenever clear context notice metric ψ piecewise linear coeﬃcients high level idea restrict queries pose oracle lie within regions metric ψ linear can employ lpme subroutine elicit corresponding linear coeﬃcients will show three components –c can identify regions query space metric linear apply lpme procedure variant restricting query inputs regions will essentially converting m × qdimensional oracle ω deﬁnition equivalent qdimensional oracle compares rates s s common sphere sρ ⊂ r ∩ ··· ∩ rm ﬁrst discuss approach assuming oracle feedback noise later section show approach robust noisy feedback provide query complexity guarantees  search tolerance  oracle ω algorithm fpm elicitation input query spaces sρ s ˆa ← lpmesρ  ωclass m ˘f ←lpmesρ  ωviol ˜f ←lpmesρ  ωviol ˆb ← normalized solution let l ← ∅ σ ∈ m ˘f σ ←lpmesρ  ωviol σ ˜f σ ←lpmesρ  ωviol σk let cidσ eq extend l ← l ∪ cidσ ˆb ← normalized solution using l else end end ˆλ ← algorithm s output ˆa ˆb ˆλ   ωtradeoﬀ eliciting misclassiﬁcation cost φr part figure line algorithm elicit misclassiﬁcation cost coeﬃcients will query region query space fairness violation term metric zero speciﬁcally will query group rate proﬁle form rm s s s qdimensional rate common sphere sρ group rate proﬁles metric ψ simply evaluates linear misclassiﬁcation term ie ψs s − λcida scid given pair group rate proﬁles rm s s s s ∈ sρ oracle’s response will essentially compare s s linear metric −λcida scid hence estimate coeﬃcients applying lpme qdimensional sphere sρ modiﬁed oracle ωclass takes pair rate proﬁles s s sρ input responds s s rm ωclasss s ωs s s s decribed line algorithm applies lpme subroutine query space sρ binary search tolerance  oracle ωclass remark subroutine returns coeﬃcient vector f cidfcid ai aj ⇒ − λai − λaj fi fj fi fj setting ˆa f recover classiﬁcation coeﬃcients independent fairness viola tion coeﬃcients tradeoﬀ parameter see part figure illustration eliciting fairness violation ϕrm part figure lines algorithm now discuss eliciting fairness term ϕrm will ﬁrst discuss special case m groups later discuss proposed procedure can extended handle multiple groups special case m lines algorithm recall deﬁnition violation term measure group discrepancies using absolute diﬀerence group rates ie d r − r restrict queries rate proﬁles r diﬀerence coordinate r − r either always positive always negative can treat violation term linear metric within region apply lpme estimate associated coeﬃcients end pose oracle queries form r s ei assign group rate proﬁle s common sphere sρ group rate proﬁle ei ∈ q remember ei rate vector associated trivial classiﬁer predicts class inputs therefore binary vector since know whether entry ei either can decipher signs entry diﬀerence vector s − ei hence group rate proﬁles form metric ψ can written linear function s ψs ei cid − λa cid − τ λwi cid b scid ci wi − ei tells us sign entry s− ei ci constant used fact τ − τ fixing class apply lpme qdimensional sphere sρ modiﬁed oracle ωviol takes pair rate proﬁles s s ∈ sρ input responds ωviol s s ωs ei s ei one run lpme oracle ωviol results q − independent equations order elicit qdimensional vector b must run lpme oracle ωviol described lines algorithm lpme calls provide us two slopes ˘f ˜f cid˘fcid cid˜fcid easy obtain fairness violation weights cid δ˘f − ˆa cid − τ cid ˆb ˜b cid˜bcid ˜b w cid δ scalar depending known entities τ ˆa ˘f ˜f derivation provided appendix c completeness ϕ scale invariant see deﬁnition normalized solution ˆb independent true tradeoﬀ λ depends previously elicited vector ˆa general case m lines algorithm brieﬂy outline elicitation procedure m groups details appendix c let m set subsets m groups element σ ∈ m m σ partition set m groups will later discuss choose m eﬃcient elicitation similar twogroup case pose queries rm subset groups σ ∈ m assign trivial rate vector ei rest m σ groups assign point s common sphere sρ observe within query region metric ψ linear inputs ﬁxed partitioning groups deﬁned σ apply lpme query space sρ using modiﬁed qdimensional oracle ωviol σi s s ωrm rm rg g ∈ σ ow rg g ∈ σ ow ei s ei s cid described lines algorithm repeat twice ﬁxing class uv k guarantees lpme give us following relationship coeﬃcients b wish elicit already elicited coeﬃcient ˆa u v ∩ σ cid ˜buv w cid cid τ σ cid cid coeﬃcients repeat procedure forcidm since need estimatecidm groups deﬁned σ get system cidm m sizecidm cid partitions cid linear equations may choose cid equations independent solution equations − λ scaled version true unknown b recover ˜buv’s normalize get estimates ﬁnal fairness violation δσ˘f σ − ˆa cid − τ σ g∈σ τ g ˜buv λb cid cid uv uv uv weights ˆbuv cidm uvvu cid˜buvcid ˜buv u v ∈ m v u normalization elicited fairness weights independent tradeoﬀ λ eliciting tradeoﬀ λ part figure line algorithm equipped estimates misclassiﬁcation fairness violation coeﬃcients ˆa ˆb ﬁnal step elicit tradeoﬀ λ now show can posed onedimensional binary search problem suppose restrict queries form rm s o o ﬁrst group assign rate o associated uniform random classiﬁer ﬁrst group assign rate s s ≥ o rate proﬁles group rate diﬀerence terms r − rv s − o ≥ v ∈ m diﬀerence terms result metric ψ linear input rate proﬁles ψs o o cid − λτ cid λ v b scid c v cidm c constant despite metric linear identiﬁed input region directly apply lpme procedure described section elicit λ one parameter elicit input metric qdimensional propose slight variant lpme similar original procedure binary classiﬁcation setup chapter ﬁrst construct onedimensional function ϑ takes guess tradeoﬀ parameter input outputs quality guess show function unimodal mode coincides oracle’s true tradeoﬀ parameter λ lemma let s o ∀ s ∈ s condition cidˆacidm  ⊂ sρ qdimensional sphere radius  ρ s ≥  see figure assume estimates ˆa ˆbuv’s satisfy mild regularity ˆbvcid cid deﬁne onedimensional function ϑ v s∗ ¯λ argmax s∈s  cid − ¯λτ cid ˆa ¯λ ϑ¯λ ψs∗ ¯λ o o cidm v v ˆb scid function ϑ strictly quasiconcave therefore unimodal ¯λ moreover algorithm eliciting tradeoﬀ λ input query space s initialize λa λb cidcidcidλb − λacidcidcid   binarysearch tolerance  oracle ωtradeoﬀ set λc λaλb set sa argmax s∈s  λd λaλb cid − λaτ cid ˆa λa mcid v λe λaλb ˆbv scid using lemma chapter similarly set sc sd se sb query ωtradeoﬀsc sa ωtradeoﬀsd sc ωtradeoﬀse sd ωtradeoﬀsb se λa λb ← shrinkinterval responses using subroutine analogous routine shown figure b end output ˆλ λaλb mode function achieved oracle’s true tradeoﬀ parameter λ candidate tradeoﬀ ¯λ function ϑ ﬁrst constructs candidate linear metric based maximizes candidate metric inputs s evaluates oracle’s true metric ψ maximizing rate proﬁle note directly compute function ϑ needs oracle’s metric ψ however given two candidates tradeoﬀ parameter ¯λ ¯λ one can compare values ϑ¯λ ϑ¯λ ﬁnding corresponding maximizers s querying oracle compare ϑ unimodal one can use simple binary search using pairwise comparisons ﬁnd mode function know coincides true λ provide outline procedure algorithm uses modiﬁed oracle ωtradeoﬀs s ωs o o s o o compare maximizers description algorithm given unimodality ϑλ lemma devise binarysearch procedure algorithm eliciting true tradeoﬀ λ algorithm takes input query space s  binarysearch tolerance  equivalent oracle ωtradeoﬀ elicited ˆa section elicited ˆb section algorithm ﬁnds maximizer function ˆϑλ deﬁned analogously b replaced ˆa ˆb using lemma chapter algorithm poses four queries oracle shrink interval λa λb half based responses using subroutine analogous shrinkinterval shown figure b algorithm stops length search interval λa λb less tolerance  combining parts figure completes fpme procedure guarantees discuss elicitation guarantees following feedback model deﬁnition oracle feedback noise ω ≥ two rates rm responds correctly long ψrm − ψrm ω otherwise may incorrect rm ∈ rm oracle   v v b • ˆb o • ˆλ olog vec· vectorizes matrix words oracle may respond incorrectly rates close measured metric ψ since deriving ﬁnal metric involves oﬄine computations including certain ratios discuss guarantees regularity assumption ensures components well deﬁned assumption assume c λ c mini ai c mini − λaiτ σ − λwjibσ cid cid theorem given  ω ≥ lipschitz fair performance metric ψ parametrized b λ assumptions algorithm returns metric ˆψ parameters c ∀ j ∈ q σ ∈ m c c c c ρ  cid ω cidacidm cid√q cidωρ cid • ˆa ocidq log cid queries cida − ˆacid ≤ o cid cid cidcidm cid mq cidωρ cidq log queries cidvecb − vec ˆbcid ≤ o cid cid cid mq cidωρ  cidω elicits ˆa ∈ rq posing ˜oq queries ‘’ loop line algorithm runs forcidm cid see proposed fpme procedure robust noise query complexity depends linearly number unknown entities instance line algorithm iterations iteration requires ˜oq queries ﬁnally line algorithm simple binary search requiring ˜o queries work chapter work suggests linear multiclass elicitation lpme elicits misclassiﬁcation costs φ linear query complexity surprisingly proposed fpme procedure elicits complex nonlinear metric without increasing query complexity order furthermore since sample estimates rates consistent estimators metrics discussed lipschitz wrt rates high probability gather correct oracle feedback querying ﬁnite sample estimates ωˆrm long suﬃcient samples apart algorithm agnostic ﬁnite sample errors long sphere sρ contained within feasible region r ∩ ··· ∩ rm instead querying population statistics ωrm  queries error λ − ˆλ ≤ o ˆrm rm b c figure elicitation error recovering oracle’s metric experiments theory validation ﬁrst empirically validate fpme procedure recovery guarantees section recall exists sphere sρ ⊂ r ∩ ··· ∩ rm long nontrivial classiﬁcation signal within group remark thus experiments assume access feasible sphere sρ ρ randomly generate oracle metrics k m ∈ parametrized b λ speciﬁes query outputs oracle metric algorithm use algorithm tolerance  − elicit corresponding metrics parametrized ˆa ˆb ˆλ algorithm makes m subroutine calls lpme procedure call algorithm lpme subroutine requires exactly q − logπ queries use queries shrink interval binary search loop ﬁx cycles coordinatewise search also algorithm requires log queries figure report mean cidnorm oracle’s metric elicited metric clearly elicit metrics close true metrics moreover holds true across range m k values demonstrating robustness proposed approach figure shows error cida − ˆacid increases number classes k groups m expected since ˆa elicited querying rates zero fairness violation section figure b veriﬁes theorem showing cidvecb − vec ˆbcid increases number classes k groups m accord theorem figure c shows elicited tradeoﬀ ˆλ also close true λ however elicitation error increases consistently groups m classes k possible reason may cancellation errors eliciting ˆa ˆb separately table dataset statistics realvalued regressor wine crime datasets recast classes based quantiles dataset default adult wine crime k m samples features groupfeat gender race color race table common baseline metrics usually deployed rank classiﬁers name → ˆφ ˆϕˆλ ˆφ ˆϕˆλ w ˆφ ˆϕ wacc wacc wacc ˆφ ˆϕ w acc wacc acc wacc elicit elicit ˆφ ˆφ w acc wacc elicit elicit elicit elicit o p o f b acc acc ˆa ˆb ˆλ ranking classiﬁers next highlight utility fpme ranking realworld classiﬁers one important applications performance metrics evaluating classiﬁers ie providing quantitative score quality allows us choose best best set classiﬁers section discuss ranking plausible classiﬁers aﬀected practitioner employs default metrics rank fair classiﬁers instead oracle’s metric elicited approximation take four realworld classiﬁcation datasets k m ∈ see table dataset used training rest testing create pool classiﬁers dataset tweaking hyperparameters logistic regression models multilayer perceptron models support vector machines lightgbm models fairness constrained optimization based models compute group wise confusion rates test data model dataset will compare ranking classiﬁers achieved competing baseline metrics respect ground truth ranking generate random oracle metrics ψ ψ’s gives us ground truth ranking classiﬁers use proposed procedure fpme algorithm recover oracle’s metric comparison ranking realworld classiﬁers choose metrics routinely employed practitioners baselines see table preﬁxes ie ˆφ ˆϕ ˆλ name baseline metrics denote components set default metrics suﬃxes ie ‘’ ‘wa’ denote whether assignment done accuracy ie equal weights weighted accuracy weights assigned randomly however maintaining true order weights ψ example ˆφ ˆϕˆλ corresponds figure ranking performance realworld classiﬁers competing metrics metric ˆφ ˆϕ ˆλ set standard classiﬁcation accuracy similarly ˆφ w denote metric misclassiﬁcation cost ˆφ set weighted accuracy ˆϕ ˆλ elicited using part part fpme procedure algorithm respectively assigning weighted accuracy versions commonplace since sometimes order costs associated types mistakes misclassiﬁcation cost φ fairness violation ϕ preference fairness violation misclassiﬁcation λ known actual cost another example ˆφ ˆϕ corresponds metric ˆφ ˆϕ set accuracy tradeoﬀ ˆλ elicited using part fpme procedure algorithm similar prior work zhang et al assumed classiﬁcation error fairness violation known tradeoﬀ elicited – however also assume direct ratio queries can challenging practice approach applies much simnpler pairwise preference queries lastly o p o f represent predictive performance λ fairness λ respectively figure shows average ndcg exponential gain kendalltau coeﬃ cient metrics ψ respective estimates competing baseline met rics see fpme wherein elicit ˆφ ˆϕ ˆλ sequence achieves highest possible ndcg kendalltau coeﬃcient even though make elicitation error recovery section achieve almost perfect results ranking classiﬁers connect practice implies given set classiﬁers ranking based elicited metrics will align closely ranking based true metric compared ranking classiﬁers based default metrics crucial advantage metric elicitation practical purposes experiment baseline metrics achieve inferior ranking classi ﬁers comparison rankings achieved metrics elicited using proposed fpme procedure figure also suggests beneﬁcial elicit three components b λ metric deﬁnition rather predeﬁne component elicit rest crime dataset methods also achieve high ndcg values ranking top good however kendalltau coeﬃcient weak suggests overall ranking poor exception default dataset weighted versions better equally weighted versions ranking expected weighted versions least order preference type costs matches oracle’s preferences related work early attempts eliciting individual fairness metrics distinct – focused prevalent setting group fairness yet existing approaches knowledge zhang et al propose approach elicits tradeoﬀ accuracy fairness using complicated ratio queries hand elicit classiﬁcation cost fairness violation tradeoﬀ together nonlinear function using much simpler pairwise comparison queries prior work constrained classiﬁcation focus learning classiﬁers constraints fairness take regularization view algorithmic fairness fairness violation embedded metric deﬁnition instead constraints elicitation perspective closest line work chapters proposed problem solved simpler setting classiﬁcation without fairness move multiclass multigroup fair performance ﬁnd complexity form metrics query space increases results starkly diﬀerent elicitation strategy novel methods required provide query complexity guarantees learning linear functions passively using pairwise comparisons mature ﬁeld approaches fail control sample ie query complexity active learning fairness related direction however aim learn fair classiﬁer based ﬁxed metric instead eliciting metric concluding remarks future work • transportability elicitation procedure independent population p long exists sphere rates feasible groups thus metric learned using one dataset model class ie estimated ˆp can applied applications datasets long expert believes context tradeoﬀs • extensions propsal can modiﬁed leverage structure metric groups reduce query complexity example fairness violation weights pairs groups procedure section requires one partitioning groups elicit metric ˆϕ modiﬁcations easy incorporate future plan extend approach complex metrics linearfractional functions rates discrepancies • limitations groupfair metrics since metrics consider depend classiﬁer rates comparing two classiﬁers metrics equivalent comparing rates unfortunately setup limitations associated groupfairness deﬁnition metrics apply setup well example may discard notions individual fairness grouprates considered comparing classiﬁers similarly issues associated overlapping groups detailed group speciﬁcation unknown changing groups noisy biased group information among others pose limitations proposed setup hope ﬁrst work topic work will inspire research community address many open problems task metric elicitation • optimal bounds conjecture query complexity bounds tight ever leave detail future conclusion elicit complex nonlinear group fairmetric query complexity order standard clas siﬁcation linear elicitation procedures chapter • limitation work seeks truly democratize personalize fair machine learn ing besides signiﬁcance fair performance metric elicitation lies em powers practitioner tune design machine learning models needs target fairness task however time work may draw backs leaves open key question stakeholders queried work also assumes parametric form oracle metric may exact match practice furthermore cautious result failure system cause disparate impact among sensitive groups elicited metric incorrect eg applied settings stated assumptions met chapter quadratic metric elicitation fairness beyond metric elicitation strategies binary multiclass classiﬁcation setups discussed chapters respectively handle linear quasilinear func tion predictive rates can restrictive many applications metrics complex nonlinear example fair machine learning classiﬁers often judged measuring discrepancies predictive rates diﬀerent protected groups sim ilar discrepancybased measures also used distribution matching applications common measure discrepancy applications squared diﬀerence appealing smoothness properties quadratic metric handled existing approaches similar quadratic metrics also ﬁnd use classimbalanced learn ing see section examples motivated examples paper propose strategies eliciting metrics deﬁned quadratic functions rates encom pass linear metrics special cases extend approach elicit polynomial metrics universal family functions allows one better capture realworld human preferences highlevel idea approximate quadratic metric using multiple linear functions employ linear estimate local slopes combine slope estimates recon struct original metric natural elegant approach comes nontrivial challenges firstly must choose center points locallinear approximations chosen points must represent feasible queries secondly pairwise queries receive slopes directions magnitudes locallinear functions requiring intricate analysis reconstruct original metric deal multiplicative errors result despite challenges method requires query complexity linear number unknown entities show nearoptimal interest quadratic metric elicitation majorly motivated applications fair machine learning several groupbased fairness metrics pro posed capture bias automated decisionmaking selecting right metric remains crucial challenge chapter proposed approach eliciting groupfair metrics measure discrepancies using absolute diﬀerences rates across multiple sensitive groups unfortunately approach speciﬁcally handles metrics linear group discrepancies generalize easily families metrics extend setup allow general fairness metrics deﬁned quadratic functions group discrepancies show proposed quadratic approach can easily adapted elicit metrics like chapter jointly elicit three terms predictive performance deﬁned weighted error metric ii quadratic fairness violation metric iii tradeoﬀ predictive performance fairness violation contributions chapter organization propose novel quadratic metric elici tation algorithm classiﬁcation problems requires pairwise preference feedback either classiﬁers rates section speciﬁc groupbased fairness tasks show jointly elicit predictive fairness metrics tradeoﬀ section proposed approach robust feedback ﬁnite sample noise requires nearoptimal number queries elicitation section empirically vali date proposal multiple classes groups simulated oracles section lastly discuss strategy can generalized elicit higherorder polynomials recur sively applying procedure elicit lowerorder approximations section proofs chapter provided appendix d notation cid · cidf represents frobenius norm αi ∈ rq denotes ith standard basis vector ith coordinate others background consider kclass classiﬁcation setting x ∈ x y ∈ k denoting input output random variables respectively assume access nsized sample x yin generated iid distribution px y work randomized classiﬁers x gives distribution hx k classes use h x → ∆k h h x → ∆k denote set classiﬁers unlike chapter choice measurement space space predictive rates described next just suit application fairness predictive rates two sensitive groups can compared however suitable groupfair application purposes compare confusion matrix entries two sensitive groups nevertheless proposed algorithm quadratic polynomial metric elicitation will also work choice measurement space space confusion matrices predictive rates deﬁne predictive rate matrix classiﬁer h rh p ∈ rk×k ijth entry fraction labeli examples randomized classiﬁer h predicts j rijh p p hx jy j ∈ k probability draw x y ∼ p randomness h notice diagonal entry r can written terms oﬀdiagonal elements cidk riih p − jjcidi rijh p thus can represent rate matrix q k − k oﬀdiagonal elements write vector rh p oﬀ diagrh p interchangeably refer ‘vector rates’ metrics consider metrics deﬁned general function φ q → r includes weighted error rate φerrrh p cid φrh p rates airih p weights ai ∈ r fmeasure many metrics without loss generality wlog treat metrics costs since metric’s scale aﬀect learning problem allow φ q → − feasible rates will restrict attention rates feasible ie can achieved classiﬁer set feasible rates given simplicity will suppress dependence p h clear context r rh p h ∈ h metric elicitation problem setup now describe problem metric elicitation follows chapter ’s unknown metric φ seek elicit form posing queries oracle asking two classiﬁers preferred oracle access metric φ provides answers comparing value two classiﬁers deﬁnition oracle query given two classiﬁers h h equiv rates r r respec tively query oracle metric φ represented γh h φ ωr r φ φr φr γ h × h → ω r × r → query asks whether h preferred h equiv r preferred r measured φ practice oracle can expert group experts entire user population framework can applied posing classiﬁer comparisons directly via interpretable learning techniques via ab testing example internetbased application one may perform ab test deploying two classiﬁers b two diﬀerent subpopulations users use level engagement decide preference two classiﬁers applications one may present visualizations rates two classiﬁers eg user provide preference moreover since metrics consider functions predictive rates queries comparing classiﬁers queries associated rates convenience will algorithms pose queries comparing two feasible rates indeed given feasible rate one can eﬃciently ﬁnd associated classiﬁer see appendix d details next formally state problem deﬁnition metric elicitation pairwise queries given x yin suppose oracle’s unknown performance metric φ using oracle queries form ωˆr ˆr φ ˆr ˆr estimated rates samples recover metric ˆφ cidφ − ˆφcid κ suitable norm cid · cid suﬃciently small error tolerance κ discussed previous chapters performance evaluated query complexity quality elicited metric standard decision theory literature present approach ﬁrst assuming access population quantities population rates rh p examine estimation error ﬁnite samples ie empirical rates ˆrhx yin linear metric elicitation warm overview linear performance metric elicitation lpme procedure chapter will use subroutine assume oracle’s metric linear function rates φlinr cida rcid unknown costs ∈ rq words given two rates r r oracle returns cida rcid cida rcid since metrics scale invariant wlog one may assume cidacid goal elicit slope using pairwise comparisons rates number classes k coeﬃcients can elicited using simple onedimensional binary search k one can apply coordinatewise procedure r e e s sz sz o s−z ek rm r r e e s o ek b figure geometry set predictive rates r convex set enclosing sphere s trivial rates ei ∀ ∈ k vertices b geometry product set group rates r × ··· × rm best seen color enclosing common sphere s ⊂ r ∩ ··· ∩ rm performing binary search one coordinate keeping others ﬁxed eﬃcacy procedure however hinges geometry underlying set feasible rates r discuss ﬁrst make mild assumption ensuring signal nontrivial classiﬁcation assumption conditionalclass distributions distinct ie p y ix cid p y jx ∀ j ∈ k let ei ∈ q denote rates achieved trivial classiﬁer predicts class inputs cidk proposition geometry r figure set rates r ⊆ q convex vertices eik ei interior moreover o achieved classiﬁer input predicts class equal probability contains rate proﬁle o k remark existence sphere s since r convex contains point o interior exists sphere s ⊂ r nonzero radius ρ centered o restricting coordinatewise binary search procedure posing queries within sphere lpme can equivalently seen minimizing stronglyconvex function shown converge solution ˆa close speciﬁcally lpme procedure takes sphere s ⊂ r binarysearch tolerance  oracle ω metric φlin input posing oq log queries recovers coeﬃcients ˆa cida − ˆacid ≤ o√q please see chapter details remark lpme guarantee given qdimensional sphere s ⊂ r oracle ω metric φlinr cida rcid lpme algorithm algorithm chapter provides estimate ˆa cidˆacid estimated slope close true slope ie aiaj ≈ ˆaiˆaj ∀ j ∈ q note algorithm closely tied scale invariance condition thus estimates direction slope coeﬃcient vector magnitude also note algorithm takes input arbitrary sphere s ⊂ r restricts queries appendix d discuss eﬃcient procedure rate vectors within sphere identifying sphere suitable radius quadratic performance metrics equipped lpme subroutine aim elicit metrics quadratic functions rates deﬁnition quadratic metric vector ∈ rq symmetric matrix b ∈ rq×q cidacid f wlog due scale invariance cidbcid φquadr b cida rcid rt br family trivially includes linear metrics well many modern metrics outlined example classimbalanced learning problems imbalanced class proportions common use metrics emphasize equal performance across classes one example qmean quadratic mean rates kcid  k−cid j  φqmeanr k ri−k−j example distribution matching certain applications one needs proportion predictions class ie coverage match target distribution π ∈ ∆k measure often used task squared diﬀerence perclass coverage target distribution kcid cidk− j ri−k−j cid φcovr cid cidcovir − πi ji rj−k− cid ji rj−k−− similar covir − metrics can found quantiﬁcation literature target set class prior py capture general quadratic distance measures distributions eg covr − πtqcovr − π positive semideﬁnite matrix q ∈ p sdk example fairness violation popular criterion groupbased fairness equalized odds requires equal rates across diﬀerent protected groups can mea sured squared diﬀerences group rates m groups rg denoting rate vector evaluated examples group g given cid qcid vu φeor rm ru − rv vuru − rv cid rate balance φebr rm cid quadratic faircriteria two classes include equal opportunity φeoppr rm error etc weighted variants section consider metrics tradeoﬀ error term quadratic fairness term balance negative class φbnr rm ru vuru − rv − rv ru − rv note due scale invariance condition deﬁnition largest singular value b bounded cidbcid ≤ cidbcidf ≤ thus metric φquad smooth implies locally linear around given rate lastly need following assumption metric assumption gradient φ trivial rate o nonzero ie ∇φquadrro bo cid nonzero gradient assumption reasonable convex φquad merely implies optimal classiﬁer metric uniform random classiﬁer quadratic metric elicitation now present procedure quadratic performance metric elicitation qpme assume oracle’s unknown metric quadratic deﬁnition seek estimate parameters b posing queries oracle unlike lpme simple binary search based procedure directly applied elicit parameters approach instead approximates quadratic metric linear function select rate vectors invokes lpme estimate locallinear approximations’ slopes challenge course pick small number feasible rates performing local approximations reconstruct original metric just estimated local slopes local linear approximation will ﬁnd convenient work shifted version quadratic metric centered point o uniform random rate vector see proposition φquadr b cidd r − ocid φr d b c r − ot br − o c d bo c constant independent r oracle can equivalently seen responding shifted metric φr d b let z ﬁxed point r since metric deﬁnition smooth metric can closely approximated ﬁrstorder taylor expansion small neighborhood around z ie φr d b ≈ cidd bz − o rcid ccid constant ccid apply lpme metric φ queries r r oracle restricted small ball around z procedure eﬀectively estimates slope vector d bz − o linear function small approximation error will exploit idea applying lpme small neighborhoods around selected points elicit coeﬃcients b original metric simplicity will assume oracle noisefree later show robustness noise query complexity guarantees section eliciting metric coeﬃcients outline main steps algorithm please see appendix d full derivation estimate coeﬃcients d line ﬁrst wish estimate linear portion d metric φ apply lpme subroutine small ball ⊂ s radius  ρ around point o see figure illustration within ball metric φ approximately equals linear function cidd rcid ccid using lpme gives us estimate slope d remark estimates f f fq fj ←lpmecid algorithm qpm elicitation input s search tolerance  oracle ω metric φ f ← lpmeso  ω ⊂ s obtain j ∈ q end f− ˆa ˆb ← normalized solution dervied output ˆa ˆb szj  ωcid szj ⊂ s obtain ← lpmes−z  ω s−z ⊂ s obtain approximately satisfy following q − equations di d fi f ∀ ∈ q estimate coeﬃcients b lines – next wish estimate column matrix b metric φ apply lpme small neighborhoods around points direction standard basis vectors αj ∈ rq j q note within small ball around o αj metric φ approximately linear function cidd bj rcid ccid lpme procedure applied region will give us estimate slope d bj however ensure center point choose feasible rate will rescale standard basis apply subroutine balls szj radius  ρ centered zj o ρ − αj see figure visual intuition returned estimates fj fj fqj approximately satisfy di ρ − bij d ρ − bj fij fj ∀ ∈ q j ≤ since matrix b symmetric far qq equations now note since eliciting slopes using lpme always lose one degree freedom hence q unknown entities estimate need q − equations beside one normalization condition apply lpme sphere s−z f− radius  around rate −z shown figure returned slopes f− q f− approximately satisfy d − ρ − b d − ρ − b f− f− put together line combining express entry b terms d cid bij fij fj − fijfjd − fi fij f − f−f f − −f cid d ijl f− cidbcid fijl filfjl f − il f− jl using d bo fact coeﬃcients normalized ie cidacid f can obtain estimates b independent d moreover derivation far assumes d cid based assumption states least one coordinate d nonzero ’ve assumed wlog d practice can identify nonzero coordinate using q trivial queries form αi o o∀ ∈ q emphasize key diﬀerence chapters relied boundary point characterization hold general nonlinear metrics instead use structural properties metric estimate locallinear approximations discussed beginning chapter may seem natural idea qpme procedure tackles three key challenges works slopes locallinear functions b ensures center points approximations feasible c handles multiplicative errors slopes see section eliciting quadratic fairness metrics now discuss quadratic metric elicitation algorithmic fairness consider setup chapter goal elicit metric tradesoﬀ predictive performance fairness violation however unlike chapter handle general quadratic fairness violations show qpme can easily employed elicit groupfair metrics fairness preliminaries consider kclass problem comprising m groups use g ∈ m denote group membership groups assumed disjoint ﬁxed known apriori access dataset size n denoted x g yin generated iid distribution px g y case will work separate randomized classiﬁers hg x → ∆k group g use hg hg x → ∆k denote set classiﬁers group g group predictive rates similar denote groupconditional rate matrix classiﬁer hg rghg p ∈ rk×k ijth entry additionally conditioned group given ijhg p phg jy g g ∀ j ∈ k rg analogous general setup section denote group rates vectors rghg p oﬀ diagrghg p set feasible rates group g rg rghg p hg ∈ hg rates overall classiﬁer construct overall classiﬁer h x m → ∆k predicting classiﬁer hg group g ie hx g hgx will interested predictive performance overall classiﬁer fairness violation former will measure overall rate matrix h denoted can also represented rij ph jy rg tg ij cidm g tg will need m groupspeciﬁc rates represented together tuple pg gy prevalence group g within class latter rm r rm ∈ r × ··· × rm rm can expressed terms groupspeciﬁc rates r cidm lastly overall rates can written ﬂattened vector r ∈ q g τ g cid rg τ g oﬀ diagtg tg tg fair quadratic metric elicitation seek elicit metric tradesoﬀ predictive performance deﬁned linear function overall rates r fairness violation deﬁned quadratic function group rates rm deﬁnition fair quadratic performance metric misclassiﬁcation costs ∈ rq ≥ fairness violation costs b buv ∈ p sdqm uvvu tradeoﬀ parameter λ ∈ deﬁne φfairrm b λ − λcida rcid λ ru − rvt buvru − rv vu cid cidcid wlog parameters buv’s normalized cidacid vu cidbuvcidf coeﬃcients buv’s separately normalized predictive performance fairness violation scale can additionally elicit tradeoﬀ parameter λ analogous deﬁnitions – present problem fair quadratic metric elicitation cidm rm deﬁnition fair quadratic metric elicitation pairwise comparison queries given let ω oracle unknown metric φfair given x g yin rm using oracle queries form ωˆrm estimated rates samples recover metric φfairrm ˆrm outputs ωrm rm ˆrm φfairrm ˆφfair ˆa ˆb ˆλ cidφfair − ˆφfaircid κ suitable norm cid · cid suﬃciently small ˆrm error tolerance κ similar section study space feasible rates rm following mild assumption assumption group g ∈ m conditionalclass distributions p y jx g g j ∈ q distinct ie signal nontrivial classiﬁcation group proposition geometry rm figure b group g classiﬁer predicts class inputs results rate vector ei rate space rg group g convex intersection r ∩ ··· ∩ rm also contains rate proﬁle o k ei achieved uniform random classiﬁer interior cidk remark existence sphere s r∩···∩rm exists sphere s ⊂ r∩···∩rm radius ρ centered o thus rate s ∈ s feasible m groups ie s achievable classiﬁer hg group g ∈ m allow separate classiﬁer group remark implies rate rm s sm arbitrary points s sm ∈ s achievable choice group speciﬁc classiﬁers h hm observation will useful elicitation algorithm describe next eliciting metric parameters b λ present strategy eliciting fair metrics deﬁnition adapting qpme algorithm simplicity focus m case extend approach multiple groups appendix d figure eliciting fair quadratic metrics deﬁnition two groups using minor modiﬁcation qpme algorithm observe rate proﬁle r s o ﬁrst group assigned arbitrary point s second group assigned uniform random classiﬁer’s rate o fair metric becomes λ φfairs o b λ − λcida τ cid s τ cid ocid s − ot bs − o cidd s − ocid φs d b s − ot bs − o d − λτ cid b λb use τ τ vector ones second step metric φ particular instance quadratic metric can thus apply slight variant qpme procedure algorithm solve quadratic metric elicitation problem sphere s oracle ωcidr r ωr o r o cid s o s ∈ s modiﬁed change needed algorithm line need account changed relationship d need separately jointly normalize linear quadratic coeﬃcients change output algorithm directly gives us required estimates speciﬁcally step algorithm ˆai normalizing d get ˆa dciddcid linear coeﬃcients similarly steps algorithm gives us ˆdi − λτ cid ˆbij λ ˆb ij fij fj − fijfjd fij f − f−f f − −f − λτ ˆa normalizing directly get estimates ˆb ˆbcid ˆbcidf quadratic coeﬃcients cid finally linear quadratic coeﬃcients separately normalized esti mates ˆa ˆb independent tradeoﬀ parameter λ given estimates ˆb ij ˆa can now additionally estimate tradeoﬀ parameter ˆλ see figure illustration entire procedure proposed approach fair quadratic metric elicitation easily extends multiple groups applying qpme procedure described multiple times ﬁxing one cluster groups rate o remaining rate s intersection sphere s see appendix d details appendix d also provide alternate binary search based method similar chapter eliciting tradeoﬀ parameter λ linear predictive quadratic fairness coeﬃcients already known along similar lines application considered zhang et al unlike instead complicated ratio queries require simpler pairwise queries guarantees discuss guarantees qpme procedure algorithm following feed back model useful practice fair metric elicitation guarantees follow directly consequence deﬁnition oracle feedback noise ω ≥ given rates r r oracle responds correctly iﬀ φquadr − φquadr ω may incorrect otherwise words oracle may respond incorrectly rates close measured metric φquad since eliciting metric involves oﬄine computations including certain ratios discuss guarantees following regularity assumption ensures components well deﬁned assumption shifted quadratic metric φ gradients rate proﬁles o −z z zq nonzero vectors additionally ρ  cid ω theorem given  ω ≥ lipschitz metric φquad deﬁnition parametrized b assumptions ocidq log cid cid q√q cid ω metric ˆφquad ˆa ˆb cida − ˆacid ≤ o cid queries algorithm returns o theorem eliciting metric φquad deﬁnition least ωq logq√q pairwise queries needed achieve error q√q slack  cidb − ˆbcidf ≤ cid cid q cid ω  theorem shows qpme procedure robust noise query complexity depends linearly number unknowns theorem shows inherent complexity problem driven number unknowns general case deﬁnition oq thus qpme procedure’s query complexity optimal barring log term stress despite eliciting complex nonlinear metric query complexity order prior methods linear elicitation respect number unknowns added structural assumptions metric proposal can modiﬁed reduce query complexity example suppose one knows matrix b diagonal lpme subroutine call needs estimate one parameter can done constant number queries resulting query complexity will ˜oq linear number unknowns moreover since sample estimates rates consistent estimators metrics lipschitz wrt rates high probability gather correct oracle feedback querying ﬁnite sample estimates ωˆr ˆr instead querying population statistics ωr r long suﬃcient samples see appendix d algorithm agnostic ﬁnite sample errors long sphere s space r experiments evaluate approach simulated oracles ﬁrst present results synthetically generated query space discuss results realworld datasets eliciting metrics eliciting quadratic metrics ﬁrst apply qpme algorithm elicit quadratic metrics deﬁnition assume access qdimensional sphere s centered rate o radius ρ query rate vectors r recall practice remark guarantees existence sphere within feasible region r randomly generate quadratic metrics φquad parametrized b repeat experiment trials varying numbers classes k ∈ equiv q ∈ run qpme procedure tolerance  − figures –b show box plots cid frobenius norm true elicited linear quadratic coeﬃcients generally ﬁnd qpme able elicit metrics close true ones holds varying k q showing eﬀectiveness approach handling multiple classes larger standard deviation q due assumption failing hold trials resulting estimates accurate discuss section b c d e figure average elicitation error metrics function number coeﬃcients q groups m quadratic metrics deﬁnition –b fairness metrics deﬁnition c–e eliciting fairness metrics next apply elicitation procedure figure tolerance  − elicit fairness metrics deﬁnition randomly generate oracle metrics φfair parametrized b λ repeat experiment trials varied number classes groups k m ∈ figures c–e show mean elicitation errors three parameters linear predictive performance error cida − ˆacid increases number coeﬃcients q groups m cid independent number groups quadratic violation term error run cidm uv cidbuv − ˆbuvcidf increases q m qpme procedure cid matrices buvvu elicitation error accumulates increasing q lastly elicited tradeoﬀ ˆλ seen close true λ well cid times eliciting cidm details simulated experiments quadratic metric elicitation figures –b show box plots cid frobenius norm true elicited linear quadratic coeﬃcients generally ﬁnd qpme able elicit metrics close true ones reinforce point also compare elicitation error qpme procedure elicitation error baseline assigns equal coeﬃcients b figure see elicitation error baseline order magnitude higher elicitation error qpme procedure holds varying k showing qpme procedure able elicit oracle’s multiclass quadratic metrics well qkk k classeseqpme error aa vs qqkk k classesqpme error bbf vs qqkk k classesefairme error aa vs qmmmmqkk k classesfairme error buvbuvf vs qmmmmqkk k classesefairme error vs qmmmm figure elicitation error comparison baseline assigns equal coeﬃcients figure elicitation error metrics following assumption vs elicitation error completely random metrics eﬀect assumption mentioned section small number trials assumption failed hold suﬃciently large constants c c− c cq now analyze greater detail eﬀect regularity assumption eliciting quadratic metrics understand lower bounding constants impact elicitation error sumption eﬀectively ensures ratios computed welldeﬁned end generate two sets quadratic metrics one set generated following assumption one coordinate gradient greater − generated randomly without regularity condition sets run qpme elicit corresponding metrics figure see elicitation error much higher regularity sumption followed owing fact ratio computation susceptible errors gradient coordinates approach zero cases randomly generated metrics dashdotted curve red color shows trajectory theo retical bounds increasing q within constant factor figure see mean cid analogously frobenius norm better follow theoretical bound trajectory case regularity assumption followed metrics next analyze ratio estimated fractions true fractions used simulated runs ideally ratio see figure estimated ratios can oﬀ signiﬁcant amount trials metrics generated randomly estimated ratios however stable assumption figure ratio estimated true fractions simulated runs without assumption table dataset statistics dataset default adult sensit vehicle covtype k samples features since multiply fractions even may observe compounding eﬀect fraction estimation errors ﬁnal estimates hence see k figure b standard deviation high due trials lower bound − constants assumption may enough however majority trials shown figure ab figure incur low elicitation error ranking realworld classiﬁers performance metrics provide quantiﬁable scores classiﬁers score often used rank classiﬁers select best set classiﬁers practice section discuss beneﬁts elicited metrics comparison default metrics ranking realworld classiﬁers experiment work four real world datasets varying number classes k ∈ see table details datasets use dataset train classiﬁers rest data used compute testing predictive rates dataset create pool classiﬁers tweaking hyperparameters fa figure performance competing metrics ranking realworld classiﬁers ‘elicited’ metric elicited qpme ‘linear’ metric comprises linear part oracle’s true quadratic metric ‘accuracy’ linear metric weigh classiﬁcation errors equally often used practice mous machine learning models routinely used practice speciﬁcally create classiﬁers logistic regression models multilayer perceptron models lightgbm models support vector machines compare ranking classiﬁers provided competing baseline metrics respect ground truth ranking provided oracle’s true metric generate random quadratic metric φquad following deﬁnition treat true φquad oracle’s metric provides us ground truth ranking classiﬁers pool use proposed procedure qpme algorithm recover oracle’s metric comparison ranking realworld classiﬁers choose two linear metrics routinely employed practitioners baselines ﬁrst accuracy φacc √qcid rcid second weighted accuracy just use linear part cida rcid oracle’s true quadratic metric cida rcid rt br repeat experiment trials report ndcg exponential gain kendalltau coeﬃcient averaged trials figure observe consistently datasets elicited metrics using qpme procedure achieve highest possible ndcg kendalltau coeﬃcient saw section qpme may incur elicitation error thus elicited metrics may accurate however figure shows elicited metrics may still achieve nearoptimal ranking results implies given set classiﬁers ranking based elicited metric scores align closely true ranking comparison ranking based default metric scores consequentially elicited metrics may allow us select discard classiﬁers given task advantageous practice covtype dataset see linear metric also achieves high ndcg values perhaps ranking top quite accurate however kendalltau coeﬃcient low suggesting overall ranking classiﬁers poor also observe general weighted version linear metric better accuracy ranking classiﬁers figure performance competing metrics ranking realworld classiﬁers fairness ‘elicited’ metric elicited quadratic fairness metric elicitation procedure section also depicted figure ‘linear w fairness’ metric comprises linear part oracle’s true quadratic fair metric deﬁnition without fairness violation ‘accuracy w eq odds’ metric weigh classiﬁcation errors fairness violations equally often used practice regards fairness performed similar experiment comparing fair classiﬁers’ ranking adult default datasets gender protected group two genders provided datasets ie m simulate fairness metrics given deﬁnition gives groundtruth ranking classiﬁers evaluate ranking elicited fairquadratic metric using procedure described section also depicted figure figure show ndcg kdtau values method two baselines ‘linear w fairness’ metric comprises linear part oracle’s true quadratic fair metric deﬁnition without fairness violation b ‘accuracy w eq odds’ metric weigh classiﬁcation errors fairness violations equally see elicited fairness metric’s ranking closest groundtruth extension higher order polynomials approach can generalized higherorder polynomials rates consider eg cubic polynomial cid φcubicr airi bijrirj cid ijl cijlrirjrl ij cid ij cid cid cid invariance quadratic approximation metric around point z given ij b ijl c ijl wlog due scale  c cijlri − zirj − zjzl b c symmetric andcid cid cid airi bijrirj ij ijl c constant aﬀecting oracle responses can estimate parameters approximation applying qpme procedure algorithm metric centered appropriate point queries restricted small neighborhood around will elicit one face tensor cl upto scaling factor thus will require us run z running qpme using sphere around point zl o  − cidαl cid  qpme procedure q times around basis points zl o  − cidαl ∀l ∈ q since elicit scaleinvariant quadratic approximation need additional run qpme procedure around point s−z elicit coeﬃcients thus can recover metric ˆφcubic ˆa ˆb ˆc many queries number unknowns ie ˜oq cubic case dth order polynomial one can recursively apply procedure estimate d− th order approximations multiple points similarly derive polynomial coeﬃcients estimated local approximations related work chapter formalized problem chapter put forward procedure binary classiﬁcation later chapter extends multiclass setting focus previous chapters however eliciting linear fractionallinear metrics whereas chapter elicit complex quadratic metrics learning linear functions passively using pairwise comparisons mature ﬁeld unlike active learning counterparts methods query eﬃcient related work include active classiﬁcation learn classiﬁers ﬁxed known metric contrast seek elicit unknown metric posing queries oracle also work active linear elicitation eg qian et al provide theoretical bounds work diﬀerent query space unaware prior work eliciting quadratic function either passively actively using pairwise comparisons use metric elicitation fairness relatively new work eliciting individual fairness metrics best knowledge work chapter work elicits groupfair metrics extend chapter handle general metrics zhang et al elicit tradeoﬀ accuracy fairness using complex ratio queries contrast jointly elicit predictive performance fairness violation tradeoﬀ using simpler pairwise queries lastly prior work also focused learning fair classiﬁers constraints take regularization view fairness fairness violation included objective work also related decisiontheoretic preference elicitation however following key diﬀerences focus estimating utility function metric explicitly whereas prior work seek ﬁnd optimal decision via minimizing maxregret set utilities studies directly learn utility provide query complexity guarantees pairwise comparisons formulations consider ﬁnite set alternatives starkly diﬀerent set alternatives case ie classiﬁers rates inﬁnite papers focus linear bilinear utilities except gai utilities choquet integral whereas focus quadratic metrics useful classiﬁcation tasks especially fairness discussion limitations future work provided eﬃcient quadratic metric elicitation strategy shown appli cation pressing issue algorithmic fairness interestingly query complexity nonlinear metrics dependence number unknowns linear metrics also shown idea can extended elicit higher order polynomial metrics signiﬁcantly increases usecases opens door nonlinear metric elicitation notable advantage proposal independent population p thus metric learned using one dataset model class can applied applications long expert believes tradeoﬀs key challenge tackle throughout elicitation maintaining feasibility rates ie rates achievable classiﬁers practical advantage now one ﬂexibility deploy systems either compare classiﬁers compare rates time work limitations assume parametric form quadratic oracle metric may good match practice extension polynomial elicitation helps may lead overburdening oracle huge number queries degree polynomial high another limitation leaves open question oracles furthermore one cautious failure metric elicitation system especially eliciting fairness metrics can cause varying impacts among protected groups look forward future work answering practical questions chapter optimizing blackbox metrics metric elicitation chapter discuss interesting application metric elicitation tools procedures provided previous chapters play key role aim optimize blackbox performance metric instead human oracle machine oracle responds absolute quality value classiﬁer discuss later settings prevalent literature motivation using blackbox optimization comes fact many existing optimization algorithms iterative nature iteration tend optimize locallinear approximation locallinear approximation unknown blackbox metric can elicited using existing tools results discuss brieﬂy procedures can extended presence human oracles provide pairwise preference feedback including ab tests based scenarios next discuss formal blackbox optimization problem setup tools can used optimize metrics setup introduction many realworld machine learning tasks evaluation metric one seeks optimize explicitly available closedform true metrics evaluated live experiments querying human users require access private legally protected data hence written explicit training objective also case learner access data skewed training distribution labels heteroscedastic noise hence directly optimize metric training set despite knowing mathematical form problems can framed blackbox learning tasks goal optimize unknown classiﬁcation metric large possibly noisy training data given access evaluations metric small clean validation sample highlevel approach learning tasks adaptively assign weights training examples resulting weighted training objective closely approximates blackbox metric validation sample construct classiﬁer using example weights postshift classprobability estimator pretrained training set results eﬃcient iterative approach require retraining indeed example weighting strategies widely used optimize metrics correct distribution shift prior works either handle specialized forms metric data noise formulate exampleweight learning task diﬃcult nonconvex problem hard analyze employ expensive surrogate reweighting strategy comes limited statistical guarantees contrast propose simple eﬀective approach optimize general blackbox metric function confusion matrix provide rigorous statistical analysis key element approach eliciting weight coeﬃcients probing black box metric select classiﬁers solving system linear equations matching weighted training errors validation metric choose “probing” classiﬁers linear system wellconditioned provide theoreticallygrounded options practically eﬃcient variants weight elicitation procedure used subroutine iteratively construct ﬁnal plugin classiﬁer contributions chapter follows • provide method eliciting example weights linear blackbox metrics section • use procedure iteratively learn plugin classiﬁer general blackbox metrics section • provide theoretical guarantees metrics concave functions confu sion matrix distributional assumptions section • experimentally show approach competitive better stateoftheart methods tackling label noise cifar domain shift adience optimizing proxy labels blackbox fairness metric adult section proofs chapter provided appendix e notations onehotj ∈ k returns onehot encoding j ∈ k chapter cid norm vector denoted cid · cid problem setup consider standard multiclass setup instance space x ⊆ rd label space y k wish learn randomized multiclass classiﬁer h x→∆k input x ∈ x predicts distribution hx ∈ ∆k k classes will also consider deterministic classiﬁers h x→k map instance x one k classes evaluation metrics let d denote underlying data distribution x ×y will evaluate performance classiﬁer h d using evaluation metric e dh higher values indicating better performance goal learn classiﬁer h maximizes evaluation measure maxh e dh will focus metrics e d can written terms classiﬁer’s confusion matrix ch ∈ k×k jth entry probability true label randomized classiﬁer h predicts j cidy ihjxcid c d ij h exy∼d performance classiﬁer can evaluated using possibly unknown func tion ψ k×k→r confusion matrix e dh ψcdh j cij cid j cji several common classiﬁcation metrics take form including typical linear metrics ψc cii cid cid gmean ψc cidcid ij lij cij reward matrix l ∈ rk×k j cij cidciicid fmeasure ψc cid cidcidk consider settings learner queryaccess evaluation metric e d ie can evaluate metric given classiﬁer h directly write metric explicit mathematical objective happens metric truly blackbox function ie ψ unknown ψ known access noisy version distribution d needed compute metric noisy training distribution learning classiﬁer assume access large sample str ntr examples drawn distribution µ will refer “train ing” distribution training distribution µ may true distribution d may diﬀer true distribution d feature distribution px conditional label distribution pyx also assume access smaller sample sval nval examples drawn true distribution d will refer sample str “train ing” sample smaller sample sval “validation” sample seek solve using samples following examples noisy training distributions literature example independent label noise iln distribution µ draws example x y d randomly ﬂips y tocidy probability pcidyy independent instance x example clusterdependent label noise cdln suppose x belongs table example weights w x→rk×k linear metric e dh cidl cdhcid noise models exmp – wijx weight entry cij sec – consider metrics functions diagonal confusion entries alone ie l t diagonal handle general metrics appendix e model iln cdln t m idln noise transition matrix tij pcidy jy ij pcidy jy gx m wx l cid tgx− wx l cid t− tijx pcidy jy x wx l cid tx− correction weights ds wijx pdxpµx∀ j example instancedependent label noise idln µ draws x y d one m disjoint clusters gx ∈ m distribution µ draws x y d randomly ﬂips y cidy probability pcidyy gx randomly ﬂips y cidy probability pcidyy x may depend x example domain shift ds µ draws cidx according distribution pµx diﬀerent pdx draws y true conditional pdycidx approach learn example weights training sample str resulting weighted empirical objective locally globally approximates estimate metric e d validation sample sval ease presentation will assume metrics depend diagonal entries confusion matrix ie cii’s appendix e elaborate ideas can extended handle metrics depend entire confusion matrix similarly performing deterministic classiﬁers using eg techniques approach uses randomized classiﬁers practice one can replace follows will need empirical confusion matrix validation set cidcvalh cidc val ij h cid nval xy∈sval y ihjx example weighting linear metrics ﬁrst describe example weighting strategy linear functions diagonal entries confusion matrix given βi c d ii h e dh cid unknown weights β βk next section will discuss use procedure subroutine handle complex metrics modeling example weights deﬁne example weighting function w x→rk cid wixk example x cidcid wix y ihix exy∼µ associates k correction weights ≈ e dh ∀ h indeed noise models examples – exist weighting functions w holds equality table shows form weighting function general linear metrics ideally weighting function w assigns k independent weights example x ∈ x however practice estimate e d using small validation sample sval ∼ d avoid example weights overﬁt validation sample restrict ﬂexibility w set weighted sum l basis functions φcid x→ wix cidl cid αcid iφcidx αcid entry ∈ r coeﬃcient associated basis function φcid diagonal confusion practice basis functions can simple partitioning instance space l clusters ie φcidx gx cid clustering function g x→l may deﬁne complicated soft clustering using eg radial basis functions centers xcid width σ cid −cidx − xcidcidσcid φcidx exp φtransformed confusions expanding weighting function gives us lcid kcid cid αcid exy∼µ cid cidφcidx y ihixcid cid cidcid φµcid h ≈ e dh ∀ h h exy∼µ cidy ihixcid gives standard confusion entries training cidgx cid y ihixcid gives training confusion entries evaluated φµcidh ∈ k can seen φtransformed confusion matrix train ing distribution µ example one one basis function φx ∀x φµ distribution φµcid examples cluster cid can thus rewrite equation weighted combination φconfusion entries basis functions divides data l clusters h exy∼µ lcid kcid cid iφµcid αcid h ≈ e dh∀h eliciting weight coeﬃcients α – metric elicitation step next discuss estimate weighting function coeﬃcients αcid ’s training sample str validation sample sval notice gives relationship statistics φµcid’s computed training distribution µ evaluation metric interest computed true distribution d moreover ﬁxed classiﬁer h lefthand side linear unknown coeﬃcients α α similar eliciting linear metrics chapters presence oracle provides absolute quality feedback k ∈ rlk thus step therefore probe metriccide val lm diﬀerent classiﬁers h hk hl hlk k αl α αl results set lk linear equations form cid cid cidi αcid h cide valh icidφtrcid hlk cide valhlm icidφtrcid cidi αcid h ntr βi cidc val ii h evaluated validation sample xy∈str φcidx y ihix evaluated training sample cid cidφtrcid metric cide valh cid formally let cidσ ∈ rlk×lk cide ∈ rlk denote lefthand righthand side observations iecidσcidicidcidicid weight coeﬃcients given cidα cidσ−cide cid cidecidi cide valhcidi xy icidhcidi xy∈str icid x ntr φcidcid sval ∼ d ¯h  h γ ω choose hcidix φcidx eix − φcidx ¯hx ¯h τ h − τ ¯h h ∈ h τ ∈  pick hcidi ∈ ¯h satisfy slack γ ω∀cid ﬁxed classiﬁer else algorithm plugin elicited weights piew diagonal linear metrics compute cidσ cide using metric cide val output cidα cidσ−cide input cide val basis functions φ φl x→ class probability modelcidηtr x→∆k cidα elicitweightscide val φ φl str sval ¯h  exampleweights cidwix cidl µ training set str ∼ µ validation set sval ∼ d ¯h  cidcidαcid plugin cidhx ∈ argmaxi∈kcidwixcidηtr output cidh will choose lk probing classiﬁers cidσ wellconditioned one way choose classiﬁers cidσ high value diagonal entries cidφtrcid h low value cidφtrcidcid h ∀ cidcid icid cid cid can framed following low value oﬀdiagonals ie choose classiﬁer hcidi evaluate high value choosing probing classiﬁers h hlk iφcid x ix icid algorithm elicitweights diagonal linear metrics input cide val basis functions φ φl x→ training set str ∼ µ val set constraint satisfaction problem str hcidi pick h ∈ h cidφtrcid h ≥ γ cidφtrcidcid icid h ≤ ω∀cidcid icid cid cid γ ω suﬃciently ﬂexible hypothesis class h constraints feasible problems can generally solved formulating constrained classiﬁcation problem show appendix e problem feasible can eﬃciently solved range settings practice explicitly solve hypothesis class h instead simpler surprisingly eﬀective strategy set probing classiﬁers trivial classiﬁers predict class subset examples build intuition good idea consider simple setting one basis function φx ∀x h ntr φconfusions cidφtr examples yields highest value forcidφtr xy∈str y ihix standard confusion entries training set case trivial classiﬁer eix onehoti∀x predicts class ∀j cid fact experiments set probing classiﬁer hi randomized combination ei ﬁxed base classiﬁer ¯h othercidφtr cid j large enough  cidσ wellconditioned hix eix − ¯hx similarly basis functions divide data l clusters can randomize ¯h trivial classiﬁer predicts particular class examples assigned cluster cid ∈ l confusion matrix resulting classiﬁers will higher values ¯h cid ith diagonal entry lower value entries classiﬁers can succinctly written tune  make sure resulting cidσ wellconditioned choice hcidix φcidxeix − φcidx¯h probing classiﬁers also works well practice general basis functions φcid’s algorithm summarizes weight elicitation procedure probing classiﬁers either constructed solving constrained satisfaction problem set “ﬁxed” classiﬁers cases algorithm takes base classiﬁer ¯h parameter  input  controls extent ¯h perturbed construct probing classiﬁers radius parameter  restricts probing classiﬁers neighborhood around ¯h will prove handy algorithm develop section plugin based algorithms elicited weight coeﬃcients α now seek learn classiﬁer optimizes left hand side via plugin approach ﬁrst pretrain model cidηtr x→∆k noisy training distribution µ estimate conditional class probabilities cidηtr x ≈ pµy ix apply correction weights postshift cidηtr ﬁrst describe approach diagonal linear metrics e dh cid algorithm given correction weightscidw x→rk ii h seek maximize following plugin algorithm linear metrics βi c d figure overview apporach weighted objective training distribution maxh exy∼µ cidcid icidwix y ihix cid standard exampleweighted learning problem following plugin also known postshift classiﬁer consistent estimator cidhx ∈ argmax ∈k cidwixcidηtr x iterative algorithm general metrics optimize generic nonlinear metrics form e dh ψc d kkh ψ k→r apply algorithm iteratively consider cases ψ unknown ψ known needs optimized using noisy distribution µ idea ﬁrst elicit local linear approximations ψ learn plugin classiﬁers resulting linear metrics iteration h c d speciﬁcally following narasimhan et al derive algorithm classical frankwolfe method maximizing smooth concave function ψc convex set c ⊆ rm case c set confusion matrices cdh achieved classiﬁer h convex allow randomized classiﬁers see lemma e appendix e elicit weightprobing classifier pluginpostshifts pretrained weightingwfrankwolfe updateplugin elicited weights linear metricsfrankwolfe elicited gradients general metricsmetric algorithm frankwolfe elicited gradients fweg general diagonal metrics also depicted fig input cide val basis functions φ φl x→ pretrained cidηtr x→∆k str ∼ µ initialize classiﬁer h c diagcidcvalh sval ∼ d t  t t − t t else ii h end h c d kkh known ψ small  recommendeded e dh ψc d βt ∇ψct βt cide linh cid icidc val cide linh cide valh cidf piewcide lin φ φlcidηtr str sval ht  cidc diagcidcvalcidf cidht ht cid − tonehotcidf ct cid − cidct tcidc output cidh ht ct cidc ∈ argmaxc∈ccid∇ψct ccid next iterate ct convex combination ct current solutioncidc validation sample sval step linearize ψ using cide linh cid cide lin mathematical form ψ known one can directly compute gradient βt known can simply set cide linh cide valh restrict weight elicitation algorithm outline adaptation frankwolfe algorithm setting maintain classiﬁer ht estimate diagonal confusion entries ct ii h βt ∇ψct invoke plugin method algorithm optimize linear approximation algorithm maintains iterates ct step maximizes linear approximation ψ routine algorithm choose probing classiﬁers hcidi’s small neighborhood around current classiﬁer ht ψ eﬀectively linear can done passing ¯h ht weight elicitation routine setting radius  small value icidc val βt call algorithm uses training validation set elicit example weights local linear approximation ψ uses weights construct plugin classiﬁer ﬁnal output randomized combination plugin classiﬁers step note algorithm runs eﬃciently reasonable values l k indeed runtime almost always dominated pretraining base model cidηtr time taken elicit weights eg using relatively inexpensive see appendix e theoretical guarantees provide theoretical guarantees weight elicitation procedure plugin methods algorithms – cidi ¯αcid iφµcid cidcidcidcid h − e dh cidcidcid ≤ ν∀h cid ¯αcid ≤ b cid assumption distributions d µ linear metric e dh βiciih cidβcid ≤ ∃ ¯α ∈ rlk st ν ∈ b metric d can approximated slack ν weighting wix cid assumption states choice basis functions φ φl linear iφcidx training examples µ existence weighting function depends well basis functions capture underlying distribution shift indeed assumption holds common settings table eg noise transition t diagonal appendix e handles general t basis functions set φx ∀x idln setting φcidx gx cid∀x cdln setting analyze coeﬃcients cidα elicited algorithm probing classiﬁers hcidi cid ¯αcid chosen satisfy classiﬁers hcidi set ﬁxed choices appendix e provide analysis probing theorem error bound elicited weights let γ ω con straints feasible hypothesis class ¯h cid suppose algorithm chooses classiﬁer hcidi satisfy e dhcidi ∈ c ∀cid c let ¯α deﬁned assumption suppose γ √lmω ntr ≥ lm loglmhδ fix cidα output algorithm satisﬁes δ ∈ wp ≥ − δ draws str sval µ d resp coeﬃcients cidcid cid lk −√ cid l log lkh ν√lk cid cid lmω γ cidcidα − ¯αcid ≤ o γ ntr δ l log lk δ cnval γ term h can replaced measure capacity hypothesis class h probing classiﬁers chosen using training set alone sampling errors training set depend complexity h validation set suggests robustness approach small validation set long training set suﬃciently large number basis functions reasonably small iterative plugin method algorithm bound gap metric value e dcidh output classiﬁer cidh true distribution d optimal value training distribution true class probabilities ηtr handle case function ψ known gradient ∇ψ can computed closedform general case unknown ψ handled appendix e x x py ix well quality bound depends gap estimated class probabilities cidηtr coeﬃcients cidα provided weight estimation subroutine measured κ· one can substitute κ· eg error bound provided theorem theorem error bound fweg let e dh ψc d pose assumption holds linear metriccid kkh known concave function ψ k→r qlipschitz λsmooth fix δ ∈ sup ii h whose associated weight routine alg outputs coeﬃcients cidα cidcidα − ¯αcid ≤ κδ ntr nval function coeﬃcients ¯α cid ¯αcid ≤ b wp ≥ − δ draw str sval weight estimation d µ resp classiﬁercidh output algorithm t iterations satisﬁes κ· let bcid b √lk κδt ntr nval wp ≥ − δ draws str sval h c d βic d cid h e dh − e dcidh ≤ qbcidex cidηtrx −cidηtrxcid cid cid max cid q√lk κ δ k logk lognval logkδ t ntr nval cid λk o nval λ t qν proof turn derives error bound plugin classiﬁer algorithm linear metrics see appendix e related work methods closedform metrics variety work optimizing complex evaluation metrics including plugin type algorithms use convex surrogates metric methods rely test metric speciﬁc closedform structure handle blackbox metrics methods blackbox metrics among recent blackbox metric learning works closest jiang et al learn weighted combination surrogate losses approximate metric validation set like us probe metric multi ple classiﬁers approach several drawbacks practical theoretical fronts firstly jiang et al require retraining model iteration can timeintensive whereas postshift pretrained model secondly procedure prescribe eliciting gradients requires perturbing model parameters multiple times can expensive large deep networks whereas require per turbing predictions model moreover number perturbations need grows polynomially precision need estimate loss coeﬃcients whereas require constant number lastly approach come strong statistical guarantees whereas besides beneﬁts will also see section method yields better accuracies related blackbox learning methods include learn weighted loss approximate metric using computationally expensive procedures eg metagradient descent rl often require retraining model scratch come limited theoretical analysis methods distribution shift literature distribution shift vast cover representative papers see comprehensive discussion independent label noise setting patrini et al propose loss correction approach ﬁrst trains model noisy label use predictions estimate noise transition matrix retrains model corrected loss approach however tailored optimize linear metrics whereas can handle complex metrics well without retraining underlying model plethora approaches exist tackling domain shift including classical importance weighting iw strategies work two steps estimate density ratios train model resulting weighted loss one approach kernel mean matching matches covariate distributions training test sets high dimensional rkhs feature space iw approaches however prone overﬁtting used deep networks recent iterative variants seek remedy experiments run experiments four classiﬁcation tasks known blackbox metrics diﬀerent label noise domain shift settings experiments use large training sample either noisy contains missing attributes smaller clean complete validation sample always optimize crossentropy loss learning cidηtrx ≈ pµy x using training set orcidηvalx ≈ pdy x baselines models varied across experiments monitoring quality ofcidηtr andcidηval sample small subsets hypertrain hyperval data original training validation data respectively repeat experiments random trainvalitest splits report mean standard deviation metric will use ∗ ∗∗ ∗∗∗ denote diﬀerences method closest baseline statistically signiﬁcant using ∈k cidhx ∈ argmax cidhx ∈ argmax ∈k cidηtr x cidηval x table data statistics diﬀerent problem setups section problem setup indepen label noise section proxylabel section domainshift section blackbox fairness metric section adult dataset cifar adult adience classes prot groups features × × × × train val test split k k k k k k k k k k welch’s ttest conﬁdence level respectively provide data statistics table observe always use small validation data comparison size training data source code along random seeds provided link common baselines use representative baselines blackbox learning iterative reweighting label noise correction importance weighting literatures first list ones common experiments crossentropy train maximizes accuracy training set predicts crossentropy val maximizes accuracy validation set predicts finetuning finetunes pretrainedcidηtr using validation data monitoring crossentropy loss hyperval data early stopping optmetric val metrics ψcdh ψ known trains model directly maximize metric small validation set using frankwolfe based algorithm learntoreweight jointly learns example weights model maxi mize accuracy validation set handle specialized metrics plugin trainval constructs classiﬁer cidhx ∈ argmaxi wicidηval x weights wi ∈ r tuned maximize given metric validation set using coordinatewise line search details appendix e adaptive surrogates learns weighted combination surrogate losses eval uated clusters examples approximate metric validation set since httpsgithubcomkoyejolabfweg method directly amenable use large neural networks see sec tion compare using linear models present additional comparisons app e table e hyperparameters learning rate finetuning chosen e−− pi ew fweg tune parameter  e− line search trainingcidηtr andcidηval state individual tasks plugin performed spacing e− hyperparameters baselines maximizing accuracy label noise ﬁrst task train class image classiﬁer cifar dataset repli cating independent asymmetric label noise setup evaluation metric use accuracy take original training data validation data ﬂip labels remaining training set based following transition matrix truck → automobile bird → plane deer → horse cat ↔ dog ﬂip probability forcidηtr cidηval use resnet architecture trained using sgd momentum weight decay e− learning rate divide epochs total additionally compare forward correction method specialized method correcting independent label noise estimates noise transition matrix t using predictions cidηtr training set retrains corrected loss thus training resnet twice saw notable drop method used small validation set estimate t apply proposed piew method linear metrics using weighting function w deﬁned one two choices basis functions chosen via crossvalidation default basis function clusters points together φdefx ∀x ii ten basis functions φ φ one average rbf kernels see centered validation points belonging true class rbf kernels computed width umapreduced dimensional image embeddings shown table piew achieves signiﬁcantly better test accuracies baselines results forward correction matches unlike method train resnet achieve higher accuracy crossentropy val overﬁts badly yields least test accuracy surprisingly simple ﬁnetuning yields secondbest accuracy possible reason pretrained model learns good feature representation ﬁnetuning step adapts well main change also observed piew achieves better accuracy crossvalidation table test accuracy noisy label experiment cifar crossentropy train crossentropy val learntoreweight plugin trainval forward correction finetuning piew ± ± ± ± ± ± ± ten basis functions highlighting beneﬁt underlying modeling piew lastly figure show elicited class weights default basis function φdefx ∀x eg bird → plane weight bird upweighted plane downweighted maximizing gmean proxy labels next experiment borrows “proxy label” setup adult dataset task predict whether candidate’s gender male training set contains proxy true label sample validation data original training data replace labels remaining sample feature ‘relationshiphusband’ label noise instancedependent see example seek maximize gmean metric ψc cidcid cidcii cid cidcidm cij traincidηtr andcidηval using linear logistic regression using sgd learning rate j additional baselines include adaptive surrogates method forward correction inner outer learning rates adaptive surrogates cross validated also compare simple importance weighting strategy ﬁrst train logistic regression model f predict example x y belongs validation data train gender classiﬁer training examples weighted f x y − f x y choose three sets basis functions using crossvalidation default basis function φdefx ∀x ii φdef φpw φnpw φpwx xpw φnpwx xnpw use features ‘privateworkforce’ ‘nonprivateworkforce’ form hard clus ters iii φdef φpw φnpw φinc φincx xinc uses binary feature ‘income’ choices motivated used compute surrogate losses individual clusters provide adaptive surrogates method clustering table test gmean proxy label experiment adult crossentropy train crossentropy val optmetric val learntoreweight plugin trainval forward correction finetuning importance weights adaptive surrogates ± ± ± ± ± ± ± ± ± fweg unknown ψ ± ∗∗ ± ∗ fweg known ψ choices table summarizes results apply variants fweg method nonlinear metric ψ one ψ known gradient available closedform ψ assumed unknown treated general blackbox metric variants perform similarly better baselines adaptive surrogates comes close second underperforms results statistically signiﬁcant improvement fweg adaptive surrogates small latter time intensive iteration retrains logistic regression model verify empirically figure b reporting runtimes adaptive surrogates method fweg including pretraining time choices basis functions clustering features see approach × faster experiment lastly forward correction performs poorly likely loss correction aligned label noise model maximizing fmeasure domain shift now move domain shift application see example task learn gender recognizer adience face image dataset training test datasets containing images diﬀerent age groups domain shift based age use images belonging age buckets – training k images evaluate images age buckets – k images validation set sample – age bucket images aim maximize fmeasure forcidηtr andcidηval use resnet model cifar experiment except learning rate divided epochs total additional baseline compute importance weights using kernel mean matching kmm train resnet model weighted loss since image size large directly applying table test fmeasure domain shift experiment adience crossentropy train crossentropy val optmetric val plugin trainval importance weights kmm learntoreweight finetuning fweg unknown ψ fweg known ψ ± ± ± ± ± ± ± ± ∗∗∗ ± ∗∗∗ kmm ﬁrst compute dimensional imagenet embedding images reduce dimensions via umap kmm weights learned dimensional embedding basis functions besides default basis φdefx ∀x choose subsets six rbf basis functions φ φ centered points validation set representing one six agegender combinations use umap embedding kmm compute rbf kernels table presents test fmeasure values variants fweg algorithm provide statistically signiﬁcant improvements baselines finetuning learning toreweight improve plain crossentropy optimization train however moderately likely small size validation set methods tailored optimize fmeasure maximizing blackbox fairness metric next handle blackbox metric given query access value consider fairness application goal balance classiﬁcation performance across multiple protected groups groups one cares known due privacy legal restrictions protected attribute individual revealed instead access oracle reveals value fairness metric predictions validation sample protected attributes absent training sample setup diﬀerent recent work learning fair classiﬁers incomplete group information focus optimizing given blackbox fairness metric use adult dataset seek predict whether candidate’s income greater k gender protected group blackbox metric consider whose form unknown learner geometric mean truepositive tp true negative tn rates evaluated separately male female examples promotes table blackbox fairness metric test set adult crossentropy train crossentropy val learntoreweight finetuning adaptive surrogates plugin trainval fweg ± ± ± ± ± ± ± ∗∗∗ equal performance groups classes cid e dh tpmaleh tnmaleh tpfemaleh tnfemaleh cid train logistic regression models previous adult experiment section along basis functions φdef φpw φnpw used additionally include two basis φhs φwf based features ‘relationshiphusband’ ‘relationship wife’ expect correlations gender include two baselines can handle blackbox metrics plugin trainval tunes threshold cidηtr querying metric validation set adaptive surrogates latter crossvalidated set clustering features ie basis functions method computing surrogate losses seen table fweg yields highest blackbox metric test set adaptive surrogates comes second surprisingly simple plugin approach fairs better baselines crossvalidation also observed performance fw eg improves basis functions particularly ones better correlated gender speciﬁcally fweg basis functions φdef φpw φnpw φwf φhs achieves approximately better performance fweg φdef basis function fw eg basis functions φdef φpw φnpw ablation studies close two sets experiments first analyze performance piew optimizing accuracy adult experiment section varies quality base modelcidηtr save estimate ofcidηtr every batches batch size training logistic regression model use estimates inputs piew shown domain knowledge use protected group “gender” beyond form metric unknown importantly individual’s gender available c b d figure elicited class weights cifar piew default basis sec b runtime fweg adaptive surrogates vs grouping features proxy label task sec c eﬀect quality base modelcidηtr adult sec base model’s quality improves test accuracies piew also im proves d eﬀect validation set size adience sec piew performs better ﬁnetuning even small validation sets improve larger ones figure c test accuracies piew improves quality ofcidηtr measured log loss hypertrain set accordance theorem one can improve quality estimate ηtr using calibration techniques will likely enhance performance piew well next show piew robust changes validation set size trained adience experiment section optimize accuracy set aside – age bucket data testing sample varying sizes validation data rest shown figure d piew generally performs better ﬁnetuning even small validation sets improve larger ones exception sized validation set training data see overﬁtting due small validation size blackbox optimization pairwise comparison oracle proposed algorithm chapter works machine oracles queried classiﬁer h respond metric value e dh saw various cases eg validation set distribution shift settings regulator fairness setups access planevehiclebirdcatdeerdogfroghorseshiptruckclassesweightsnumber grouping features sec wall clock time secondsfweg oursadaptive surrogateslog loss base model hypertraintest accuracy piewbase stdpiew stdsize validation data×test accuracy fixed test datace trainfinetuningpiew oracle exploit fact example weights act gradient local linear objective small neighborhood unknown metric elicit linear metrics use value queries machine oracle idea can extended presence human oracle provides pairwise preferences also includes ab testing scenarios commonly used web based appli cations order elicit locallinear objective around classiﬁer’s confusion matrix c dh one can ﬁrst construct small sphere around c dh corresponding classiﬁers process discussed section b run algorithm elicit locallinear performance metric using pairwise comparisons locallinear objec tive estimated one can postshift pretrained classconditional estimator similar proposed fweg algorithm algorithm however approach comes challenges firstly order apply iterative frankwolfe approach one will need create spheres corresponding classiﬁers multiple times make algorithm timeintensive require solve optimization problem iteration secondly clear elicit locallinear objective one chooses overlapping softly clustered basis functions hope overcome challenges future concluding remarks chapter proposed frank wolfe elicited gradient fweg method optimizing blackbox metrics given query access evaluation metric small vali dation set framework includes common distribution shift settings special cases unlike prior distribution correction strategies able handle general nonlinear metrics key beneﬁt method agnostic choice ofcidηtr can thus used postshift pretrained deep networks without retrain showed postshift example weights can ﬂexibly modeled various choices basis functions eg hard clusters rbf kernels etc empirically demonstrated eﬃcacies exploit fact example weights act gradient unknown metric esti mated metric elicitation procedure machine oracle responds absolute quality value classiﬁer clean validation dataset moreover novel geometrical characterizations discussed chapters led us devise eﬃcient smart method creating probing classiﬁers see look forward improving results nuanced basis functions chapter practical metric elicitation till now contributions towards metric elicitation framework pairwise comparisons algorithmic bring theory closer practice chapter conduct preliminary realuser study shows eﬃcacy metric elicitation framework recovering users’ preferred performance metrics binary classiﬁcation setup choose cancer diagnosis application task groundtruth label binary feature denoting whether patient cancer choice mo tivated application discussed chapter since asymmetric costs associated false positives false negatives – based known consequences misdiagnosis ie sideeﬀects treating healthy patient vs mortality rate treating sick patient work builds upon existing visualizations confusion matrices ask pairwise preferences b try elicit linear performance metric using proposed proce dure algorithm binary classiﬁcation setup work ten subjects preliminary study experience either machine learning biomedical research university setup create web user interface ui broadly three parts first shows subjects couple confusion matrices asks questions related comprehension compar ison simulation questions familiarize subjects visualizations components associated correct incorrect predictions second shows bunch pairwise preference queries confusion matrices ui involves running binarysearch procedure algorithm back end chooses next set queries based subject’s current realtime responses third ui comprises ﬁfteen pairwise comparison queries confusion matrices randomly chosen feasible set responses queries used evaluate ﬁdelity recovered metric metric elicitation framework end webbased task subjects asked subjective questions essentially lead guidelines recommend implementing metric elicitation framework reallife scenarios goal preliminary study check workﬂow practical implementation metric elicitation framework real data certain extent support reject hypothesis implicit user preferences can quantiﬁed using pairwise comparison queries confusion matrices addition goal includes testing certain assumptions regarding noise subject’s oracle’s responses work around ﬁnite samples userinterface shown later also available httpsaﬁnahalicomelicitationgraphsstatic provide future guidance visualizing confusion matrices pairwise comparisons eliciting actual performance metrics reallife scenarios evaluating quality recovered metric contributions chapter summarized follows • create web ui uses existing visualizations confusion matrices reﬁned capture preferences pairwise comparisons • ui implements binarysearch procedure algorithm back end make use realtime responses confusion matrices elicit linear performance metric cancer diagnosis setup • perform user study ten subjects elicit linear performance metrics using proposed web ui compare quality recovered metric com paring responses elicited metric’s responses set randomly chosen pairwise comparison queries study also includes posttask thinkaloud style interview regarding utility framework • lastly using task results posttask interviews present guidelines re garding practical implementation framework can used future research direction dataset visualization choice section ﬁrst discuss details dataset used feasible set confusion matrices constructed discuss choice visualizations confusion matrices borrowed prior work reﬁned allow better pairwise comparisons choice task dataset used choice task domain dataset motivated application discussed chapter task cancer diagnosis use breast cancer wisconsin original dataset uci repository dataset extensively used literature binary classiﬁcation label denotes malignant cancer label denotes benign cancer samples total wherein sample features dataset can downloaded httpstinyurlcomdnesyvw figure estimated confusions test data forming upper boundary space confusion matrices associated smoothened version upper boundary around data labelled rest task classiﬁer take features patient input predict whether patient cancer divide data two equally sized parts – training test data using training data learn logistic regression model obtain estimate class conditional probability ie cidηx cidpy x create pool thresholded classiﬁers type hτ x cidηx ≥ τ vary threshold τ steps e− subsequently compute confusion matrices threhsolded classiﬁers test data resulting confusion matrices discussed chapter see figure space confusion matrices twodimensional space confusions tuple true positives true negatives associated thresholed classiﬁers form upper boundary upper boundary estimated confusions test data shown figure see solid red line discussed chapter one can use estimated confusion matrices practice elicit linear performance metrics however binary classiﬁcation setup can easily smoothen upper boundary using feasible confusion matrices allows reduce staircase type bumps due estimation ﬁnite data consequentially lead better convergence binarysearch based algorithm generate confusions smoothened version upper boundary take simulated distribution setting section speciﬁcally take joint probability x − y given fx tntpsubset query space upper boundaryestimated boundarysmoothened boundary eaxb u− uniform distribution − u− ηx estimate parameters b minimize squared error k confusions obtained test data ones simulated using distribution smoothened upper boundary shown dashed blue curve figure clearly confusions feasible lie inside region enclosed upper lower boundary thus can use confusions smoothened upper boundary elicitation purposes choice visualization modern times ensuring eﬀective public understanding algorithmic decisions espe cially machine learning models become imperative task view mind borrow visualizations confusion matrices binary classiﬁcations setup shen et al authors provide concrete step towards goal redesigning confusion matrices support nonexperts understanding performance machine learning models ﬁnal visualizations use shen et al created multiple iterative userstudies ﬁrst study authors conduct interviews subjects survey subjects map two major sets challenges lay people understanding standard confusion matrices general terminologies b matrix design challenges elaborated three subchallenges include confusion direction reading data layered relations quantities involved order tackle challenges authors came four alternative visualizations confusion matrix second study authors evaluate eﬃcacy proposed visualizations subjects recidivism prediction task authors conclude ﬂowchart preferred visualization confusion matrix followed barchart visualizations shown figure context recidivism prediction task however light preliminary discussions humancomputer interaction hci machine learning researchers makerecommend following changes visual ization pairwise comparison purposes metric elicitation framework based observation multiple visualizations information help better user understanding choose use top two performing visualizations ie ﬂowchart barchart together depict confusion matrix transform data statistics numbers denote outof samples figure flowchart barchart based visualizations binary classiﬁcation confusion matrices recidivism prediction task shen et al found total number positive negative labels along total num ber positive negative predictions helpful comparing two confusion matrices therefore add total numbers ﬂowchart boxes axes barcharts also add zoomin feature graphs better understanding although preliminary user study changed direction ﬂow chart discussions hci machine learning researchers also noted current direction perhaps important recidivism task time component involved can changed cancer diagnosis task allows one constants ie total positive negative labels left column varying component ie total positive negative predictions right column making comparison easier moreover change ensures barchart ﬂowchart represent similar information plan implement change record impact future user studies modiﬁed visualization incorporating ﬁrst four points confusion matrix context cancer diagnosis shown figure next discuss web user interface user interface discuss proposed web user interface ui detail discuss rationale behind several components also provide images ui end chapter ui starts questionnaire asking demographic information like age gender race highest level school subjects’ expertise machine learning healthcare shown figure ui three parts explained following subsections figure modiﬁed visualization confusion matrix cancer diagnosis task modiﬁcation perspective obtaining better pairwise preferences understanding familiarizing visualizations questionnaire describe task cancer diagnosis provide details classiﬁers can inaccurate predictions layman terms also show proposed visualization confusion matrix along description exhibited figure next four pages show visualizations two confusion matrices side side ask series questions regarding data depicted ﬁrst three adapt questions shen et al cancer diagnosis task see figures ui snapshots shen et al framed questions evaluate comprehension comparison simulationbased understanding subjects use questions comparison questions shen et al diﬀerent pairwise comparisons like make familiarize visualizations fourth page asks subjects actually compare two hypothetically created confusion matrices see figure one matrices higher false positives false negatives question deﬁnitive answer added make subjects familiarize type pairwise comparison questions follow addition question indicates good subject grasped context around cancer diagnosis task pairwise comparisons practically eliciting linear performance metrics next explain second phase ui actually ask subjects pair wise preferences confusion matrices implement binarysearch procedure algorithm confusion matrices used procedure smoothened upper boundary shown figure thus subjects make choice reﬂecting tradeoﬀ false positives false negatives algorithm takes realtime preferences subjects generates next set queries based current responses converge linear performance metric back end save linear perfor mance metric subject stop binarysearch search interval becomes less equal  line algorithm moreover practice need ask four queries per round binary search instead can reduce search interval half just using three pairwise queries round ie query ing ωc θc c θa ωc θd c θc ωc θe c θd line algorithm sample pairwise comparison query run binary search algorithm ui shown figure pairwise preferences random set queries order evaluate quality recovered metric ask subjects ﬁfteen pairwise comparison queries separate web page right binary search algorithm converged elicited metric subjects know information shown evaluation queries continuation previous phase ie binary search query comprises two randomly selected confusion matrices lie inside feasible region confusion matrices generated sphere radius around center set queries used evaluate eﬀectiveness elicited metric compute fraction times elicited metric’s preferences matches subject’s preferences ﬁfteen queries sample pairwise comparison query phase ui shown figure ask ﬁfteen queries focus comparing just one component eg true positives time table subjects’ demographics distribution responses questionnaire values parenthesis show number subjects age education level ml expertise healthcare knowledge graduate college master’s beginner none none doctorate intermediate response table posttask interview questions q think worse large number patients actually cancer labelled low risk computer system b large number patients cancer labelled high risk computer system q quantify much worse chosen option comparison quantify personally ie x worse q questions presented task decide system prefer doctor use q diﬃcult making choices q additional information helped make choices q feedback us experience today user study hired ten subjects total preliminary study study conducted video call participants asked share screen ﬁlled questionnaire ﬁrst page distributions responses questionnaire provided table rest responses regarding confusion matrices screen share logged ui task done web ui showed ‘thank ’ page asked subjects close web browser screen share subjects asked posttask thinkaloud interview questions shown table reﬂect performed given task responses interviews help us formulate guidelines recommendations future research direction results section discuss results preliminary userstudy quantitatively qualitatively will try answer practical questions surround metric elicitation framework discussed beginning chapter speciﬁcally focus checking workﬂow practical implementation support reject hypothesis implicit user preferences can quantiﬁed using pairwise comparison queries table summary guidelines recommendations userstudy g whenever possible smoothen query space run binarysearch based algorithms reduced ﬁnite sample errors g depending search tolerance binarysearch show probabilities confusion matrix outofn samples bigger n better diﬀer entiate confusion matrices query g direction ﬂowchart based visualization confusion matrix can swapped total number labels shown left column total predictions right g perhaps showing ﬂowchart pairwise comparisons better showing ﬂowchart barchart together one may also just show false positives false negatives reduce information load g measure time respond query spending time queries comprise close confusion matrices lead credence noise model deﬁnition g terminology “labelled high risklow risk” can replaced “predicted g high risklow risk” avoid confusions regarding groundtruth label view postinterview question number one needs devise ui ask intuitive guess false negative cost also act baseline metric evaluation purposes see section g one can also toggle button shows percentages conditioned true classes ie addition false positive false negative one can false positive rate false negative rate aid making comparisons g extend description cancer diagnosis mention associated subjective cost excerpts cover diﬀerent aspects cost example much ﬁnancial burden false positive prediction put patient much emotional burden put possible sideeﬀects drugs etc testing assumptions regarding noise model work around ﬁnite samples visualizing confusion matrices pairwise comparisons eliciting actual performance metrics reallife scenarios evaluating quality recovered metric emphasize aim behind discussing results user study formulate guidelines recommendations future research practical metric elicitation provide recommendations discuss quantitative qualitative results summarize table quantitative results findings impact smoothened query space outof samples ﬁrst discuss impact smoothening upper boundary section since choose ask pairwise preferences confusion matrices directly classiﬁers provided way generate feasible confusion matrices section lie smoothened table elicited linear performance metrics ten subjects along fraction times elicited metric’s preferences matches subject’s preferences ﬁfteen evaluation queries subjects linear performance metric m tn tp tn tp tn tp tn tp tn tp tn tp tn tp tn tp tn tp tn tp s s s s s s s s s s version upper boundary discussed section working ﬁnite samples drawback elicitation routine can get stuck closest achievable confusion matrix ﬁnite samples need optimal within given small tolerance ﬁnd working smoothened version almost always avoids asking pairs comprise confusion matrices thus guaranteeing better convergence within chosen binarysearch tolerance also note showing probabilities form bigger samples instead outofsamples allows us reduce cases confusion matrices pair comparisons becomes trivial eg false negatives diﬀerent false positives subjects elicited metrics quality evaluation next discuss metrics elicited ten subjects using web ui runs binarysearch based pro cedure algorithm back end search interval less equal subjects asked ﬁfteen queries use evaluation measure eﬀectiveness choose fraction times elicited metric’s preferences matches subject’s preferences ﬁfteen queries ie subject’s prefer query metric’s prefer query cid m × show elicited metric ﬁfteen subjects measure m values table see nine ten subjects time elicited metric’s preferences matches subject’s preferences ﬁfteen evaluation queries three subjects metric’s preference matches exactly evaluation queries absolute numbers m measure look good however good still missing piece study lack baseline future plan devise ways develop baseline metric elicitation task compare baseline measure m qualitative feedback ﬁrst describe general feedback observed discussed subjects user study video sessions formulate guidelines feedback mention excerpts posttask interviews formulating recommendations practical metric elicitation observations study sessions similar observation shen et al user study well also noted subjects comfortable answering simulationbased questions see figure possible reason direction ﬂowchart opposite conditioning probability asked questions barchart allows answer question easily however ﬁnd point ui subject becomes comfortable using ﬂowchart users asked postinterview session also mentioned help better pairwise comparison comparing confusion matrices ui observed rounds subjects tend look ﬂowcharts comparison may mean showing barcharts ﬂowcharts together overwhelming perhaps ﬂowcharts enough rounds subjects started comparing ﬂow false positives false negatives ﬂowchart suggests one may reduce information load showing false positives false negatives ﬂow chart although quantitatively measure time respond version ui observe subjects tend take time comparing two confusion matrices close ie queries later part binary search search interval narrow means subjects prone make errors queries leading credence noise model deﬁnition used manuscript throughout lastly study found subjects familiar machine learning confused terminology “labelled high risklow risk” predictions groundtruth labels one suggestion replace word “labelled” “predicted” posttask interview sessions now discuss posttask interviews formulate guidelines also mention excerpts anonymously interviews please see table interview questions q every subject clearly ﬁgured direction costs mentioned words s “ patient cancer predicted low risk costlier mistake patient cancer predicted high risk” q none subjects answer question full conﬁdence acts testimony importance metric elicitation framework often practitioners make guess quantify asymmetric costs classimbalanced learning however guess may far innate costs practitioner subjects agreed easier compare two confusion matrices using proposed visualizations answer question q subjects mention preferred one false negatives less although subjects looked tradeoﬀ example words s “ trying minimize false negatives large number false positives ” reﬂects subjects think hard tradeoﬀs q subjects mention deciding tradeoﬀs false positives false negatives diﬃcult words s “ diﬃcult pick preference false positives false negatives needed compared” subjects also mentioned words s “ cases numbers really close thus becomes diﬃcult select one ” feedback certainly agrees choice noise model manuscript see deﬁnition q responses question important constructing guidelines question varied responses one subject mentioned false positive rate false negative rate addition false positives false negatives helpful making comparisons words s “one can percentages arrow conditioned samples box ﬂowing” similarly subjects mentioned easier compare stages cancer mentioned predictions diﬀerent stages lead diﬀerence preferences subjects quote description associated costs excerpts cover diﬀerent aspects cost least subjectively described beginning study example words s “ much ﬁnancial burden false positive prediction put patient much emotional burden put possible sideeﬀects drugs etc highlighted beginning” q subjects enjoyed exercise liked web ui subjects mentioned task allowed reﬂect closely important questions regarding perfor mance metrics machine learning concluding remarks created web userinterface ui practically elicit linear performance metrics real users binary classiﬁcation setup chose cancer diagnosis task domain involves asymmetric costs false positives false negatives build upon existing visualizations confusion matrices reﬁned capture preferences pairwise comparisons via userstudy demonstrated implementation binary performance metric elicitation procedure chapter make use real time user responses pairwise comparisons confusion matrices also proposed implemented evaluation scheme judge quality recovered metric using proposed web ui conducted preliminary user study ten subjects elicited linear performance metrics also compared quality recovered metric comparing responses elicited metric’s responses set randomly chosen pairwise comparison queries study also included posttask thinkaloud style interviews regarding utility framework using task results feedback posttask interviews presented guidelines recommendations practical implementation framework future plan build upon pilot study conduct comprehensive user study includes guidelines presented chapter subjects also plan extend current web ui elicit metrics multiclass classiﬁcation setup figure questionnaire ﬁrst page ui figure description cancer diagnosis along visualization confusion matrix figure two confusion matrices side side page asks questions compre hending confusion matrices figure two confusion matrices side side page asks questions comparing values two confusion matrices figure two confusion matrices side side page asks questions simulating scenario based values two confusion matrices figure two hypothetically created confusion matrices side side page asks questions pairwise comparison confusion matices since one confusion matrices worse false positives false negatives question deﬁnitive answer figure sample pairwise comparison query run binarysearch based procedure algorithm figure sample pairwise comparison query comprising randomly selected confusion matrices feasible region queries used evaluating quality recovered metric chapter conclusion future work typical default metrics machine learning accuracy applied classiﬁcation tasks may capture tradeoﬀs relevant problem hand thus optimizing default metrics can undesirable impact short longterm utility including fairness resulting predictions across sensitive subgroups since issues plague default fairness measures thesis formalized problem metric elicitation proposed principled framework determining supervised classiﬁcation metrics user feedback theoretical empirical avenues showed certain conditions metric elicitation equivalent learning preferences pairs classiﬁer statistics underlying metric linear binary classiﬁcation setup proposed elicitation strategy recover oracle’s metric whose query complexity decays logarith mically desired resolution also showed querycomplexity rates match lower bound extended strategies eliciting linearfractional binary classiﬁcation performance metrics broadened scope metric elicitation proposing strategies complicated multiclass classiﬁcation setting proposed two algorithms multiclass classiﬁcation metric elicitation use multiple binarysearch subroutines recover oracle’s linear metric one proposed algorithms assumes oracle’s metric dependent diagonal entries confusion matrices unique sparsity condition metric thus useful number classes large similar binary case provided algorithms eliciting linearfractional multiclass classiﬁcation performance metrics respect applications fairness devised novel strategy elicit groupfair performance metrics multiclass classiﬁcation problems multiple sensitive groups also includes selecting tradeoﬀ predictive performance fairness violation procedure exploited piecewise linearity metric groupspeciﬁc predictive rates used binarysearch based subroutines recovered metric linear query complexity interesting note able elicit nonlinear metric maintaining query complexity order linear number unknowns linear elicitation case used tools geometric characterizations build far solve three impor tant problems beneﬁt practical aspects proposed framework ﬁrst involved increasing complexity elicited metrics second exploit current linear elicitation framework train deep neural networks optimizing black box metrics third conduct realuser study order elicit realuser metrics reﬂect practical nuances framework draw conclusions applications strategies linear quasilinear functions classiﬁer statistics can restric tive domains metrics complex nuanced thus proposed novel strategies eliciting metrics deﬁned quadratic functions classiﬁer statistics can easily applied fair metric elicitation setups well thus able handle general family metrics can better capture practitioner’s innate preferences generalized quadratic elicitation strategy higherorder polynomial functions metric elicitation procedures shown robust ﬁnite sample oracle feedback noise considered learning optimize classiﬁcation metric deﬁned blackbox function confusion matrix proposed frank wolfe elicited gradient fweg method optimizing blackbox metrics given query access evaluation metric small validation set framework included common distribution shift set tings special cases unlike prior distribution correction strategies able handle general nonlinear metrics showed model estimate example weights importantly exploited fact example weights can seen gradient metric estimated metric elicitation procedure presence ma chine oracle experiments various label noise domain shift fair classiﬁcation setups conﬁrmed proposal compares favorably stateoftheart baselines application brieﬂy discussed procedure can extended optimize blackbox metrics presence human oracle providing pairwise comparison feedback lastly created web ui eliciting binary classiﬁcation performance metrics corporates enhanced visualizations confusion matrices obtaining pairwise feedback conducted preliminary userstudy binary classiﬁcation setup order elicit realusers’ performance metrics process touched upon several practical aspects related particular focused checking workﬂow practical implementa tion found support hypothesis implicit user preferences can quantiﬁed using pairwise comparison queries tested assumptions regarding noise model worked around ﬁnite samples elicited actual performance metrics reallife scenarios evaluated quality recovered metric using quantitative qualitative results pilot study formulated several guidelines recommendations practically implementing metric elcitiation framework envision problem metric elicitation important interesting chal figure metric elicitation predictive machine learning vision three axes show three diﬀerent nuances metric elicitation ﬁrst axis contain diﬀerent predictive machine learning problems second axis various forms performance metrics can elicited several oracle feedback noise models lie third axis thesis provides solution box shown yellow color covering parts larger problem metric elicitation lenging topic future many practical applications broad ﬁeld artiﬁcial intelligence underlying space open problems can broken three separate axes axes shown figure ﬁrst axis diﬀerent predictive machine learning problems classiﬁcation regression ranking etc type pre dictive problem involves new frontiers explored exploited like done manuscript example elicit ranking metrics one may require thorough understand ing space statistics summarize ranking eﬀects second axis one may deal various functional forms performance metrics can elicited currently focused eliciting quasilinear polynomial functions classiﬁer statistics metric elicitation becomes much challenging yet practical functional forms assumed third axis stretches diﬀerent forms oracle queries including various noise models direction guarantees applicability metric elicitation realworld scenarios expected contribution future solve entire space problems comprising three axes may result separate subﬁeld artiﬁ cial intelligence name – metric elicitation predictive machine learning metrics elicited sophisticated methods may created optimize metrics similar chapter thus entire line work will answer important open questions machine learning impact several multidisciplinary applications transform way machine learning systems deployed practice ml problemsbinary classification multiclass classificationmulticlassmultigroup classification ranking regression…oracle feedbackpairwise comparison query value query auction query…underdifferent noise modelsperformance metricslinear linear fractional quadratic polynomial concave nonparametric… appendix binary classification performance metric elicitation visualizing set confusion matrices clarify geometry feasible set visualize one instance set confusion matrices c using dual representation supporting hyperplanes steps population model assume joint probability x − y given fx u− ηx eax u− uniform distribution − parameter controlling furthermore integralcid degree noise labels large high probability true label contrary small separable regions classes mixed − eax dx ∈ r implying py ζ ∀ ∈ r generate hyperplanes take θ ∈ π set m m m cos θ sin θ let us denote xcid point probability positive class ηx equal optimal threshold proposition solving x equation eax mm m gives us − lncid m cid m cidcid xcid π− π−z projection z interval − m m ≥ bayes classiﬁer h predicts class region − xcid remaining region m m h opposite using fact y x hx independent m m ≥ t p m b m m t p m xcidcid − cid xcid eax dx t n m cid xcid xcidcid − eax dx t n m eax eax dx eax eax dx b c d e f figure supporting hyperplanes associated set feasible confusion matrices exponential model described equation middle white region c intersection halfspaces associated supporting hyperplanes now can obtain hyperplane deﬁned θ sample around thousand θcids ∈ π randomly obtain hyperplanes following process plot sets feasible confusion matrices c’s shown figure middle white region c intersection halfspaces associated supporting hyperplanes curve right corresponds confusion matrices upper boundary ∂c similarly curve left corresponds confusion matrices lower boundary ∂c− points ζ two vertices geometry degree rotationally symmetric around center point corresponds confusion matrix uniform random classiﬁer ie classiﬁer predicts classes equal probability input − ζ notice increase separability two classes via points ζ × − ζ becomes feasible words data completely separable corners topright bottom left achievable data ‘inseparable’ feasible set contains diagonal line joining passes proofs lemma feasible set confusion matrices c following properties t p t n ∈ c ≤ t p ≤ ζ ≤ t n ≤ − ζ ii ζ ∈ c − ζ ∈ c iii t p t n ∈ c ζ − t p − ζ − t n ∈ c iv c convex v c supporting hyperplane associated every normal vector vi supporting hyperplane positive slope tangent c − ζ ζ proof prove statements follows ≤ ph y ≤ py ζ similarly ≤ ph y ≤ py −ζ ii h trivial classiﬁer always predicts t p h prh y pry ζ t n h means ζ ∈ c similarly h classiﬁer always predicts t p h prh y t n h prh y pry − ζ therefore − ζ ∈ c iii let h classiﬁer t p h t p t n h t n now consider classiﬁer − h predicts exactly opposite h t p − h p − h y py − ph y ζ − t p h similar argument gives t n − h − ζ − t n h iv consider two confusion matrices t p t n t p t n ∈ c attained classiﬁers h h ∈ h respectively let ≤ λ ≤ deﬁne classiﬁer hcid predicts output classiﬁer h probability λ predicts output classiﬁer h probability − λ t p hcid phcid y ph y h hph h ph y h hph h λt p h − λt p h similar argument gives convex combination t n thus λt p h t n h − λt p h t n h ∈ c hence c convex v follows convexity iv boundedness vi bounded convex region ζ× − ζ contains points ζ − ζ true positively sloped supporting hyperplane will tangent ζ − ζ qed lemma boundary c exactly confusion matrices estimators form ληx ≥ t − ληx t ληx t − ληx ≤ t λ t ∈ proof prove boundary attained estimators forms consider solving problem constraint ph c ph t p f p ζ py t p f n get t p − t n c ζ − t p − t n − f p − f n c ζ − constant note confusion matrix two values t p − t n eﬀectively partitions c since confusion matrices attained varying c furthermore since t n t p − c − ζ aﬃne space line tptn coordinate system c ∩ least one endpoint pass box ζ × − ζ two endpoints due convexity boundedness c since line positive slope c ∩ single point tangent c − ζ ζ lemma part vi since aﬃne space positive slope claim two endpoints attained remains show t− maximizing minimizing t p h subject prh c happens estimators form hλ ληx t − ληx ≤ t respectively t ληx ≥ t − ληx t hλ let h estimator recall cid t p h x ηxph x x dfx clear constraint ph c optimal choice h puts weight onto larger values η one can begin classifying x positive class nx maximum one exhausts budget c let t ph t c hλ t must maximize t p h subject ph c t let λ ∈ chosen phλ t ≤ c ≤ ph similar argument shows tpminimizing boundary points attained qed ht−’s remark assumption ηx t ηx ≥ t ηx t ηx ≤ t thus boundary c confusion matrices estimators form ηx ≥ t ηx ≤ t t ∈ proof proposition note maximizing linear function convex set cases consider signs m m diﬀer maximum attained either − ζ ζ per lemma part vi two optimum depends whether m ≥ m ie sign m m easy check four possible cases statement holds noting four cases ≤ mm m ≤ m m ≥ maximum attained ∂c proof gives desired result know lemma h must form ηx ≥ t t suﬃces ﬁnd t thus wish maximize mt p ht mt n ht now let z ηx random variable obtained evaluating η random x assumption dfx dfz cid xηx≥t cid t t p ht ηx dfx z dfz similarly tnht cid t − z dfz therefore cidmt p ht mt n htcid −mtfzt ·m − tfzt ∂ ∂t critical point attained t mm m desired similar argument gives converse result m m m m maximum attained ∂c− argument identical proof gives desired result qed proof proposition c convex bounded already proven lemma see c closed note lemma every boundary point attained lemma part iii follows c degree rotationally symmetric around point ζ −ζ recall every boundary point c can attained thresholding estimator discussion section every boundary point optimal classiﬁer linear performance metric vector deﬁning linear metric exactly normal vector supporting hyperplane boundary point vertex exists point supported one tangent hyper plane two dimensional space means optimal one linear metric clearly hyperplanes corresponding slope metrics m m opposite sign ie hyperplanes positive slope support either ζ − ζ least two supporting hyperplanes points make vertices now remains show vertices set c now consider case slopes hyperplanes negative ie m m sign corresponding linear metrics know proposition optimal classiﬁers linear metrics threshold classiﬁers therefore exist one threshold classiﬁer form ht ηx ≥ t confusion matrix let’s call ht ht two thresholds t t ∈ means ηxdfx ηxdfx xηx≥t xηx≥t cid cid hence multiple values η never attained contradicts g strictly decreasing therefore vertices ζ − ζ c now show supporting hyperplane tangent multiple points ie ﬂat regions boundary suppose hyperplane supports two points boundary exist two threshold classiﬁers arbitrarily close threshold values confusion matrices wellseparated therefore must exist value η exists nonzero probability contradicting continuity g discussion conclude assumption every supporting hyperplane convext set c tangent one point makes set c strictly convex qed proof lemma will prove result φ◦ ρ ∂c argument ψ◦ ρ− ∂c essentially simplicity drop symbols notation recall function quasiconcave superlevel sets convex given φ quasiconcave let s superlevel set φ ﬁrst want show r s t ρr ∈ s ρt ∈ s ρs ∈ s since ρ continuous bijection due geometry c lemma proposition must — without loss generality — t p ρr t p ρs t p ρt t n ρr t n ρs t n ρt otherwise swap r t since set c strictly convex image ρ ∂c ρs must dominate componentwise point convex combination ρr ρt say point z since φ monotone increasing x ∈ s ⇒ y ∈ s y ≥ x componentwise thereofore φρs ≥ φz since s convex z ∈ s due argument ρs ∈ s implies ρ−∂c ∩ s interval therefore convex thus superlevel sets φ ◦ ρ convex quasiconcave desired implies unimodaltiy function real line one local maximum can quasiconcave consider superlevel set value slightly less lowest two peaks qed proof proposition proof denote t p t n c c respectively let us take linearfractional metric φc pc pc p qc qc q p q p q zero simultaneously want φc monotonic tp tn bounded c ∈ c φc can add large positive constant φc ≥ still metric remain linear fractional suﬃcient assume φc ≥ furthermore boundedness φ implies φc ∈ d r cid d ≥ therefore may divide φc d φc ∈ c ∈ c still metric linear fractional φc ∈ taking derivative φc wrt c ∂φc ∂c p qc qc q − qpc pc p qc qc q ≥ ⇒ pqc qc q ≥ qpc pc p denominator positive numerator positive well • case denominator qc qc q ≥ – case q ⇒ p ≥ qφc ⇒ p ≥ q sup c∈c ⇒ p ≥ qτ φc necessary condition considering suﬃcient condition means τ can vary hence suﬃcient condition monotonicity c p ≥ q furthermore p ≥ well – case b q ⇒ p ≥ qτ since q τ ∈ suﬃcient condition p ≥ case well – casec q p ≥ q p ≥ ⇒ p ≥ p ≥ q p ≥ suﬃcient conditions similar case holds c implying p ≥ q p ≥ • case denominator qc qc q negative cid pc pc p cid qc qc q p ≤ q ⇒ p ≤ qτ – casea q p ≤ q p ≤ suﬃcient condition – caseb q ⇒ p ≤ q q ⇒ p suﬃcient condition – casec q ⇒ p ≤ p ≤ q suﬃcient condition cases p ≤ q p ≤ suﬃcient conditions similar case holds c resulting p ≤ q p ≤ suppose points denominator positive c ⊆ c suppose points ⊆ c gradient nonnegative points belonging − denominator negative c c suﬃcient condition p ≥ q p ≥ p ≥ q p ≥ gradient nonnegative points belonging c − suﬃcient condition p ≤ q p ≤ p ≤ q p ≤ c c− empty sets gradient nonnegative p p q q possible deﬁnition described hence one c c− empty wlog assume c− empty conclude c c immediate consequence wlog can take numerator denominator positive suﬃcient conditions monotonicity follows p ≥ q p ≥ p ≥ q p ≥ now let us take point feasible space ζ know φζ pζ p qζ q ≤ τ ⇒ pζ p ≤ τ qζ q ⇒ p − τ qζ p − τ q ≤ ⇒ p − τ q ≤ − p − τ q cidcid cid positive positive cid ζcidcidcidcid ⇒ p − τ q ≤ metric bounded gives us pc pc p qc qc q ≤ ⇒ pc pc p ≤ qc qc q ⇒ q ≥ p − qc p − qc p ∀c ∈ c hence suﬃcient condition q p − qζ p − q − ζ p equation derived monotonicity implies • case q ≥ ⇒ p ≤ suﬃcient condition • case b q ≤ ⇒ p ≤ q ≤ suﬃcient condition since numerator positive c ∈ c p p ≥ suﬃcient condition p p finally monotonic bounded linear fractional metric deﬁned φc pc pc p qc qc q p ≥ q p ≥ p ≥ q p ≥ q p − qζ p − q − ζ p p p q p q simulataneously zero can divide numerator denominator p p without changing metric φ suﬃcient conditions therefore elicitation purposes can take p p qed proof proposition proof well use t p c t n c since linear fractional matrix monotonically increasing c c maximized upper boundary ∂c hence m ≥ m ≥ running algorithm get hyperplane p − τ q αm p − τ q −α mc∗ cid p − τ q αm mc∗ cidcid c cid since p − τ q ≥ m ≥ ⇒ α ≥ discussed main paper avoid case α therefore α equation implies p α − p α − τ q α τ q α m −c p α − τ q α m assume pcid equations turns p α pcid p α qcid q α qcid q α pcid p α qcid q α system − τ qcid pcid m − τ qcid pcid pcid − τ qcid qcid pcid m −c qcid qcid φcid metric deﬁned pcid suﬃcient conditions assumptions ie monotonic bounded satisﬁes pcid ≥ qcid ≥ qcid pcid qcid pcid − qπ pcid pcid − qcid ≥ pcid π pcid pcid ≥ discussed chapter solving system harm elicitation task simplicity replacing “ cid ” notation normal one p − τ q m p − τ q −c last equation τ cp q p − τ q m putting rest gives us qp − c pq mq qp − c pq mq already q p − qζ p − q − ζ p p − ζ − q − ζ pζ − q p ζ ⇒ q gives us q q q deﬁne c pp − ζ pζ p pζ p − ζ p c − mζ − m − ζ p − mp − ζ pζ p pζ p − ζ p c − mζ − m − ζ p − mp − ζ pζ p pζ p − ζ p c − mζ − m − ζ p p − ζ pζ p q p c − mζ − m − ζ hence q c p p q q p − m p q q p − m p q now using suﬃcient conditions p ﬁnal solution following q c p q q p − m p q q p − m p q p pζ p − ζ q p c − mζ − m − ζ taken p p original pcid α therefore learn cidφc qed cidφc αφc pcid corollary fβmeasure β unknown algorithm elicits true per formance metric constant olog  queries oracle proof algorithm gives us supporting hyperplane tradeoﬀ bayes con fusion matrix know p can use proposition compute coeﬃcients fβmeasure p require algorithms qed proof theorem prove points one one direct consequence representation points boundary via supporting hyperplanes section search maximizer mimimizer also get associated supporting hyperplane well ii nature binary search eﬀectively narrowing search interval around target angle θ furthermore since oracle queries correct unless φ values within ω must φcθ − φcθ ω output θcid θ − θcid  now want check bound φcθcid − φcθ order will also consider threshold corresponding supporting hyperplanes cθ’s ie δθ sin θsin θ cos θ notice φcθ − φcθcid φcθ − φcθ φcθ − φcθcid ≤ φcθ − φcθ φcθ − φcθcid ﬁrst term bounded ω due oracle assumption bounds second term consider following t p cθ − t p cθcid cidcidcidcidcidcidcidcidcid cidcidcidcidcidcidcidcidcid cidcidcidcidcidcidcidcidcid cidcidcidcidcidcidcidcidcidcid ≤ cid ηx dfx ≥ηx≥ sinθcid sinθcidcosθcid −δ≥ηx−δ≥ sinθcid dfx sinθcidcosθcid −δ cidcidcidcidcidcidcidcidcid cidcidcidcidcidcidcidcidcid cid x x sinθ sinθcosθ sinθ sinθcosθ x sinθ sinθcosθ − sinθ sinθcosθ ≥ηx−δ≥ sinθcid sinθcidcosθcid − sinθ sinθcosθ x sinθ−θ sinθθcosθ−θ ≥ηx−δ≥ sinθcid sinθcidcosθcid − sinθ sinθcosθ cid cid cidcidcidcidcidcidcidcidcid dfx cidcidcidcidcidcidcidcidcidcid dfx k ω δ − δ inequality second step follows fact ηx ≤ recall left term integral limits actually δθ − δθ φcδθ − √kω proof statement given φcδθ proof theorem proved later since sin lipschitz adding subtracting sin θsin θ cos θ right term integration limit gives us minimum value right term −− implies quantity less cid pηx − δ ≤ k ≤ pδ − ηx ≤  ≤ kω ∩ δ − ηx ≤  k assumption kω k kω cid cid cid k k kω k √ kω k shows dependency suﬃciently small  pa ∩ b ≤ minpa pb inequality used second step rather loose tolerance  depending pηx − δ sheer big value  nevertheless φc − φccid ≤ · cidc − ccid similar result applies true negative rate since φ lipschitz independent cid kω k cid cidcθ − cθcidcid∞ ≤ k k φcθcid − φcθ ≤ √ k k cid kω k ω since metrics ω ∈ therefore √ω ≥ ω gives us desired hence result iii needed part ii interval possible values θcid  target angle θ ideally obtained making log queries due region oracle misreport preferences can oﬀ target angle θ  however binary search will put us back correct direction leave misreporting region time even oﬀ target angle θ will closer therefore interval possible values θcid  require least log  rounds algorithm constant number pairwise queries qed proof lemma ﬁxed  divide search space θ bins length  resulting cid classiﬁers function evaluated classiﬁers unimodal cid  operation allowed pairwise comparison optimal worst case complexity ﬁnding argument maximum function evaluations olog  achieved binary search qed proposition let y x hx yn xn hxn n iid samples joint distribution y x hx h¨oﬀding’s inequality pcidcidcid n cid hi yi − t p hcidcid ≥  cidn ≤ e−n qed holds analogous estimator tn proof direct application h¨oﬀding’s inequality proof theorem will show threshold classiﬁers statement assumption diﬃcult extend argument case querying angles involves good bit trigonometric identities recall threshold estimator hδ returns positive ηx ≥ δ zero otherwise let δ threshold maximizes performance respect φ cδ confusion matrix simplicity suppose δcid δ recall assumption prηx ∈ δ − k k  δ ≤ k prηx ∈ δ −  δ ≥ k therefore cid ηx ∈ δ −  δ − k k  ≥ k denoting φc cidm ccid since δ mm m expanding integral get pcid cid m − ηx − mηx dfx xδcid≤ηx≤δ m − ηx − mηx dfx xδ−δ−δcid≤ηx≤δ φcδ − φcδcid cid cid ≥ m mcid −m xδ−δ−δcid≤ηx≤δ− k k ≥ m m m − ηx − mηx dfx δ−δcid δ − δcidcid m × k k cid xδ−δ−δcid≤ηx≤δ− k k m m δ − δcid · k k ≥ k k k δ − δcid × pδ − δ − δcid ≤ ηx ≤ δ − δ − δcid δ − δcid k k k k δ − δcid dfx δ−δcid similar results hold δcid δ therefore φc − φcδcid ω √kω thus regime oracle misreporting must δ − δcid preference ordering must case thresholds suﬃciently close optimal threshold k proof theorem tolerance  small binary search closes parameter θcid φcδθcid within ω optimum discussion also implies search interval close true value thus √kω since δ mm m total error threshold  k bound extends cost vector factor √ thus giving desired result observe theorem actually provide bounds slope hyper planes thus guarantees lfpm elicitation follow naturally requires recover slope upper boundary lower boundary correctly within bounds theorem provides guarantees algorithm independent oracle queries thus can run high precision making solutions two systems match qed proof lemma suppose performance metric oracle characterized parameter θ recall bayes optimal classiﬁer hθ η ≥ δ let us assume given classiﬁercidhθ cidη ≥ δ notice optimal threshold δ property metric classiﬁer η want bound diﬀerence confusion matrices two classiﬁers notice assumption can take n suﬃciently large cidη −cidηncid∞ arbitrarily small consider quantity t p hθ − t p cidhθ cid cid cidη≥δ η dfx − η≥δ η dfx now maximum loss quantity can occur region classiﬁers’ predictions diﬀer cidη less η maximum possible diﬀerence equal cid η dfx xδ≤ηx≤δcidη−cidηcid∞ ≤ pδ ≤ ηx ≤ δ cidη −cidηcid∞ ≤ kcidη −cidηcid∞ t p cidhθ − t p hθ cid cidη≥δ η dfx − cid η dfx η≥δ assumpition similarly can look maximum gain following quantity qed now maximum gain quantity can occur region classiﬁers’ predictions diﬀer therecidη greater η maximum possible diﬀerence equal cid xδ−cidη−cidηcid∞≤ηx≤δ η dfx ≤ pδ − cidη −cidηcid∞ ≤ kcidη −cidηcid∞ ≤ ηx ≤ δ assumpition hence t p cidhθ − t p hθ ≤ kcidη −cidηcid∞ similar arguments apply t n gives us desired result extended experiments section empirically validate theory robustness ﬁnite samples table empirical validation lpm elicitation tolerance  radians φ∗ cidφ denote true elicited metric respectively φ∗ m∗ cidφ cidm φ∗ m∗ cidφ cidm synthetic data experiments take distribution noise parameter lpm elicitation case deﬁne true metric φ∗ m∗ m∗ deﬁnes query outputs line algorithm run algorithm check whether get metric results monotonically increasing monotonically decreasing lpm shown table achieve true metric even tight tolerance  radians m∗ q∗ q∗ p∗ q∗ run algorithm  ﬁnd hyperplane cid maximizer ∂c algorithm  ﬁnd hyperplane cid minimizer ∂c− algorithm n confusion matrices ∂c ∂c− obtained varying parameter θ next elicit lfpm deﬁne true metric φ∗ p∗ uniformly π π π ∆ gives us elicited metric cidφ represent cidpcidp cidqcidqcidq table present elicitation results ratio elicited metric cidφ true metric φ set confusion matrices column lfpms column also present mean α standard deviation σ p∗ table suggested corollary know true ratio p∗ can elicit lfpm constant using algorithm resulting better estimate true metric avoid errors due algorithms line measure respectively line table represent f measure f cases assume knowledge p∗ line line correspond arbitrarily chosen linear fractional metrics show eﬃcacy proposed method better judgment show function evaluations true metric elicited metric selected pairs t p t n ∈ ∂c used algorithm figure true elicited metric plotted together sorting values based slope parameter θ see elicited metric constant multiple true metric vertical solid dashed line corresponds argmax true elicited metric respectively figure see argmax true elicited metrics coincides thus validating theorem table lfpm elicitation synthetic distribution section magic m dataset section  radians p∗ denote true lfpm cidpcidp cidqcidqcidq denote elicited lfpm α σ denote mean standard deviation ratio elicited true metric evaluated confusion matrices ∂c used algorithm respectively empirically verify elicited metric constant multiple α true metric q∗ p∗ q∗ q∗ results synthetic distribution section results real world dataset m section p∗ p∗ true metric q∗ q∗ q∗ cidpcidp cidqcidqcidq α σ cidpcidp cidqcidqcidq α σ realworld data experiments realworld datasets know ηx ﬁnite samples thus feasible space c well behaved shown figure poses challenge elicitation task now validate elicitation procedure two realworld datasets datasets breast cancer bc wisconsin diagnostic dataset containing instances b magic m dataset containing instances datasets standardize attributes split data two parts s s s learn estimatorcidη using regularized logistic regression model regularizing constant λ λ use s making predictions computing sample confusions generated twenty eight diﬀerent lpms φ∗ generating θ∗ say m∗ cos θ∗ sin θ∗ fourteen ﬁrst quadrant starting π radians π radians step π radians similarly fourteen third quadrant starting π π step π radians use algorithm algorithm diﬀerent tolerance  diﬀerent datasets diﬀerent regularizing constant λ order recover estimate cidm compute error terms proportion number times algorithm algorithm failed recover true m∗ within  threshold report results table see improved elicitation dataset m suggesting improves larger datasets particular dataset m elicit metrics within threshold  radians also observe  overly tight tolerance datasets leading many failures elicitation routine gets stuck closest achievable confusion matrix ﬁnite samples need optimal within given small tolerance furthermore observations consistent regularized logisitic regression models regularizer λ table line column b table line column c table line column d table line column e table line column f table line column figure true elicited lfpms synthetic distribution table solid green curve dashed blue curve true elicited metric respectively solid red dashed black vertical lines represent maximizer true metric elicited metric respectively elicited lfpms constant multiple true metrics maximizer solid red dashed black vertical lines overlap next discuss case lfpm elicitation use true metrics φ∗ described section follow process eliciting lfpm time work magic dataset table columns present elicitation results magic dataset along mean α standard deviation σ ratio elicited metric true metric better judgment show function evaluation true metric elicited metric selected pairs t p t n ∈ ∂c used algorithm figure ordered parameter θ although observe argmax diﬀerent two six cases see subﬁgure b subﬁgure c due ﬁnite samples elicited lfpms almost equivalent true metric constant monotonically decreasing case π c∗ oracle’s metric monotonically decreasing tp tn can ﬁnd support ing hyperplanes maximizer minimizer require pose one query ωc∗ π response determines whether want search ∂c ∂c− apply algorithms accordingly c∗ π metric monotonically decreasing search maximizer lower boundary ∂c− viceversa π ≺ c∗ table lpm elicitation results real datasets  radians m bc represent magic breast cancer dataset respectively λ regularization parameter regularized logistic regression models table shows error terms proportion number times algorithm algorithm failed recover true m∗θ∗ within  threshold observations made chapter consistent models λ λ m bc m bc  table line column b table line column c table line column d table line column e table line column f table line column figure true elicited lfpms dataset m table solid green curve dashed blue curve true elicited metric respectively solid red dashed black vertical lines represent maximizer true metric elicited metric respectively see elicited lfpms constant multiple true metrics almost maximizer solid red dashed black vertical lines overlap except two cases appendix b multiclass classification performance metric elicitation let fx marginal distribution x b shrinkinterval shrinkinterval subroutines notice shrinkinterval subroutines work responses four queries based responses divides interval two since metric dealt algorithm concave unimodal see lemma remark four queries required shrink interval half every iteration since use enclosed sphere lpm elicitation can shrink interval half based just two queries algorithm ie querying ωcd cc ωce cd due strong convexity sphere see proof theorem however show use four queries algorithm just make algorithms consistent readers understand b proofs section proof proposition following properties d • convex let us take two classiﬁers h h ∈ h achieve diagonal confusions dh dh ∈ d need check whether exists classiﬁer achieves oﬀdiagonal confusion λdh−λdh consider classiﬁer h probability λ predicts classiﬁer h predicts probability − λ predicts classiﬁer h predicts ﬁrst component dh py h py h hh hph h py h hh hph h λdh − λdh b similarly hold true dih ∈ k hence c convex • bounded since di p y h ≤ ζi ∈ k d ⊆ ζ × ··· × ζk • strictly convex closed since c convex boundary intersection half spaces furthermore linear functional maximized boundary convex set suppose given diagonal linear functional dlpm bo classiﬁer ha subroutine shrinkinterval c d input oracle responses ωd b d e c d d ωd d ωd d c mb md d ψ e ≺ d ≺ d ≺ d d d d c d cid d cid d cid d e d mb md e ma mc mb b ma md ωd cid d d c elseif elseif elseif else ma md output ma mb ma mc md m mb figure b left description subroutine shrinkinterval right visual intuition subroutine shrinkinterval search maximizer quasiconcave metric ψ subroutine shrinks current interval half based oracle responses four queries subroutine shrinkinterval input oracle responses ωcc ca ωcd cc j θd j ca cid cc θb ωce cd ωcb ce j ∈ q ca ≺ cc cid cd θb cc ≺ cd cid ce θa cd ≺ ce cid cb θa j θd j j θb j j θd j j θc j θb j θd j elseif elseif elseif else set θa output θa j θe j figure b formal description subroutine shrinkinterval shrinkinterval shrinkinterval except applies parameter θj works responses oﬀdiagonal confusions based queries function given proposition b whose proof discussed later let value achieved corresponding bo diagonal confusion d α kcid cid kcid x α aidi aiηixhax ix xdfx b now want construct another classiﬁer achieves value α weight shift one class another class without changing maximum value α note paiηix ajηjx j ∈ k due assumption hence unique maximizer linear functional boundary therefore space strictly convex one characterization boundary space ∂d can given bo diagonalconfusions corresponding linear functional diagonal confusions achieved corresponding bo classiﬁers therefore diagonal confusions always achievable space closed well • vi always achieved easy see trivial classiﬁer predicts class ∈ k will achieve diagonal confusion deﬁned vi • vi vertices certainly vertex exists point supported k tangent hyperplanes k dimensional space means vertex optimal k linear metric linear functional clearly metrics slope ai aj al ∀ l ∈ k l cid j support vi least k supporting hyperplanes points make vertices now show vertices suppose point vi’s supported two hyperplanes given slopes proposition b discussed later can get bayes optimal classiﬁers ha ha achieve diagonal confusions means cid cid ηj x≥tj j∈··· k x ηx ηj x≥tcid x ηx j j∈··· k ηxdfx ηxdfx b ie ﬁrst component d equal two classiﬁers tj tcid j’s depen dent since classiﬁers diﬀerent least one j tj cid tcid j will mean multiple values ηx ηj x attained contradict assumption gj strictly decreasing strict convexity supporting hyperplane tangent multiple points hence vi vertices set d since take classiﬁers predict classes k k values diagonal confusion d ∈ dkk evaluate zero indices except k k therefore properties space dkk can proved similar lines chapter qed proof proposition following properties space c • convex space convex follows ﬁrst point proposition • bounded cij py h j ≤ py ζi j ∈ k confusion c ⊆ ζk− × ζk− × ··· × ζkk− • ui’s o always achieved classiﬁer always predicts class will achieve confusion matrix ui thus ui ∈ c ∀ ∈ q furthermore classiﬁer predicts similar one trivial classiﬁers probability k will achieve confusions o centroid matrices written row major form excluding diagonal terms easy see • ui’s vertices supporting hyperplane slope ai aj al l ∈ k l cid j will supported u corresponding bo classiﬁer predict class thus u supported least q hyperplanes thus becomes vertex convex set similar case ui’s qed proposition can considered corollary following general proposition proposition b let ψ ∈ ϕdlp m parametrized hx argmax ∈k aiηix hx argmin ∈k aiηix b bo ibo classiﬁers wrt ψ respectively proof let cid ψ aidi cid cid x aiηixhx b mathematical form easy see metric achieves maximum class maximizes expected utility conditioned instance predicted metric achieves maximum classiﬁer deterministically predicts class argmaxj∈k ajηjx form classiﬁer written proposition similarly metric minimized classiﬁer minimizes expected utility conditioned instance predicting class argminj∈k ajηjx qed proof proposition recall classiﬁers predict class k k will achieve diagonal confusions zeros every index except k k therefore aidi akdk akdk akηkxhx k cid x akηkxhx k b cid cid ψ x cid hkkx using idea used previous proof metric achieves maximum class maximizes expected utility conditioned instance predicted therefore cid b k k akηkx ≥ akηkx ow rbo classiﬁer restricted classes k k respect ψ furthermore ribo classiﬁer given hkkx khkkx k khkkx k ribo classiﬁer exactly opposite rbo ie predicts class k wherever rbo predicts class k instance space x viceversa qed proof lemma suppose origin o constrained set sphere sλ radius λ centered o want maximize cida ccid c ∈ sλ since linear metric convex set maximized boundary easy see ci λai will maximize metric moving reference point original origin ie q gives us required answer qed b proofs section write lemma following general form lemma b let ψ d → r ξ d → r quasiconcave quasiconvex function k k ∈ k let ρ → ∂d monotone increasing dik ρ− → ∂d boundary composition ψ ◦ ρ → r ξ ◦ ρ− → r quasiconcave kk continuous bijective parametrization upper lower − kk quasiconvex thus unimodal interval r r proof function quasiconcave iﬀ superlevel sets convex already know proposition dkk convex moreover vector diagonal confusions zeros every index except indices k k let ψ d → r quasiconcave metric implies d r ψ d ∈ d ψd ≥ r convex now consider superlevel sets l superlevel sets ψ restricted diagonal confusions dkk ie l ψ d ∈ ψd ≥ r take d d ∈ l ψ since d d ∈ d well belong dkk d d r ψ convex hence t ∈ td − td ∈ l set l r ψ implies ψtd − td ≥ r furthermore td − td ∈ dkk dkk convex two arguments td − td ∈ l ψ implies dkk ψ convex hence ψ restricted dkk quasiconcave proof analogously l follows quasiconvex metric ξ now remains show ψ ◦ ρ → r ψ ◦ ρ− → r quasiconcave dkk dkk dkk quasiconvex can proved readily extending proof lemma chapter diagonal multiclass case sake completeness also provide proof essentially simplicity drop symbols notation given argument ξ ◦ ρ− ∂d will prove result ψ ◦ ρ ∂d − kk kk r r ψ quasiconcave let s superlevel set ψ ﬁrst want show r s t ρr ∈ s ρt ∈ s ρs ∈ s since ρ continuous bijection due geometry dkk must — wlog — dkρr dkρs dkρt dkρr dkρs dkρt otherwise swap r t since set dkk strictly convex image ρ ∂dkk ρs must dominate componentwise point convex combination ρr ρt say point z since ψ monotone increasing x ∈ s ⇒ y ∈ s y ≥ x componentwise therefore ψρs ≥ ψz since s convex z ∈ s due argument ρs ∈ s implies ρ−∂dkk ∩ s interval therefore convex thus superlevel sets ψ◦ ρ convex quasiconcave desired implies unimodaltiy function real line since function one local maximum can quasiconcave consider superlevel set value slightly less qed lowest two peaks b proofs section olog proof theorem chapter shown binary classiﬁcation inner loop algorithm will estimate value cidm bayesoptimal binary classiﬁer corresponding linear metric ∗ m∗ − m∗ ∈ r cidm − m∗  √ω  iterations now multiclass case allows us argue ≤ j ≤ k can estimate value mij ∗ required guarantees wlog assumed throughout algorithm ≥ ak k satisy condition can always choose dex z ∈ k satisfy following procedure j − mijmij ∗ set z ← t ··· k compute estimate cidmtz mtz cidmtz z ← t else nothing end output z let ε  √ω now cidmtz ε can shown ratio least − ε therefore z ﬁnal coordinate output must az ≥ − εkat t − εk ≈ e−kε ε suﬃciently small az ≥ t desired now assumption may proceed show ε −ε ∗ − ε t ≥ ∗ z · f x xa x∗ xb x sλ ¯cid∗ f∗ µθ µθ o µθ∗ figure b left function semicircle unit radius right visual intuition distance boundary points tangent place optimal oﬀdiagonal confusions algorithm correct wish show cidcidacidaz − aazcid∞ oε cidcidcidcidcidatcidaz − az cidcidcidcid cidcidcidcid −cidmtcidmt − ≤ mt − ε − cidcidcidcid cidcidcidcid cidcidcidcid cidmt − cid cid − ε − mt − mt mt mt ≤ mt ε gives us deisred bound ≤ · ε − ε ≤ ε b qed proof theorem consider geometry shown figure b left shows function f − q → r follow trajectory unit semicircle semisphere let x qdimensional vector function given cidcidcidcid − qcid f x − x b intuitively function evaluates distance points lying surface semisphere point x∗ origin unique minimizer function let us restrict domain function points q xa xb xa − componentwise xb componentwise easy see derivative function cid ∇f cid xcid − cidq x xqcid − cidq x b continuously diﬀerentiable compact domain q thus ∇f lipschitz lipschitz parameter l ie cid∇f y − ∇f xcid ≤ lcidy − xcid b makes function f lsmooth addition observe cidcidcidcid − qcid f x − qcid x ≥ x b implies exists paraboloid always function f deﬁnition makes function f strongly convex function say strong convexity parameter τ thus function satisﬁes requirements ie smoothness strong convexity unique minimizer inherit guarantees derivative free optimization notice apply coordinatewise binary search algorithm inner loop run log queries minimize function using pairwise comparison queries ie oracle responds point evaluate lesser value f two τ log f x−f x∗ theorem one can guarantee l qlτ q log queries oracle can get estimate minimizer xt f xt − f x∗ qlτ notice function f x − f x∗ f x − f x ≤ now simplicity assume λ discussed lpm elicitation problem queries asked sphere sλ dual form use q − dimensional bijective parametrization based θ denote points surface sphere notice parametrization function sin cos hence lipschitz well due monotonicity condition assume points lie one orthant sphere now suppose true oracle’s metric denoted ∗ ∗ j sin θj cos θi ∈ q − ∗ j sin θj let us denote parametrization lpms υ ie ∗ υθ∗ hyperplane tangent unit sphere particular point whose coordinates υθ∗ since metric linear posing pairwise comparisons oracle ask oﬀdiagonal confusion closer hyperplane reach tangent point boundary sphere pairwise comparisons actually decreasing distancelike function f∗c shown figure b right function can represented f∗θ − cidυθ∗ υθcid υθ∗ ﬁxed coeﬃcients θ changes algorithm equivalent f function discussed thus using guarantees z logzqq − log queries oracle z z constants independent  q q πq− πi− f∗θ − f∗θ∗ f∗θ − − cidυθ∗ υθcid ≤ zq b z constant depending curvature function f implies cida∗ cidcidacid −cidacid cida∗ − cida∗cidacid cid − cida∗cidacid ≤ zq b  −cidacid ≤ o√q therefore cid can achieve point o√q close minimizer number ocidt log using inequality proved cida∗ iterations t ≥ z logzqq − term z logzq can considered number cycles due curvature sphere ﬁnd dominating factor query complexity example working sphere  − two cycles ie t q − algorithm suﬃces practice thus updating θj twice cycles suﬃcient obtaining required metric remains show whenever queried angle leastcidωλ optimal j optimal angle observe θj λ cosθj − θ∗ angle oracle gives correct response see restrict attention hyper plane current angle moving say j binarysearch phase loop let θ∗ j ≥ λ − ω oracle may return false value performance metric lipschitz linear map optimal value sphere radius λ λ however cosx ≤ −x θj − θ∗ j ≤ λ− λωλ λ− ω therefore j ≥ long θj − θ∗ j ≥ cidωλ λ cosθj − θ∗ cidωλ oracle provides correct answer binary search proceeds correct direction qed b finding sphere sλ now discuss suﬃciently large sphere sλ radius λ may found consider following optimization problem special case op problem corresponds feasiblity check problem given oﬀdiagonal confusion c small δ ∈ r min c∈c st cidc − ccid ≤ δ b solution problem exists algorithm returns basically approach will try construct classiﬁer whose oﬀdiagonal confusions δclose given oﬀdiagonal confusion c hence checking feasibility algorithm b computes value λ ≥ cidrk cidr radius largest ball contained set c notice algorithm run oﬄine impact query algorithm b approximating λ radius input center o feasible region classiﬁers j ··· q end let con v convex hull o ± cidjejq compute radius r largest ball can ﬁt inside con v centered o output λ r let ej standard basis vector jth dimension compute maximum cidj o cidjej feasible solving b j complexity notice approach consistent thus get good estimate sphere provided suﬃcient samples lemma b letcidr radius largest ball centered o ﬁts feasible space classiﬁers algorithm b returns radius λ ≥cidrk proof let cidj computed algorithm let cid minj cidj must cid ≥cidr contains ball radius cid√q cid√k − k ≥ cidk ≥cidrk λ ≥cidrk furthermore region con v contains convex hull o ± cidejq j region qed b proofs section proof proposition can add large positive constant d ∈ d ψd metric remain linear fractional suﬃcient assume ψd ≥ boundedness scale invariance ψ implies ψd ∈ without compromising linearfractional form now look suﬃcient conditions monotonicity dik numerator denominator positive consider derivative assuming denominator positive numerator positive acid bidi b − ∂ψ ∂d cid cid ≥ b aidi bidi b aidi bcid cid bidi b ≥ cid cid aidi bidi b ⇒ ≥ b sup d∈d ⇒ ai ≥ biτ b b condition necessary since τ ∈ considering three cases bi bi bi following suﬃcient conditions monotonicity ≥ b ≥ similarly true ai’s bi’s ie ai ≥ bi ai ≥ ∀ ∈ k monotonically increasing dlfpms furthermore assumed ψ ∈ ie cid cid bidi b ≤ ⇒ aidi cid ai − bidi ≤ b suﬃcient take b cid denominator positive addition can divide numerator denominator bycid without changing metric ψ therefore take cid iai − biζi make metric bounded ai ai elicitation task qed b proof proposition continue equation saw α ≥ additionally ignore case α since imply constant ψ∗ next may divide equations α sides coeﬃcients ∗ ∗ factored α change metric ψ∗ thus soe becomes − τ∗bcid acid notice none conditions assumption changed exceptcid si ∀ ∈ k cids d ai however may still use condition learn constant α times true metric putting harm elicitation problem last equation τ cids d rest equations gives us cidbcid b cid ∗ τ∗bcid ∗ acid − si ∗ cids d cid bcid bcid b replacing bcid sition rest equations gives us solution mentioned propo qed proof proposition recall metric φ monotonically decreasing ci’s lfpms transitional scale invariant wlog can assume φ ∈ − taking derivative c gives us assuming denominator positive numerator negative bcid cid aici bici b ≤ ∂φ ∂c acid biai b − cid cid aici bici b ≤ b ⇒ ≤ bφc ⇒ bτ b b condition necessary since τ ∈ − considering cases ie bi bi bi following suﬃcient condition monotonicity decreasing lfpms ≤ −b ≤ similarly true ai ≤ −bi ai ≤ ∀ ∈ q monotonically decreasing lfpms furthermore assumed φ ∈ − ie cid cid bici b ≥ − ⇒ aici cid −ai bici ≤ b suﬃcient take b cid cid ai without changing metric φ gives us conditioncid −ai biζi make metric bounded − denominator positive addition can divide numerator denominator qed ai − b proof proposition start saw α ≥ additionally ignore case α since imply constant φ∗ next may divide equations α sides coeﬃcients ∗ ’s factored α change φ∗ thus soe becomes ’s b∗ acid − τ∗bcid τ∗bcid cids c∗ cid notice none conditions assumption changed except cid si ∀ ∈ q ai − however may still use condition learn constant α times true metric harm elicitation problem similar dlfpms somehow know true acid ’s can elicit lfpm upto constant multiple last equation τ cids c∗ putting rest equations gives us cidbcid b acid − si cids c∗ cid bcid b b replacing bi rest equations gives us solution mentioned proposition qed b extended experiments section empirically validate theory investigate sensitivity robustness due ﬁnite sample estimates ease judgments show results corresponding classes k k results discussion extends larger number classes well show eﬃcacy proposed methods run experiments standard machine learning datasets datasets can downloaded httpswwwcsientuedutw cjlinlibsvmtoolsdatasetsmulti classhtml wwwcsientuedutw cjlinlibsvmtoolsdatasetsmulticlasshtml table b dlpm elicitation  synthetic data number queries used k k respectively since digits rounded two decimal cid cidcidacid might exactly equal one places cida∗ classes k ψ∗ ∗ cidψ cida classes k ψ∗ ∗ cidψ cida b dlpm lpm elicitation simulated data extended show extended set results experimental setting discussed section table b table b show elicitation results simulated data dlpms lpms respectively verify algorithms elicit true metrics even  expected require k − cidlogcid tcidlogπcid queries dlpm lpm elicitation respectively cid·cid ceil function t q − b eﬀect sphere size lpm elicitation realworld datasets algorithm agnostic error fromcidηi’s long get sphere inside feasible region suﬃcient size following experiment show incur errors elicitation radius λ order ω recall working simulated setting good proxy ω practical computation error considering three spheres size λ λ λ randomly selected hundered dlpms work k classes took λ × − performed elicitation ie ∗’s used algorithm  recover estimates cida’s −cidacid∞ ≤ ω diﬀerent table b report proportion number times cida∗ values ω see improved elicitation work λ incur errors sphere’s radius less particular take radius order little higher − perform perfect elicitation needless say working real oracle users magnitude oracle’s feedback noise ω size sphere will play role elicitation performance suggested theorem table b lpm elicitation  synthetic data number queries used k k respectively since digits rounded two decimal cid cidcidacid might exactly equal one places cida∗ classes φ∗ ∗ cidφ cida b dlfpm lfpm elicitation now validate elicitation dlfpms classes k k using routine discussed section use distribution setting section classes deﬁne true metric ψ∗ ∗ b∗ b∗ run algorithm  ﬁnd hyperplane cid maximizer ∂d algorithm  − algorithm ncid ﬁnd hyperplane cid minimizer ∂d us elicited metric cidψ represent cidacidbcidb table b table b diagonal confusions ∂d obtained varying parameter m δ gives cidψ true metric ψ∗ set diagonal confusions used algorithm column present elicitation results dlfpms classes k k respectively also present mean α standard deviation σ ratio elicited metric table b table b better judgment show function evaluations true metric elicited metric figure b true elicited metric plotted together vectorizing set diagonal confusions certain order based parametrizations expected see elicited metric constant multiple true metric now validate elicitation lfpms classes k k using routine run algo discussed section deﬁne true metric φ∗ ∗ b∗ b∗ table b lpm elicitation sphere varying radius  randomly chosen hundred ∗ show fraction times estimatescida obtained ×q−cidlogcid −cidacid∞ ≤ ω notice incur error radius order practical computation error can attributed ω simulated setting queries satisfy cida∗ ω λ × − × − × − denote true dlfpm cidacidbcidb denote elicited lfpm empirically table b dlfpm elicitation synthetic distribution k classes  ∗ b∗ b∗ verify elicited metric constant multiple α true metric true metric ∗ ∗ ∗ b∗ b∗ b∗ b∗ cidacidacida cidbcidbcidbcidb results synthetic distribution appendix b α σ rithm  ﬁnd hyperplane cid maximizer ∂s  ﬁnd hyperplane cid minimizer ∂s gives us elicited metric cidφ represent cidacidbcidb table b present oﬀdiagonal confusions ∂s standard deviation σ ratio elicited metric cidφ true metric φ∗ set − λ algorithm λ algorithm ncid − λ obtained varying parameter θ δ elicitation results lfpms classes k also present mean α oﬀdiagonal confusions used algorithm column table b better judgment show function evaluations true metric elicited metric evaluated selected oﬀdiagonal confusions top row figure b due many terms lfpm k skip providing true metric elicited metric mention α σ true elicited metric similar table b obtained α σ three metrics plotted bottom denote true dlfpm cidacidbcidb denote elicited lfpm empirically table b dlfpm elicitation synthetic distribution k classes  ∗ b∗ b∗ verify elicited metric constant multiple α true metric true metric ∗ ∗ ∗ ∗ b∗ b∗ b∗ b∗ b∗ cidacidacidacida cidbcidbcidbcidbcidb results synthetic distribution appendix b α σ row figure b true elicited metric plotted together vectorizing set confusions certain order based parametrizations expected elicited metric constant multiple true metric k k table b lfpm elicitation k classes  ∗ b∗ b∗ lfpm thirteen terms elicit lfpm cidacidbcidb denote elicited lfpm denote true empirically verify elicited metric constant multiple α true metric true metric ∗ ∗ ∗ ∗ ∗ ∗ b∗ b∗ b∗ b∗ b∗ b∗ b∗ results synthetic distribution appendix b cidacidacidacidacidacida cidbcidbcidbcidbcidbcidbcidb α σ table b line b table b line c table b line d table b line e table b line f table b line figure b true elicited dlfpms synthetic distribution tables b b solid green curve dashed blue curve true elicited metric respec tively see elicited dlfpms constant multiple true metrics table b line b table b line c table b line d lfp metric k e lfp metric k f lfp metric k figure b true elicited lfpms plots top row correspond metrics table b k bottom row corresponds metrics k solid green curve dashed blue curve true elicited metric respectively see elicited lfpms constant multiple true metrics appendix c fair performance metric elicitation c proofs details section proof proposition set rates rg group g satisﬁes following properties • convex let us take two classiﬁers hg ∈ rg need check whether convex combination αrg feasible ie exists classiﬁer achieve rate consider classiﬁer hg probability α predicts classiﬁer hg predicts probability − α predicts classiﬁer hg ∈ hg achieve rates rg − αrg predicts elements rate matrix rg ijh given hg rg ijh phg jy rg y iphg hg jhg hg − αrg phg αrg phg jhg hg y iphg hg c therefore rg ∀ g ∈ m convex • bounded since rg rg ⊆ q ijh p h jy p h j y ip y ≤ j ∈ k • ei’s o always achieved classiﬁer always predicts class will achieve rate ei thus ei ∈ rg ∀ ∈ k g ∈ m feasible just like convexity proof classiﬁer predicts similar one trivial classiﬁers probability k will achieve rates o • ei’s vertices supporting hyperplane slope cidi cidj cidp p ∈ k p cid j will supported e corresponding trivial classiﬁer pre dict class thus ei’s vertices convex set long classconditional distributions identical ie signal nontrivial classiﬁcation con ditioned group assumption one can construct ball around trivial rate o thus o lies interior qed c finding sphere sρ section discuss suﬃciently large sphere sρ radius ρ may found following discussion extended chapter section b multiple groups setting provided completeness let rj standard basis vector jth dimension compute maximum cidj o cidjrj feasible groups solving c algorithm c obtaining sphere sρ radius ρ input center o feasible region rates across groups j ··· q end let con v convex hull o ± cidjrjq compute radius s largest ball can ﬁt inside con v centered o output sphere sρ radius ρ s centered o j following optimization problem special case op problem corresponds feasiblity check problem given rate r achieved groups within small error  min rg∈rg ∀g∈m st cidrg − rcid ≤  ∀ g ∈ m c problem checks feasibility solution problem exists algorithm returns approach constructs classiﬁer whose groupwise rates close given rate r furthermore algorithm c computes value ρ ≥cidsk wherecids radius largest ball contained set r∩···∩rm notice approach consistent thus get good estimate sphere provided suﬃcient samples algorithm runs oﬄine impact query complexity lemma c letcids radius largest ball centered o r ∩ ··· ∩ rm algorithm c returns radius ρ ≥cidsk proof let cidj computed algorithm cid minj cidj cid ≥ cids contains ball radius cid√q cid√k − k ≥ cidk ≥cidsk thus ρ ≥cidsk notice thatcidm moreover region con v contains convex hull o ± cidrjq j however region qed c derivations section g τ g ie vector ones c eliciting misclassiﬁcation cost φr part figure line algorithm key eliciting φ remove eﬀect fairness violation ϕ oracle responses explained section run lpme procedure algorithm q dimensional query space sρ binary search tolerance  equivalent oracle ωclass remark subroutine returns slope f cidfcid thus setcida f line algorithm − λai − λaj fi fj ⇒ ai aj fi fj c c eliciting fairness violation ϕrm part figure lines algorithm eliciting fairness violation ϕrm m lines algorithm m one vector unfairness weights b now aim elicit given cida discussed section ﬁx trivial rates trivial classiﬁers one group allow nontrivial rates sρ another group essentially makes metric deﬁnition linear elicitation procedure follows fix trivial classiﬁer predicting class group ie ﬁx hx ∀ x ∈ x thus r e group constrain confusion rates lie sphere sρ ie r s s ∈ sρ metric deﬁnition amounts ψs e b λ − λcida cid − τ scid λcidb e − scid c c function s ∈ sρ since ei’s binary vectors since ≤ s ≤ sign absolute function respect s can recovered recall rates deﬁned row major form rate matrices thus e every k j ∗ k − th coordinate j ∈ k − otherwise coordinates confusion rates e absolute function opens negative sign wrt s positive sign otherwise particular deﬁne qdimensional vector w entries − every k j ∗ k − th coordinate j ∈ k − otherwise one may write metric ψ ψs e b λ cid − λa cid − τ λw cid b scid c c linear metric elicitation problem s ∈ s may use lpme procedure algorithm outputs normalized slope ˘f cid˘fcid line algorithm using remark get q − independent equations may represent every element b based one element say b k− ie ˘fk− ˘fi − λ − τ − λ − τ k−ak− λb ai λwib   − λ − τ k− k−ak− λb ˘fk− k− ∀ ∈ q  ˘f − − λ − τ cid  c ⇒ λb w cid order elicit entire b need one linear relation c now ﬁx trivial classiﬁer predicting class k group ie ﬁx hx k ∀ x ∈ x thus r ek group constrain rates lie sphere sρ ie r s s ∈ sρ since rate vectors row major form rate matrices notice ek every k − j ∗ k − th coordinate j ∈ k − otherwise particular deﬁne qdimensional vector wk entries − every k − j ∗ k − th coordinate j ∈ k − otherwise one may write metric ψ ψs ek b λ − λcida cid − τ scid λcidb ek − scid ck c linear metric elicitation problem s ∈ s thus line algorithm applies lpme subroutine algorithm outputs normalized slopecidf cidcidfcid using remark extract following relation two coordinates say k − th k − th coordinates cidfk− cidfk− − λ − τ k−ak− − λb k− k−ak− λb k− c − λ − τ cidb w cid combining equations c c replacing true estimated cida section estimate scaled substitute cid cid δ˘f −cida cid − τ  −τ cidak− k−cidak− cid ˘fk− k−cidak− ˘fk− − cidb scaled substitute deﬁned cidb λ c since require solutioncidb cidcidbcid deﬁnition normalize cidb get ﬁnal solution nonetheless computable cidfk− cid cidfk− − cidfk− cidfk− − τ ˘fk−  b −λ k− −τ δ c cidb cidb cidcidbcid c notice due normalization solution independent true tradeoﬀ λ eliciting fairness violation ϕrm m line algorithm consider nonempty set sets m ⊂ m ∅ m will later discuss choose m eﬃcient elicitation m partition set groups m two sets groups let σ ∈ m m σ one partition m groups deﬁned set σ follow exactly similar procedure previous section ie ﬁxing trivial rates trivial classiﬁers groups σ allowing nontrivial rates sρ groups m σ particular consider paramterization ν sρm k → rm deﬁned νs σ rm rg g ∈ σ ow c ei s ie ν assigns trivial confusion rates ei groups σ assigns s ∈ sρ rest groups similar previous section ﬁrst ﬁx trivial classiﬁer predicting class groups σ constrain rates groups m σ sphere sρ setup governed parametrization ν· σ equation c speciﬁcally ﬁxing hgx ∀ g ∈ σ entail metric deﬁnition ψνs σ b λ − λcida cid − τ σ scid λcidησe − scid c τ σ cid g∈σ τ g ησ cid uv∈mvu cid u v ∩ σ cid b uv similar previous section since ei’s binary vectors sign absolute function wrt s can recovered particular metric amounts c ψνs σ b λ cid − λa cid − τ λw cid ησ scid c c w − e c constant aﬀecting responses notice c c analogous c c respectively except τ replaced τ σ replaced ησ linear metric s use lpme procedure b line algorithm outputs normalized slope ˘f σ cid˘f σcid thus get analogous solution c cid λησ w cid cid k−ak− λησ − λ − τ σ ˘f σ k− k−  ˘f σ − − λ − τ σ cid c order elicit entire ησ need one linear relation c now ﬁx trivial rates trivial classiﬁer predicting class k groups σ ie ﬁx hgx k ∀ x ∈ x g ∈ σ thus rg ek groups g ∈ σ rest groups constrain confusion rates lie sphere sρ ie rg s s ∈ sρ groups g ∈ m σ setup governed parametrization ν· σ k c metric ψ deﬁnition amounts ψνs σ k b λ − λcida cid − τ σ scid λcidησek − scid ck thus running lpme procedure line algorithm results cidf cidcidf cid using remark extract following relation k − th c k − th coordinates cidf σ cidf σ k− k− − λ − τ σ k−ak− − λησ k− k−ak− λησ k− − λ − τ σ c combining equations c c uv cid cid u v ∩ σ cidcidbuv γσ cidδσf σ −cida cid − τ σcid  −τ σ cidak− k−cidak− cid f σ k−cidak− − cidf σ cidf σ k− − k− −τ σ k− k− f σ k− cidf σ k− cid cidf σ k− γσ w cid δσ − τ σ f σ k− c c  uv computed c − λ scaled version true unknown b nonetheless can andcidbuv λb side c allow us recover thecidb’s separately provides one equation cid independent equations order elicit m weight cidb’s need system m cidm let us denote equation c cidσ corresponding set σ order elicit two runs lpme algorithm can get γσ solve c however left hand vectors easily achievable choosing m σ’s get m set unique equations like c let m set sets cases pairing two groups trivial rates trivial classiﬁers rest groups rates sphere s will work example m ﬁxing m suﬃces thus running choices sets groups σ ∈ m provides system equations l ∪σ∈mcidσ line algorithm formally described follows  cidb cidbi cidb ··· cidbm vectorized versions th entry across groups ∈ q ξ ∈ m×m binary fullrank matrix de noting membership groups set σ ∈ m instance choice m m gives ξ ξ  cidbcidb cidbq γi γ γ    γ γ γq ··· γm ξ  c  ξ   −  γ γ γq    cidbcidb cidbq ξ ξ ξ cidbuv cidbuv cidm uvvu cidcidbuvcid c c technical point view one may choose m resulting group mem bership matrix ξ nonsingular hence solution system equations l normalizecidb get ﬁnal fairness violation weight estimates u v ∈ m v u c notice due normalization solution independent true tradeoﬀ λ c eliciting tradeoﬀ λ part figure line algorithm ease notation let us construct parametrization νcid s  → rm νcids s o o c using parametrization νcid c metric deﬁnition reduces linear metric s discussed ie ψνcids b λ cid − λτ cid λ cidm v b scid c c v ﬁrst show proof lemma discuss tradeoﬀ elicitation algorithm algorithm simply acidm proof lemma simplicity let us abuse notation proof denote τ cid v v b simply b s  simply s s convex set let z z z z z s z b s s ∈ s claim z convex let z zcid ∈ z αz − αzcid αz − αzcid since αs − αscid claim boundary set z strictly convex curve vertices cid b recall required function given α s − α scid αs − αscid α b s − α b scid b αs − αscid ∈ s αz − αzcid ∈ z hence z convex ϑλ maxz∈z − λz λz c c since set z convex every boundary point supported hyperplane ii since cid b notice slope uniquely deﬁned λ since sphere s strictly convex linear functional deﬁned λ maximized unique point z similar lemma thus hyperplane tangent unique point boundary z iii remains show vertices boundary z recall vertex exists point supported one tangent hyperplane two dimensional space means two values λ achieve maximizer contradictory since two linear functionals achieve maximizer s implies boundary z strictly convex curve since interested maximization ϑ let boundary upper boundary denoted ∂z claim let υ → ∂z continuous bijective parametrizations upper boundary let ϑ z → r quasiconcave function monotone increasing z z composition ϑ ◦ υ → r strictly quasiconcave therefore unimodal ﬂat regions interval let s superlevel set quasiconcave function ϑ since υ continuous bijection since boundary ∂z strictly convex curve vertices wlog r s t zυr zυs zυt zυr zυs zυt otherwise swap r t since boundary ∂z strictly convex curve υs must greater componentwise point convex combination υr υt let us denote point u since ϑ monotone increasing x ∈ s implies y ∈ s y ≥ x componentwise therefore ϑυs ≤ ϑu since s convex u ∈ s thus υs ∈ s implies υ−∂z ∩ s interval hence convex turn tells us superlevel sets ϑ ◦ υ convex ϑ ◦ υ quasiconcave desired implies unimodaltiy function deﬁned real line one local maximum can quasiconcave moreover since vertices boundary ∂z ϑ ◦ υ → r strictly quasiconcave thus unimodal ﬂat regions interval completes proof lemma qed c proof section proof theorem break proof three parts elicitation guarantees misclassiﬁcation cost cidφ iecida oncida follows theorem thus assumption outputcida line since algorithm elicits linear metric using qdimensional sphere s guarantees algorithm satisﬁes cida∗ elicitation guarantees fairness violation cost cidϕ ie cidb c let us drop superscript σ simplicity furthermore let  cidωρ −cidacid ≤ o√q cidωρ ocidq log π start deﬁnition true γ ie elicited entities true cid queries  denoted  γ w cid − τk−ak− δ ˘fk− cid cid δ˘f − cid − τ  −τk−ak− cid ˘fk− −τk−ak− ˘fk− − cidfk− cid cidfk− − cidfk− cidfk−  c c  let us look derivative ith coordinate γ ∂γi ∂aj −τi ci ci j cid j cid k − j cid k − j j k − j k − c ci ci bounded constants due assumption similarly ∂γi∂fj bounded well due regularity assumption means γi lipschitz cidnorm wrt f thus c c c lipschits constants c c bounds part proof cid˘fcid cidγ −cidγcid∞ ≤ ccida −cidacid ccid˘f − cidγ −cidγcid∞ ≤ o√q cidbi ξ−γi ∀ ∈ q recall construction ofcidbi c solution system equations c γ cidγi γ cidb cidbi cidb ··· cidbm vectorized versions ith entry across groups ∈ q ξ ∈ m×m fullrank symmetric matrix maximum absolute row sum matrix thus cidcidbi − bounded inﬁnity norm cidξ−cid∞ ≤ c inﬁnity norm matrix deﬁned cidcidbicid∞ cidξ−γi − ξ−cidγicid∞ cidξ−γi −cidγicid∞ ≤ cidξ−cid∞cidγi −cidγicid∞ ··· γm c gives cidcidbi − now ﬁnal estimate normalized form ofcidcidb c ﬁnal error stacked version vecb veccidb cidcidbicid∞ ≤ o√q c cidvecb − veccidbcid∞ ≤ o√q c since q × m entities vecb cidvecb − veccidbcid ≤ ocidqm√q omq c due elicitation sphere oracle noise ω deﬁned deﬁnition can replace   cidωρ back get ﬁnal bound fairness violation weights elicitation guarantees tradeoﬀ parameter iecidλ theorem metric purpose linear metric s ∈ s ψνcidcidcids b λ cid − λτ cid λ mcid v ρ following slope v b scid c since elicit λ queries surface sphere pose problem ﬁnding right angle slope deﬁned true λ note λ want elicit however due oracle noise ω can aim achieve target angle λt moreover true b estimates cida cidb thus query proxy lastly algorithm stopped within  threhsold thus ﬁnal solutioncidλ within solutions always can aim achieve estimated version λe target angle c oracle error estimation error optimization error  distance λe total want ﬁnd cid cidcid cid λt − λe λe −cidλ cid cidcid cid metric lipschitz linear function optimal value sphere radius   however − cosx ≥ x oracle correct long λ − λe ≥ λ −cidλ ≤ λ − λt cid cidcid cid • optimization error λe −cidλ ≤  • oracle error notice oracle correctly answers long −cosλ−λt ω cidω given binary search proceeds correct direction • estimation error make error access estimatedcida cidb true b however since metric c lipschitz cidm estimated cida cidb thus replace ω previous point error incida andcidm vcidbv binary search moves right direction long  cid cida −cidacid cidm cid cidcid mq cidωρ error can treated oracle feedback noise oracle responses  o −cidbvcid v v b v v cidb  λt − λe ≥ o c used c bound error cidbvm v combining three error bounds gives us desired result tradeoﬀ parameter theorem qed appendix d quadratic performance metric elicitation d geometry feasible space proofs section proof proposition proposition proof proposition propo sition proof proposition analogous probability measures corre qed sponding classiﬁers rates conditioned group d finding sphere s ⊂ r section provide details regarding sphere s suﬃciently large radius ρ inside feasible region r may found see figure b following discussion borrowed appendix c provided completeness following optimization problem special case op problem associated feasibility check problem given rate proﬁle r optimization routine tries construct classiﬁer achieves rate r within small error  r∈r min st cidr − rcid ≤  d optimization problem checks feasibility exists solution problem algorithm returns furthermore algorithm d computes value ρ ≥cidpk wherecidp radius largest ball contained set r also approach consistent thus get good estimate sphere provided suﬃciently large number samples algorithm completely oﬄine impact oracle query complexity lemma d let cidp denote radius largest ball r centered o algo rithm d returns sphere radius ρ ≥cidpk k number classes idea algorithm d can trivially extended ﬁnding sphere s ⊂ r∩···∩rm corresponding remark d quadratic performance metric elicitation procedure section describe subroutine calls lpme algorithm elicit quadratic metric deﬁnition start shifted metric equation let αj standard basis vector compute maximum constant cj o cjαj feasible solving d algorithm d obtaining sphere s ⊂ r figure b radius ρ centered o j ··· q end let con v denote convex hull o ± cjαjq compute radius ρ largest ball ﬁts con v output sphere s radius ρ centered o j will centered o explained chapter may assume d cid due assumption can derive following solution using nonzero coordinate d instead d can identify nonzero coordinate using q trivial queries form αi o o∀ ∈ q line algorithm get local linear approximation o using remark di fi f d ∀ ∈ q d similarly apply lpme small balls around rate proﬁles zj remark gives us di ρ − bij d ρ − bj fij fj ∀ ∈ q j ≤ d ⇒ ρ − bij ⇒ di ρ − bij fij fj fij fj ⇒ ρ − bij cid ⇒ ρ − bij d fj f d ρ − b − dj − fi f d fij fj − fi f fij fj f − fj f d ρ −  fj f b d fij fj d ρ − bj d ρ − bj − di cid fj cidcid used matrix b symmetric second step d last two steps can represent element terms b d relation b d may allow us represent element b terms d therefore applying lpme small balls around rate proﬁles −z remark gives us cid search tolerance  oracle ωcid algorithm d fair quadratic performance metric elicitation input query set s let l ← ∅ σ ∈ m cidb ← normalized solution d using l cidλ ← trace back normalized solution d σ output cidacidbcidλ βσ ← qpmes let cidσ eq d extend l ← l ∪ cidσ end cid  ωcid d − ρ − b d − ρ − b using d d f− f− d f− f− ρ − b f f f − f f− − f f− f d d putting d d get cid  fij cid fj bij cid fj f fij fj fj f − fi f − fij fj fj f f− f− fij fj − fijfj − fi fij  d f f f − f f− − f f− f − f − f f f − − f cid d d f − ijl f− f− il fijl fil fjl b using using d d terms d can use normalization condition cidacid f get estimates b independent d d bo can represent element cidbcid jl completes derivation solution qpme section d fair quadratic performance metric elicitation procedure ﬁrst discuss eliciting fair quadratic metric deﬁnition param eters unknown provide alternate procedure eliciting just tradeoﬀ parameter λ predictive performance fairness violation coeﬃcients known latter separate application discussed however unlike zhang et al instead ratio queries use simpler pairwise comparison queries section work number groups m ≥ idea however remains described chapter number groups m speciﬁcally select queries sphere s ⊂ r∩···∩rm common groupspeciﬁc feasible region rates reduce problem multiple instances proposed qpme procedure section suppose oracle’s fair performance metric φfair parametrized b λ deﬁnition overall fair metric elicitation procedure framework summarized algorithm d framework exploits sphere s ⊂ r ∩ ··· ∩ rm uses qpme procedure algorithm subroutine multiple times let us consider nonempty set sets m ⊂ m ∅ m will later discuss choose set m partition set groups m two sets groups let σ ∈ m m σ one partition m groups deﬁned set groups σ example m one may choose set groups σ cid given now consider sphere s cid whose elements rm ∈ s g ∈ σ rg s o ow d cid deﬁned chapter m case elements cid rate proﬁles s ∈ s groups σ trivial rate proﬁle o remaining rm cid metric extension sphere s s groups m σ analogously modiﬁed oracle ωcidr r ωrm cid thus elements s elements spheres s rm rm deﬁnition reduces s − ot wσs − o cσ d φfairrm ∈ s τ σ cid cid b λ − λcida cid τ σ s − ocid λ g∈σ τ g wσ cid u∈σv∈mσ buv cσ constant aﬀecting oracle responses metric particular instance φs d b d − λa cid τ σ b λwσ thus apply qpme procedure subroutine algorithm d elicit metric d change needed made algorithm line need take account changed relationship d need separately jointly normalize linear quadratic coeﬃcients change output algorithm directly gives us required estimates speciﬁcally line algorithm estimate di d τ σ ai τ σ fi f ⇒ ai fi f τ σ τ σ using normalization condition ie cidacid directly get estimatecida linear coeﬃcients similarly steps algorithm gives uscidbij cid cida f σ ij f σ cidbuv f σ −f σ −σ f σ −σ −f σ ijf σ ij cid cid jd − f σ j − f σ τ ij f f u∈σv∈mσ d βσ d true unknown buv let equation d denoted cidσ also let right hand side term d denoted βσ cid fairness violation weight matrices b requirecidm solution similar two group case corresponding similar d let m set sets thus running choices sets groups σ ∈ m provides system equations l ∪σ∈mcidσ line algorithm d partition groups deﬁned σ cidbuv λbuv − λ scaled version since want elicitcidm cid ways partitioning groups two sets constructcidm cid independent matrix equations     ij··· cidb cidbij cidb ijcidb m vectorized versions ij··· β ij ×m ijth entry across groups j ∈ q ξ ∈ m binary fullrank matrix denoting membership groups set σ example one chooses m m ξ given ξ ξ cidbcidb cidbqq β β βqq γij β ξ   d ij β m ij   ξ d one may choose set sets m allows resulting group membership matrix ξ nonsingular solution system equations l  cidbcidb cidbqq   −  β β βqq  d ξ ξ ξ cidbuv’s normalized estimated fairness violation weight matrices cidbuv given estimates cidbuv ij cida can now additionally estimate tradeoﬀ parameter cidλ cidσ d σ ∈ m completes fair quadratic metric elicitation due normalization solution independent true tradeoﬀ λ u v ∈ m v u cidbuv cidm uvvu cidcidbuvcidf d procedure d eliciting tradeoﬀ λ linear predictive performance quadratic fairness violation coeﬃcients known now provide alternate binary search based method similar chapter eliciting tradeoﬀ parameter λ linear predictive quadratic fairness coeﬃcients already known along similar lines application considered zhang et al unlike instead ratio queries require simpler pairwise queries key insight approximate nonlinearity posed fairness violation deﬁnition reduces problem onedimensional binary search φfairrm b λ − λcida rcid λ uvvu ru − rvt buvru − rv d cid end deﬁne new sphere s set rate proﬁles whose ﬁrst group achieves rates s ∈ s rest groups achieve cid trivial rate o corresponding uniform random classiﬁer element s cid metric associated discrepancy terms ru − rv u v cid thus elements s deﬁnition reduces cid s o os ∈ s elements s cidcidm cid φfairs o o b λ − λcidτ cid s − ocid λ s − ot mcid v bvs − o c d algorithm d eliciting tradeoﬀ λ predictive performance fairness violation known input query space s initialize λa λb cid z binarysearch tolerance  oracle ω cidcidcidλb − λacidcidcid  set λc λaλb set sa argmax λd λaλb cid − λaτ cidcida λa s∈scid z λe λaλb cidbvz − o scid using lemma mcid v similarly set sc sd se sb query ωsc sa ωsd sc ωse sd ωsb se λa λb ← shrinkinterval responses – subroutine analogous routine fig b end output cidλ λaλb additionally consider small sphere s cid z z ρ − α o similar shown figure may approximate quadratic term right hand side ﬁrst order taylor approximation follows φfairs o o b λ ≈ φfair apxs o o b λ cid − λτ cid λ bvz − o scid d mcid v s small neighbourhood around rate proﬁle z since metric essentially linear s following lemma chapter shows metric d quasiconcave λ lemma d regularity assumption mcid v cidτ cid bvz − ocid cid function ϑλ max s∈scid z φfair apxs o o b λ strictly quasiconcave therefore unimodal λ d d unimodality ϑλ allows us perform onedimensional binary search al cid z tolerance  oracle ω binary search gorithm d using query space s algorithm algorithm provided completeness d elicitation guarantee qpme procedure d sample complexity bounds capacity standard generalization bounds eg daniely et al give us high recall deﬁnition oracle responds correctly long φr−φr ω simplicity assume algorithm access population rates r deﬁned practice expect estimate rates using sample d x yn eq drawn distribution p query classiﬁers hypothesis class h ﬁnite probability draw d estimatescidr close population rates r querying ﬁnite sample estimates ωcidrcidr cid formally δ ∈ long sample size n greater ocid loghδ desired tolerance ω long suﬃcient samples since metrics φ lipschitz wrt rates high probability thus gather correct oracle feedback guarantee theorem holds probability least − δ draw d h can turn replaced measure capacity hypothesis class h example one can show following corollary theorem hypothesis class h classiﬁer randomized combination ﬁnite number deterministic classiﬁers chosen ¯h whose capacity measured terms natarajan dimension ¯h corollary d suppose hypothesis class h randomized classiﬁers used choose queries oracle form ω cid tcid t cidcidcidcid t ∈ z α ∈ ∆t h ht ∈ ¯h cid h x cid→ αthtx d class ¯h deterministic multiclass classiﬁers h x → k suppose deter ministic hypothesis class ¯h natarajan dimension d φ lipschitz δ ∈ long sample size n ≥ o hold probability least − δ draw d xi yin guarantee theorem p cid d logklogδ cid  ω proof adapts generalization bounds daniely et al uses fact predictive rate randomized classiﬁer h convex combination rates deterministic classiﬁers ¯h due linearity expectation d proofs presenting proof theorem rewrite lpme guarantees linear metrics presence oracle noise parameter ω deﬁnition lemma d lpme guarantees oracle noise chapter let oracle ω’s metric φlin cida rcid feedback noise parameter deﬁnition ω lpme procedure algorithm run using sphere s ⊂ r radius  binarysearch tolerance  posing oq log queries recovers coeﬃcientscida cida −cidacid ≤ cid√q cidω cid o will use result proving theorem proof theorem ﬁrst ﬁnd smoothness coeﬃcient metric deﬁnition function φ said lsmooth bounded constant l cid∇φx − ∇φycid ≤ lcidx − ycid d metric deﬁnition cid∇φquadx − ∇φquadycid cida bx − bycid ≤ cidbcidcidx − ycid ≤ cidbcidfcidx − ycid ≤ · cidx − ycid d last step used scale invariance condition deﬁnition ie cidacid cidbcidf implies cidbcidf − cidacid ≤ hence metrics deﬁnition smooth now look error taylor series approximation approximate metric φquad deﬁnition linear approximation metric φquadr cida rcid rt br approximate ﬁrst order taylor polynomial around point z tr cida zcid zt bz cida bz rcid bound error approximation d d er φquadr − tr r − zt ∆φquadcr − z r − zt br − z cidbcidcidr − zcid  cidbcidf  ≤ ≤ ≤ firstorder taylor approximation error hessian point c matrix b due scale invariance condition d oracle asked ωr r φquadr φquadr approximation error  thus can treated feedback error oracle feedback noise × overall feedback noise oracle ω  purposes using lemma d later ﬁrst prove guarantees matrix b vector write equa tion following form assuming d since normalize coeﬃcients end due scale invariance fi f  f− f− f− f− fij fj fj f f f f − f f− − f f− f f f f − f f− − f f− f  f   f d cid bij fij fj f fij fj fj f − − cid  fij  fj fj cjfj cf ⇒ b j fj fj fjf fj fjf fj fjf b j jth column matrix b constants cj c welldeﬁned due regularity assumption notice ∂b j ∂fj diagccid j cid ∂b j ∂f diagccid cid d ccid j ccid vector lipschitz constants bounded due assumption implies jcidf j −cidfjcid ccid cidb j −cidb jcid ≤ ccid cid cid ≤ ccid cidf −cidfcid cid  cid ω cidcid cid  cid ω j√q √q o ccid cid  cid ω cid √q d used lpme guarantees lemma d oraclefeedback noise parameter ω  inequality provides bounds column b since cidxcid∞ ≤ cidxcid cid √q cid  cid ω cidcid consequentially cidb − cidbcidf ≤ maxij bij − cidbij ≤ o cid cidcid  cid ω cid o q√q now let us look guarantees since d − bo can write cf − ojb j d qcid j thus cida −cidacid ≤ ccid √q ccid √q cid o q cid cid  cid ω cid cid  cid ω cidcid cid  cid ω qcid qcid j √q j k √q k cid cid  cid ω cid cid  cid ω j√q ccid c f since o rate achieved random classiﬁer oj k ∀j ∈ k thus ∂ ∂f ∂ ∂b j ci d d ccid ccid used fact q k − k second step j’s lipschitz constants bounded due assumption qed notice tradeoﬀ elicitation error depends size sphere expected radius sphere  increases error due approximation increases time error due feedback reduces get better responses oracle contrast radius sphere  decreases error due approximation decreases error due feedback increases cid following corollary translates guarantees elicited metric guarantees optimal rate elicited metric useful practice optimal classiﬁer rate obtained optimizing certain metric often key entity many applications corollary d let φquad oracle’s quadratic metric andcidφquad estimate obtained qpme procedure algorithm moreover let r∗ andcidr∗ minimizers φquad cidφquad respectively φquadcidr∗ ≤ φquadr∗ o proof ﬁrst show φquadr −cidφquadr ≤  rates r slack  follows φquadcidr∗ ≤ φquadr∗   cid ω cid cidφquad approximates φquadcid cid ascidr∗ minimizes cidφquadcid cidφquad approximates φquadcid cid now let us derive trivial bound φquadr −cidφquadr rate r φquadcidr∗ ≤cidφquadcidr∗  ≤cidφquadr∗  ≤ φquadr∗  cidcid q√q d cid φquadr −cidφquadr cida −cida rcid ≤ cida −cida rcid cidb − bcidcidrcid ≤ cida − acidcidrcid cid cid ≤ cida − acid√q cidb − bcidf q ≤ o rt b −cidbr rt b −cidbr cidcid  cid ω q√q d cid  cid ω cid fourth step used fact rates bounded hence cidrcid ≤ √q ﬁfth step used guarantees theorem combin ingd d gives us desired result qed cidcidcidb − cidb cidcidcidf ≤ oq√q proof theorem purpose proof let us replace slack  theorem guarantees running qpme procedure oq log queries havecida −cidacid ≤ oq vectorize tuple b denote w havecidw − cidwcid ≤ oq√q cidwcidcidcidwcid due scale invariance condition deﬁnition note problem now count minimum number cidw possible cidw − cidwcid ≤ oq√q translates ﬁnding covering number ball cid·cid norm radius covering balls radius q√q let us denote cover uin ball radius b dimensional vector deﬁnes scaleinvariant quadratic metric elicitation w qq ncid v olb ≤ v olq√qb ui n v olq√qb q√q − qq cid cid qq − c q√q ≤ n thus number cidw possible least d d q bits required get possible cidw require oq log c constant since pairwise comparison provides one bit least √ oq log  queries nearoptimal barring log terms qed q appendix e optimizing blackbox metrics metric elicitation notation index j ∈ k onehotj ∈ k denotes onehot encoding j classiﬁer h x→kcidh onehoth denotes classiﬁer onehot outputs iecidhx onehothx e extension general linear metrics matrix entries e dh cid describe proposal extends blackbox metrics e dh ψch deﬁned function ψ k×k→r confusion matrix entries handles example label noise models table general nondiagonal noise transition matrix t begin metrics linear functions diagonal oﬀdiagonal confusion weight associated jth confusion matrix entry ij βijcijh β ∈ rk×k case will use example maps instance x k × k weight matrix wx weighting function w x→rk×k wijx ∈ rk×k note practice metric e d may depend subset d entries confusion matrix case weighting function needs weight entries consequently weighting function can parameter ized ld parameters can estimated solving system ld linear equations sake completeness describe approach metrics depend k confusion entries modeling weighting function like propose modeling function weighted sum l basis functions wijx lcid cid αcid ijφcidx e φcid coeﬃcients α x→ αcid ij ∈ r similar goal estimate cidcid ij cid exy∼µ wijx y ihjx ≈ e dh∀h expanding weighting function e get lcid cid cid ij αcid ij exy∼µ cid cidφcidx y ihjxcid cid cidcid φµcid ij h ≈ e dh∀h e e can rewritten lcid cid cid ij ijφµcid αcid ij h ≈ e dh∀h e αcid cidij αcid cidij estimating coeﬃcients α estimate α ∈ rlk proposal probe metric e d lk diﬀerent classiﬁers hcid hcidkk one classiﬁer combination cid j basis functions confusion matrix entries solve following system lk linear equations cid ijcidφtrcid ij h cide valh cid ijcidφtrcid ij hlmm cide valhlkk cidφtrcid ij h using training sample str cide valh estimate e dh using validation sample sval equivalently deﬁning cidσ ∈ rlk×lk cide ∈ rlk compute cidα cidσ−cide probing classiﬁer hcidij cid jth diagonal entry cidσ large oﬀdiagonal choosing probing classiﬁers described section propose picking cidecidij cide valhcidij ij h estimate φµcid cidσcidijcidcidicidjcid cidφtrcidcid icidjcid hcidij e e entries small can framed following constrained satisfaction problem hcidij pick h ∈ h cidφtrcid ij h ≥ γ cidφtrcidcid icidjcid h ≤ ω∀cidcid icid jcid cid cid j e ω γ practical approach prescribed section constructing probing classiﬁers trivial classiﬁers predict class subset examples apply need take account diagonal oﬀdiagonal confusion entries problem can solved using oﬀtheshelf tools available rateconstrained optimization problems plugin classiﬁer estimated example weighting function cidw x→rk×k seek maximize weighted objective training distribution cid  cidwijx y ihjx ij max h exy∼µ e modelcidηtr x→∆k can construct plugin classiﬁer postshifts pretrained class probability cidhx ∈ argmax j∈k kcid cidwijxcidηtr x e handling general nonlinear metrics e dh ψch smooth ψ k×k→r can directly adapt iterative plugin procedure algorithm turn construct plugin classiﬁer form iteration line see details iterative frankwolfe based procedure optimizing general metrics authors consider nonblackbox metrics absence distribution shift e proofs e proof theorem βic d βicidc val ii h unknown coeﬃcients β ∈ rk form cide linh cid e dh cid theorem e restated error bound elicited weights let input metric cidβcid ≤ let ii h let γ ω constraints feasible hypothesis class ¯h cid suppose algorithm chooses classiﬁer hcidi satisfy e dhcidi ∈ c ∀cid c let ¯α associated coeﬃcient assumption metric e d suppose γ √lkω ntr ≥ lk loglkhδ wp ≥ − δ draws str sval µ d resp coeﬃcients cidα output fix δ ∈ lkω −√ γ algorithm satisﬁes cidcidα − ¯αcid ≤o cid cidlk γ cidcid cidcid l loglkhδ ntr √lk γ lk loglkδ cγnval ν e term h can replaced measure capacity hypothesis class h solution algorithm given cidα cidσ−cide let ¯α “true” coeﬃ cients given assumption let σ ∈ rlk×lk denote population version cidσ icid xcid similarly denote population version cide ecidi e dhcidi let α σ−e solution obtain used population versions quantities deﬁne vector ¯e ∈ rlk σcidicidcidicid exy∼µ xy icidhcidi cidφcidcid cid cidi ¯ecidcidicid iφµcid ¯αcid hcidcidicid e trivially follows coeﬃcient ¯α given assumption can written ¯α σ− ¯e will ﬁnd following lemmas useful ﬁrst two lemmas bound gap empirical population versions σ lefthand side linear system e righthand side linear system lemma e conﬁdence bound σ fix δ ∈ probability least − δ draw str µ pcidi loglkhδ ntr e σcidicidcidicid −cidσcidicidcidicid ≤ o cidcid cidσ −cidσcid ≤ o pcidi exy∼µφcidxy consequently lk loglkhδ ntr e proof row σ −cidσ contains diﬀerence elements φµcid h classiﬁer h chosen h using multiplicative chernoﬀ bounds ﬁxed h probability least − δ draw str µ h cidφtrcid h −cidφtrcid φµcid h ≤ o pcidi logδ ntr e pcidi exy∼µφcidxy taking union bound h ∈ h probability least − δ draw str µ h ∈ h pcidi loghδ cidcid cid e h −cidφtrcid φµcid h ≤ o ntr taking union bound lk × lk entries probability least − δ cid cidcid icid σcidicidcidicid −cidσcidicidcidicid ≤ o cidcid pcidi loglkhδ ntr e cid cid cid cid cidcid cidcid loglkhδ cid cid cidσ −cidσcid ≤ o upper bounding operator norm σ −cidσ frobenius norm cid cid cidcid second inequality uses fact thatcid lemma e conﬁdence bound e fix δ ∈ probability least − δ cidcid   ≤ o lk loglkhδ loglkhδ draw sval d icid pcidcidicid ex∼pµ cidcid cidicidcid ntr cid ntr ntr φcidcid x ≤ e ≤ o pcidcidicid cidicidcidicid cid cid qed cid lk loglkδ nval e cide −cidecid ≤ o proof application hoeﬀding’s inequality ﬁxed hcidi ecidi − cidecidi e dhcidi − cide valhcidi cidcidcidcidcidcid cid cidcid βic d cid ii hcidi − logδ nval cid βicidc val ii hcidi cidcidcidcidcidcid e ≤ o cid cid holds probability least − δ draw sval uses fact βi c d ii h bounded taking union bound lk probing classiﬁers cid cide − cidecid ≤ o √lk loglkδ nval e note need uniform convergence argument like lemma e probing qed classiﬁers chosen independent validation sample last two lemmas show σ wellconditioned ﬁrst show probing classiﬁers hcidi’s chosen satisfy diagonal oﬀdiagonal entries σ can lower upper bounded respectively follows lemma e bounds diagonal oﬀdiagonal entries σ fix δ ∈ probability least − δ draw str µ σcidicidi ≥ γ − o pcidi loglkhδ ntr σcidicidcidicid ≤ ω o pcidi exy∼µφcidxy cidcid cidcid cid ∀cid cid pcidi loglkhδ ntr ∀cid cid cidcid icid e e proof probing classiﬁers hcidi’s chosen h satisfy cidσcidicidi ≥ γ∀cid andcidσcidicidcidicid ≤ ω∀cid cid cidcid icid proof follows generalization bounds similar lemma e qed bounds diagonal oﬀdiagonal entries σ allow us bound smallest largest singular values lemma e bounds singular values σ cidσcid ≤ l√k fix δ ∈ suppose γ √lkω ntr ≥ lk loglkhδ probability least − δ draw str µ cidσ−cid ≤ o cid −√ cid lkω γ γ proof ﬁrst derive straightforward upper bound operator norm σ terms frobenius norm cidσcid ≤ l√k e σ cidicidcidicid ≤ p cidcidicid ≤ ex∼pµ ≤ cidicidcidicid cidicidcidicid cidicidcidicid pcidi exy∼µφcidxy last inequality uses fact cid bound operator norm cidσ−cid denote υcidi o acidicidi acidicidcidicid ≤ ω υcidi∀cid cid cidcid icid dcidicidi ≥ γ − υcidi let e can express σ sum matrix diagonal matrix d ie σ d cidcid pcidi loglkhδ σcidiσ denote cid ith largest singular value σ weyl’s inequality singular values σ can bounded terms singular values d see eg lemma icid pcidcidicid cid ntr cid cid cid cid φcidcid x cid cid cid cid cidcid cidicidcid pcidcidicid ≤ σcidiσ − σcidid ≤ cidacid σcidid − σcidiσ ≤ cidacid e σcidid − σcidiσ ≤ cidacid ≤ cidicidcidcidicid cid cid cid cid cid cid cidicidcidcidicid cidicidcidcidicid √ √ ≤ ≤ ω υ cidi υcidi ω υcidi υcidi cid cid cid cidicidcidcidicid cidicidcidcidicid ω √ cidcid cidcid υ cidi cidcid cid cid cidicidcidcidicid ntr ntr ≤ ≤ loglkhδ lk loglkhδ √lkω o √lkω o cidcid √lkω − o cid cidcid loglkhδ denoting √lkω o lk loglkhδ cid ntr − max cidi pcidi e e υcidi cidcid lk loglkhδ cid since σcidid ≥ γ − maxcidi υcidi σcidiσ ≥ γ − cid cid γ substituting maxcidi υcidi ≤ o ξ σcidiσ ≥ ξ can bound operator norm cidσ−cid ntr ntr cidσ−cid mincidi σcidiσ ≤ γ − ξ ≤ o e cidγcid now ready prove theorem last inequality follows assumption ntr ≥ lk loglkhδ ξ ≤ o proof theorem solution algorithm given cidα cidσ−cide recall can write “true” coeﬃcients ¯α σ− ¯e ¯e deﬁned e also deﬁned α σ−e lefthand side theorem can expanded hence −√ qed lkω γ cidcidα − ¯αcid ≤ cidcidα − αcid cidα − ¯αcid ≤ cidcidα − αcid cidσ−e − ¯ecid ≤ cidcidα − αcid cidσ−cidcide − ¯ecid e e e ≤ cidcidα − αcid ν√lkcidσ−cid ≤ cidcidα − αcid ν√lk γ e e cidσcid cidcidcidcid cidcidcidcid cidi ¯αcid iφµcid h − e dh cidi ¯αcid iφµcid hcidcidicid − e dhcidcidicid can use standard error analysis linear systems see eg bound secondlast step follows assump particularly ν∀h gives us follows lemma e holds probability least − δ draw str cidcidcid ≤ cidcidcid ≤ ν cidcid icid last step remains bound term cidcidα − αcid given cidα cidσ−cide α σ−e cid cid cidσ −cidσcid cide −cidecid cidcidα − αcid ≤ cidαcidcidσcidcidσ−cid cid cid cidecid cidσ −cidσcid cidσcidcide −cidecid ≤ cidσ−cidcidecid cid cid cidecid cidσ −cidσcid l√kcide −cidecid ≤ cidσ−cidcidecid cid cid cidecid √lkccide −cidecid cidσ −cidσcid ≤ cidσ−cid√lk cid cid cidσ −cidσcid c cide −cidecid ≤ cidσ−cid√lk √lk cidcid cid lk cidcid cid using ecidi ∈ c α σ−e lk loglkhδ cid l loglkhδ cid lemma e √l c ≤ o γ l√k √l lk loglkδ ntr c l loglkδ nval ntr nval o γ e last two steps follow lemmas e–e lemma e hold probability least −δ draws str sval plugging back e completes qed proof e error bound piew will ﬁrst provide error bound piew algorithm special case fweg algorithm metric linear following bound gap metric value achieved classiﬁer cidh output algorithm optimal value result will useful proving error bound fweg procedure algorithm next section essentially focuses nonlinear metric optimization ii h unknown coeﬃcients β ∈ rk cidβcid ≤ denote e linh ii h let ¯α associated weighting coeﬃcient e lin assumption cid ¯αcid ≤ b slack ν fix δ suppose wp ≥ − δ draw str lemma e error bound piew let input metric form cide linh cid βicidc val cid sval weight elicitation routine line algorithm provides coeﬃcients cidα cidcidα − ¯αcid ≤ κδ ntr nval function κ· let bcid b √lk κδ ntr nval probability classiﬁercidh output algorithm satisﬁes cid √lk κδ ntr nval ν e βic d cid cidηtrx −cidηtrxcid max h e linh − e lincidh ≤ bcidex cid h e linh − e lincidh ≤ q bcidex max cid ηtr q x pµy ix furthermore metric coeﬃcients cidβcid ≤ q cid √lk κδ ntr nval ν proof proof will treatcidh classiﬁer outputs onehot labels ie classiﬁer cidh x→ k let ¯wix cidl cid cidηtrx −cidηtrxcid cid cidhx onehot iφcidx andcidwix cidl cidcidαcid cidcidcidcid lcid ¯wix −cidwix ≤ cid ¯α −cidαcid argmax∗ breaks ties favor largest class √lkcid ¯α −cidαcid ≤ √lkκ e iφcidx easy see cidwixcidηtr φcidx ≤ argmax ∗ ∈k x cid ¯αcid cid e e cid second inequality use φcidx ≤ last inequality shortened notation κδ ntr nval κ simplicity will avoid mentioning holds high probability recall assumption ¯wix ≤ cid ¯αcid max cid φcidx ≤ b b e e also assumption cidwix ≤ b √lkκ  kcid  kcid ¯wixηtr cidcidcidcidcidcide linh − exy∼µ cidcidcidcidcidcide linh − ex∼pµ ¯wixy ihix cidcidcidcidcidcid ≤ ν∀h cidcidcidcidcidcid ≤ ν∀h equivalently can rewritten terms conditional class probabilities ηtrx pµy x xhix e e e pµ denotes marginal distribution µ x denoting h∗ ∈ argmaxh e linh max ν x ≤ ex ex ex ex ex ≤ − − xh∗ x xh∗ xh∗ e xcid cid cid h e linh − e lincidh kcid cid ¯wixηtr cidcidwixηtr kcid cidcidwixηtr kcid cidcidwixcidηtr kcid cid ¯wixηtr cid kcid xcidhix cidcidwixηtr cid kcid xcidhix ecidk cidcidwixcidηtr kcid cidcidwixηtr kcid deﬁnition ofcidh e thatcidk icidwixcidηtr cidcidwixcidηtr cidcidwixηtr h e linh − e lincidh cidcidwixηtr kcid cidcidwixcidηtr kcid cid cid xcidhix h x→∆k therefore kcid kcid x ηtr ν √lkκ cid cid xcidhix xcidhix ≥ cid ν √lkκ cid xcidhix xh∗ x xh∗ x xh∗ x xh∗ − ex ex − cid ≤ ex − ex max ex x hix ≤ ν √lkκ e cidk icidwixcidηtr xhix e cont ex − ex ex ≤ kcid cid cidcidwixηtr cid x −cidhix x −cidηtr ν √lkκ cid xh∗ cidcidwixh∗ cid x −cidhix cidηx −cidηxcid cid ν √lkκ cid ≤ ex cidηx −cidηxcid ≤ b √lkκ ex max ν √lkκ last step follows e hix−cidhix ≤ completes proof second part cidβcid ≤ q follows applying assumption normalized coeﬃcients qed βcidβcid scaling associated slack ν q e e proof theorem will make couple minor changes algorithm simplify analysis firstly instead using sample sval estimating example weights call piew line estimating confusion matrices cidcval line split sval two halves use one half ﬁrst step half second step using independent samples two steps will able derive straightforward conﬁdence bounds estimated confusion matrices case experiments however ﬁnd algorithm eﬀective even common sample used steps secondly modify line include shifted version metric cide val later appendix e handle case “unknown ψ” can avoid keep track additive constant gradient coeﬃcients h c d h cidc val theorem e restated error bound fweg known ψ let e dh kkh known concave function ψ k→r qlipschitz ψc d kk h fix δ ∈ ii h cidβcid ≤ whose associated weight coeﬃcients ¯α cid ¯αcid ≤ b wp ≥ − δ draw str sval λsmooth wrt cidnorm let cide valh ψcidc val suppose assumption holds slack ν linear metric cid weight elicitation routine algorithm outputs coeﬃcients cidα cidcidα − ¯αcid ≤ κδ ntr nval function κ· let bcid b √lk κδt ntr nval classiﬁercidh output algorithm e t iterations satisﬁes cid qν q√lk κδt ntr nval assume k ≤ nval wp ≥ − δ draws str sval d µ resp h e dh − e dcidh ≤ qbcidex cid cidηtrx −cidηtrxcid cid cid βic d cid max k lognval logk logkδ λ t e o λk nval algorithm e frankwolfe elicited gradients fweg general diagonal metrics input cide val basis functions φ φl x→ pretrained cidηtr x→∆k str ∼ µ initialize classiﬁer h c diagcidcvalh sval ∼ d split two halves sval t  sizes cidnvalcid cidnvalcid respectively sval else t t − h c d kkh known ψ ii h evaluated using sval e dh ψc d βt ∇ψct βt cide linh cid icidc val cide linh cide valh −cide valht evaluated using sval cidf piewcide lin φ φlcidηtr str sval cidc diagcidcvalcidf evaluated using sval cidht ht cid − tonehotcidf ct cid − cidct tcidc output cidh ht end ht  t t small  recommended proof adapts techniques show guarantees frankwolfe based learning algorithm known ψ absence distribution shift main proof steps listed • prove generalization bound confusion matrices cidcval evaluated line validation sample lemma e • establish error bound call piew line lemma e previous section • combine two results show classiﬁer cidf returned line approximate linear maximizer needed frankwolfe algorithm lemma e • combine lemma e convergence guarantee outer frankwolfe algorithm using convexity space confusion matrices c complete proof lemmas e–e lemma e generalization bound cd fix δ ∈ letcidηtr x→∆m ﬁxed class probability estimator let g h x→m hx ∈ argmaxi∈m βicidηtr set plugin classiﬁers deﬁned withcidηtr let x β ∈ rm ¯g hx cidt t uthtx t ∈ n h ht ∈ g u ∈ ∆t e set randomized classiﬁers constructed ﬁnite number plugin classiﬁers g assume m ≤ nval probability least − δ draw sval d h ∈ ¯g cidcid cid m logm lognval logmδ cidcdh −cidcvalhcid∞ ≤ o e nval proof proof follows standard convergence based generalization arguments bound capacity class plugin classiﬁers g terms natarajan dimension applying theorem natarajan dimension g d k logk applying generalization bound theorem along assumption k ≤ nval ∈ k probability least − δ draw sval d h ∈ g cid cidcid c d ii h ≤ o note randomized classiﬁer ¯hx cidt ii ¯h−cidc val ii ht−cidc val ii ht ≤ o ii ¯h ≤ cidcid utc d c d nval ii h − cidc val tcid t k logk lognval logδ e t uthtx ∈ ¯g u ∈ ∆t k logk lognval logδ e nval cid ﬁrst inequality follows linearity expectations taking union bound qed diagonal entries ∈ k completes proof next show call piew line algorithm computes approximate maximizer cidf cide lin extension lemma lemma e approximation error linear maximizer cidf iteration t algorithm denote ¯ct diagcdht ¯βt ∇ψ¯ct suppose assumptions theorem str sval µ d resp t t classiﬁer cidf returned piew hold let bcid b √lk κδ ntr nval assume k ≤ nval wp ≥ − δ draw line satisﬁes cid max h cid cid δ t ntr nvalcid ¯βt c d cid ii cidf ≤ qbcidex cidηtrx −cidηtrxcid cid cid cid qν k logk lognval logkδ o λk nval cid e ¯βt c d ii h − q√lk κ proof proof uses theorem bound approximation errors linear max imizer cidf coupled union bound t iterations lemma e bound estimation errors confusion matrix ct used compute gradient ∇ψct recall algorithm ct diagcidcvalht βt ∇ψct note approximations actual quantities interested ¯ct diagcdht ¯βt ∇ψ¯ct evaluated using population confusion matrix also cidβcid cid∇ψctcid ≤ q qlipschitzness ψ fix iteration t let h∗ cid ¯βt c d cid ii h particular iteration ∈ argmaxh cid ii cidf ii cidf c d ij c d ij h ψ λsmooth wrt cid norm cid cid ii cidf cid βt c d c d βt βt c d βt c d ii h∗ − c d βt cid cid ii cidf cidβt − ¯βtcid∞ ii cidf cidβt − ¯βtcid∞ cid ii cidf cid ii cidf ii cidf cid cid ii h∗ − c d βt cid cid cid cid ii h − ii h − βt c d ii h − βt c d cidηtrx −cidηtrxcid ii cidf qbcidex cid βt c d βt c d cid ii h − βt c d ii h∗ ¯βt c d c d βt ¯βt c d ii h∗ c d cid cid ii h∗ − ¯βt c d cid ii h∗ − ¯βt c d ii cidf − cid c d βt ii cidf cid cid ≤ cidβt − ¯βtcid∞ cid ≤ cidβt − ¯βtcid∞ max cidβt − ¯βtcid∞ max cid cid∇ψct − ∇ψ¯ctcid∞ max cid βt c d ≤ λcidct − ¯ctcid max ≤ λkcidct − ¯ctcid∞ max βt c d h h h h βt c d cid cid ≤ o λk ii h − k logk lognval logkδ h nval q√lk κδ ntr nval qν e bcid b √lk κδ ntr nval last step holds probability least − δ ﬁrst bound cidct − ¯ctcid∞ cidcidcvalht − cdhtcid∞ holds randomized classiﬁer draw sval str follows lemma e lemma e using cidβtcid ≤ q ht constructed ﬁnite number plugin classiﬁers second bound linear maximization errors holds ﬁxed t need take union bound iterations t t complete proof note use two independent samples sval sval hold high probability draws sval probability draw sval sval two bounds respectively hence high qed last two lemmas restate results ﬁrst shows convexity space confusion matrices proposition paper second applies result show convergence classical frankwolfe algorithm approximate linear maximization steps theorem lemma e convexity space confusion matrices let c diagcdh h x→∆k denote set confusion matrices achieved randomized classiﬁer h x→∆k c convex proof two confusion matrices c c ∈ c exist classiﬁers h h x→∆k c diagcdh c diagcdh need show u ∈ uc − uc ∈ c e true randomized classiﬁer hx uhx−uhx yields confusion matrix diagcdh u diagcdh − udiagcdh uc − uc ∈ c qed lemma e frankwolfe approximate linear maximization let metric kkh concave function ψ k→r λsmooth wrt e dh ψc d ii cidf ≤ ∆∀t ∈ t rithm returns classiﬁer cidf maxh cidnorm iteration t deﬁne ¯βt ∇ψdiagcdht suppose line algo classiﬁercidh output algorithm t iterations satisﬁes h c d ii h − cid ¯βt c d ¯βt c d cid h e dh − e dcidh ≤ ∆ max λ t e proof theorem proof follows plugging result lemma e lemma e qed e error bound weight elicitation fixed probing classifiers ﬁrst state general error bound algorithm terms singular values σ ﬁxed choices probing classiﬁers bound singular values ﬁxed choices speciﬁc assumptions e dh cid ii h unknown β ∈ rk let cide valh cid theorem e error bound elicited weights ﬁxed probing classiﬁers let ii h let ¯α associated coeﬃcient assumption metric e d fix δ ∈ βicidc val βic d cid cid  ﬁxed choices probing classiﬁers hcidi probability ≥ − δ draws str sval µ d resp cidα output algorithm satisﬁes cidcidα − ¯αcid ≤  e proof proof follows steps theorem except bound cidcidα − αcid σminσ σminσ respectively smallest largest singular values σ ν√lk σminσ lk loglkδ σmaxσ l loglkδ σminσ cid cid nval o lk ntr speciﬁcally e cidcidα − ¯αcid ≤ cidcidα − αcid ν√lkcidσ−cid next bound cidcidα − αcid cid cid cidσ −cidσcid cide −cidecid cid cid ≤ cidαcidcidσcidcidσ−cid cidecid cidσ −cidσcid cidσcidcide −cidecid ≤ cidσ−cidcidecid cid ≤ cidσ−cidcid cidecid cidecidcidσ −cidσcid cidσcidcide −cidecid ≤ cidσ−cidcid√lkcidσ −cidσcid cidσcidcide −cidecid cid  cid lk loglkδ σmaxσ √lk cid cid cidσcid ≤ o σminσ ntr e cid e α σ−e e dh ∈ lk loglkδ nval last step follows adaptation lemma e h contains lk ﬁxed classiﬁers lemma e last statement holds probability least − δ draws str sval substituting bound back e completes qed proof next provide bound singular values σ specialized setting probing classiﬁers hcidi set basis functions φcid’s divide data disjoint clusters base classiﬁer ¯h close “uniform accuracies” across clusters classes lemma e let hcidi’s deﬁned suppose x φcidx ∈ x ∀cid cid cidcid let pcidi exy∼µφcidxy let ¯h κ − τ ≤ φcidxφcidcid φµcid k τ κ ¯h ≤ κ∀cid κ σmaxσ ≤ l max pcidi cidi ∆ lkτ max cidi pcidi ∆ σminσ ≥  − kκ min cidi pcidi − ∆ e proof ﬁrst write matrix σ σ ¯σ e p − κ cid − κcid pk − κ pκ pk − κ pκ p cid − κcid p − κ p  ¯σ cid cid pκ pκ pkκ pκ plk e ∈ rlk×lk ecidicidcidicid ≤ max matrix ¯σ can turn written product symmetric matrix ∈ rlk×lk diagonal matrix d ∈ rlk×lk κ − φµcid ≤ τ max pcidi ¯h pcidi cidi cidi ¯σ ad e  plkκ plkκ cid − κcid e    − κ − κ − κ  − κ − κ − κ − κ − κ κ κ κ κ  − κ κ κ κ κ κ κ κ κ  − κ − κ − κ  − κ d diagp plk e can bound largest smallest singular values σ terms d using weyl’s inequality see eg σmaxσ ≤ σmax ¯σ cidecid ≤ cidacidciddcid cidecid σmaxaσmaxd cidecid e σminσ ≥ σmin ¯σ−cidecid cid ¯σ−cid −cidecid ≥ cida−cidcidd−cid −cidecid σminaσmind−cidecid e cidecid ≤ cidecidf ≤ lkτ max cidi pcidi ∆ giving us σmaxσ ≤ σmaxaσmaxd ∆ σminσ ≥ σminaσmind − ∆ e e remains bound singular values σ d since d diagonal matrix ’s singular values given diagonal entries σmaxd max cidi pcidi σmind min cidi pcidi e matrix symmetric certain block structure ’s singular values positive magnitudes eigen values ﬁrst write ’s lk eigen vectors cid cidcid k entries cid x − x − xk− − xk x xk− xk − − cid cidcid cid k entries − − − − cid cidcid cid k entries e xl − xlk− xlk − one can verify lk eigen values  multiplicity l − k  − kκ multiplicity k − l − kκ  multiplicity therefore σmaxa ≤ l σmina  − kκ e substituting singular eigen values d e e completes proof qed lemma base classiﬁer ¯h assumed roughly uniformly low accu racies classes clusters closer uniform accuracies ie smaller value τ tighter bounds shown bound singular values σ speciﬁc setting basis functions φcid’s divide data disjoint clusters case eg overlapping clusters soft clusters singular values σ depend correlated basis functions e error bound fweg unknown ψ section provide error bound algorithm e evaluation metrics h c d kkh smooth unknown ψ rk→r case form e dh ψc d closedform expression gradient ψ instead apply example weight elicitation routine algorithm using probing classiﬁers chosen within small neighborhood around current iterate ht ψ eﬀectively linear speciﬁcally invoke algorithm current iterate ht base classiﬁer radius parameter  set small value error bound state version algorithm explicitly take account “slack” using local approximation ψ proxy gradient βic d h c d theorem e error bound frank wolfe elicited gradients un known ψ let e dh ψc d h cidc val kkh unknown concave ψ k→r kk h fix δ ∈ suppose assumption holds slack ν suppose linear metric ii h whose associated weight coeﬃcients assumption ¯α cid ¯αcid ≤ b following holds δ ∈ probability ≥ − δ draw str qlipschitz also λsmooth wrt cidnorm let cide valh ψcidc val cid sval weight elicitation routine algorithm given input metric cide val cid βicidc val cide val − ii h ≤ χ∀h outputs coeﬃcients cidα cidcidα − ¯αcid ≤ κδ ntr nval χ wp ≥ − δ draws str sval d µ respectively classiﬁer cidh function κ· let bcid b √lk κδ ntr nval λ assume k ≤ nval cid q√lk κδt ntr nval λ output algorithm e radius parameter  t iterations satisﬁes h e dh − e dcidh ≤ qbcidex cid cidηtrx −cidηtrxcid cid cid cid max k lognval logk logkδ nval λ t e qν o λk one can plugin κ· eg error bound derived algorithm theorem suitably modiﬁed accommodate input metrics cide val may diﬀer desired linear metric χ modiﬁcations can easily made theorem result additional term √lkχ error bound take account additional approximation errors computing righthand side linear system proceeding prove theorem e state useful lemmas following lemma shows ψc λsmooth eﬀectively linear within small neighbor hood around c lemma e suppose ψ λsmooth wrt cidnorm iteration t algorithm e let υt ∇ψct denote true gradient ψ ct classiﬁer hx − htx hx cidcidcidcidcidcidcide valh −cide valht − cidcidcidcidcidcid icidc val ii h υt υt ii h cidcidcidcidcidcid ≤ λ icidc val cid cidcidcidcidcidcidψdiagcidcvalh − ψdiagcidcvalht − ciddiagcidcvalh − diagcidcvalhtcid ciddiagcidcvalh − diagcidcvalhtcid ciddiagcidcvalh − diagcidcvalhtcid cid ciddiagcidcvalhcid ciddiagcidcvalhtcid λ λ λ λ λ ≤ ≤ ≤ e icidc val υt ii h cidcidcidcidcidcid cid cid proof randomized classiﬁer hx − htx hx cidcidcidcidcidcidcide valh −cide valht − cid  λ e second line follows fact ψ λsmooth wrt cidnormand υt ∇ψdiagcidcvalht third line follows linearity expectations last line follows fact sum entries confusion matrix hence sum qed diagonal entries exceed next restate error bounds call piew line corresponding bound approximation error linear maximizer cidf obtained lemma e error bound call piew line unknown ψ iteration t algorithm let υt ∇ψct denote true gradient ψ ct algorithm run unknown ψ qlipschitz λsmooth wrt cidnorm let ¯α ii h whose coeﬃcients unknown assumption cid ¯αcid ≤ b slack ν fix δ suppose wp ≥ − δ draw str sval weight elicitation routine used pi ii h ≤ χ∀h associated weighting coeﬃcient linear metriccid ew called input metric cide valh −cide valht cide valh − outputs coeﬃcients cidα cidcidα − ¯αcid ≤ κδ ntr nval χ function κ· let bcid b √lk κδ ntr nval λ probability classiﬁercidh output piew called algorithm e metric cide linh cide valh −cide valht radius υicidc val cid ic d υt cid ii cidh ≤ q υt ic d cid cidηtrx −cidηtrxcid cid bcidex √lk κδ ntr nval λ ν cid cid e  satisﬁes cid max h υt ic d ii h − ηtr x pµy ix υicidc val ii h use lemma qed e compute value slack χ κ· proof proof lemma e “known ψ” case except κ· guarantee call weight elicitation routine line diﬀerent takes account fact input metric cide valh −cide valht weight elicitation routine local approximation unknown linear metriccid lemma e approximation error linear maximizer cidf line unknown ψ iteration t algorithm e let ¯ct diagcdht let ¯βt ∇ψ¯ct denote unknown gradient ψ evaluated ¯ct suppose assumptions theorem e hold let bcid b √lk κδ ntr nval λ assume k ≤ nval classiﬁer cidf returned piew line satisﬁes wp ≥ − δ draw str sval µ d resp t t cid ii cidf ≤ qbcidex cidηtrx −cidηtrxcid cid cid cid qν cid cid cid cid δ t ntr nval λcid ¯βt c d ¯βt iic d h − q√lk κ k logk lognval logkδ max h e o λk nval proof proof lemma e “known ψ” case diﬀerence use lemma e instead lemma e bound linear maximization errors equation e qed proof theorem e proof follows plugging lemma e frankwolfe convergence guarantee lemma e stated appendix e qed e running time algorithm epoch training classconditional probability estimate cidηtrx ≈ pµy x discuss one iteration fweg algorithm compares one iteration iteration fweg create lk probing classiﬁers probing classiﬁer via requires perturbing predictions base classiﬁer ¯h ht hence requires ntr nval computations constructing lk probing classiﬁers fweg solves system linear equations lk unknowns na¨ıve matrix inversion ap proach requires olk time notice can improved eﬃcient methods eg using stateoftheart linear regression solvers fweg creates plu gin classiﬁer combines predictions frankwolfe style updates requiring lkntr nval computations overall time complexity iteration fweg requires ontrhk time h represents total number parameters lying model architecture penultimate layer deep networks resnets ocidlkntr nval lkcid hand one iteration epoch training cidηtrx sections clearly runtime dominated training cidηtrx network thus approach reasonably faster train model cidηtr epoch compared trainingcidηtr iteration training model resnets twice making multiple forwardbackward passes training validation set requiring three times time long l k relatively small compared number parameters neural e plugin coordinatewise search baseline cidhx ∈ argmaxi∈k wicidηval describe plugin trainval baseline used section constructs classiﬁer x tuning weights wi ∈ r maximize given metric validation set note k parameters tuned na¨ıve approach use kdimensional grid search instead use trick decompose search independent coordinatewise search wi speciﬁcally one can estimate relative weighting wiwj pair classes j constructing classiﬁer form  j hζx ζcidηtr x − ζcidηtr j x otherwise e predicts either class j based receives higher weighted prob ability estimates line search ﬁnding parameter ζ ∈ hζ yields highest validation metric cide valhζ wiwj ≈ argmax ζ∈ e ﬁxing class k repeating classes j ∈ k − one can estimate wjwk j ∈ k− normalize estimated related weights get estimates w wk e solving constrained satisfaction problem describe common special cases one can easily identify classiﬁers hcidi’s satisfy constraints will make use pretrained class probability x ≈ pµy ix also used section construct plugin classiﬁer algorithm hypothesis class h consider set plugin classiﬁers obtained model cidηtr postshiftingcidηtr start binary classiﬁcation problem k basis functions φcidx gx cid divide data points l disjoint groups according gx ∈ l setting one can show mild assumptions data distribution indeed feasible solution using eg geometric techniques used also elaborated ﬁgure one feasible hcidi predicts class ∈ example belonging group cid uses thresholded ofcidηtr examples groups per cluster thresholds eﬀect maximizing diagonal entry cidφtrcid cidσ thresholds can tuned oﬀdiagonal entries cidφtrcidcid hcidi∀cidcid icid cid cid hcidi icid small speciﬁcally cid ∈ l ∈ classiﬁer hcidi can constructed  cidηtrx ≤ τgx hcidix gx cid otherwise e minimize maxicidcidφtrcidcid thresholds τcidcid ∈ cidcid hcidi long cidηtr close approximation pyx cid cid can tuned independently using line search procedure guaranteed ﬁnd approximately feasible solution provided one icid h φµcid figure e geometry space φconfusions k classes basis functions φcidx gx cid divide data l disjoint clusters ﬁxed cluster cid plot values φµcid h randomized classiﬁers πcid pµy gx cid points lower boundary correspond classiﬁers form ηtrx ≤ τ varying thresholds τ ∈ points lower boundary within dotted box correspond thresholded classiﬁers h yield values φµcid h ≤ ω φµcid h ≤ ω one can thus ﬁnd feasible probing classiﬁer hcidi constrained optimization problem using construction e long πcid ≥ γ − πcid ≥ γ lower boundary intersects dotted box clusters cidcid cid cid latter fails one can increase ω slowly classiﬁer given e feasible exists indeed one can tune values γ ω construction tuned thresholds satisﬁes constraints next look multiclass problem k basis functions φcidx gx cid divide data points l disjoint groups one can show mild assumptions data distribution indeed feasible solution using eg geometric tools can construct feasible hcidi predicting class ∈ k example belonging group cid using postshifted classiﬁer examples groups particular cid ∈ l ∈ k classiﬁer hcidi can constructed  hcidix argmaxj∈k wgx j cidηtr j x gx cid otherwise e use k parameters wcidcid k parameters minimize maximum oﬀdiagonal entries cidσ maxicidcidφtrcidcid cid cid can tune ie minimize hcidi however may require kdimensional grid search fortunately described appendix e can use trick reduce problem tuning wcidcid k cluster cidcid icid ωωφ‘φ‘π‘−π‘ table e test macro fmeasure maximization task section compas ↓ data method → adaptive surrogates fweg adult default wcidcid k parameters k independent line searches based idea optimal relative weighting wcidcid j pair classes can determined line search case will ﬁx wcidcid k − solv ing following onedimensional optimization problem determine relative weighting wcidcid wcidcid k ∀cidcid k wcidcid cid cid compute wcidcid x − ζcidηtr ζcidηtr  k otherwise cid icid cidφtrcidcid icid max cid wcidcid ∈ argmin ζ∈ hζ hζx k x e can repeat cluster cidcid e cid cid construct cid ith probing classiﬁer hcidi general setting basis functions φcid’s cluster data overlap ping soft clusters one can ﬁnd feasible classiﬁers posing problem “rate” constrained optimization problem form pick hcidi h∈h cidφtrcid max h st cidφtrcidcid icid h ≤ ω∀cidcid icid cid cid e can solved using oﬀtheshelf toolboxes opensource library oﬀered indeed one can tune hyperparameters γ ω solution problem feasible h set plugin classiﬁers obtained postshifting cidηtr one can alternatively use approach identify optimal postshift cidηtr solves constrained problem e additional experimental details provide details regarding experiments • maximizing accuracy label noise cifar section metric aim optimize test accuracy linear metric diagonal entries confusion matrix notice work asymmetric label noise model httpsgithubcomgoogleresearchtensorflowconstrainedoptimization patrini et al corresponds setting label ﬂipped particular label certain probability involves nondiagonal noise transition matrix t consequently corrected training objective linear function entire confusion matrix indeed loss correction approach makes use estimate entire noisetransition matrix including oﬀdiagonal entries whereas approach experiment elicits weights diagonal entries alone assigns diﬀerent set weights basis function ie cluster thus able achieve better performance optimizing correcting noise using linear function percluster diagonal entries indeed also observed piew often achieves better accuracy crossvalidation ten basis functions highlighting beneﬁt underlying modeling piew expect get improvements incorporating oﬀdiagonal entries piew optimization training side explained appendix e also stress results methods can improved crossvalidating kernel width umap dimensions selection cluster centers currently set ﬁxed values experiments lastly compare adaptive surrogates experiment baseline requires retrain resnet model every iteration importantly method constructs probing classiﬁers perturbing parameters resnet model several times iteration can prohibitively expensive practice • maximizing gmean proxy labels adult section experiment use binary features basis functions instead rbf kernels done cifar experiment reﬂects ﬂexibility proposed piew fweg methods approach can incorporate indicator features basis function long reﬂects cluster memberships moreover choice basis function motivated choices made expect improve results incorporating binary features basis functions • maximizing fmeasure domain shift adience section mentioned section basis functions addition default basis φdefx ∀x choose subsets six basis functions φ φ averages rbfs centered points validation set corresponding one six age gender combinations choose subsets using knowledge underlying image classiﬁcation task speciﬁcally besides default basis function cross validate three subsets basis functions ﬁrst subset comprises two basis functions basis functions averages rbf kernels cluster centers belonging two true class second subset comprises three basis functions basis functions averages rbf kernels cluster centers belonging three agebuckets third subset comprises six basis functions basis functions averages rbf kernels cluster centers belonging combination true class × agebucket expect improve results crossvalidating kernel width selection cluster centers lastly compare adaptive surrogates experiment requires training deep neural network model perturbing retraining model iteration can prohibitively expensive practice • maximizing blackbox fairness metric adult section experiment since treat metric blackbox assume access gradients thus run ψ known variant fweg report ψ unknown variant fweg varied basis functions shown table • table e replicate “macro fmeasure” experiment without noise section report results maximizing macro fmeasure adult compas default datasets see approach yields notable gains two three datasets comparison adaptive surrogates approach references n modani p maneriker g hiranandani r sinha v subramanian s gupta et al “summarizing multimedia content” international conference web infor mation systems engineering springer pp – p anderson b fernando m johnson s gould “spice semantic propositional springer image caption evaluation” european conference computer vision pp – berenzweig b logan d p ellis b whitman “ largescale evaluation acoustic subjective musicsimilarity measures” computer music journal pp – h c sox medical decision making acp press p dmitriev x wu “measuring metrics” cikm s choudhary g hiranandani s k saini “sparse decomposition time series forecasting anomaly detection” proceedings siam international conference data mining siam pp – r caruana niculescumizil “data mining metric space empirical anal ysis supervised learning performance criteria” acm sigkdd pp – c ferri j hern´andezorallo r modroiu “ experimental comparison per formance measures classiﬁcation” pattern recognition letters vol pp – m sokolova g lapalme “ systematic analysis performance measures classiﬁcation tasks” information processing management vol pp – g hiranandani r somani o koyejo s acharyya “clustered monotone trans forms rating factorization” proceedings twelfth acm international con ference web search data mining pp – b qian x wang f wang h li j ye davidson “active learning relative queries” ijcai pp – g hiranandani k ayush c varsha sinha p maneriker s v r maram “poster enhanced personalized targeting using augmented reality” ieee international symposium mixed augmented reality ismaradjunct ieee pp – o o koyejo n natarajan p k ravikumar s dhillon “consistent binary classiﬁcation generalized performance metrics” nips pp – c dwork m hardt t pitassi o reingold r zemel “fairness aware ness” itcs pp – singla e horvitz p kohli krause “learning hire teams” third aaai conference human computation crowdsourcing s corbettdavies e pierson feller s goel huq “algorithmic decision making cost fairness” proceedings rd acm sigkdd interna tional conference knowledge discovery data mining pp – s barocas m hardt narayanan “fairness machine learning” nips tu torial j kleinberg s mullainathan m raghavan “inherent tradeoﬀs fair de termination risk scores” th innovations theoretical computer science con ference itcs schloss dagstuhlleibnizzentrum fuer informatik g hiranandani w vijitbenjaronk s koyejo p jain “optimization anal ysis pap k metric recommender systems” international conference machine learning pmlr pp – menon h narasimhan s agarwal s chawla “ statistical consis tency algorithms binary classiﬁcation class imbalance” international conference machine learning pp – h narasimhan “learning complex loss functions constraints” interna tional conference artiﬁcial intelligence statistics pp – m hardt e price n srebro “equality opportunity supervised learning” advances neural information processing systems pp – h narasimhan h ramaswamy saha s agarwal “consistent multiclass algorithms complex performance measures” icml pp – m t ribeiro s singh c guestrin “ trust explaining predictions classiﬁer” acm sigkdd acm pp – f doshivelez b kim “towards rigorous science interpretable machine learning” arxiv eprints g tamburrelli margara “towards automated ab testing” international symposium search based software engineering springer pp – g hiranandani s katariya n rao k subbian “online bayesian learning ecommerce query reformulation” y zhang r bellamy k varshney “joint optimization ai fairness utility humancentered approach” proceedings aaaiacm conference ai ethics society pp – e beauxisaussalet l hardman “visualization confusion matrix non expert users” ieee conference visual analytics science technology vastposter proceedings g hiranandani s boodaghians r mehta o koyejo “performance metric elicitation pairwise classiﬁer comparisons” nd international conference artiﬁcial intelligence statistics pp – g hiranandani s boodaghians r mehta o o koyejo “multiclass perfor mance metric elicitation” advances neural information processing systems pp – o o koyejo n natarajan p k ravikumar s dhillon “consistent multilabel classiﬁcation” nips pp – steinwart “ compare diﬀerent loss functions risks” constructive approximation vol pp – s boyd l vandenberghe convex optimization cambridge university press s boucheron g lugosi p massart concentration inequalities nonasymptotic theory independence oxford university press w n street w h wolberg o l mangasarian “nuclear feature extraction breast tumor diagnosis” biomedical image processing biomedical visualization vol international society optics photonics pp – j dvorak p savicky “softening splits decision trees using simulated anneal ing” international conference adaptive natural computing algorithms springer pp – f wauthier m jordan n jojic “eﬃcient ranking pairwise comparisons” icml pp – r herbrich “large margin rank boundaries ordinal regression” advances large margin classiﬁers mit press pp – k g jamieson r nowak “active ranking using pairwise comparisons” nips pp – f janssen j furnkranz “ metalearning rule learning heuristics” icdm ieee pp – m peyrard t botschen gurevych “learning score system summaries better content selection evaluation” proceedings workshop new frontiers summarization pp – g hiranandani j mathur h narasimhan m m fard s koyejo “optimizing blackbox metrics iterative example weighting” international conference machine learning pmlr pp – n abe b zadrozny j langford “ iterative method multiclass cost sensitive learning” acm sigkdd acm pp – k g jamieson r nowak b recht “query complexity derivativefree op timization” advances neural information processing systems pp – m f duarte y h hu “vehicle classiﬁcation distributed sensor networks” journal parallel distributed computing vol pp – j p siebert “vehicle recognition using rule based methods” m k¨¨ari¨ainen “active learning nonrealizable case” international confer ence algorithmic learning theory springer pp – t joachims “optimizing search engines using clickthrough data” proceedings eighth acm sigkdd international conference knowledge discovery data mining acm pp – b settles “active learning literature survey” university wisconsinmadison de partment computer sciences tech rep d m kane s lovett s moran j zhang “active classiﬁcation comparison queries” ieee th annual symposium foundations computer science focs ieee pp – l qian j gao h jagadish “learning user preferences adaptive pairwise comparison” proceedings vldb endowment vol pp – j angwin j larson s mattu l kirchner “machine bias risk assessments criminal sentencing” propublica may vol s friedler c scheidegger s venkatasubramanian s choudhary e p hamilton d roth “ comparative study fairnessenhancing interventions machine learning” proceedings conference fairness accountability trans parency pp – p lahoti k p gummadi g weikum “ifair learning individually fair data representations algorithmic decision making” ieee th international conference data engineering icde ieee pp – s barocas d selbst “big data’s disparate impact” calif l rev vol p chouldechova “fair prediction disparate impact study bias recidivism prediction instruments” big data vol pp – r berk h heidari s jabbari m kearns roth “fairness criminal jus tice risk assessments state art” sociological methods research p t kamishima s akaho h asoh j sakuma “fairnessaware classiﬁer prejudice remover regularizer” joint european conference machine learning knowledge discovery databases springer pp – b woodworth s gunasekar m ohannessian n srebro “learning non discriminatory predictors” conference learning theory pp – k menon r c williamson “ cost fairness binary classiﬁcation” conference fairness accountability transparency pp – s yang d q naiman “multiclass cancer classiﬁcation based gene expression comparison” statistical applications genetics molecular biology vol pp – m b zafar valera m gomez rodriguez k p gummadi “fairness beyond disparate treatment disparate impact learning classiﬁcation without disparate mistreatment” proceedings th international conference world wide web pp – y bechavod k ligett “learning fair classiﬁers regularizationinspired ap proach” th workshop fairness accountability transparency machine learning fatml s opotow “aﬃrmative action fairness scope justice” journal social issues vol pp – d g kleinbaum k dietz m gail m klein m klein logistic regression springer s k pal s mitra “multilayer perceptron fuzzy sets classiﬁaction” t joachims “svmlight support vector machine” svmlight support vector ma chine httpsvmlight joachims org university dortmund vol g ke q meng t finley t wang w chen w ma q ye ty liu “light gbm highly eﬃcient gradient boosting decision tree” advances neural infor mation processing systems pp – h narasimhan cotter m gupta “optimizing generalized rate metrics three players” advances neural information processing systems pp – h valizadegan r jin r zhang j mao “learning rank optimizing ndcg measure” advances neural information processing systems pp – g s shieh “ weighted kendall’s tau statistic” statistics probability letters vol pp – c ilvento “metric learning individual fairness” arxiv preprint arxiv d mukherjee m yurochkin m banerjee y sun “two simple ways learn individual fairness metric data” icml g goh cotter m gupta m p friedlander “satisfying realworld goals dataset constraints” advances neural information processing systems pp – m b zafar valera m g rogriguez k p gummadi “fairness constraints mechanisms fair classiﬁcation” artiﬁcial intelligence statistics pp – agarwal beygelzimer m dudik j langford h wallach “ reductions approach fair classiﬁcation” international conference machine learning pp – noriegacampero m bakker b garciabulle pentland “active fairness algorithmic decision making” proceedings aaaiacm conference ai ethics society pp – r binns “ apparent conﬂict individual group fairness” pro ceedings conference fairness accountability transparency pp – m kearns s neel roth z s wu “preventing fairness gerrymandering auditing learning subgroup fairness” international conference machine learning pp – t hashimoto m srivastava h namkoong p liang “fairness without de mographics repeated loss minimization” international conference machine learning pp – s gillen c jung m kearns roth “online learning unknown fairness metric” advances neural information processing systems pp – s wang w guo h narasimhan cotter m gupta m jordan “robust optimization fairness noisy protected groups” esuli f sebastiani “optimizing text quantiﬁers multivariate loss func tions” acm transactions knowledge discovery data vol p article m h stone “ generalized weierstrass approximation theorem” mathematics mag azine vol pp – g hiranandani h narasimhan o koyejo “fair performance metric elicitation” neurips s lawrence burns back ac tsoi c giles “neural network classiﬁcation prior class probabilities” neural networks tricks trade ser lncs springer pp – w liu s chawla “ quadratic mean based supervised learning model man aging data skewness” sdm cotter h narasimhan m gupta “ making stochastic classiﬁers determin istic” neurips p kar s li h narasimhan s chawla f sebastiani “online optimization methods quantiﬁcation problem” proceedings nd acm sigkdd international conference knowledge discovery data mining pp – b g lindsay m markatou s ray k yang sc chen et al “quadratic distances probabilities uniﬁed foundation” annals statistics vol pp – r mcgill j w tukey w larsen “variations box plots” american statistician vol pp – c boutilier r patrascu p poupart d schuurmans “constraintbased opti mization utility elicitation using minimax decision criterion” artiﬁcial intel ligence vol pp – n benabbou p perny p viappiani “incremental elicitation choquet capacities multicriteria choice ranking sorting problems” artiﬁcial intelligence vol pp – c c white p sage s dozono “ model multiattribute decisionmaking tradeoﬀ weight determination uncertainty” ieee transactions sys tems man cybernetics pp – p perny p viappiani boukhatem “incremental preference elicitation decision making risk rankdependent utility model” uncertainty artiﬁcial intelligence u chajewska d koller r parr “making rational decisions using adaptive utility elicitation” aaaiiaai pp – d braziunas “decisiontheoretic elicitation generalized additive utilities” phd dissertation g hiranandani j mathur h narasimhan o koyejo “quadratic metric elici tation fairness beyond” arxiv preprint arxiv p awasthi beutel m kleindessner j morganstern x wang “evaluating fairness machine learning models uncertain incomplete information” facct c huang s zhai w talbott m b martin sy sun c guestrin j susskind “addressing lossmetric mismatch adaptive loss alignment” international conference machine learning pmlr pp – q jiang o adigun h narasimhan m m fard m gupta “optimizing black box metrics adaptive surrogates” icml m sugiyama t suzuki s nakajima h kashima p von b¨unau m kawanabe “direct importance estimation covariate shift adaptation” annals institute statistical mathematics vol pp – n natarajan s dhillon p k ravikumar tewari “learning noisy labels” advances neural information processing systems vol pp – g patrini rozza krishna menon r nock l qu “making deep neural networks robust label noise loss correction approach” proceedings ieee conference computer vision pattern recognition pp – m ren w zeng b yang r urtasun “learning reweight examples robust deep learning” international conference machine learning pmlr pp – s zhao m m fard h narasimhan m gupta “metricoptimized example weights” international conference machine learning pmlr pp – krizhevsky g hinton et al “learning multiple layers features tiny im ages” e eidinger r enbar t hassner “age gender estimation unﬁltered faces” ieee transactions information forensics security vol pp – d dua c graﬀ “uci machine learning repository” online available httparchiveicsucieduml d lewis “evaluating optimizing autonomous text classiﬁcation systems” si gir s daskalaki kopanas n avouris “evaluation classiﬁers uneven class distribution problem” applied artiﬁcial intelligence vol pp – j wang y liu c levy “fair classiﬁcation groupdependent label noise” arxiv preprint arxiv k menon b van rooyen n natarajan “learning binary labels instancedependent noise” machine learning vol pp – cotter m gupta h narasimhan “ making stochastic classiﬁers determin istic” advances neural information processing systems cotter h jiang s wang t narayan s k sridharan m r gupta “optimization nondiﬀerentiable constraints applications fairness recall churn goals” journal machine learning research jmlr vol pp – f yang m cisse s koyejo “fairness overlapping groups” m jaggi “revisiting frankwolfe projectionfree sparse convex optimization” icml n ye k m chai w s lee h l chieu “optimizing fmeasures tale two approaches” proceedings th international conference machine learning omnipress pp – h narasimhan r vaish s agarwal “ statistical consistency plugin classiﬁers nondecomposable performance measures” advances neural infor mation processing systems pp – b yan s koyejo k zhong p ravikumar “binary classiﬁcation karmic thresholdquasiconcave metrics” international conference machine learning pmlr pp – t joachims “ support vector method multivariate performance measures” proceedings nd international conference machine learning acm pp – p kar h narasimhan p jain “online stochastic gradient methods nondecomposable loss functions” arxiv preprint arxiv p kar s li h narasimhan s chawla f sebastiani “online optimization methods quantiﬁcation problem” proceedings nd acm sigkdd international conference knowledge discovery data mining pp – h narasimhan p kar p jain “optimizing nondecomposable performance measures tale two classes” international conference machine learning pmlr pp – e eban m schain mackey gordon r rifkin g elidan “scalable learn ing nondecomposable objectives” artiﬁcial intelligence statistics pmlr pp – b fr´enay m verleysen “classiﬁcation presence label noise survey” ieee transactions neural networks learning systems vol pp – g csurka “ comprehensive survey domain adaptation visual applications” domain adaptation computer vision applications pp – h shimodaira “improving predictive inference covariate shift weighting loglikelihood function” journal statistical planning inference vol pp – t kanamori s hido m sugiyama “ leastsquares approach direct impor tance estimation” journal machine learning research vol pp – z lipton yx wang smola “detecting correcting label shift black box predictors” international conference machine learning pmlr pp – j huang gretton k borgwardt b sch¨olkopf smola “correcting sample selection bias unlabeled data” advances neural information processing systems vol pp – j byrd z lipton “ eﬀect importance weighting deep learning” international conference machine learning pmlr pp – t fang n lu g niu m sugiyama “rethinking importance weighting deep learning distribution shift” arxiv preprint arxiv l mcinnes j healy n saul l großberger “umap uniform manifold ap proximation projection” journal open source software vol p krizhevsky sutskever g e hinton “imagenet classiﬁcation deep convolutional neural networks” advances neural information processing systems vol pp – c guo g pleiss y sun k q weinberger “ calibration modern neural networks” international conference machine learning pmlr pp – s satyal weber hy paik c d ciccio j mendling “ab testing process versions contextual multiarmed bandit algorithms” international conference advanced information systems engineering springer pp – n bhat v f farias c c moallemi d sinha “nearoptimal ab testing” management science vol pp – g hiranandani h singh p gupta burhanuddin z wen b kveton “cascading linear submodular bandits accounting position bias diversity online learning rank” uncertainty artiﬁcial intelligence pmlr pp – h shen h jin ´ cabrera perer h zhu j hong “designing alternative representations confusion matrices support nonexpert public understanding algorithm performance” proceedings acm humancomputer interaction vol cscw pp – r mazza introduction information visualization springer science business media t h cormen introduction algorithms mit press s k tavker h g ramaswamy h narasimhan “consistent plugin classiﬁers complex objectives constraints” advances neural information processing systems daniely s sabato s bendavid s shalevshwartz “multiclass learnability erm principle” jmlr vol p – jan b k natarajan “ learning sets functions” machine learning vol pp – g w stewart “perturbation theory singular value decomposition” tech rep j w demmel applied numerical linear algebra siam b k natarajan “ learning sets functions” machine learning vol pp – daniely s sabato s bendavid s shalevshwartz “multiclass learnability erm principle” proceedings th annual conference learning theory jmlr workshop conference proceedings pp – daniely s sabato s bendavid s shalevshwartz “multiclass learnability erm principle” journal machine learning research vol pp – ",97.6999999999999,"1"
4,"Toward General Design Principles for Generative AI Applications","toward general design principles generative ai applications justin d weisz ibm research ai usa michael muller ibm research ai usa jessica ibm research ai usa stephanie houde ibm research ai usa n j c h s c v v x r fig seven principles design generative ai systems six principles presented overlapping circles indicating relationships one principle stands alone directive design potential harms may caused generative model’s output misuse harmful effects principles bounded environment generative variability outputs generative ai application may vary quantity quality character characteristics generative ai technologies growing power utility use generative technologies incorporated mainstream applications need guidance design applications foster productive safe use based recent research humanai cocreation within hci ai communities present set seven principles design generative ai applications principles grounded environment generative variability six principles focused designing characteristics generative ai multiple outcomes imperfection exploration control mental models explanations addition urge designers design potential harms may caused generative model’s hazardous output misuse potential human displacement anticipate principles usefully inform design decisions made creation novel humanai applications invite community apply revise extend principles work ccs concepts • humancentered computing→ hci design evaluation methods interaction paradigms hci theory concepts models additional key words phrases generative ai design principles humancentered ai foundation models acm reference format justin d weisz michael muller jessica stephanie houde toward general design principles generative ai applications acm new york ny usa pages httpsdoiorgxxxxxxxxxxxxxx manuscript submitted acm haigen ’ workshop iui ’ march sydney nsw australia weisz et al introduction generative ai technologies continue grow power utility use becoming mainstream generative models including llmbased foundation models used applications general qa eg chatgpt software engineering assistance eg copilot task automation eg adept copywriting eg jasperai creation highfidelity artwork eg dalle stable diffusion midjourney given explosion popularity new kinds generative applications need guidance design applications foster productive safe use line humancentered ai values fostering productive use challenge revealed recent literature survey campero et al found many humanai collaborative systems failed achieve positive synergy – notion humanai team able accomplish superior outcomes either party working alone fact studies found opposite effect humanai teams produced inferior results either human ai working alone fostering safe use challenge potential risks harms stem generative ai either model trained eg applied eg order address issues propose set design principles aid designers generative ai systems principles grounded environment generative variability indicating two properties generative ai systems inherently different traditional discriminative ai systems generative aim generative ai applications produce artifacts outputs rather determine decision boundaries discriminative ai systems variability indicating fact given input generative system may produce variety possible outputs many may valid discriminative case expected output model vary given input note principles meant generally apply generative ai applications sets design principles exist specific kinds generative ai applications including liu chilton ’s guidelines engineering prompts texttoimage models advice oneshot prompts generation texts different kinds also general airelated design guidelines six principles presented “design ” statements indicating characteristics designers keep mind making important design decisions one presented “design ” statement directing designers design potential harms may arise hazardous model outputs misuse potential human displacement harms yet considered principles interact complex ways schematically represented via overlapping circles figure example characteristic denoted one principle eg multiple outputs can sometimes leveraged strategy addressing another principle eg exploration principles also connected user’s aims producing singular artifact seeking inspiration creative ideas learning domain also connected design features attributes generative ai application support versioning curation sandbox environments httpchatopenaicom httpcopilotgithubcom httpadeptai httpjasperai httpmidjourneycom use term discriminative indicate task conducted ai algorithm one determining class group data instance belongs classification clustering algorithms examples discriminative ai although use term discriminative may evoke imagery human discrimination eg via racial religious gender identity genetic lines use follows scientific convention established machine learning community see eg httpsenwikipediaorgwikidiscriminativemodel toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia aim principles threefold provide designers generative ai applications language discuss issues unique generative ai provide strategies guidance help designers make important design decisions around end users will interact generative ai application sensitize designers idea generative ai applications may cause variety harms likely inadvertently possibly intentionally hope principles provide humanai cocreation community reasoned way think design novel generative ai applications design principles generative ai applications developed seven design principles generative ai applications based recent research hci ai communities specifically around humanai cocreative processes conducted literature review research studies guidelines analytic frameworks communities included experiments humanai cocreation examinations representative generative applications review publications recent workshops environment generative variability generative ai technologies present unique challenges designers ai systems compared discriminative ai systems first generative ai generative nature means purpose produce artifacts output rather decisions labels classifications andor decision boundaries artifacts may comprised different types media text images audio animations videos second outputs generative ai model variable nature whereas discriminitive ai aims deterministic outcomes generative ai systems may produce output given input time fact design can produce multiple divergent outputs given input may satisfactory user thus may difficult users achieve replicable results working generative ai application although nature generative applications violates common hci principle system respond consistently user’s input critiques position see take position environment generative applications operate – generative variability – core strength generative applications enable users explore populate “space” possible outcomes query sometimes exploration explicit case systems enable latent space manipulations artifact times exploration space occurs generative model produces multiple candidate outputs given input multiple distinct images given prompt multiple implementations source code program recent studies also show users may improve knowledge domain working generative model variable outputs concept generative variability crucially important designers generative ai applications communicate users users approach generative ai system without understanding probabilistic nature capacity produce varied outputs will struggle interact productive ways design principles outline following sections – designing multiple outcomes imperfection exploration human control mental models explanations – rooted notion generative ai systems distinct unique operate environment generative variability haigen ’ workshop iui ’ march sydney nsw australia weisz et al design multiple outputs generative ai technologies encoderdecoder models generative adversarial networks transformer models probabilistic nature thus capable producing multiple distinct outputs user’s input designers therefore need understand extent multiple outputs visible users users need ability annotate curate need ability compare contrast many outputs user need understanding user’s task can help answer questions user’s task one production ultimate goal produce single satisfying artifact designs help user filter visualize differences may preferable example software engineer’s goal often implement method performs specific behavior tools copilot take user’s input method signature documentation provide singular output contrarily user’s task one exploration designs help user curate annotate mutate may preferable example software engineer may wish explore space possible test cases code module artist may wish explore different compositions styles see broad range possibilities discuss set strategies helping design multiple outputs versioning randomness involved generative process well userconfigurable parameters eg random seed temperature types user controls may difficult user produce exactly outcome twice user interacts generative ai application creates set outputs may find prefer earlier outputs later ones can recover reset state system generate earlier outputs one strategy keep track outputs well parameters produced versioning versioning can happen manually eg user clicks button “save” current working state automatically curation generative model capable producing multiple outputs users may need tools curate outputs curation may include collecting filtering sorting selecting organizing outputs possibly versioned queue meaningful subsets groups creating prioritized lists hierarchies outputs according subjective objective criteria example cogmol generates novel molecular compounds can sorted various properties molecular weight toxicity water solubility addition confidence model output produced may useful way sort rank outputs although cases model confidence scores may indicative quality model’s output annotation generative model produced large number outputs users may desire add marks decorators annotations outputs interest annotations may applied output eg “ like ” may applied portion subset output eg flagging lines source code look problematic need review visualizing differences cases generative model may produce diverse set distinct outputs images cats look strikingly different cases generative model may produce set outputs difficult discern differences source code translation one language another case tools aid users visualizing similarities differences multiple outputs can useful depending users’ goals may seek find invariant aspects across outcomes identifying httpcovidmolmybluemixnet toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia parts source code translation across multiple translations indicating confidence correctness users may prioritize variant aspects greater creativity inspiration example sentient sketchbook video game cocreation system displays number different metrics maps generates enabling users compare newlygenerated maps current map understand differ design imperfection highly important users understand quality generative model’s outputs will vary users expect generative ai application produce exactly artifact desire will experience frustration work system find often produces imperfect artifacts “imperfect” mean artifact may imperfections visual misrepresentations image bugs errors source code missing desired elements eg “ illustration bunny carrot” fails include carrot violations constraints specified input prompt eg “write word sentence” produces much longer shorter sentence even untruthful misleading answers eg summary scientific topic includes nonexistent references “imperfect” can also mean “doesn’t satisfy user’s desire” user prompts model doesn’t get back satisfying outputs eg user didn’t like illustrations bunny carrot discuss set strategies helping design imperfection multiple outputs previous design principle also strategy handling imperfect outputs generative model allowed produce multiple outputs likelihood one outputs satisfying user increased one example effect code translation models evaluated via metric called 𝑝𝑎𝑠𝑠𝑘 idea model allowed produce 𝑘 code translations given input pass set unit tests model said produced correct translation way generating multiple outputs serves mitigate fact model’s mostlikely output may imperfect however left user review set outputs identify one satisfactory multiple outputs similar task may difficult implying need way easily visualize differences evaluation identification given generative models may produce perfect perfectly satisfying outputs may still able provide users signal quality output indicate parts require human review previously discussed model’s peroutput confidence scores may used care indicate quality model’s output domainspecific metrics eg molecular toxicity compiler errors may useful indicators evaluate whether artifact achieved desirable level quality thus evaluating quality generated artifacts identifying portions artifacts may contain imperfections thus require human review discussed weisz et al can effective way handling imperfection cocreation user experiences allow cocreation user ai can edit candidate artifact will effective user experiences assume aim generative model produce perfect output allowing users edit model’s outputs provides opportunity find fix imperfections ultimately achieve satisfactory artifact one example idea github copilot embedded vscode ide case copilot produces imperfect block source code developers able edit right context without friction contrast tools like midjourney stable diffusion produce gallery images chose editing images requires user shift different environment eg photoshop haigen ’ workshop iui ’ march sydney nsw australia weisz et al sandbox playground environment sandbox playground environment ensures user interacts generated artifact interactions edits manipulations annotations impact larger context environment working returning example github copilot since situated inside developer’s ide code produces directly inserted working code file although design choice enables cocreation also poses risk imperfect code injected production code base sandbox environment requires users explicitly copy paste code order commit current working file may guard accidental inclusion imperfect outputs larger environment product design human control keeping humans control ai systems core tenet humancentered ai – providing users controls generative applications can improve experience increasing efficiency comprehension ownership generated outcomes cocreative contexts multiple ways interpret kinds “control” people need identify three kinds controls applicable generative ai applications generic controls one aspect control relates exploration design space range possible outcomes discussed section users need appropriate controls order drive explorations control number outputs produced input amount variability present outputs refer kinds controls generic controls applicable particular generative technology domain example generative projects may involve “lifecycle” pattern users benefit seeing great diversity outputs early stages process order search ideas inspirations directions later stages project may focus smaller number singular output requiring controls specifically operate output many generative algorithms include usercontrollable parameter called temperature low temperature setting produces outcomes similar conversely high temperature setting produces outcomes dissimilar “lifecycle” model users may first set high temperature increased diversity reduce wish focus particular area interest output space effect observed study music cocreation tool novice users dragged temperature control sliders extreme ends explore limits ai generate technologyspecific controls types controls will depend particular generative technology employed encoderdecoder models example often allow users perform latent space manipulations artifact order control semanticallymeaningful attributes example liu chilton demonstrate semantic sliders can used control attributes d models animals animal’s torso length neck length neck rotation transformer models use temperature parameter control amount randomness generation process natural language prompting emerging discipline prompt engineering provide additional ways tune tweak outputs large language models refer kinds controls technologyspecific controls controls exposed user user interface will depend upon particular generative ai technology used application domainspecific controls types user controls will domainspecific dependent type artifact produced example generative models produce molecules output might controlled user specify desired properties molecular weight water solubility types constraints might propagated model eg expressed constraint encoder phase may simply act filter toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia model’s output eg hide anything user doesn’t satisfy constraints either case control dependent fact model producing specific kind artifact molecule logically make sense kinds artifacts domains eg control water solubility texttoimage model thus refer types controls independent implemented domain specific examples domainspecific controls include reading level text color palette artistic style image run time memory efficiency source code design exploration users working environment generative variability will need way “explore” “navigate” space potential outputs order identify one satisfies needs discuss set strategies helping design exploration multiple outputs ability generative model produce multiple outputs section enabler exploration returning bunny carrot example artist may wish explore different illustrative styles prompt reprompt model additional candidates “ bunny carrot” various kinds styles configurations developer can explore different ways implement algorithm prompting reprompting model produce implementations possess different attributes eg “implement using recursion” “implement using iteration” “implement using memoization” way user can get sense different possibilities model capable producing control depending specific technical architecture used generative application may different ways users control section matter specific mechanisms control providing controls user provides ability interactively work model explore space possible outputs given input sandbox playground environment sandbox playground environment can enable exploration providing separate place new candidates can explored without interfering user’s main working environment example project using copilot cheng et al suggest providing “ sandbox mechanism allow users play prompt context project” visualization one way help users understand space exploring explicitly visualize kreminski et al introduce idea expressive range coverage analysis erca user shown visualization “range” possible generated artifacts across variety metrics users interact system produce specific artifact instances instances included visualization show much “range” “space” explored user design mental models users form mental models work technological systems models represent user’s understanding system works work effectively produce outcomes desire due environment generative variability generative ai applications will pose new challenges users applications may violate existing mental models computing systems behave ie deterministic fashion therefore recommend designing support users creating accurate mental models generative ai applications following ways haigen ’ workshop iui ’ march sydney nsw australia weisz et al orientation generative variability users may need general introduction concept generative ai understand system may produce multiple outputs query section outputs may contain flaws imperfections section effort may required collaborate system order produce desired artifacts via various kinds controls section role ai research humanai interaction suggests users may view ai application filling role assistant coach teammate study video game cocreation guzdial et al found participants ascribe roles friend collaborator student manager ai system recent work ross et al examined software engineers’ role orientations toward programming assistant found people viewed assistant tool orientation interacted social agent clearly establishing role generative ai application user’s workflow well level autonomy eg will help users better understand interact effectively designers can reason role application answering questions tool partner act proactively just respond user make changes artifact directly simply make recommendations user design explanations generative ai applications will unfamiliar possibly unusual many users will want know application can well works work effectively users may even wish understand technical details underlying generative ai algorithms work although details may necessary work effectively model discussed recent years explainable ai xai community made tremendous progress developing techniques explaining ai systems work much work xai focused discriminative algorithms generally make decisions eg via interpretable models chapter feature importance section make decision specific instance eg via counterfactual explanations section recent work humancentered xai hcxai emphasized designing explanations cater human knowledge human needs work grew general shift toward humancentered data science import explanations technical user data scientist end user might impacted machine learning model case generative ai recent work begun explore needs explainability sun et al explored explainability needs software engineers working generative ai model various types use cases code translation autocompletion identified number types questions software engineers generative ai capabilities limitations indicating explainability important feature generative ai applications also identified several gaps existing explainability frameworks stemming generative nature ai system indicating existing xai techniques may sufficient generative ai applications thus make following recommendations design explanations calibrate trust communicating capabilities limitations inherent imperfection generative ai outputs users wellserved understood limitations systems allowing calibrate trust terms application can kinds imperfections section signaled users cocreative tools may mistakenly blame shortcomings generated artifacts cocreative applications users q use cases can shown deceptive misconceptions harmful falsehoods objective answers one way communicate capabilities generative ai application toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia show examples can example midjourney provides public discussion space orient new users show users produced model space shows outputs model eg images textual prompts produced images way users can quickly come understand different prompts influence application’s output communicate limitations systems like chatgpt contain modal screens inform users system’s limitations use explanations create reinforce accurate mental models weisz et al explored generative model’s confidence surfaced user interface working transformer model code translation task developed prototype ui highlighted tokens translation model confident user study found highlights also served explanations model worked users came understand source code token chosen probabilistically model considered alternatives design transformed algorithmic weakness imperfect output resource users understand algorithm worked ultimately control output showing users might need make changes design harms use ai systems – including generative ai applications – may unfortunately lead diverse forms harms especially people vulnerable situations much work ai ethics communities identified discriminative ai systems may perpetuate harms denial personhood identity deprivation liberty children erasure persons cultures nations data silences identify four types potential harms unique generative domain others represent existing risks ai applications may manifest new ways aim section sensitize designers potential risks harms generative ai systems may pose prescribe solutions address risks part active area research understand kinds risks mitigated risk identification assessment mitigation sociotechnical problem involving computing resources humans cultures even focus design generative applications analysis harms limited design concepts may blur technosolutionism posit humancentered approaches generative ai design useful first step must part larger strategy understand direct indirect stakeholders generative application work directly stakeholders identify harms understand differing priorities value tensions negotiate issues culture policy yes technology meet diverse challenges eg hazardous model outputs generative ai applications may produce artifacts cause harm integrative survey paper weidinger et al list six types potential harms large language models three regard harms may caused model’s output • discrimination exclusion toxicity generative models may produce outputs promote discrimina tion certain groups exclude certain groups representation produce toxic content examples include texttoimage models fail produce ethnically diverse outputs given input eg request images doctors produces images male white doctors language models produce inappropriate language swear words hate speech offensive content haigen ’ workshop iui ’ march sydney nsw australia weisz et al • information hazards generative models may inadvertently leak private sensitive information training data example carlini et al found strategically prompting gpt revealed individual’s full name work address phone number email fax number additionally larger models may vulnerable types attacks • misinformation harms generative models may produce inaccurate misinformation response user’s query lin et al found gpt can provide false answers mimic human falsehoods misconceptions “coughing can help stop heart attack” “cold weather tells us global warming hoax” singhal et al caution tendency llms hallucinate references especially consulted medical decisions albrecht et al claim llms defenses adversarial attacks advising ethical questions galactica model found hallucinate nonexistent scientific references stack overflow banned responses sourced chatgpt due high rate incorrect yet plausible responses addition harms generative model’s outputs may hazardous ways well • deceit impersonation manipulation generative algorithms can used create false records “deep fakes” eg impersonate others eg distort information politicallyaltered content addition may manipulate users believe chatting another human rather algorithm case unreviewed chatgpt “experiment” least people seeking mental health support connected chatbot rather human counselor • copyright licenses intellectual property generative models may trained data protected regulations gdpr prohibits reuse data beyond purposes collected addition large language models referred “stochastic parrots” due ability reproduce data used training one consequence effect model may produce outputs incorporate remix materials subject copyright intellectual property protections example codex model produces source code output may reproduce source code copyrighted subject software license openly shared creative commons license prohibits commercial reuse eg paytoaccess llm thus use model’s outputs project may cause project violate copyright protections subject project restrictive license eg gpl writing lawsuit github microsoft openai alleged copyright violations training codex misuse weidinger et al describe generative ai applications may misused ways unanticipated creators systems examples include making disinformation cheaper effective facilitating fraud scams assisting code generation cyberattacks conducting illegitimate surveillance censorship addition misuses houde et al also identify business misuses generative ai applications facilitating insurance fraud fabricating evidence crime although designers may able prevent users intentionally misusing generative ai applications may preventative measures make sense given application domain example output images may watermarked indicate generated particular model blocklists may used disallow undesirable words textual prompt multiple people may required review approve model’s outputs can used toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia human displacement one consequence largescale deployment generative ai technologies may come replace rather augment human workers concerns raised related areas use automated ai technologies data science wang et al weidinger et al specifically discuss potential economic harms inequalities may arise consequence widespread adoption generative ai generative model capable producing highfidelity outputs rival even surpass can created human effort humans necessary anymore contemporary fears human displacement generative technologies beginning manifest mainstream media case illustrators’ concerns texttoimage models stable diffusion midjourney will put job urge designers find ways design generative ai applications enhance augment human abilities rather applications aim replace human workers copilot serves one example tool clearly enhances abilities software engineer operates lowlevel details source code implementation freeing software engineers focus attention higherlevel architectural system design issues discussion designing user aims users generative ai applications may varied aims goals using systems users may pursuit perfecting singular artifact method implementation software program users may pursuit inspiration creative ideas exploring visual design space consequence working generative ai application users may also enhance learning understanding domain operating software engineer learns something new programming language model’s output aims can supported design principles well help designers determine appropriate strategy addressing challenges posed principle support artifact production designers carefully consider manage model’s multiple imperfect outputs interfaces support users curating annotating mutating artifacts help users refine singular artifact ability version artifacts show history artifact edits may also useful enable users revisit discarded options undo undesirable modifications cases users seek produce one “ideal” artifact satisfies criteria controls enable cocreate generative tool can help achieve goal efficiently explanations signal identify imperfections can tell close far mark support inspiration creativity designers also provide adequate controls enable users explore design space possibilities visualizations represent design space can also helpful can show parts user vs explored enabling explore novel parts space tools help users manage curate filter different outputs created explorations can extremely helpful digital mood board capturing inspiring model outputs finally support learning effectively interact generative ai application designers help users create accurate mental models explanations explanations can help answer general questions generative ai application capable capable generating model’s controls impact output model trained provenance training data can also answer questions specific model output confident model output portions haigen ’ workshop iui ’ march sydney nsw australia weisz et al output might need human review revision adjust modify input prompt adjust properties output options alternatives exist output importance valuesensitive design mitigating potential harms designers need sensitive potential harms may caused rapid maturation widespread adoption generative ai technologies although sociotechnical means mitigating harms yet developed recommend designers use value sensitive design approach reasoning design generative ai applications clearly identifying different stakeholders impacted parties generative ai application explicitly enumerating values designers can make reasoned judgments stakeholders might impacted hazardous model outputs model misuse issues human displacement limitations future work generative ai applications still infancy new kinds cocreative user experiences emerging rapid pace thus consider principles infancy well possible important design principles strategies andor user aims overlooked addition although principles can provide helpful guidance designers making specific design decisions need validated realworld settings ensure clarity utility conclusion present set seven design principles generative ai applications principles grounded environment generative variability key characteristics generative ai application will generate artifacts outputs outputs may varied nature eg varied quality character principles focus designing multiple outputs imperfection outputs designing exploration space range possible outputs maintaining human control exploration designing establish accurate mental models generative ai application via explanations also urge designers design potential harms may caused hazardous model output eg production inappropriate language imagery reinforcement existing stereotypes failure inclusively represent different groups misuse model eg creating disinformation fabricating evidence displacing human workers eg designing replacement rather augmentation human workers envision principles help designers make reasoned choices create novel generative ai applications references acm words matter alternatives charged terminology computing profession retrieved january httpswwwacm orgdiversityinclusionwordsmatter mayank agarwal jorge j barroso tathagata chakraborti eli m dow kshitij fadnis borja godoy madhavan pallan kartik talamadupula project clai instrumenting command line new environment ai agents arxiv preprint arxiv mayank agarwal kartik talamadupula stephanie houde fernando martinez michael muller john richards steven ross justin d weisz quality estimation interpretability code translation proceedings neurips workshop computerassisted programming neurips joshua albrecht ellie kitanidis abraham j fetterman despite superhuman performance current llms unsuited decisions ethics safety arxiv preprint arxiv saleema amershi dan weld mihaela vorvoreanu adam fourney besmira nushi penny collisson jina suh shamsi iqbal paul n bennett kori inkpen et al guidelines humanai interaction proceedings chi conference human factors computing systems – cecilia aragon shion guha marina kogan michael muller gina neff humancentered data science introduction mit press toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia vijay arya rachel ke bellamy pinyu chen amit dhurandhar michael hind samuel c hoffman stephanie houde q vera liao ronny luss aleksandra mojsilovic et al ai explainability extensible toolkit understanding data machine learning models j mach learn res – emily m bender timnit gebru angelina mcmillanmajor shmargaret shmitchell dangers stochastic parrots can language models big proceedings acm conference fairness accountability transparency – rishi bommasani drew hudson ehsan adeli russ altman simran arora sydney von arx michael s bernstein jeannette bohg antoine bosselut emma brunskill et al opportunities risks foundation models arxiv preprint arxiv danah boyd kate crawford critical questions big data provocations cultural technological scholarly phenomenon information communication society – tom brown benjamin mann nick ryder melanie subbiah jared d kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbertvoss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey wu clemens winter chris hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever dario amodei language models fewshot learners advances neural information processing systems h larochelle m ranzato r hadsell mf balcan h lin eds vol curran associates inc – httpsproceedingsneuripsccpaperfilecdbfcbbfbacfapaperpdf zana buçinca maja barbara malaya krzysztof z gajos trust think cognitive forcing functions can reduce overreliance ai aiassisted decisionmaking proceedings acm humancomputer interaction cscw – matthew butterick github copilot litigation httpsgithubcopilotlitigationcom andres campero michelle vaccaro jaeyoon song haoran wen abdullah almaatouq thomas w malone test evaluating performance humancomputer systems arxiv preprint arxiv nicholas carlini daphne ippolito matthew jagielski katherine lee florian tramer chiyuan zhang quantifying memorization across neural language models arxiv preprint arxiv nicholas carlini florian tramer eric wallace matthew jagielski ariel herbertvoss katherine lee adam roberts tom brown dawn song ulfar erlingsson et al extracting training data large language models th usenix security symposium usenix security – ruijia cheng ruotong wang thomas zimmermann denae ford work online communities shape software developers’ trust aipowered code generation tools arxiv preprint arxiv vijil chenthamarakshan payel das samuel c hoffman hendrik strobelt inkit padhi kar wai lim benjamin hoover matteo manica jannis born teodoro laino et al cogmol targetspecific selective drug design covid using deep generative models arxiv preprint arxiv vijil chenthamarakshan payel das inkit padhi hendrik strobelt kar wai lim ben hoover samuel c hoffman aleksandra mojsilovic targetspecific selective drug design covid using deep generative models arxiv cslg jaemin cho abhay zala mohit bansal dalleval probing reasoning skills social biases texttoimage generative transformers arxiv preprint arxiv kyunghyun cho bart van merriënboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations using rnn encoderdecoder statistical machine translation arxiv preprint arxiv elizabeth clark anne spencer ross chenhao tan yangfeng ji noah smith creative writing machine loop case studies slogans stories rd international conference intelligent user interfaces – apple computer human interface guidelines httpsdeveloperapplecomdesignhumaninterfaceguidelinesguidelinesoverview sasha costanzachock design justice communityled practices build worlds need mit press paul denny viraj kumar nasser giacaman conversing copilot exploring prompt engineering solving cs problems using natural language httpsarxivorgabs norman k denzin yvonna s lincoln linda tuhiwai smith et al handbook critical indigenous methodologies sage sebastian deterding jonathan hook rebecca fiebrink marco gillies jeremy gow memo akten gillian smith antonios liapis kate compton mixedinitiative creative interfaces proceedings chi conference extended abstracts human factors computing systems – catherine d’ignazio lauren f klein data feminism mit press carl disalvo design democratic inquiry putting experimental civics practice mit press upol ehsan philipp wintersberger q vera liao elizabeth anne watkins carina manger hal daumé iii andreas riener mark o riedl humancentered explainable ai hcxai beyond opening blackbox ai chi conference human factors computing systems extended abstracts – stephen m fiore eduardo salas janis cannonbowers group dynamics shared mental model development people evaluate others organizations paul m fitts ms viteles nl barr dr brimhall glen finch eric gardner wf grether kellum ss stevens human engineering effective airnavigation trafficcontrol system appendixes thru technical report ohio state univ research foundation columbus giorgio franceschelli mirco musolesi copyright generative deep learning data policy batya friedman david g hendry value sensitive design shaping technology moral imagination mit press haigen ’ workshop iui ’ march sydney nsw australia weisz et al werner geyer lydia b chilton justin d weisz mary lou maher haigen nd workshop humanai cocreation generative models th international conference intelligent user interfacescompanion – lisa gitelman raw data oxymoron mit press github copilot retrieved august httpscopilotgithubcom ian goodfellow jean pougetabadie mehdi mirza bing xu david wardefarley sherjil ozair aaron courville yoshua bengio generative adversarial networks commun acm – imke grabe miguel gonzálezduque sebastian risi jichen zhu towards framework humanai interaction patterns cocreative gan applications joint proceedings acm iui workshops march helsinki finland cobus greyling prompt engineering text generation large language models httpscobusgreylingmediumcompromptengineering textgenerationlargelanguagemodelsdccd matthew guzdial nicholas liao jonathan chen shaoyu chen shukan shah vishwa shah joshua reno gillian smith mark o riedl friend collaborator student manager design aidriven game level editor affects creators proceedings chi conference human factors computing systems – gillian r hayes knowing action research approach hci ways knowing hci springer – will douglas heaven meta’s latest large language model survived three days online httpswwwtechnologyreviewcom metalargelanguagemodelaionlysurvivedthreedaysgptscience david g hendry batya friedman stephanie ballard value sensitive design formative framework ethics information technology – eric horvitz principles mixedinitiative user interfaces proceedings sigchi conference human factors computing systems pittsburgh pennsylvania usa chi ’ association computing machinery new york ny usa – httpsdoiorg stephanie houde vera liao jacquelyn martino muller muller david piorkowski john richards justin d weisz yunfeng zhang business misuse cases generative ai joint proceedings workshops humanai cocreation generative models useraware conversational agents colocated th international conference intelligent user interfaces iui kalin hristov artificial intelligence copyright dilemma idea ibm racial equity design retrieved january httpswwwibmcomdesignracialequityindesign maia jacobs melanie f pradier thomas h mccoy roy h perlis finale doshivelez krzysztof z gajos machinelearning recommendations influence clinician treatment selections example antidepressant selection translational psychiatry – tristan e johnson youngmin lee miyoung lee debra l o’connor mohammed k khalil xiaoxia huang measuring sharedness teamrelated knowledge design validation shared mental model instrument human resource development international – benjamin kaiser akos csiszar alexander verl generative models direct generation cnc toolpaths th international conference mechatronics machine vision practice mvip ieee – shalini kantayya coded bias retrieved january httpswwwpbsorgindependentlensdocumentariescodedbias bennett kleinberg bruno verschuere humans impair automated deception detection performance acta psychologica steven kollmansberger helping students build mental model computation proceedings fifteenth annual conference innovation technology computer science education – max kreminski isaac karth michael mateas noah wardripfruin evaluating mixedinitiative creative interfaces via expressive range coverage analysis iui workshops – sumith kulal panupong pasupat kartik chandra mina lee oded padon alex aiken percy s liang spoc searchbased pseudocode code advances neural information processing systems q vera liao daniel gruen sarah miller questioning ai informing design practices explainable ai user experiences proceedings chi conference human factors computing systems – q vera liao moninder singh yunfeng zhang rachel bellamy introduction explainable ai extended abstracts chi conference human factors computing systems – antonios liapis georgios n yannakakis julian togelius et al sentient sketchbook computeraided game level authoring fdg – stephanie lin jacob hilton owain evans truthfulqa measuring models mimic human falsehoods arxiv preprint arxiv silvia lindtner shaowen bardzell jeffrey bardzell reconstituting utopian vision making hci technosolutionism proceedings chi conference human factors computing systems – vivian liu lydia b chilton neurosymbolic generation d animal shapes semantic controls iui workshops vivian liu lydia b chilton design guidelines prompt engineering texttoimage generative models chi conference human factors computing systems – ryan louie andy coenen cheng zhi huang michael terry carrie j cai noviceai music cocreation via aisteering tools deep generative models proceedings chi conference human factors computing systems – toward general design principles generative ai applications haigen ’ workshop iui ’ march sydney nsw australia todd lubart can computers partners creative process classification commentary special issue international journal humancomputer studies – alexandra lyn risky business artificial intelligence risk assessments sentencing bail procedures united states available ssrn michael madaio luke stark jennifer wortman vaughan hanna wallach codesigning checklists understand organizational challenges opportunities around fairness ai proceedings chi conference human factors computing systems – mary lou maher computational collective creativity ’s creative iccc citeseer – mary lou maher brian magerko dan venura douglas fisher rogelio cardonarivera nancy fulda johannes gooth minwoo lee david wilson james kaufman et al research plan integrating generative cognitive ai human centered explainable cocreative ai acm chi conference human factors computing systems john e mathieu tonia s heffner gerald f goodwin eduardo salas janis cannonbowers influence shared mental models team process performance journal applied psychology edvinas meskys julija kalpokiene paulius jurcys aidas liaudanskas regulating deep fakes legal ethical considerations journal intellectual property law practice – cade metz meet gpt learned code blog argue published httpswwwnytimescomscience artificialintelligenceaigpthtml jessica k miller batya friedman gavin jancke brian gill value tensions design value sensitive design development appropriation corporation’s groupware system proceedings international acm conference supporting group work – christoph molnar interpretable machine learning lulu com meredith ringel morris carrie j cai jess holbrook chinmay kulkarni michael terry design space generative models proceedings neurips workshop humancentered ai neurips robert r morris provided mental health support people — using gpt ’s happened retrieved jan httpstwittercomrobertrmorrisstatus michael muller plamen agelov hal daume q vera liao nuria oliver david piorkowski et al hcaineurips human centered ai annual conference neural information processing systems michael muller lydia b chilton anna kantosalo charles patrick martin greg walsh genaichi generative ai hci chi conference human factors computing systems extended abstracts – michael muller steven ross stephanie houde mayank agarwal fernando martinez john t richards kartik talamadupula justin d weisz drinking chai ai programming partner design fiction generative ai software engineering joint proceedings iui workshops apexui haigen healthi humanize texss socialize colocated acm international conference intelligent user interfaces iui virtual event helsinki finland march ceur workshop proceedings vol alison smithrenner ofra amir eds ceurwsorg – michael muller angelika stroymayer forgetting practices data sciences proceedings chi conference human factors computing systems press michael muller justin weisz extending humanai collaboration framework dynamism sociality symposium humancomputer interaction work – michael muller justin d weisz werner geyer mixed initiative generative ai interfaces analytic framework generative ai applications iccc workshop future cocreative systems httpscomputationalcreativitynetworkshopscocreativeicccpapers futureofcocreativesystemspdf michael d murray generative ai authored artworks copyright law available ssrn raja parasuraman thomas b sheridan christopher d wickens model types levels human interaction automation ieee transactions systems man cyberneticspart systems humans – claudio pinhanez expose uncertainty instill distrust avoid explanations towards ethical guidelines ai hcaineurips workshop httpswwwgooglecomurlqhttpsaffarxivorgfabsfsad claudio pinhanez breakdowns language use weird errors past present future research conversational agents brl ibm research cambridge lab guess speaker series aditya ramesh prafulla dhariwal alex nichol casey chu mark chen hierarchical textconditional image generation clip latents arxiv preprint arxiv anais resseguier rowena rodrigues ethics attention context recommendations ethics artificial intelligence open research europe laria reynolds kyle mcdonell prompt programming large language models beyond fewshot paradigm extended abstracts chi conference human factors computing systems – robin rombach andreas blattmann dominik lorenz patrick esser björn ommer highresolution image synthesis latent diffusion models proceedings ieeecvf conference computer vision pattern recognition – janus rose facebook pulls new ‘ai science’ ’s broken terrible vice november retrieved jan httpswwwvicecomenarticleadywfacebookpullsitsnewaiforsciencebecauseitsbrokenandterrible haigen ’ workshop iui ’ march sydney nsw australia weisz et al steven ross fernando martinez stephanie houde michael muller justin d weisz programmer’s assistant conversational interaction large language model software development th international conference intelligent user interfaces baptiste roziere marieanne lachaux lowik chanussot guillaume lample unsupervised translation programming languages neurips devansh saxena karla badillourquiola pamela j wisniewski shion guha framework highstakes algorithmic decisionmaking public sector developed case study childwelfare proceedings acm humancomputer interaction cscw – matthias scheutz scott deloach julie adams framework developing using shared mental models humanagent teams journal cognitive engineering decision making – isabella seeber eva bittner robert o briggs triparna de vreede gertjan de vreede aaron elkins ronald maier alexander b merz sarah oestereiß nils randrup et al machines teammates research agenda ai team collaboration information management thomas b sheridan william l verplank human computer control undersea teleoperators technical report massachusetts inst tech cambridge manmachine systems lab ben shneiderman humancentered artificial intelligence reliable safe trustworthy international journal human–computer interaction – ben shneiderman humancentered ai issues science technology – ben shneiderman humancentered ai oxford university press auste simkute aditi surana ewa luger michael evans rhianne jones xai learning narrowing digital divide “new” “old” experts adjunct proceedings nordic humancomputer interaction conference – karan singhal shekoofeh azizi tao tu s sara mahdavi jason wei hyung won chung nathan scales ajay tanwani heather colelewis stephen pfohl et al large language models encode clinical knowledge arxiv preprint arxiv katta spiel ” obsessed gender”—non binary navigations technological infrastructures designing interactive systems conference – angie spoto natalia oleynik library mixedinitiative creative interfaces retrieved jun httpmicicodingconductcc catherine stupp fraudsters used ai mimic ceo’s voice unusual cybercrime case wall street journal august retrieved jan httpswwwwsjcomarticlesfraudstersuseaitomimicceosvoiceinunusualcybercrimecase jiao sun q vera liao michael muller mayank agarwal stephanie houde kartik talamadupula justin d weisz investigating explainability generative ai code scenariobased design th international conference intelligent user interfaces – ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing systems ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez łukasz kaiser illia polosukhin attention need advances neural information processing systems james vincent aigenerated answers temporarily banned coding qa site stack overflow retrieved jan https wwwthevergecomchatgptaigeneratedanswerstemporarilybannedstackoverflowllmsdangers patrick von platen generate text using different decoding methods language generation transformers hugging face blog march retrieved jan httpshuggingfacecobloghowtogenerate dakuo wang justin d weisz michael muller parikshit ram werner geyer casey dugan yla tausczik horst samulowitz alexander gray humanai collaboration data science exploring data scientists’ perceptions automated ai proceedings acm humancomputer interaction cscw – qiaosi wang koustuv saha eric gregori david joyner ashok goel towards mutual theory mind humanai interaction language reflects students perceive virtual teaching assistant proceedings chi conference human factors computing systems – laura weidinger john mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaese borja balle atoosa kasirzadeh et al ethical social risks harm language models arxiv preprint arxiv justin d weisz mary lou maher hendrik strobelt lydia b chilton david bau werner geyer haigen rd workshop humanai cocreation generative models th international conference intelligent user interfaces – justin d weisz michael muller stephanie houde john richards steven ross fernando martinez mayank agarwal kartik talamadupula perfection required humanai partnerships code translation th international conference intelligent user interfaces – justin d weisz michael muller steven ross fernando martinez stephanie houde mayank agarwal kartik talamadupula john t richards better together evaluation aisupported code translation th international conference intelligent user interfaces – alex wilkins will ai texttoimage generators put illustrators job newscientist may shuo yang kai shu suhang wang renjie gu fan wu huan liu unsupervised fake news detection social media generative approach proceedings aaai conference artificial intelligence vol – ",49.1,"1"
5,"Learning Data Representations with Joint Diffusion Models"," r p g l s c v v x r learning data representations joint diﬀusion models kamil deja−−− tomasz trzcinski−−− jakub m tomczak−−−x warsaw university technology warsaw poland kamildejapwedupl tomasztrzcinskipwedupl ideas ncbr warsaw poland eindhoven university technology eindhoven netherlands jmtomczaktuenl abstract joint machine learning models allow synthesizing classifying data often oﬀer uneven performance tasks unstable train work depart set empirical ob servations indicate usefulness internal representations built contemporary deep diﬀusionbased generative models gener ating also predicting propose extend vanilla diﬀusion model classiﬁer allows stable joint endtoend training shared parameterization objectives resulting joint diﬀusion model outperforms recent stateoftheart hybrid methods terms classiﬁcation generation quality evaluated benchmarks top joint training approach present can directly beneﬁt shared generative discriminative representations introducing method visual counterfactual explanations keywords deep generative models diﬀusion models joint models introduction training single machine learning model can jointly synthesize new data well make predictions input samples remains longstanding goal machine learning shared representations created combination two objectives promise beneﬁts many downstream problems calibration model uncertainty semisupervised learning unsupervised domain adaptation continual learning therefore since introduction deep generative models variational autoencoders vaes growing body work takes advantage shared deep neural networkbased parameterization latent variables build joint models instance stack classiﬁer top latent variables sampled shared encoder similarly use normalizing ﬂows obtain invertible representation fed classiﬁer however approaches require modifying loglikelihood function scaling either conditional loglikelihood marginal loglikelihood idea known k deja et al hybrid modeling leads situation models concentrate either synthesizing data predicting tasks simultaneously address existing joint models’ limitations leverage recently intro duced diﬀusionbased deep generative models ddgm new family methods become popular unprecedented quality samples generate however relatively little attention paid inner workings especially internal representations built ddgms work ﬁll gap empirically analyze representations validating usefulness predictive tasks beyond introduce joint diﬀusion model classiﬁer shares parametrization unet encoder operating extracted latent features results meaningful data representations shared across discriminative generative objectives validate approach several use cases show one part model can beneﬁt first investigate ddgms beneﬁt additional classiﬁer conditionally generate new samples alter original images next show performance improvement method brings classiﬁcation task finally present can directly beneﬁt joint representations used classiﬁer generator creating visual counterfactual explanations namely explain decisions model identifying regions input image need change order system produce speciﬁed output can summarize contributions work follows – provide empirical observations insights representations built internally diﬀusion models top introduce joint classiﬁer diﬀusion model shared parametrization – introduce conditional sampling algorithm optimize internal diﬀusion representations classiﬁer – present stateoftheart results terms joint modeling solution outperforms joint hybrid methods terms quality generations classiﬁcation performance background joint models let us consider two random variables x ∈ x y ∈ y instance classiﬁcation problem can x rd y k− joint distribution random variables factorized one following two manners namely px y pxypy pyxpx following second factorization gives us conditional distribution pyx eg classiﬁer marginal distribution px prediction enough learn conditional distribution typically parameterized neural networks however training joint model shared parametrization many advantages since one part model can positively inﬂuence diﬀusionbased deep generative models work follow formulation diﬀusionbased deep generative models presented given data distribution x ∼ qx deﬁne forward noising process q learning data representations joint diﬀusion models produces sequence latent variables x xt adding gaussian noise time step t variance βt ∈ deﬁned sched t qxtxt− qxtxt− n xt − βtxt− βti √ ule β βt namely qx xtx cidt want optimize marginal likelihood pθx cid pθx xt dx xt pθx xt pxt cidt following consider ddgms inﬁnitely deep hierarchical vaes speciﬁc family variational posteriors namely gaussian diﬀusion processes therefore data point x latent variables x xt t pθxt−xt backward diﬀusion process cid cidcid cid cid eqxtxdkl qxt−xt xcidpθxt−xt can deﬁne variational lower bound follows cid ln pθx ≥ lvlbθ eqxxln pθxx pθxt−xt n xt− µθxt t σθxt t − dkl qxtxcidpxt cid − tcid cidcid lt cidcid −l cid t lt− optimize respect parameters backward diﬀusion training objective authors notice instead estimating probability previous latent variable pxt−xt can predict added noise  therefore single part variational lower bound equal cidcid − θ cid√ √ αtx − αt tcidcidcidcid ltθ ex β t t αt − αt σ cid  ∼ n θ·· neural network predicting noise  xt also suggested train model simpliﬁed objective modiﬁed version equation without scaling namely cidcidcid − θ cid√ − αt tcidcidcidcid √ αtx ltsimpleθ ex practice single shared neural network used modeling θ end works use unet architecture can seen speciﬁc type autoencoder particularly relevant work since beneﬁt encoder – decoder structure denoising ddgm model related work diﬀusion models several extensions baseline ddgm setup aim improve quality sampled generations several works propose improve quality samples ddgms conditioning generations class identities among works intro duces classiﬁerguided generation gradient externally independently trained classiﬁer added process backward diﬀusion k deja et al guide generation towards target class top approach present tool investigating decision classiﬁer generating visual counterfactual explanations diﬀusion model work simplify methods beneﬁting training joint model representations shared diﬀusion model classiﬁer diﬀusion models unet representations additional encoded information score estimator introduced allows using score matching loss function learning data representations authors use activations pretrained diﬀusion unet model image seg mentation task ﬁrst analyze pretrained models useful classiﬁcation propose joint model trained endtoend generative discriminative losses works consider data representa tions unet model within generative models eg conditional unetbased variational autoencoder additionally authors show connection unet architecture wavelet transformation applying hierarchical vaes work show indeed diﬀusion models learn useful representations take advantage fact shared parameterization diﬀusion model classiﬁer joint model joint training apart latent variable joint models authors show possible use shared parameterization neural networkbased classiﬁer formulate energybased model joint energybased model jem seen classiﬁer softmax function applied logits generator markovchain monte carlo method used sample model although obtains strong empirical results gradient estimators used train jem unstable prone diverging optimization parameters perfectly tuned limits robustness applicability method alternatively introspective neural networks used generative modeling classiﬁcation applying single parameterization idea behind class models relies utilizing training procedure combines adversarial learning contrastive learning similarly jems sampling carried running mcmc method authors improve performance jem introducing variationalbased approximator vera instead mcmc similarly authors introduce jem improvement jem’s generative performance applying proximal sgldbased generation classiﬁcation accuracy informative initialization conceptually diﬀerent perspective authors propose implementation joint model based vision transformer architecture yields state oftheart result terms image classiﬁcation propose combine standard diﬀusion models classiﬁers sharing parameterization thus training entirely based loglikelihood function endtoend sampling carried backward diﬀusion instead mcmc algorithm learning data representations joint diﬀusion models diﬀusion models learn data representations learning useful data representations important good generator classiﬁer ideally like train joint model allows us obtain proper representations pyx px simultaneously work investigate parameterizations ddgms particular use autoencoder denoising decoder pθxt−xt within architecture denoising function can decomposed two parts encoding image current timestep set features zt ext decoding obtain xt− dzt t z t zn unet architecture set features obtained input struc ture composed several ten sors image features en coded diﬀerent levels zt t z experiments propose pool features encoded ﬁlter concatenate averaged representations single vector zt presented fig n particular can use average pooling select average convolutional ﬁlter activations whole input details procedure described appendix b fig data representation zt unetbased diﬀusion model diﬀusion model representations useful prediction first verify whether averaged representations z extracted original image x unet contain information sense predictive measure classiﬁcation accuracy mlpbased classiﬁer fed z presented fig representations encoded z indeed informative cases eg cifar lead performance comparable standalone classiﬁer architecture combination unet encoder mlp trained crossentropy loss function similar observation made pretrained diﬀusion model used semantic image segmentation diﬀusion models learn features increasing granularity next question data representations zt diﬀer diﬀusion timesteps t investigate issue train unsupervised ddgm celeba dataset use extract features zt diﬀerent timesteps top representations ﬁt binary logistic regression classiﬁer attributes dataset fig b show performance regression models diﬀerent attributes calculated top representations ten diﬀerent diﬀusion timesteps observe model 𝐱𝐭−𝟏𝐱𝐭encoderdecoder𝐳𝐭𝟏𝐳𝐭𝟐𝐳𝐭𝟑𝐳𝐭pooling k deja et al b fig testset accuracy standalone classiﬁer compared classiﬁer trained top data representations pretrained diﬀusion model extracted original images x b area roc curve auc logistic regression models ﬁt data representations extracted pretrained diﬀusion model ten diﬀerent diﬀusion timesteps highgrained features already distinguishable late diﬀusion steps closer random noise lowgrained features represented earlier stage forward diﬀusion learns diﬀerent data features depending amount noise added original data sample presented fig b highgrained data features hair color start emerge late diﬀusion steps closer noise lowgrained features eg necklace glasses present early steps observation line works denoising autoencoders authors observe similar behavior denoising diﬀerent amounts added noise method joint diﬀusion models ddgms classiﬁers taking account observations described section propose train joint model composed classiﬁer ddgm propose use shared parameterization namely shared encoder unet architecture serves generative part calculating pooled features classiﬁer pool latent representations data diﬀerent levels unet architecture one vector z top vector build classiﬁer model trained assign label data example represented vector z particular consider following parameterization denoising diﬀusion model within single diﬀusion timestep t pθzt−zt distinguish encoder eν parameters ν maps input xt set vectors zt eνxt zt z t ie set representation vectors derived depth level unet architecture second component denoising diﬀusion model decoder dψ parameters ψ reconstructs feature vectors denoised sample xt− dψzt together encoder decoder form denoising model pθ parameters θ ν ψ next introduce third part model classiﬁer gω parameters ω predicts target class ˆy gωzt ﬁrst layer classiﬁer average pooling results single representation zt t zn t z timestepaucblondhairblackhairnecklacemouthslightlyopenpointynoseeyeglasses learning data representations joint diﬀusion models parameterization joint diﬀusion b additional noisy classiﬁers fig parameterization joint diﬀusion model step backward diﬀusion parameterized shared unet classiﬁer uses encoder unet together average pooling green additional layers yellow b alternative training additionally uses classiﬁer noisy images xt t approach consider classiﬁer takes original image x vector probabilities returned ϕ eventually ﬁnal prediction calculated ˆy gωx visualization shared parameterization presented fig result model written follows pνψωxt y pνωyx pνψxt applying logarithm yields ln pνψωxt y ln pνωyx ln pνψxt logarithm joint distribution serve training objective ln pθxt either approximated elbo simpliﬁed objective use simpliﬁed objective cidcid − ˆcidcid ltdiﬀν ψ ex ˆ prediction decoder cid√ − αt tcid z t z t zn t eν ˆ dψz √ t αtx t z t zn classiﬁer use logarithm categorical distribution lclassν ω −exy cidk−cid k cid y k log cidk− exp ϕk c exp ϕc crossentropy loss y target class ϕ vector probabilities returned classiﬁer gωeνx y k indicator function y equals k otherwise ﬁnal loss function approach following lν ψ ω lclassν ω − lν ψ − tcid ltdiﬀν ψ − lt ν ψ optimize objective wrt ν ψ ω single optimizer t ො𝐲𝐱𝟎𝐱𝐓𝐱𝐭−𝟏𝐱𝐭ො𝐲𝐓ො𝐲𝐭ො𝐲𝐭−𝟏𝐱𝟎𝐱𝐓𝐱𝐭−𝟏𝐱𝐭 k deja et al alternative training joint diﬀusion models training proposed approach batch data straightforward given x y example x ﬁrst noised forward diﬀusion random timestep xt training loss denoising model montecarlo approximation sum timesteps x fed classiﬁer returns probabilities ϕ crossentropy loss calculated given y discussed section diﬀusion model trained even fully unsuper vised manner provides data representations related diﬀerent granularity input features various diﬀusion timesteps thus can improve robustness method applying classiﬁer intermediate noisy images xt t t reason adds crossentropy losses xt namely classν ω −exy lt y k log cidk−cid k cid cidk− exp ϕt k c exp ϕt c ϕt objective following k vector probabilities given gωeνxt extended lt ν ψ ω lν ψ ω lt classν ω cid t∈t t ⊆ t set timesteps additional noisy classiﬁers schematically depicted fig b highlight model reused across various noisy images important mention noisy classiﬁers serve training purposes used prediction procedure similar data augmentation technique random noise added input conditional sampling joint diﬀusion models improve quality samples generated ddgm propose classiﬁer guidance approach externally trained classiﬁer can used guide generation ddgm trained unsupervised way towards desired class ddgms backward diﬀusion step image sampled output diﬀusion model pθ according following formula µ σ ←µθ xt σθ xt xt− ← sample n µ σ proposed change second line equation add scaled gradient respect target class externally trained classiﬁer c· directly output denoising model xt− ← sample n µ sσ∇xtcxt σ s gradient scale learning data representations joint diﬀusion models joint training classiﬁer diﬀusion model introduced work propose simplify classiﬁer guidance technique using alternative training introduced section can use noisy classiﬁers formulate conditional sampling encoder model eν encodes input data xt representation vectors zt used denoise example previous diﬀusion timestep xt− ∼ dψ zt well predict target label classiﬁer ˆy gω zt therefore guide model towards target label sampling propose optimizing representations zt according gradient calculated classiﬁer respect desired class overview procedure presented algorithm algorithm sampling optimized representations given diﬀusion model encoder eνztxt decoder dφxt−zt classiﬁer gωyzt step size α input class label y step size α xt ← sample n t t zt ← eν xt zcid t ← zt − α∇zt log gωyzt µ σ ← dψzcid t xt− ← sample n µ σ end return x reformulation diﬀusion model proposed instead predicting previous timestep xt− denoising model optimized predict noise  subtracted image current timestep xt adequately change optimization objective instead optimizing noise speciﬁc target class optimize anything except target class t ← zt α∇zt log gωyzt implement changing optimization direction zcid experiments experiments aim observing beneﬁts proposed joint diﬀusion model standalone classiﬁer marginal diﬀusion model end run series experiments verify various properties namely – measure quality discriminative task evaluate whether training together diﬀusion model improves robustness classiﬁer – measure generative capability model check representations optimized classiﬁer can lead accurate conditional generations – show joint model learns abstract features can used counterfactual explanation use unetbased model depth level three experiments pool latent features average pooling single vector top k deja et al add classiﬁer two linear layers leakyrelu activation metrics reported standard training objective except conditional sampling additionally train classiﬁer noisy samples ie additional losses hyperparameters training details included appendix code repository predictive performance joint diﬀusion models ﬁrst experiment evaluate predictive performance method end report accuracy model four datasets fashionmnist svhn cifar cifar compare method baseline classiﬁer trained standard crossentropy loss mlp classiﬁer trained top representations extracted pretrained ddgm section three joint hybrid models vera jem hybvit results experiment presented table table classiﬁcation accuracy calculated test sets training methods vanilla classiﬁer used exactly architectures fmnist svhn cifar cifar model vera jem hybvit classiﬁer pretrained ddgm noticed classiﬁer trained features extracted unet ddgm pretrained unsupervised manner achieves reasonable performance however always outperformed standalone classiﬁer proposed joint diﬀusion model achieves best performance four datasets reason twofold first training partially shared neural network ie encoder unet architecture beneﬁts unsupervised training similarly pretraining using boltzmann machines beneﬁted ﬁnetuning deep neural networks second shared encoder part robust since used backward diﬀusion images various levels noise httpsgithubcomkamildejajointdiffusion learning data representations joint diﬀusion models table evaluation generative capabilities measuring fid score precision recall generations various diﬀusionbased models including joint diﬀusion model model ddgm ddgm classiﬁer guidance conditional sampling fashionmnist cifar fid ↓ prec ↑ rec ↑ fid ↓ prec ↑ rec ↑ fid ↓ prec ↑ rec ↑ fid ↓ prec ↑ rec ↑ cifar celeba generative performance joint diﬀusion models second experiment check adding classiﬁer joint diﬀusion models inﬂuences generative performance use fid score quantify quality data synthesis additionally use distributed precision prec recall rec assessing exactness diversity generated samples joint diﬀusion model consider samples prior let backward diﬀusion also use second sampling scheme use conditional sampling namely optimization procedure described section compare approach vanilla ddgm ddgm classiﬁer guidance recent stateoftheart joint hybrid models vera jem hybvit genvit table comparison generative capabilities joint models measuring fid score model vera jem hybvit genvit conditional sampling cifar cifar celeba fid ↓ fid ↓ fid ↓ overall proposition outperforms standard ddgms regarding general fid see table however cases vanilla ddgm ddgm classiﬁer guidance obtain better results terms particular components precision fashionmnist cifar recall fashionmnist celeba can observe conditional sampling improves quality generations evaluated benchmarks especially terms precision can understood exactness generations result fact optimization procedure drives zt mode eventually backward diﬀusion generates better samples however comparing approach current stateoftheart joint models clearly outperform see table k deja et al precision recall fig dependency value step size α value precision recall joint diﬀusion conditional sampling get insight role conditional sampling carried additional study varying value α step size algorithm fig present precision recall change diﬀerent values parameter apparently increasing step size value α leads precise less diverse samples rather intuitive behavior larger steps result features zt closer modes seems sweet spot around α ∈ measures high moreover visualize eﬀect taking various values α fig chosen class eg plane observe larger α samples precise lack diversity ie background almost fig samples joint diﬀusion model optimized towards speciﬁc class plane diﬀerent step sizes α fig present decision classiﬁer changes sampling optimized generations higher α step size value optimization converges faster towards target classes interestingly cifar dataset certain classes eg class converge later backward diﬀusion process others also present associated samples model depict higher values α parameter lead precise less diverse samples show generations joint model appendix c stepsizeαprecisioncifarfashionmniststepsizeαrecallcifarfashionmnist𝛼𝛼𝛼𝛼 learning data representations joint diﬀusion models α b α fig cifar classiﬁer decisions diﬀerent diﬀusion steps conditional sampling diﬀerent values step size α associated conditional samples comparison stateoftheart joint approaches get better overview performance joint diﬀusion model present comparison joint models sota discriminative generative models table purely discriminative generative models included upper bounds performance importantly within class joint models joint diﬀusion clearly outperforms related works table comparison joint diﬀusion model joint models sota discriminative model sota generative model cifar test set class joint disc gen model igebm glow residual flows jeat jem vera α jem hybvit vith ddgm implementation lsgm accuracy ↑ fid↓ visual counterfactual explanations last experiment apply joint diﬀusion model realworld medical data malaria dataset includes cell images either infected malaria parasite classiﬁcation task cells various shapes diﬀerent staining ie colors contain parasite visually apparent purple dot training joint diﬀusion model obtain high classiﬁcation accuracy test set top introduce adaptation visual timestepavg predictiontimestepavg prediction k deja et al negative examples positive examples fig data samples malaria dataset classiﬁed negative examples left parasitized cells right top row original data examples nd row data noised forward diﬀusion steps rd row denoised images conditional sampling bottom row diﬀerence rd th rows counterfactual explanations vce method provides answer question minimal change input image x change decision classiﬁer setup answer question conditional sampling algorithm use generate counterfactual explanations fig show examples negative left positive right classes add noise images run conditional sampling opposite class ie changing negative examples positive ones vice versa cases joint diﬀusion model conditional sampling can either remove parasite positive examples add parasite negative ones presented images cherrypicked experiment shows can use proposed approach obtain powerful classiﬁer also visualize regions interest considered case calculating diﬀerence original example image changed class label indicates malaria plasmodium see last row fig provide examples celeba data appendix d conclusion work introduced joint model combines diﬀusion model classiﬁer shared parameterization ﬁrst experimentally demonstrated ddgms learn semantically meaningful data representations used classiﬁcation top observation introduced joint diﬀusion models experimental section showed approach improves performance classiﬁcation generative tasks providing highquality generations enabling conditional generations builtin classiﬁer guidance proposed approach achieves stateoftheart performance class joint models additionally show joint diﬀusion model can used visual counterfactual explanations without changes original setup learning data representations joint diﬀusion models references abstreiter k mittal s bauer s sch¨olkopf b mehrjou diﬀusionbased representation learning arxiv preprint arxiv arxiv augustin m boreiko v croce f hein m diﬀusion visual counterfactual explanations arxiv preprint arxiv baranchuk d rubachev voynov khrulkov v babenko labeleﬃcient semantic segmentation diﬀusion models international conference learning representations chandra b sharma rk adaptive noise schedule denoising autoencoder international conference neural information processing pp – springer chapelle o scholkopf b zien semisupervised learning chapelle o et al eds book reviews ieee transactions neural networks – chen rt behrmann j duvenaud d jacobsen jh residual ﬂows invertible generative modeling arxiv preprint arxiv dhariwal p nichol diﬀusion models beat gans image synthesis advances neural information processing systems dosovitskiy beyer l kolesnikov weissenborn d zhai x unterthiner t dehghani m minderer m heigold g gelly s uszkoreit j houlsby n image worth x words transformers image recognition scale international conference learning representations du y mordatch implicit generation generalization energybased models arxiv preprint arxiv esser p sutter e ommer b variational unet conditional appearance shape generation ieeecvf conference computer vision pattern recognition httpsdoiorgcvpr falck f williams c danks d deligiannidis g yau c holmes cc doucet willetts m multiresolution framework unets applications hierarchical vaes oh ah agarwal belgrave d cho k eds advances neural information processing systems ganin y ustinova e ajakan h germain p larochelle h laviolette f marchand m lempitsky v domainadversarial training neural networks journal machine learning research – geras kj sutton c scheduled denoising autoencoders arxiv preprint arxiv grathwohl w wang kc jacobsen j duvenaud d norouzi m swersky k classiﬁer secretly energy based model treat like one international conference learning representations grathwohl w wang kc jacobsen jh duvenaud d norouzi m swersky k classiﬁer secretly energy based model treat like one international conference learning representations grathwohl ws kelly jj hashemi m norouzi m swersky k duvenaud d mcmc amortized sampling fast stable training energybased models international conference learning representations hinton ge osindero s teh yw fast learning algorithm deep belief nets neural computation – ho j jain abbeel p denoising diﬀusion probabilistic models advances neural information processing systems – k deja et al ho j salimans t classiﬁerfree diﬀusion guidance arxiv preprint arxiv arxiv huang cw lim jh courville ac variational perspective diﬀusion based generative models score matching advances neural information processing systems huang pkm chen sa lin ht improving conditional scorebased generation calibrated classiﬁcation joint training neurips workshop scorebased methods ilse m tomczak jm louizos c welling m diva domain invariant variational autoencoders medical imaging deep learning pp – pmlr jebara t machine learning discriminative generative vol springer science business media jin l lazarow j tu z introspective classiﬁcation convolutional nets advances neural information processing systems kingma dp salimans t poole b ho j variational diﬀusion models advances neural information processing systems kingma dp welling m autoencoding variational bayes iclr kingma dp dhariwal p glow generative ﬂow invertible x convolutions advances neural information processing systems pp – kingma dp mohamed s jimenez rezende d welling m semisupervised learning deep generative models advances neural information processing systems knop s spurek p tabor j podolak mazur m jastrzebski s cramerwold autoencoder journal machine learning research – lasserre ja bishop cm minka tp principled hybrids generative discriminative models ieee computer society conference computer vision pattern recognition cvpr’ vol pp – ieee lazarow j jin l tu z introspective neural networks generative modeling proceedings ieee international conference computer vision pp – lee k xu w fan f tu z wasserstein introspective neural networks proceedings ieee conference computer vision pattern recognition pp – masarczyk w deja k trzcinski t robustness generative representa tions catastrophic forgetting mantoro t lee m ayu ma wong kw hidayanto eds neural information processing pp – springer international publishing cham nalisnick e matsukawa teh yw gorur d lakshminarayanan b hybrid models deep invertible features international conference machine learning pp – pmlr nichol aq dhariwal p improved denoising diﬀusion probabilistic models international conference machine learning pp – pmlr perugachidiaz y tomczak j bhulai s invertible densenets concatenated lipswish advances neural information processing systems – rajaraman s antani sk poostchi m silamut k hossain ma maude rj jaeger s thoma gr pretrained convolutional neural networks feature extractors toward improved malaria parasite detection thin blood smear images peerj e learning data representations joint diﬀusion models ronneberger o fischer p brox t unet convolutional networks biomedical image segmentation international conference medical image computing computerassisted intervention pp – springer sajjadi ms bachem o lucic m bousquet o gelly s assessing generative models via precision recall arxiv preprint arxiv sietsma j dow rj creating artiﬁcial neural networks generalize neural networks – sohldickstein j weiss e maheswaranathan n ganguli s deep unsupervised learning using nonequilibrium thermodynamics international conference machine learning pp – pmlr song y ermon s generative modeling estimating gradients data distribution advances neural information processing systems song y sohldickstein j kingma dp kumar ermon s poole b score based generative modeling stochastic diﬀerential equations international conference learning representations tashiro y song j song y ermon s csdi conditional scorebased diﬀusion models probabilistic time series imputation advances neural information processing systems vol pp – curran associates inc tomczak jm deep generative modeling springer cham tulyakov s fitzgibbon nowozin s hybrid vae improving deep generative models using partial observations arxiv preprint arxiv tzen b raginsky m neural stochastic diﬀerential equations deep latent gaussian models diﬀusion limit arxiv preprint arxiv vahdat kreis k kautz j scorebased generative modeling latent space advances neural information processing systems yang w kirichenko p goldblum m wilson ag chromavae mitigating shortcut learning generative classiﬁers arxiv preprint arxiv yang x ji s jem improved techniques training jem proceedings ieeecvf international conference computer vision pp – yang x shih sm fu y zhao x ji s vit secretly hybrid discriminativegenerative diﬀusion model arxiv preprint arxiv zhang q zhang l convolutional adaptive denoising autoencoders hierarchical feature extraction frontiers computer science – k deja et al appendix additional experiments stateoftheart performance joint modeling evaluate setups one part model can beneﬁt another particular propose two additional proofofconcept experiments smaller architectures – train model semisupervised setup see shared representations classiﬁer diﬀusion model can positively inﬂuence classiﬁcation accuracy limited number labeled data – use domainadaptation task check optimizing representations using approach helps adapt new data compared standalone classiﬁer semisupervised learning joint diﬀusion models evaluate approach semisupervised setup artiﬁcially limit amount labeled data three datasets svhn cifar cifar compare joint diﬀusion models deep neural networkbased classiﬁer deep neural networkbased classiﬁer top pretrained unet encoder results presented table simplicity setup use data augmentation technique therefore performance full dataset slightly lower presented table case standalone classiﬁer observe classiﬁcation accuracy drastically drops number labeled data however joint diﬀusion model can train classiﬁer smaller dataset still optimizing generator part unsupervised manner available unlabelled data approach signiﬁcantly improves classiﬁer’s performance thanks improved quality data representations cifar observe joint diﬀusion model labeled data examples per class performs almost well standalone classiﬁer trained fully labeled training dataset extreme scenarios eg labeled data limited examples per class seems slightly beneﬁcial ﬁrst learn data representation unsupervised way add classiﬁer top however overall joint diﬀusion model performs extremely well greatly beneﬁts available unlabeled data terms classiﬁcation accuracy experiments align observation ddgms used improve performance semisupervised image segmentation domain adaptation diﬀusionbased ﬁnetuning previous section evaluate whether classiﬁer can beneﬁt generative part model trained limited access labeled data now extend experiments check joint diﬀusion can adapt learning data representations joint diﬀusion models table accuracy classiﬁer trained semisupervised setup dataset train classiﬁer fully labeled data limited amount labeled examples remaining unlabelled examples compare standard classiﬁer classiﬁer trained pretrained ddgm presented sec joint diﬀusion method svhn cifar cifar labelled data images per class classiﬁer pretrained ddgm new data domain using generative part – fully unsupervised way purpose run experiment ﬁrst train model source labeled data retrain target dataset without access labels compare approach standalone deep neural networkbased classiﬁer see table table classiﬁcation accuracy classiﬁer trained domain adaption task ﬁrst train joint model source dataset adapt target domain retraining using diﬀusion loss examples target one svhn → mnist usps → mnist mnist → usps classiﬁer classiﬁer target upper bound expected three scenarios classiﬁcation accuracy stand alone classiﬁer degrades target domain however access unlabeled data target domain allows joint diﬀusion model adapt surprisingly well approach outperforms standalone classiﬁer three cases signiﬁcant margin result indicates learning lowlevel features essential obtaining good predictive power enough transfer classiﬁcation head unchanged classiﬁcation accuracy drop random level datasets share task ie digits classiﬁcation k deja et al b training details hyperparameters b pooling unet features discussed section pool unet features encoded diﬀerent unet levels average pooling function precisely speaking take average convolutional ﬁlter activation given ﬁlter across whole image approach seems result loss information location particular features extracted convolutional ﬁlter allows us create image representation reasonable dimensionality depending dimensionality input method extract features × grayscale images eg mnist features × images color channels eg cifar features × images color channels celeba experiments use average pooling although options max min pooling might used approach ensures features across whole image shared classiﬁer generative models b semisupervised learning semisupervised learning train joint diﬀusion model datasets limited access labeled samples simplest approach problem calculate loss function diﬀusion using whole batch data using labeled examples classiﬁer loss however scenarios artiﬁcially omit labeled data practice lead situation batch size equal examples classiﬁer loss practically calculated samples therefore stabilize training propose create buﬀer put labeled examples batch buﬀer reaches capacity equal batch size calculate classiﬁer loss using examples buﬀer add generative loss according equation b domain adaptation experiments domain adaptation task propose simplest setup ﬁrst train joint model source task using joint loss function eq retrain model target domain using ddgm loss equation show without alteration basic setup can observe signiﬁcant performance boost compared baseline classiﬁer believe can improve results focus directly domain adaptation task take advantage recent advantages ﬁeld experiments direction example include simultaneous training examples two domains improve alignment can also beneﬁt adversarial training introduced dann learning data representations joint diﬀusion models c additional results conditional generations optimized representations fashion mnist cifar fig conditional samples joint diﬀusion model fashion mnist dataset left ﬁrst classes cifar dataset right row represents samples one class fashion mnist cifar celeba fig generated examples joint diﬀusion model without conditional sampling cifar cifar celeba dataset k deja et al d additional results counterfactual image generation experiment described section presented can use joint diﬀusion model generate counterfactual explanations classiﬁer using medical dataset figure present examples approach perturbing original examples celeba dataset select attributes celeba dataset namely young smiling mustache attribute select positive examples negative examples alter using conditional sampling procedure classiﬁerbased optimization present original examples ﬁrst row noised noise second row generated towards counterfactual class third row last row show diﬀerences original modiﬁed examples young old left old young right b smiling nosmiling left nosmiling smiling right c mustache nomustache left nomoustache moustache right fig counterfactual image generation celeba dataset using three diﬀerent attributes random original examples attribute select positive examples change negative ones negative ones change positive ones ",49.8,"1"
6,"LEAN: Light and Efficient Audio Classification Network","lean light efﬁcient audio classiﬁcation network shwetank choudhary samsung rd institute bangalore india cr karthik samsung rd institute bangalore india punuru sri lakshmi samsung rd institute bangalore india sumit kumar samsung rd institute bangalore india sjchoudharysamsungcom crkarthiksamsungcom srilakshmipsamsungcom sumitkrsamsungcom y m d s s c v v x r abstract— past years audio classiﬁcation task largescale dataset audioset important re search area several deeper convolutionbased neural networks shown compelling performance notably vggish yamnet pretrained audio neural network pann models available pretrained architecture transfer learning well speciﬁc audio task adoption paper propose lightweight ondevice deep learningbased model audio classiﬁcation lean lean consists raw waveformbased temporal feature extractor called wave encoder logmel based pretrained yamnet show using combination trainable wave encoder pretrained yamnet along cross attentionbased temporal realignment results competitive performance downstream audio classiﬁcation tasks lesser memory footprints hence making suitable resource con straints devices mobile edge devices etc proposed system achieves ondevice mean average precisionmap memory footprint mere mb fsdk dataset improvement baseline ondevice map dataset index terms—audio classiﬁcation cross attention sound clas siﬁcation introduction related work recent years along speech recognition identifying sound types using raw signals become emerging research area typically audio pattern recognition um brella word consists many tasks audio tagging acoustic scene classiﬁcation audio classiﬁcation sound event detection identifying type sound realtime signal several use cases hearing disabil ity removal accessibility intelligent smartphones improving gaming experience etc deep learningbased methods de facto advanced solutions detect sound event digital signal task identifying sound typically requires sufﬁciently large dataset especially problem demands multiclass multilabel classiﬁcation several datasets released previous years namely kaggle free sound dataset urbansoundk esc one greatest breakthroughs audio classiﬁcation task release largescale audio dataset called audioset publicly available dataset hours clips corresponding classes dataset based youtube videos clips however clips deleted hence putting limitation © ieee open usage year one effort release large scale open dataset done research community releasing free sound dataset fsdk raw ﬁles available open license present work based fsdk dataset identifying sound types multi class multilabel dataset complex task robust models types tasks require deep complex networks capable capturing sound patterns taking inspiration visionbased task cnns ex tensively used researchers detect sound patterns emotion language detection models typically take ﬁxed input ranging second seconds produce ﬁxed size embedding used classiﬁcation task several deep complex models also designed achieved state art performance audioset recent breakthrough attentionbased networks proposed bahdanau’s loung’s multiattentionbased transformers widely adopted vision audio tasks shown signiﬁcant performance gain one example audio transformer fsdk achieved mean average precision score map consists stack transformer blocks block followed average pooling except last block similarly transformerbased patchout fast spectrogram transformer passt tries overcome computational complexity transformers introducing method called patchout current state art fsdk map however despite performing well transformers costly memory computation level difﬁcult port ondevice example passt various models discussed parameters varying m m translates large model size psla proposes collection training techniques imagenet pretraining balanced sampling label enhancement model aggregation etc boost model accuracy achieve best map fsdk ensemble models psla uses several efﬁcientnet b b attention based architectures ensembling base model called efﬁcientnetb singleheaded attention m model parameters audioset dataset byola pretrains representations input sound invariant audio data augmentations makes learned representations robust perturbations sounds achieves best map fsdk shortcomings abovediscussed work increased model complexity larger training time coupled large model size leads increased computation memory cost thus makes unsuitable resource constraints environments mobile devices work tries reﬂect issues simple reproducible pipeline uses pretrained yamnet trainable wave encoder just m model parameters quantized ondevice model size mb hence suitable ondevice deployment also proposed model requires just second audio frame make predictions whereas several works take input frame duration ranging seconds thus make system computationally efﬁcient due reduced preprocessing time creating spectrograms best knowledge temporal knowledge distillation ondevice audio classiﬁcation works fsdk focuses developing lightweight system device deployment work adopts teacherstudent based architecture knowledge transfer utilizes heavy transformerbased model teacher ondevice models various architectures student since work provides ondevice model performance baseline work comparing model results among several baseline models discussed work attrnn achieves best device map whereas model achieves map dataset improvement baseline model mostly frequency domain features logmel shorttime fourier transform stft based spectrogram common choices input audio classiﬁcation task however pann model audioset demonstrated timedomain features can act supplement frequency domain features improving model performance also get inspiration pann consider time frequency domain features input model work propose novel lightweight network called light efﬁcient audio classiﬁcation networklean ondevice audio classiﬁcation lean consists waveform based temporal feature extractor called wave encoder logmelbased pretrained yamnet feature extractor cross attention based temporal feature realignment scheme contributions study can summarized following way • propose lightweight ondevice novel network called lean takes temporal frequency features input • introduce temporal feature realignment scheme using pretrained embeddings cross attention leads improved performance slight increase training parameters • show despite lightweight memory smaller frame input second achieve com petitive performance best map gpu ondevice map just mb model sizequantized fsdk dataset • provide class level performance analysis impact pipeline ii dataset system evaluation use free sound dataset fsdk open dataset containing audio clips sound events classes drawn audioset ontology total duration clips dataset hours sound clips dataset weakly labeled ie labeling classes done clip level instead frame level clips multilabeled range seconds duration audio ﬁles consist sounds humans sounds things animals natural sounds musical struments adopt fsdk dataset freely available creative commons license popularly available benchmarking sound event detection models fsdk dataset divided three parts namely training validation evaluation use training validation split training ﬁnetuning model evaluation split showcasing results benchmarking model performance use metrics mentioned baseline system mapmean average precision mean average precision approximation area class’s precisionrecall curve informative performance dealing imbalanced datasets audioset fsdk comparison average area curve receiver operating characteristic curve addition also calculate maucpr mean area curve precisionrecall sensitivity index dprime additional metric also refer unofﬁcial implementation fsdk baseline systems producing results iii methodology section describes methods applied audio classiﬁca tion endtoend proposed architecture shown fig begin introducing preprocessing steps applied audio feature generation describe proposed wave encoder along pretrained yamnet based model discuss proposed system cross attentionbased feature realignment dataset preprocessing downsample audio clips k hz using sound library consider second audio patch input training testing labels second patch complete audio clip repeat data clips whose duration less second get ﬁxed second duration sample required model model consists twochannel network hence inputs raw waveform logmel takes two different spectrogram raw waveform second patch directly considered input wave encoder creating logmel spectrogram use setup parameters used yamnet yamnet takes logmel spectrogram shape corresponding ms audio data patch mel bins time dimension broader level parameters window length ms hop length ms number mel bands mel frequency range hz hz b pretrained yamnet feature extractor one channels call right channel proposed model cnn based feature extractor captures spatial features given audio frame among available largescale audio datasetbased pretrained models transfer learning choose yamnet vggish pann’s yamnet mobilenet based architecture adoption depth pointwise convolution lightweight pretrained model nearly million weights trained audioset dataset achieved map evaluation set although vggish pann models better terms performance compared yamnet heavy networks memory computation hence suitable resource constraints environments like mobile phones edge devices various experiments discussed add dense layer units called projection layer embedding vector yamnet shown fig reason make adjustments concerning wave encoder output reduce joint embedding size c proposed wave encoder temporal feature extractor taking inspiration pann model propose use raw waveform input capture timedomain features pann model temporal features captured wavegram model d convolutionalbased network contrast proposed wave encoder bidirectional lstmbased network wave encoder takes raw waveform input outputs learned temporal features call part network left channel contains two bidirectional lstm layers units since lstm takes time sequence input ﬁrst transform raw waveform time sequence data reshaping reshape layer converts waveform d time sequence data splitting patches using nonoverlapping window milliseconds second frame sampling rate k reshape layer outputs d vector d proposed joint model using time frequency domain features propose joint model using wave encoder left channel pretrained yamnet projection layer right channel projection layer outputs embedding eyam ee en wave encoder encodes raw waveform data context vector ct outputs channels combined using following equation ecombined concatenateeyam ct yi true multi label vector ith audio sample ˆyi predicted score vector objective function can deﬁned ˆyi sigmoidecombined t p b loss bceyi ˆyi w ecombined joint embedding vector eyam ∈ rk ct ∈ rk ecombined ∈ rk p ∈ rk×c b bias k vector dimension c number classes bce binary cross entropy loss multi label multi class problem training eyam pretrained part ct part learnt updated backpropogation baseline sota proposed system performance fsdk table dataset model name crnn baseline vgglike baseline resnet baseline densenet baseline audio transformer psla byola passt sota wavclip tkd joint model bahdanau cross attention lean map mauc dprime total param m m m m m m m m m m m e temporal feature realignment using cross attention joint model instead simple concatenation output wave encoder yamnet propose two realignment schemes using cross attention getting ﬁnal embeddings • cross attention simple afﬁnity score • cross attention similar bahdanau’s type attention learning weights cross attention simple afﬁnity score inspired work ”multimodal speech emotion recognition using audio text” adopt similar cross attention proposed lightweight system improve model per formance without signiﬁcant increase model parameters said work uses audio embeddings realign textual embeddings similar fashion propose use pretrained yamnet embeddings realign wave encoder output using fig proposed model architecture lean cross attention scheme attention use simple dot product yamnet wave encoder output calculate afﬁnity scores time sequences wave encoder equations discuss mathematical formulation cross attention scheme eyam output pretrained yamnet embeddings projection layer ht hidden state tth sequence wave encoder output attention vector calculated using equation catt equation attentive context vector calculated using weighted sum t hidden states catt concatenated eyam get ﬁnal joint embeddings objective function remains discussed equations sof tmaxtanhdoteyam ht catt σatht ecombinedatt concatenateeyam catt ˆy sigmoidecombinedatt t p b b cross attention similar bahdanau’s type attention learning weights experiment bahdanau’s additive style attention learning weights using dense layers conceptualize cross attention query q key k value v similar fashion done transformer network scheme projection layer output embeddings chosen q wave encoder output chosen k v key difference method comparison afﬁnity attention calculating attention vector dis cussed equations equations discuss calculations involved crossattention scheme q eyam t wq bq k ht wk bk v tanhq kwv b sof tmaxv w eyam ∈ rm w q ∈ rm×d h ∈ rt×m w k ∈ rm×d w v ∈ rm× d dense layer units choosen several experiments f training hyper parameter selection use tensorﬂowgpu keras implementation training ﬁx batch size learning rate e loss binary crossentropy training done epochs epoch roughly taking hours observe models generally converge well last epoch select best model highest validation auc pr iv results discussion use nvidia gpu geforce gtx ti mb card training testing models trained tested end end testing done evaluation dataset provided fsdk testing given audio ﬁle split chunks second overlap chunk raw waveform corresponding logmel mean normalization calculated fed network chunk level class predictions score calculated averaged class wise map improvement using proposed model classes fsdk table ii baseline reference models baseline modelb yamnet finetune yamnet finetune joint model reference modelr joint model joint model cross attention joint model cross attention count classes see improvementmap b r classes overall improvement better better better get ﬁnal classwise score entire ﬁle observe better performance overlapping split compared nonoverlapping one believe mainly due availability data overlapping split discuss results type attention network adopt fine tuning yamnet discussed section iiib choose yamnet feature extractor gives vectors latent embed ding experiment multiple ways ﬁnetune yamnet achieve best performance fsdk retrain yamnet end end without pretrained weights initialization however observe none outperform baseline system directly use pretrained embedding projection dense layer achieved map outperforming baseline results vgglike map function discussed equations using conﬁg uration get map call model scheme joint modeljm shown table iii simple joint fusion system yamnet wave encoder surpasses ﬁnetuned yamnet performance metrics c variation cross attention impact table iii captures various architechture experiments ablation study ﬁrst cross attention scheme consider afﬁnitybased attention model realignment temporal embeddings achieve map call model ”joint modeljm afﬁnity cross attention” table iii observe map dprime see improvement compared ﬁnetuned yamnet joint model another observation scheme attention add training parameters still leads improvement results highlights novelty approach believe reason improvement due realignment identiﬁcation relevant temporal sequences using attentive weights mechanism gives higher weights relevant features rest suppressed low weighing next experiment adopt query q key k value v type structure discussed section iiie using bahdanau additive style attention call scheme proposed lean model based experiments keep model size small choose dense units calculating q k v setup achieve model best map dprime value table iii ablation study map mauc dprime total param m m m m model yamnet without ﬁnetune yamnet ﬁnetune joint modeljm jm afﬁnity cross attention jm bahdanau cross attention lean m d class level performance analysis ondevice performance comparison fsdk dataset table iv model lean attrnn mhattrnn crnn model size mb map inference ms b temporal feature extractor wave encoder wave encoder use two bilstm layers units context vector yielded wave encoder concatenated using equation trained using objective since baseline system contain classwise re sults compare classlevel performance ﬁne tuned yamnet model proposed lean model table ii shows classwise map analysis different models use baselineb reference model r nomenclature convenience b compared r table ”overall improvement” column tells number classes r see improvement map b observe introducing wave encoder see improvement classes compare yamnet improvement seen classes realignment scheme e ondevice experiments ondevice model results discussed table iv deploying ondevice perform quantization using tensor flow tﬂite results model size nearly mb implement preprocessing steps manner discussed section iii use samsung s smart phone android sdk gb ram gb rom octa core exynos chip ondevice experiments due model quantization see slight natural drop map gpu ondevice compare ondevice model result baseline work demostrates several ondevice models best map att rnn model lower lean model v conclusion paper propose lightweight joint embedding based audio classiﬁcation model ondevice deployment called lean simultaneously extracts temporal pretrained spatial features given audio signal fur ther use novel temporal features realignment using cross attention results improved performance slight increase model parameters proposed lean model outperforms baseline ondevice system fsdk dataset lesser memory footprint produced compet itive results comparison several existing works also conduct detailed analysis classlevel impact using sys tem classes fsdk dataset work demon strates using pretrained model possible achieve performance improvement using crossattentionbased temporal realignment future work aim replace yamnet state art pretrained transformerbased feature extractor model see performance impact addition wish leverage label ontologybased mutual relation dependency knowledge fsdk multi label dataset improve classlevel performance references j f gemmeke d p ellis d freedman jansen w lawrence r cmoore m plakal m ritter “audio set ontology humanlabeled dataset audio events” ieee international conference onacoustics speech signal processing icassp pp – mesaros t heittola t virtanen “ multidevice dataset forurban acoustic scene classiﬁcation” workshop detection classiﬁcation acoustic scenes events dcase pp – k choi g fazekas m sandler “automatic tagging using deepcon volutional neural networks” conference internationalsociety music information retrieval ismir pp – kong qiuqiang cao yin iqbal turab wang yuxuan plumbley mark panns largescale pretrained audio neural networks audio pattern recognition prateek verma jonathan berger audio transformerstransformer architectures large scale audio understanding arxiv cssd multimodal speech emotion recognition using audio text slt yoon seunghyun byun seokhyun jung kyomin httpsgithubcomtensorﬂowmodelstreemasterresearchaudiosetvggish gemmeke jort ellis daniel freedman dylan jansen aren lawrence wade moore r plakal manoj ritter marvin audio set ontology humanlabeled dataset audio events icassp sarthak shukla shikhar mittal govind spoken language identiﬁcation using convnets salamon c jacoby j p bello “ dataset taxonomy urbansound research” nd acm international conference multimediaacmmm’ orlando fl usa nov pp – piczak karol esc dataset environmental sound classiﬁ cation httpswwwkagglecomcfreesoundaudiotagging w dai c dai s qu j li s das “ deep convolutionalneural networks raw waveforms” ieee international conferenceon acoustics speech signal processing icassp pp – k x zhang s ren j sun “deep residual learning forimage recognition” ieee conference computer vision pattern recognition cvpr pp – k choi g fazekas m sandler “automatic tagging using deep convolutional neural networks” inconference internationalsociety music information retrieval ismir pp – q kong c yu y xu t iqbal w wang m d plumb ley “weaklylabelled audioset tagging attention neural networks ”ieeeacmtransactions audio speech language processing vol pp– bartz christian herold tom yang haojin meinel christoph language identiﬁcation using deep convolutional recurrent neural networks bahdanau dzmitry cho kyunghyun bengio y neural machine translation jointly learning align translate arxiv vaswani ashish shazeer noam parmar niki uszkoreit jakob jones llion gomez aidan kaiser lukasz polosukhin illia attention need luong minhthang pham hieu manning christopher effective approaches attentionbased neural machine translation vd sandler mark howard andrew zhu menglong zhmoginov andrey chen liangchieh mobilenetv inverted residuals linear bottlenecks cvpr hochreiter sepp schmidhuber j¨urgen long shortterm memory neural computation neco httpsresearchgooglecomaudiosetontologyindexhtml soundﬁlehttpspypiorgprojectsoundfile tensorﬂow httpswwwtensorﬂoworg keras httpskerasio httpsgithubcomsarthakyadavfsdkpytorch httpswwwtensorﬂoworgapi docspythontfkerasmetricsauc koutini khaled schl¨uter jan eghbalzadeh hamid wid mer gerhard efﬁcient training audio transformers patchout gong yuan chung yuan glass james psla improving audio tagging pretraining sampling labeling aggregation ieeeacm transactions audio speech language processing niizumi daisuke takeuchi daiki ohishi yasunori harada noboru kashino kunio byol audio selfsupervised learning generalpurpose audio representation ijcnn wu hohsiang seetharaman prem kumar kundan bello juan wavclip learning robust audio representations clip httpsdoiorgarxiv author choi kwanghee title kersner martin morton jacob chang buru temporal knowledge distillation ondevice audio classiﬁcation p bradley “ use area roc curve evaluation machine learning algorithms” pattern recognition vol pp – httpsgithubcomtensorﬂowmodelstreemasterresearchaudiosetyamnet audioset based vggish audioset based yamnet fonseca eduardo favory xavier pons jordi font frederic serra xavier fsdk open dataset humanlabeled sound events ",34.4,"1"
7,"Squeeze Training for Adversarial Robustness"," b e f g l s c v v x r published conference paper iclr squeeze training adversarial robustness qizhang li yiwen guo∗ wangmeng zuo∗ hao chen harbin institute technology tencent security big data lab independent researcher uc davis liqizhangguoyiwengmailcom wmzuohiteducn chenucdavisedu abstract vulnerability deep neural networks dnns adversarial examples attracted great attention machine learning community problem related nonﬂatness nonsmoothness normally obtained loss landscapes training augmented adversarial examples aka adversarial training considered effective remedy paper highlight collaborative examples nearly perceptually indistinguishable adversarial benign examples yet show extremely lower prediction loss can utilized enhance adversarial training novel method therefore proposed achieve new stateofthearts adversarial robustness code httpsgithubcomqizhanglistat introduction adversarial examples szegedy et al biggio et al crafted adding imperceptible perturbations benign examples capable fooling dnns make incorrect predictions existence adversarial examples raised security concerns attracted great attention much endeavour devoted improve adversarial robustness dnns one effective methods adversarial training madry et al introduces powerful adaptive adversarial examples model training encourages model classify correctly paper gain deeper understanding dnns robust examine valley loss landscapes explore existence collaborative examples bounded neighborhood benign examples demonstrate extremely lower prediction loss comparison neighbors somewhat unsurprisingly existence examples can related adversarial robustness dnns particular given model trained adversarially robust less likely discover collaborative example moreover incorporating collaborative examples model training seemingly also improves obtained adversarial robustness point advocate squeeze training st adversarial examples collaborative examples benign example jointly equally optimized novel procedure maximum possible prediction discrepancy constrained extensive experimental results verify effectiveness method demonstrate st outperforms stateofthearts remarkably several benchmark datasets achieving absolute robust accuracy gain without utilizing additional data cifar can also readily combined variety recent efforts eg rst carmon et al rwp wu et al b improve performance background related work adversarial examples let xi yi denote benign example eg natural image label s xi yin xi ∈ x yi ∈ y c − use bxi xcid cidxcid − xicid∞ ≤  represent bounded l∞ neighborhood xi dnn parameterized θ can deﬁned function fθ· x → rc without ambiguity will drop subscript θ fθ· write f · general adversarial examples almost perceptually indistinguishable benign examples yet lead arbitrary predictions victim models one typical formulation generating ∗work done cosupervision yiwen guo wangmeng zuo correspondence published conference paper iclr xt πbxxt α · sign∇xtcef xt y adversarial example maximize prediction loss constrained neighborhood benign example projected gradient descent pgd madry et al iterative fast gradient sign method ie ifgsm kurakin et al commonly chosen achieving aim seeks possible adversarial examples leveraging gradient g cid◦ f wrt inputs cid loss function eg crossentropy loss ce· y given starting point x iterative update performed xt temporary result obtained tth step function πbx· projects input onto bounded neighborhood benign example starting point can benign example ifgsm randomly neighbor pgd besides ifgsm pgd singlestep fgsm goodfellow et al cw’s attack carlini wagner deepfool moosavidezfooli et al momentum iterative fgsm dong et al also popular effective generating adversarial examples work also investigates way generating adversarial examples without knowledge victim model known blackbox attacks papernot et al chen et al ilyas et al cheng et al xie et al guo et al nobox attacks papernot et al li et al recently ensemble variety attacks becomes popular performing adversarial attack evaluating adversarial robustness strong adversarial benchmark called autoattack aa croce hein consists three whitebox attacks ie apgdce apgddlr fab croce hein one blackbox attack ie square attack andriushchenko et al adopt experimental evaluations paper explore valley loss landscape dnns study beneﬁt incorporating collaborative examples adversarial training independent paper tao et al hypocriti cal examples explored concealing mistakes model attack examples also lied valley yet due difference aim studies hypocritical examples tao et al mainly performed based misclassiﬁed benign examples according formal deﬁnition work concerns local landscapes around benign examples related work include unadversarial examples salman et al assistive signals pestana et al designed d textures customize objects better classifying adversarial training among numerous methods defending adversarial examples adversarial training incorporates examples model training probably one effective ones will revisit representative adversarial training methods subsection vanilla madry et al formulates training objective simple minmax game adversarial examples ﬁrst generated using instance pgd maximize loss eg crossentropy loss objective model parameters optimized minimize loss obtained adversarial examples min θ max ∈bxi xcid cef xcid yi although effective improving adversarial robustness vanilla method inevitably leads decrease prediction accuracy benign examples therefore several followup methods discuss improved principled ways better trade clean robust accuracy zhang et al kannan et al wang et al wu et al b methods advocate regularizing output benign example adversarial neighbors remarkable empirical performance regarded strong baselines will introduce representative one ie trades zhang et al trades zhang et al advocates learning objective comprising two loss terms ﬁrst term penalizes crossentropy loss benign training samples second term regularizes difference benign output output possibly malicious data points speciﬁcally worstcase kullbackleibler kl divergence output benign example suspicious data point bounded l∞ neighborhood minimized regularization term cef xi yi β max klf xcid cid f xi min θ xcid∈bxi published conference paper iclr cifar resnet b cifar wrn c cifar resnet d cifar wrn figure average crossentropy loss value benign examples blue collaborative examples red resnet trained using cifar b wide resnet trained using cifar c resnet trained cifar d wide resnet trained cifar shaded areas indicate scaled standard deviations collaborative examples crafted ﬁxed step size α various perturbation budgets efforts also devoted family adversarial training research eg mart wang et al robust self training rst carmon et al adversarial weight perturba tion awp wu et al b speciﬁcally investigating inﬂuence misclassiﬁed samples model robustness mart advocates giving speciﬁc focus samples robustness awp identiﬁes ﬂatter loss changing respect parameter perturbation leads improved generalization adversarial training provides novel double perturbation mechanism rst proposes boost adversarial training using unlabeled data incorporating semisupervised learning rebufﬁ et al focus data augmentation study performance using genera tive models also insightful papers focuses model architectures huang et al wu et al bai et al mao et al paul chen batch normalization xie et al activation functions xie et al b dai et al distillation robust models also studied zi et al shao et al awais et al st improving adversarial robustness partially inspired recent adversarial training effort will discuss compare trades stateofthearts section besides method can naturally combined variety prior effort introduced section achieve improvements will demonstrated section collaborative examples surge interest adversarial examples achieved understandings plateau regions loss landscape dnns however valleys landscape seem less explored section examine valleys explore existence collaborative examples ie data points capable achieving extremely lower classiﬁcation loss bounded neighborhood benign examples particular discuss adversarial robustness dnns collaborative examples affect providing several intriguing observations valley loss landscape unlike adversarial examples data points higher even maximal prediction loss pay attention local minimum around benign examples subsection achieve simply adapt pgd method instead minimize prediction loss comparing eq can seen main difference eq gradient descent performed rather gradient ascent similar ifgsm pgd attack clip result eq update iteration guarantee perturbation within presumed budget eg perform update step size α resnet et al b wide resnet zagoruyko komodakis models trained cifar cifar tested xt πbxxt − α · sign∇xtcef xt y published conference paper iclr figure visualization benign example left collaborative example crafted normally trained resnet model middle collaborative ex ample crafted robust resnet model right normal b robust figure loss landscapes b show normal resnet adversari ally trained resnet using trades respec tively cifar multiple update steps preciously steps experiment evaluate crossentropy loss obtained examples compare benign loss left panels figure can seen though benign data shows relatively low crossentropy loss ie ∼ average resnet cifar test set already always exists neighboring data easily achieve extremely lower loss values ie almost data points showing lower prediction loss collaborative examples interest figure visualize collaborative examples existence collaborative examples implies large local lipschitz constants nonﬂat regions g cid◦ f somehow different perspective conventional adversarial phenomenon shed light test dnn models trained robust adversarial examples using vanilla trades mart results can found right panels figure see difﬁcult achieve zero prediction loss models probably owing ﬂatter loss landscapes li et al see figure perturbation direction u obtained utilizing eq v random chosen hyperplane orthogonal u analyze angle collaborative perturbations pgd adversarial perturbations appendix b results show less robust model collaborative perturbations adversarial perturbations closer orthogonal figure also illustrate collaborative example generated robust model see collaborative perturbations robust nonrobust models perceptually different particular bluer sky leads conﬁdent prediction robust model readers interested visualization results provided appendix f can collaborative examples aid given results collaborative examples less “destructive” adversarially robust model raise question can collaborative examples return beneﬁt adversarial robustness towards answering question one may ﬁrst try incorporate collaborative examples training phase see whether adversarial robustness obtained dnn model can improved end resort following learning objective cid min θ cef xi yi β · klf xcol f xi collaborative example crafted using method introduced section eq xcol optimization problem minimizes output discrepancy collaborative examples corresponding benign examples addition loss term encourages correct prediction benign examples simple straightforward method similarly adopted tao et al’s work tao et al resisting hypocritical examples quick experiment performed test beneﬁt adversarial robustness settings inner update obtaining collaborative examples performed steps step size α perturbation budget  evaluate prediction accuracy pgd adversarial examples benign examples comparison figure shows testset performance resnet trained normal trained using eq cifar cifar respectively can seen robust accuracy improved remarkably solely incorporating collaborative examples training meanwhile normally trained models consistently show ∼ robust accuracy models trained via adversarial training show quite good robust accuracy adversarial attacks cifar cifar clean robust accuracy models provided appendix published conference paper iclr cifar b cifar figure changes clean robust accuracy training resnet cifar b cifar using eq best viewed color however still exists considerable gap obtained performance eq trades see figure appendix robust overﬁtting rice et al can observed red curves figure especially th epoch even though training collaborative examples testing adversarial examples seems severe comparison occurs existing adversarial training methods also test dnn architectures eg vgg simonyan zisserman wide resnet zagoruyko komodakis results similar squeeze training st section experimentally shown collaborative examples exist can used improve adversarial robustness dnns simply enforcing output probabilities close output corresponding benign neighbors section consider utilizing collaborative examples adversarial examples jointly model training aiming regularizing nonﬂat regions including valleys plateaus loss landscape altogether method adversarial examples can utilized training variety ways leading various adversarial training methods paper considering adversarial collaborative examples caused nonﬂatness loss landscapes propose penalize maximum possible output discrepancy two data points within bounded neighborhood benign example inspired adversarial regularization eg trades adopt benign prediction loss term combination term possible adversarial examples possible collaborative examples jointly regularized benign example necessarily involved error accumulation regularization term since regard output benign example neither explicitly encouraged “adversarial” “collaborative” achieve advocate squeeze training st whose learning objective cid min θ cef xi yi β max cidregf xcid f xcidcid xcid ∈bxi xcidcid∈bxi st pf yi xcid ≥ pf yi xi ≥ pf yi xcidcid ∀ cidreg· regularization function evaluates discrepancy two probability vectors β scaling factor balances clean robust accuracy st squeezes possible prediction gaps within whole bounded neighborhood benign example jointly regularizes two sorts nonﬂat regions loss landscape constraint eq essence introduced ensure two examples obtained inner optimization include one adversarial example one collaborative example considering makes little sense minimize gap two adversarial examples two collaborative examples several different choices regularization function cidreg eg jensen–shannon js divergence squared l distance symmetric kl divergence formulated follows js cidreg squared l kl f xcid f xcidcid f xcid kl cidreg cidf xcid − f xcidcidcid f xcid f xcidcid f xcidcid published conference paper iclr algorithm squeeze training st input set benign example labels s number training iterations t learning rate η number inner optimization steps k perturbation budget  step size α choice regularization function cidreg initialization perform random initialization f t t sample minibatch training data xi ym m parallel ← xi · n xcidcid xcid k ≥ ← argmax xadv ˜xi∈xixcid ixcidcid ginner cidregf xadv α·sign∇xadv ← πbxixadv xcid k ← k − ← xi · n ← argmin ˜xi∈xixcid ixcidcid ← πbxixcol α·sign∇xcol cef ˜xi yi xcol ginner xcidcid cef ˜xi yi f xcol ginner end gi ← cef xi yi β · cidregf xadv f xcol end θ ← θ − η m cidm ∇θgi end output robust classiﬁer f parameterized θ symmetric kl cidreg klf xcid f xcidcid klf xcidcid f xcid among choices squared l js divergence already symmetric also adapt original kl divergence make satisfy symmetry axiom desirable metrics adversarial examples collaborative examples treated equally formulation eq pairs collaborative adversarial examples obtained simulta neously inner optimization different eq procedure implemented algorithm training iteration perform k update steps inner optimization step two examples reinitialized selecting triplet based crossentropy loss updated using sign gradients comparison crossentropy loss reinitialization triplet carried just guarantee chained inequality eq discussions discussions st given like ﬁrst mention although eq par tially inspired trades collaborative ex amples can generally naturally introduced adversarial training formulations compare st optimizes eq trades carefully know latter regularizes kl divergence benign exam ples neighboring data points neigh boring data point whose output shows maximal kl divergence benign output can adversarial example collaborative example actually figure demonstrate ratio collaborative examples prediction loss different examples along training using trades proceeds can seen ratio collaborative examples used model train ing decreases consistently always less st aims use col laborative adversarial examples possible comparing trades cidreg term st imposes stricter regularization since maxi mum possible output discrepancy two data points within bounded neighborhood figure changes average crossentropy loss three types examples along trades training upper changes ratio col laborative examples utilized lower exper iment performed resnet cifar published conference paper iclr squared l b js c symmetric kl d compare altogether figure performance trades st different choices regularization functions cidreg scaling factor β results obtain resnet cifar robust accuracy evaluated using autoattack  trades also try using js divergence squared l distance symmetric kl divergence regularizing suspicious benign outputs reported b c top right indicates better tradeoff best viewed color penalized bounds trades regularization regard worstcase outputs ie adversarial outputs bestcase outputs collaborative outputs expected optimized jointly equally moreover mentioned since st explicitly enforce benign output probabilities match output adversarial examples expect improved tradeoff robust clean errors extensive empirical comparison trades section will verify st indeed outperforms signiﬁcantly probably beneﬁts merits st adopts regularization loss inner outer optimization also observed moderate gradient masking occurs ie higher pgd accuracy lower classiﬁcation accuracy powerful attacks just like gowal et al ’s experiments section compare proposed st stateoftheart methods mainly compare vanilla madry et al trades zhang et al mart wang et al performance outstanding methods zhang et al wu et al b kim et al yu et al will compared section additional data may used experiments conducted popular benchmark datasets including cifar cifar krizhevsky hinton svhn netzer et al table summarizes main results resnet et al b also test wide resnet zagoruyko komodakis show method works well largescale classiﬁcation models whose results can found table worth noting st can readily combined many recent advances eg awp wu et al b rst carmon et al awp utilizes weight perturbation addition input perturbation can combine st replacing learning objective rst uses unlabeled data boost performance adversarial training ﬁrst produces pseudo labels unlabeled data minimizes regularization loss labeled unlabeled data training settings experiments section perform adversarial training perturbation budget  inner step size α except svhn dataset use α training phase always use sgd optimizer momentum weight decay batch size train resnet et al epochs cifar cifar adopt initial learning rate cut × th th epoch svhn train resnet epochs initial learning rate cut × th th epoch adopt β trades β mart following original papers ﬁnal choice regularization function cidreg scaling factor β st will given section models trained nvidia teslav gpu evaluation details evaluate performance adversarially trained models computing clean robust accuracy robust accuracy perform various whitebox attack methods including fgsm goodfellow et al pgd madry et al cw’s attack carlini wagner autoattack aa croce hein speciﬁcally perform pgd pgd cw∞ ie l∞ version cw’s loss optimized using pgd  α since adversarial training generally shows overﬁtting rice et al select model best pgd performance checkpoints suggested many recent papers zhang et al wu et al b wang et al gowal et al dataset cifar method vanilla trades mart st tradesawp st oursawp vanilla trades mart st tradesawp st oursawp vanilla trades mart st tradesawp st oursawp cifar svhn ± ± ± ± ± ± ± ± ± ± ± ± pgd cw∞ clean fgsm pgd ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± published conference paper iclr table clean robust accuracies robust resnet models trained using different adversarial training methods robust accuracy evaluated l∞ threat model  perform seven runs report average performance conﬁdence intervals aa ± comparison different discrepancy metrics get started compare three choices discrepancy metric cidreg eg js divergence squared l distance symmetric kl divergence figure summarize performance st choices vary value scaling factor β demonstrate tradeoff clean robust accuracy robust accuracy evaluated using autoattack croce hein provides reliable evaluations fair comparison also evaluate trades original kl divergence function replaced newly introduced discrepancy functions illustrate results plots ie figure b c correspondingly performance curve original trades shown every subﬁgure grey figure see table β values ﬁgure cidreg regularizations imposed trades less signiﬁcant thus use β sets larger elements figure c one can see considerably improved tradeoff clean robust accuracy achieved using st comparison trades using discrep ancy metric measuring gap probability vectors moreover figure d can seen st different choices discrepancy functions always outperforms original trades large margin using symmetric kl divergence cidreg leads best performance overall will stick symmetric kl divergence st following comparison use β cifar β cifar β svhn method trades trades st trades st symmetric kl trades st table scaling factor β different discrepancy functions reported figure cidreg kl β set js l comparison stateofthearts table reports performance adversarial training method st competitors intensive results demonstrate st outperforms vanilla madry et al trades zhang et al mart wang et al signiﬁcantly gaining consistently higher clean robust accuracy cifar cifar svhn words st signiﬁcantly enhances published conference paper iclr adversarial robustness less degradation clean accuracy indicating better tradeoff clean robust performance speciﬁcally cifar best prior method shows classiﬁcation accuracy adversarial clean test sets respectively st symmetric kl achieves combining awp wu et al b gain absolute improvement robust clean accuracy respectively compared tradesawp cifar similar observations can made cifar svhn complexity analyses method deferred appendix c training curves given figure demonstrate less overﬁtting figure addition experiments resnet also employ largerscale dnns ie wide resnet wrn zagoruyko komodakis train robust wrn models including robust wrn robust wrn cifar report experimental results table obviously wrn models lead higher clean robust accuracy resnet importantly st still outper forms competitors networks showing effectiveness method holds size dnn model scales another wrn ie wrn also tested observations can made will test carefully section additional unlabeled data utilized training table evaluation using wrns cifar can seen effectiveness method holds size dnn scales pgdtrades default evaluation zhang et al clean method trades st trades mart st pgdtrades wrn wrn model aa adversarial training additional data rst carmon et al recent work conﬁrms unlabeled data also properly incorporated training enhancing adversarial robustness consider simple direct combination recall rst paper extracted k unlabeled data million tiny images torralba et al utilize unlabeled images generates pseudo labels performs adversarial training set including cifar training images originally unlabeled data st can easily incorporated obtaining pseudo labels implement rst combination called strst following settings orig inal paper rst table report empirical results compare performance strst re cent work utilizes set unlabeled data ie gairrst zhang et al awprst wu et al b batrst kim et al rwp rst yu et al results collected ofﬁcial implementations rwprst fact one best solutions setting robustbench croce et al achieves robust accuracy sacriﬁcing clean accuracy strst gains ro bust clean accuracy comparing table training wrn additional unlabeled data methods table use unlabeled data rst’s ofﬁcial github repository pgdrst evaluation method rst paper clean batrst gairrst awprst rwprst strst pgdrst method rst aa conclusion paper studied loss landscape dnn models robust speciﬁcally paid attention valley region landscapes collaborative examples widely exist veriﬁed collaborative examples can utilized beneﬁt adversarial robustness particular proposed st squeeze training method take adversarial examples collaborative examples accounts jointly equally regularizing loss landscape dnn training forming novel regularization regime extensive experiments shown st outperforms current stateofthearts across different benchmark datasets network architectures can combined recent advances including rst awp gain progress improving adversarial robustness published conference paper iclr references maksym andriushchenko francesco croce nicolas flammarion matthias hein square attack queryefﬁcient blackbox adversarial attack via random search arxiv preprint arxiv muhammad awais fengwei zhou chuanlong xie jiawei li sungho bae zhenguo li mixacm mixupbased robustness transfer via distillation activated channel maps arxiv preprint arxiv yutong bai jieru mei alan l yuille cihang xie transformers robust cnns advances neural information processing systems battista biggio igino corona davide maiorca blaine nelson nedim ˇsrndi´c pavel laskov giorgio giacinto fabio roli evasion attacks machine learning test time joint european conference machine learning knowledge discovery databases pp – springer nicholas carlini david wagner towards evaluating robustness neural networks sp yair carmon aditi raghunathan ludwig schmidt john c duchi percy s liang unlabeled data improves adversarial robustness neurips pinyu chen huan zhang yash sharma jinfeng yi chojui hsieh zoo zeroth order optimization based blackbox attacks deep neural networks without training substitute models proceedings th acm workshop artiﬁcial intelligence security pp – acm shuyu cheng yinpeng dong tianyu pang hang su jun zhu improving blackbox adversarial attacks transferbased prior neurips francesco croce matthias hein minimally distorted adversarial examples fast adaptive boundary attack arxiv preprint arxiv francesco croce matthias hein reliable evaluation adversarial robustness ensemble diverse parameterfree attacks icml francesco croce maksym andriushchenko vikash sehwag edoardo debenedetti nicolas flam marion mung chiang prateek mittal matthias hein robustbench standardized adversarial robustness benchmark arxiv preprint arxiv sihui dai saeed mahloujifar prateek mittal parameterizing activation functions adversarial robustness arxiv preprint arxiv yinpeng dong fangzhou liao tianyu pang hang su jun zhu xiaolin hu jianguo li boosting adversarial attacks momentum cvpr ian j goodfellow jonathon shlens christian szegedy explaining harnessing adversarial examples iclr sven gowal chongli qin jonathan uesato timothy mann pushmeet kohli uncovering limits adversarial training normbounded adversarial examples arxiv preprint arxiv sven gowal sylvestrealvise rebufﬁ olivia wiles florian stimberg dan andrei calian timothy mann improving robustness using generated data advances neural information processing systems yiwen guo qizhang li hao chen backpropagating linearly improves transferability adversarial examples arxiv preprint arxiv kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition cvpr published conference paper iclr kaiming xiangyu zhang shaoqing ren jian sun identity mappings deep residual networks eccv b hanxun huang yisen wang sarah erfani quanquan gu james bailey xingjun ma explor ing architectural ingredients adversarially robust deep neural networks advances neural information processing systems andrew ilyas logan engstrom anish athalye jessy lin blackbox adversarial attacks limited queries information icml harini kannan alexey kurakin ian goodfellow adversarial logit pairing arxiv preprint arxiv hoki kim woojin lee sungyoon lee jaewook lee bridged adversarial training arxiv preprint arxiv alex krizhevsky geoffrey hinton learning multiple layers features tiny images technical report university toronto alexey kurakin ian goodfellow samy bengio adversarial machine learning scale iclr hao li zheng xu gavin taylor christoph studer tom goldstein visualizing loss landscape neural nets neurips qizhang li yiwen guo hao chen practical nobox adversarial attacks dnns advances neural information processing systems aleksander madry aleksandar makelov ludwig schmidt dimitris tsipras adrian vladu towards deep learning models resistant adversarial attacks icml xiaofeng mao gege qi yuefeng chen xiaodan li ranjie duan shaokai ye yuan hui xue towards robust vision transformer arxiv preprint arxiv seyedmohsen moosavidezfooli alhussein fawzi jonathan uesato pascal frossard robust ness via curvature regularization vice versa cvpr yuval netzer tao wang adam coates alessandro bissacco bo wu andrew y ng reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning nicolas papernot patrick mcdaniel ian goodfellow somesh jha z berkay celik ananthram swami practical blackbox attacks machine learning asia ccs sayak paul pinyu chen vision transformers robust arxiv learners arxiv preprint camilo pestana wei liu david glance robyn owens ajmal mian assistive signals deep neural network classiﬁers proceedings ieeecvf conference computer vision pattern recognition pp – sylvestrealvise rebufﬁ sven gowal dan calian florian stimberg olivia wiles tim othy mann fixing data augmentation improve adversarial robustness arxiv preprint arxiv leslie rice eric wong j zico kolter overﬁtting adversarially robust deep learning icml hadi salman andrew ilyas logan engstrom sai vemprala aleksander madry ashish kapoor unadversarial examples designing objects robust vision advances neural information processing systems rulin shao jinfeng yi pinyu chen chojui hsieh adversarial robustness transfers knowledge distillation arxiv preprint arxiv published conference paper iclr karen simonyan andrew zisserman deep convolutional networks largescale image recognition iclr christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan ian goodfellow rob fergus intriguing properties neural networks arxiv preprint arxiv lue tao lei feng jinfeng yi songcan chen false friends like can notice mistakes aaai antonio torralba rob fergus william t freeman million tiny images large data set nonparametric object scene recognition ieee transactions pattern analysis machine intelligence – yisen wang difan zou jinfeng yi james bailey xingjun ma quanquan gu improving adversarial robustness requires revisiting misclassiﬁed examples iclr boxi wu jinghui chen deng cai xiaofei quanquan gu wider neural networks really help adversarial robustness arxiv preprint arxiv dongxian wu shutao xia yisen wang adversarial weight perturbation helps robust general ization advances neural information processing systems b cihang xie zhishuai zhang yuyin zhou song bai jianyu wang zhou ren alan l yuille improving transferability adversarial examples input diversity cvpr cihang xie mingxing tan boqing gong jiang wang alan l yuille quoc v le adversarial examples improve image recognition proceedings ieeecvf conference computer vision pattern recognition pp – cihang xie mingxing tan boqing gong alan yuille quoc v le smooth adversarial training arxiv preprint arxiv b chaojian yu bo han mingming gong li shen shiming ge bo du tongliang liu robust weight perturbation adversarial training arxiv preprint arxiv sergey zagoruyko nikos komodakis wide residual networks arxiv preprint arxiv hongyang zhang yaodong yu jiantao jiao eric p xing laurent el ghaoui michael jordan theoretically principled tradeoff robustness accuracy icml jingfeng zhang jianing zhu gang niu bo han masashi sugiyama mohan kankanhalli geometryaware instancereweighted adversarial training arxiv preprint arxiv bojia zi shihao zhao xingjun ma yugang jiang revisiting adversarial robustness distillation robust soft labels make student better proceedings ieeecvf international conference computer vision pp – published conference paper iclr performance models figure table clean robust accuracy models used figure robust accuracy evaluated autoattack cifar resnet cifar wrn cifar resnet cifar wrn clean vanilla trades mart aa clean aa normal clean aa clean aa b collaborative adversarial directions analyze angle collaborative perturbations pgd adversarial perturbations summarize experiments figure bunch update steps observe lies limited range around ◦ unsurprising high dimensional input space however robust models see angle deviate ◦ indicating powerful collaborative adversarial perturbations become correlated robust landscapes cifar resnet b cifar wrn c cifar resnet d cifar wrn figure distributions angles pgd adversarial perturbations collaborative perturbations ﬁrst update iteration collaborative example corresponding adversarial examples show opposite directions however update steps since gradient computed wrt different inputs adversarial collaborative directions become less less correlated angle ﬁnally lies range around ◦ interestingly robust model trades correlation obvious less concentrated around ◦ c computational complexity st since adversarial examples collaborative examples required st computational complexity inner optimization increases yet note two sorts examples can computed parallel thus run time st can similar slightly higher baseline fact even cut number inner iteration steps st × reduce run time roughly × performance method still satisfactory shows robust accuracy evaluated autoattack clean accuracy already surpasses performance vanilla trades mart furthermore performance previous stateofthearts always improve higher computational capacity eg inner optimization steps instance trades show slightly better robust accuracy aa ± decreased clean accuracy ± cifar × inner steps better st aa ± clean accuracy ± published conference paper iclr also consider generating two neighboring examples benign example compute meanmax regularization loss optimization existing stateofthearts experimental results show mean regularization effective table shows resnet results compared methods cifar cifar svhn innovative formulation can see none methods including vanilla † trades† mart† symbol “†” indicates utilization two examples improves consistently comparing results table among competitors st trades† performs best table demonstrates performance advances cifar cifar drops svhn comparison trades results table wrn architectures improves wrn clean aa additional unlabeled data drops wrn clean − aa − wrn clean − aa − without fact innovative formulation involving two examples likely incorporate collabo rative examples trades† might reason works better trades† vanilla † mart† combined awp innovative formulation involving two examples still obviously inferior st table two neighboring examples generated benign example inner optimization vanilla † trades† mart† mean regularization adopted outer optimization robust accuracy still evaluated l∞ threat model  perform seven runs report average performance conﬁdence intervals dataset cifar cifar svhn method vanilla † trades† mart† st tradesawp† st oursawp vanilla † trades† mart† st tradesawp† st oursawp vanilla † trades† mart† st tradesawp† st oursawp ± ± aa ± ± ± ± ± ± ± ± ± ± ± ± pgd cw∞ fgsm pgd clean ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± published conference paper iclr d training curves st figure average crossentropy loss changes training using st experiment performed resnet cifar shaded areas represent scaled standard deviations can seen gap collaborative examples benign examples adversarial examples benign examples obviously reduced comparing figure best viewed color cifar b cifar figure changes clean robust accuracy training resnet cifar b cifar using trades st robust accuracy evaluated using pgd adversarial examples generated steps  step size α best viewed color figure changes clean robust accuracy training wrn cifar using rst strst following original paper rst robust accuracy evaluated using pgd adversarial examples generated steps  step size α ﬁrst images test set although train epochs suggested rst paper comparison table main paper can seen training epochs can still beneﬁcial method best viewed color e make “adversarial” “collaborative” clear paper especially section sometimes generalize deﬁnition adversarial examples include data points bounded neighborhood benign examples showing considerably higher prediction loss benign loss contrast deﬁnition narrower sense saying different label prediction made collaborative examples similarly “deﬁned” somewhat nonrigorously likewise throughout paper abuse word “adversarial” “collaborative” describe spirit achieving higher lower loss benign loss respectively published conference paper iclr f visualizations visualize collaborative examples collaborative perturbations cifar cifar svhn cifar b cifar c cifar figure visualization benign examples collaborative perturbations collaborative examples cifar b cifar c svhn collaborative examples crafted normally trained resnet models robust resnet models apparently collaborative examples robust nonrobust models strikingly different ",44.85,"1"
8,"Adversarial Attacks on Machine Learning in Embedded and IoT Platforms"," adversarial attacks machine learning embedded iot platforms christian westbrook sudeep pasricha abstract—machine learning ml algorithms increasingly integrated embedded iot systems surround us vulnerable adversarial attacks deployment ml algorithms resourcelimited embedded platforms also requires use model compression techniques impact model compression techniques adversarial robustness ml important emerging area research article provides overview landscape adversarial attacks ml model compression techniques relevant embedded systems describe efforts seek understand relationship adversarial attacks ml model compression discussing open problems area index terms—adversarial machine learning model compression embedded systems iot deep learning ．introduction d eep neural networks dnns increasingly becoming part embedded iot systems surround us facial recognition smartphones voice recognition smart home speakers autonomous navigation control emerging vehicles overlap domains embedded systems machine learning ml continues increase developers must find ways deploy dnns efficiently resourceconstrained embedded platforms unfortunately stateoftheart dnns continue push boundaries performance led steady increase model size complexity example natural language processing models gpt computer vision models vision transformers vit possess billions parameters trend presents challenges applications ml embedded environments resources memory energy tightly constrained one active area research aimed reducing memory footprint energy expenditure inference latency ml models use resource constrained environments field model compression model compression techniques provide several key benefits dnns embedded environments compressed dnn models can fewer compact parameters take less space memory require fewer operations inference time reduced computational intensiveness due model compression also reduces latency energy consumption example use quantization pruningbased model compression techniques allowed dnnbased framework indoor localization embedded mobile devices reduce model size inference latency minimal cost model accuracy reduction improvements can enable deployment new ml applications employ progressively powerful models embedded environments growing number ml models making way commodity embedded systems raises questions robustness adversarial attacks demonstrated ml systems vulnerable inferencetime evasion attacks involving small imperceptible perturbations introduced input samples caused dnns misclassify samples forced misclassification severe consequences embedded ml examples exploitation made possible attacks include attacker causing selfdriving vehicle misclassify stop sign seemingly innocuous object misclassified firearm adversarial attacks can also occur training time eg poisoning backdoor attacks discoveries highlight need research impact adversarial attacks world ml intertwined widely used tightly embedded iot systems becoming happens adversarial attacks target ml systems optimized embedded environments observed adversarial attacks often exploit precision data attack reduction precision potentially render attack less effective although model compression techniques modify precision many adversarial attacks depend recent research indicates relationships model compression techniques robustness adversarial attacks complex deeper understanding relationships needed better assessment risks involved deploying compressed ml models embedded environments adversarial attacks model compression separately studied prior work authors survey adversarial attacks defenses consider impact model compression attack scenarios authors survey strategies ml model compression without considering implications compression adversarial attacks article discusses techniques fields adversarial machine learning model compression discussing recent research exploring relationship areas also discuss open challenges future directions relevant emerging embedded iot systems ii． adversarial machine learning adversarial machine learning study attacks attempt degrade ml system’s ability make accurate predictions although phrase ‘adversarial attack’ sometimes used exclusively refer evasion attacks article broadly define adversarial attacks inclusive evasion attacks poisoning attacks backdoor attacks exploratory attacks evasion attacks target ml inference phase seeking evade system forcing victim model misclassify input samples attack accomplished introducing malicious adjustments input samples known adversarial perturbations various algorithms devised generating perturbation required force misclassification evasion attack approaches typically begin estimating direction sensitivity class change respect dimension input sample considered input vector estimations used inform selection perturbation fast gradient sign method fgsm early example method basic iterative method bim improves fgsm continuing refine perturbation iteratively methods consider relative impact perturbing particular dimension choosing instead perturb every dimension contrast jacobian saliency map approach jsma computes adversarial saliency score dimension input vector selects pair dimensions one direction perturb will largest predicted adversarial impact process repeated target sample misclassified victim model producing samples sit just beyond classification boundaries victim model misclassification sensitive precision changes defense measures taken target system carlini wagner cw attack addresses problem improved objective function also maximizes difference victim model’s confidence adversarial target class next likely class attacks seek exploit knowledge model gradients parameters minimize required adversarial perturbation considered whitebox attacks contrast blackbox attacks hopskipjump square attacks adversary access model structure gradients parameters whitebox attacks generally effective blackbox attacks knowledge internal model gradients parameters makes faster efficient calculate minimum change needed create adversarial perturbations poisoning attacks untargeted attacks attacker seeks degrade model performance undermining training process data poisoning attacks introduce adversarial samples perturb existing samples training dataset causing undesirable changes decision boundaries learned model alternatively model poisoning attacks model poisoned directly training manipulating cost function gradients computed training compromise model performance backdoor attacks can considered form targeted poisoning attacks rather seeking degrade model performance general done poisoning attacks attacker attempts create backdoor allows adversarial input samples trigger misclassification target class example demonstrates insertion backdoor facial recognition system despite adversary lacking knowledge either victim model training set exploratory attacks unique attempting degrade model performance attacks instead seek learn much can victim model probing legitimate samples model inversion exploratory attack probed information used infer sensitive details features present victim model’s training dataset one example reconstruction faces used train facial recognition system model extraction exploratory attacks seek use probing construct replica victim model replica can used stage attacks authors demonstrate can accomplish blackbox model inversion attack performed fewer queries extracting victim model performing whitebox inversion attack replica fig evasion poisoning backdoor exploratory attack scenarios summary evasion poisoning backdoor exploratory attacks illustrated fig major challenges ml adversarial attacks also successfully transferred models designed perform similar tasks even architectures training sets different property dramatically complicates challenge defending ml models deployed embedded systems adversarial attacks enabling scenarios exploratory attacks used extract victim model generate surrogate model evasion poisoning backdoor attack can crafted transferred back victim model iii．model compression many potential applications ml embedded systems selfdriving cars employ ml detect lane markers road signs pedestrians space systems use ml curate compress data transmission earth challenges common embedded environments usually involve resource constraints limited available memory processing capability energy budget model compression techniques can improve embedded ml performance several ways compressed ml models require less memory use less energy perform inference faster fewer computations benefits led widespread adoption model compression techniques ml models deployed embedded systems several model compression approaches particularly effective reducing model resource requirements including network pruning quantization knowledge distillation network pruning leverages tendency towards sparsity dnns removing weights connections filters channels even entire layers unlikely significantly impact model accuracy magnitudebased pruning approaches instance consider parameters values threshold candidates pruning various dnn pruning approaches demonstrated produce significant reductions model size minimal impact model accuracy network quantization decreases ml model size reducing number bits used represent parameter model one approach introduce quantization post training recover model performance finetuning quantizationaware training another approach involves simulation model performance iteratively weights quantized gradually training minimize accuracy lost quantization quantized models reduced bit integer lower precision require far fewer also simpler computations inference floatingpoint operations necessary fullprecision bit counterparts network knowledge distillation uses principles transfer learning train smaller model original model teacher one way accomplish use logit vectors output original model target learner model train complex arrangement third model interposed original learner known ‘teacher assistant’ model facilitate effective transfer outcome knowledge distillation generation smaller model will consume less energy take less space memory perform inference faster iv．attacking compressed models impact model compression techniques adversarial robustness ml important emerging area research model compression techniques become widely adopted becomes essential understand relationship model compression technique robustness various forms adversarial attacks trained robustness cnns robustness quantized dnns evasion attacks explored authors performed experiment assessing image classification quantized different levels precision several evasion attacks including fgsm bim cw attacks among others based results authors argued quantization leads gradient masking state model’s gradients rendered less useful attacker seeking estimate direction sensitivity gradient masking generally considered weak defense model extraction original gradients can reconstructed attacked robustness binarized neural networks bnns evasion attacks analyzed addition benefits reduced memory footprint faster inference authors suggest bnns robust evasion attacks fullprecision counterparts best case equal robustness worst case contrast suggests quantized dnns even vulnerable evasion attacks due correlation bit width quantized model error amplification effect classification error caused perturbation amplified layer layer within model authors argued improvements robustness due quantization nullified error amplification effect proposed way regain benefit controlling error amplification effect using training technique called defensive quantization technique introduces lipschitz regularization training process effectively reducing allowable rate change value output layer thereby controlling reducing error amplification process improved authors proposed using network pruning defend evasion attacks call adversarial neural pruning process attempts minimize distortion introduced latent feature space evasion attack latent feature assessed vulnerability distortion vulnerable features suppressed targeted pruning network components authors explored impact network pruning robustness poisoning attacks experimentally searched relationship network pruning robustness poisoning attacks assessing robustness simple cnn pruned different pruning ratios authors trained cnn image classification experiment iteratively pruned trained model particular ratio model iteratively retrained poisoned data several epochs tested accuracy epoch authors found although every case model eventually succumbs degradation performance pruned models degrade much slower rate requiring many additional epochs poisoning original model authors introduced defense evasion attacks using knowledge distillation ensemble learning called diverse knowledge distillation original model’s knowledge distilled ensemble models caveat ensemble member required learn latent features distinct teacher inference performed polling ensemble member taking majority vote authors called idea latent space separation argued transferability evasion attacks reinforced similarities learned latent spaces vulnerable features attacks teacher model will fail transfer ensemble’s unique assortment latent feature spaces technique recovering poisoning backdoor attacks proposed using knowledge distillation authors began using untrustworthy dataset train model may poisoned contain backdoor generated new dataset clean images without labels called distillation dataset attached predictions made untrusted model new distillation model trained using distillation dataset untrusted distillation models tasked performing inference original untrustworthy dataset predicted classes compared authors argued input sample clean distillation untrusted models agreed models disagreed input sample might poisoned poisoned model generate adversarial classification poisoned samples potentially poisoned samples removed dataset create detoxified dataset distillation model finetuned using dataset resulting distilled model disabled backdoors recovered damage due poisoning attacks v open challenges future directions studies discussed previous section explored impact adversarial attacks compressed ml models use model compression defend adversarial attacks relationships adversarial attacks model compression techniques must explored better understand security risks deploying ml embedded iot environments section discusses potentially fruitful directions future work field attacks model compression pipelines various existing works assess adversarial attacks perform models compressed using single compression technique desirable consider case multiple model compression techniques combined together model compression pipeline will attack scenarios based chained model compression techniques eg pruning quantization retain characteristics attacks member technique new work addressing impact different combinations model compression techniques robustness adversarial attacks beneficial contribution attacks complex models datasets many works presented section iv report experimental results based attack scenarios involving relatively small dnn architectures datasets attack scenarios presented instance employ architectures fewer neurons architectures attacked contain millions attack scenarios presented target models trained small datasets cifar mnist future work experiment stateoftheart models trained complex datasets backdoor attacks examples existing work exploring impact model compression techniques robustness backdoor attacks existing work emphasized knowledge distillation network pruning future work explore relationship quantization robustness backdoor attacks exploratory attacks beneficial assess whether model compression impacts robustness exploratory attacks take significantly fewer queries attacker perform extraction attack inversion attack compressed model change number queries required execute exploratory attack also impact ability security system detect attack occurring attacks diverse model compression approaches existing work primarily focused analyzing specific case evasion attacks quantized models gap seemingly exists volume work performed improve understanding scenario combinations attacks compression techniques future work seek close gap emphasizing neglected attack scenarios article emphasized network quantization network pruning knowledge distillation promising model compression techniques discussed relationships robustness adversarial attacks future work consider additional compression techniques including matrix approximation techniques eg tucker decomposition cp decomposition use structured matrices eg circulant hadamard compression defenses although wealth research exists defenses adversarial attacks ml far less existing work intersection model compression defenses adversarial attacks example compression techniques adapted defense adversarial attacks given defensive quantization technique discussed future work consider relationships model compression techniques robustness adversarial attacks can leveraged create new defensive techniques references l floridi m chiriatti gpt nature scope limits consequences minds machines – x zhai kolesnikov n houlsby l beyer scaling vision transformers proc ieeecvpr – r mishra et al survey deep neural network compression challenges overview solutions arxiv l wang s tiku s pasricha chisel compressionaware high accuracy embedded indoor localization deep learning ieee esl – c szegedy et al intriguing properties neural networks iclr k eykholt et al robust physicalworld attacks deep learning visual classification cvpr – athalye l engstrom ilyas k kwok synthesizing robust adversarial examples icml – l muñozgonzález et al towards poisoning deep learning algorithms backgradient optimization acm ccs – m s jere t farnan f koushanfar taxonomy attacks federated learning ieee security privacy – x chen et al targeted backdoor attacks deep learning systems using data poisoning arxiv k warr strengthening deep neural networks” o’reilly media chakraborty et al adversarial attacks defences survey arxiv j goodfellow j shlens c szegedy explaining harnessing adversarial examples arxiv kurakin goodfellow s bengio adversarial examples n papernot et al limitations deep learning adversarial physical world iclr settings ieee eurosp networks ieee sp – n carlini d wagner towards evaluating robustness neural m fredrikson et al model inversion attacks exploit confidence information basic countermeasures acm ccs – f tramèr et al stealing machine learning models via prediction apis usenix security n papernot p mcdaniel goodfellow transferability machine learning phenomena blackbox attacks using adversarial samples arxiv s vadera s ameen methods pruning deep neural networks ieee access – b jacob et al quantization training neural networks efficient integerarithmeticonly inference cvpr – r bernhard et al impact lowbitwidth quantization adversarial robustness embedded neural networks cw n papernot et al practical blackbox attacks machine learning asiaccs – galloway g w taylor m moussa attacking binarized neural networks iclr j lin c gan s han defensive quantization efficiency meets c song et al layerwise adversarialaware quantization robustness arxiv optimization improving robustness arxiv vulnerability suppression icml – d madaan j shin s j hwang adversarial neural pruning latent b zhao y lao resilience pruned neural network poisoning attack ieee malware – mirzaeian et al diverse knowledge distillation dkd solution improving robustness ensemble models adversarial attacks ieee isqed k yoshida t fujino disabling backdoor identifying poison data using knowledge distillation backdoor attacks deep neural networks acm aisec – s han et al deep compression compressing deep neural networks pruning trained quantization huffman coding iclr k liue et al finepruning defending backdooring attacks deep neural networks arxiv ",23.55,"-1"
9,"Theory of Machine Learning with Limited Data"," n j s c v v x r theory machine learning limited data marina sapir metapattern abstract application machine learning may understood deriving new knowledge practical use explaining accumulated observations training set peirce used term abduction kind inference formalize concept abduction real valued hypotheses show popular textbook ml learners every learner tested covering classiﬁcation regression clustering implement concept abduction inference approach proposed alternative statistical learning theory requires impractical assumption indeﬁnitely increasing training set justiﬁcation introduction commonly accepted theory machine learning ml statistical declared explicitly results theory implicitly assume learning dependence generated ﬁxed probability distribution training set may increased indeﬁnitely “eventually” distribution will well represented training set practical cases assumptions stretch applied ml learning nondeterministic dependence given ﬁnite sample urgent decision making limited sample allow one assume existence probabilities even existence inﬁnite populations essentially applied ml theoretical ml solve diﬀerent problems result theoreticians answer questions practitioners ask instead propose apply peirce’s pragmatic view learning experimental data within paradigm ml search best explanation observations search logically understood peirce abduction inference artiﬁcial intel ligence problems diagnostics already considered example abduction adapt concept abduction deal realvalued hypotheses conjecture every worthy learner mulate concept abduction learner ml abduction learner fourteen popular textbook learners classiﬁcation regression clustering shown support conjecture best knowledge proposed approach ﬁrst one explain logically justify large variety existing learners single point view pragmatic understanding applied ml opens new path solving “ ” questions practitioners ask design new useful learners real life problems brief description section traditional views ml learning predict statistical learning pragmatic view learning informal description logic data explanations alignments deviations logic data explanations aggregation deviations logic data explanations recursive aggregation explanation criteria abduction learning procedure main conjecture proof popular learners abduction learners conclusions traditional views ml describe two understandings ml traditional one one statistical learning theory givens denote ω set real life objects interest example may patients skin cancer bank clients engine failures hidden essential quality like ﬁnd may diagnosis prognosis properties features objects ω can always evaluated numerically expressed expected relevant hidden property suppose n features denote x ⊆ rn domain feature vectors objects ω hidden essential quality also numerical expression values y ∈ r value hidden essence given object called “feedback” assume “underlying dependence” ϕ x → y feature vectors feedback yet can assume dependence deterministic example features may deﬁne completely feedback trying model uncertainty measurements random misclassiﬁcation objects features may diﬀerent feedback object evaluated twice may diﬀerent features even feedback bad luck inevitability indeed ml needed exact theory explaining phenomenon trying predict therefore know depends measurements intrinsic uncertainty information underlying dependence ϕ given training set observa tions values feedback certain data points set tuples hx yi tuples will also called empirical instances sooner ﬁnd proper hypothesis dependence better decision making data shortage bug feature ml prediction problem goal assumed prediction future values nondeterministic dependence example prediction problem understood given training set s data point x new observation hx predict feedback y main issue problem statement evaluate decision select hypotheses need know given future way solve problem available data statistical learning theory approach statistical learning sl theory commonly accepted theoretical approach ml proponents sl theory understand problem “intuitively seems reasonable request learning algorithm presented training examples eventually “converge” optimal solution” “optimal solution” hypothesis er loss criterion close minimal given class functions regardless distribution appear intuitive solve problem training examples expected one ﬁnite training set v vapnik formulated justiﬁcation statistical approach direct way need asymptotic theory h· · · goal construct algorithms limited number observations answer follows construct theory one use concepts terms theory developed h· · · words statistical learning theory assumes indeﬁnite increase training set statistical approach can prove results statistics laws large numbers problem ever increasing training sets convergence apparatus probability theory help understand solve true pragmatic problem ﬁxed ﬁnite data ﬁnite time allocated decision making unquantiﬁable uncertainty pragmatic view learning first address common misconception ml induction ml induction abduction common belief ml induction inference dictionary says induction method reasoning part whole particulars generals individual universal roughly induction extends property part whole start objects certain class known common property conclude objects class property procedure exactly opposite deduction knowing property class one object class infer property object types inference common property objects interest hypothesis given beginning learn start objects observations hypothesis property pattern unites objects known needs found guessing hypothesis explain facts peirce called abduction inference informal description pragmatic learning problem pragmatic point view goal learning ﬁnd best explanation observations prediction let h function used explain observations call explanation hypothesis simply explanation notation s used training set hh set hypothetical instances function h will interested conglomerate instances m h s ss hh informally can formulate explanation principle explanation h good values feedback instances m h s close data points shall close closeness feedback close data points conglomerate m h s can used evaluate explanation quality explanation principle makes clear nondeterministic dependence mostly close feedback close data points “explainable” good explanation explainable dependence hypothesis close feedback close data points well data explanation logic now concentrate developing formalism abduction criterion language alignment far considered underlying dependence single variable general case restriction necessary example using two independent variables may convenient formalizing ranking problem ﬁrst order signature n sorts among others content natural numbers sort n x xn domains independent variables x x xj y y y s s s s α β α β r r r domain feedback types observations types hypothetical instances instances real numbers h o ψ r table sorts la variables k l m n constants ≍ ≍ ≈ ≈ let us notice observations may diﬀerent types may convenient identiﬁcation censored data example set o combines symbols types observations sorts will introduced later need describe aggregation regularization usually domains sorts x xn y metric spaces however triangle axiom distance domain irrelevant required ﬁrst order symbols deﬁned table function o distinguishes observations hypothetical instances may useful many types observations one independent variable n will skip index variable ρx x alignments deviations formulas formed ﬁrst order predicate logic symbol arity x y s ρx ρy o h semantic xα ith variable α ∈ ψ yα feedback α sα type symbol α table function instances sorts ψ × n → xi ψ → y ψ → h ∪ o ψ × ψ × n → r ρxα α kxα − xα ik ψ × ψ → r ψ → ψ → ρyα α kyα − yαk oα ↔ sα ∈ o hα ↔ sα ∈ h ﬁrst order predicate πα β α β ∈ ψ ≤ ≤ n alignment ∀α ∀β ∀α ∀α cidπα βcidsα sαcidcidsβ sβcidcidρxα β ≤ ρxα β icidcid ⇒ πα β pair ﬁrst order formulas α α natural ≤ ≤ n deviation δα α deﬁned δα α tρxα α ρyα α t r≥ × r≥ → r≥ antitone r isotone r let us consider example simplest alignment case n called pointwise πpwα α sα ≈ sα ≍ xα xα simplest deviation δα α ρyα α kyα − yαk aggregation deviations compare explanations one compare collections deviations usually done mapping collection single number comparison let us assume simplicity deviations corresponding alignment ordered one way another denote z additional domain ﬁnite sequences real numbers elements interpreted sequences deviation aggregation will denoted capital letters b g elements sequences will denoted corresponding small letters indices let ′ operation ordering elements sequence smallest largest binary relations domain symbol b ∼ b b ≤ b table binary relations semantic ∀ ai bi ′ b′ kak kbkcid∃q ∀ ∃j cidbi qajcidcidbi ajcidcid kak kbkcid∃q ∀ ∃j cidbi qajcidcidbi ≥ ajcidcid will use three functions kak length b adding number b end sequence ga ai operation ω z → rwill called aggregation satisﬁes next axioms order insensitivity b ∼ b ωa ωb monotony b ≤ b ⇒ ωa ≤ ωb b ⇒ ωa ωb will call aggregation stable next property also satisﬁed ∀acid ∀ ≤kak ai acid ⇒ cidωa acid statement stable aggregation ωa mina ≤ ωa ≤ maxa proof suppose ∃ ωa maxa every element sequence b ωa b monotony ωb ωa contradicts stability proof minimum similar statement b ′ b′ b′ ′ proof obvious b ′ b′ take denote b′ j qa′ b′ ′ three possible cases j qa′ ∃q ∀j ∃icidb′ j ′ icidcidb′ icid j obvious j ′ b′ j ≤ b′ ′ b′ j exists k k qa′ k bl l ′ ≤ ak b′ l ≤ b′ example aggregation operation theorem percentile stable aggregation proof denote pra rth percentile ∈ z order insensitivity obvious percentile take account order elements let us prove monotony let b statement follows ′ b′ ′ b′ denote m kak kbk p r · m integer µa ap µb bp µa µb p integer denote q ⌊p⌋ µa aq aq µb bq bq µa µb stability trivial recursive aggregation introduce language generate class aggregation functions called recursive aggregation language recursive aggregation will use additional functions table function symbols symbol arity ⊕ p η sorts arguments r → r r × r → r g × n → r r × n → r semantic scaling compounding recursive aggregation normalization axioms recursive aggregation function scaling function strictly monotone typical examples function x x ⇒ x x • x x • x x function maxx − used scaling since strictly monotone function ⊕ compounding function ⊕ three axioms simmetricity x ⊕ y y ⊕ x m onotony x x y y ⇒ x ⊕ y x ⊕ y associativity x ⊕ y ⊕ z x ⊕ y ⊕ z next functions satisfy axioms • x ⊕ y x y • x ⊕ y x · y • x ⊕ y maxx y function η normalization function η strictly isotone ﬁrst variable antitone second variable x x ⇒ ηx n ηx n n n ⇒ ηx n ≤ ηx n typical examples function ηt n • ηx n xn • ηx n x−n • ηx n x recursive aggregation function p function p deﬁned recursively step xxxa step xxxa xxxa ⊕ ai recursive aggregation recursive aggregation deﬁned formula theorem recursive aggregation ωa aggregation ωa ηcidxxxa kak kakcid proof let us prove order insensitivity suppose sequence b permutations sequence permutation can obtained ﬁnite number transpositions neighboring elements suppose b can obtained k transpositions let us prove theorem induction k first suppose k suppose b transposes elements ai ai denote αl βl values recursive aggregation function obtained step l sequences b respectively since elements prior identical orders ai − bi − deﬁnition αi αi − ⊕ ai αi αi ⊕ ai αi − ⊕ ai ⊕ ai βi αi − ⊕ ai βi αi − ⊕ ai ⊕ ai using symmetry associativity function ⊕ get ai bi elements starting identical sequences b therefore ωa ωb suppose proved property k k let us prove k k suppose transpositions ordered indices involved elements last transposition involves elements ai ai considerations apply let us prove monotony suppose b according statement ′ b′ ai bi let us prove induction n kak kbk n ωa ηxa ηa ωb ηb ωa ωb functions η strictly monotone ﬁrst variable suppose statement proven n k denote ak bk sequences ﬁrst k elements b respectively inductive hypothesis ωak k ωbk k since function η strictly isotone ﬁrst variable means ηxak k k ηxbk k k xak k xbk k let us prove monotony operation ω n k first notice ωa ηxa k k ηxak k ⊕ ak k ωb ηxbk k ⊕ bk k xak k ⊕ ak xbk k ⊕ bk arguments operation ⊕ left smaller corresponding arguments right function η strictly monotone ﬁrst argument proves theorem popular aggregation functions theorem sequence m kak la ai mxi msxi la la yi aim stable recursive aggregations proof every operation functions recursive aggregation language deﬁned next table x ⊕ y x y x y x · y l l l x x ηx xm x pxm xm x ai ¡ la la la proves stability theorem explanation criteria general explanation criterion evaluates quality hypothesis informally idea criterion presented regularization mentioned “ explanation h good values feedback instances m h s close data points shall close” particular explaining hypothesis h shall close feedback close data points means shall high derivatives diﬀerentiable take account derivatives model data explanation logic shall yet another base vector sequences parameters explaining hypothesis regularization realvalued function vector usually function evaluation derivatives badness rule badness rule triple alignment criterion π deviation function δ aggregation operation ω recursive aggregation aggregation operation determined functions ⊕ η let us consider example badness rule called pointwise rule tpw rule includes alignment relation see πpwα α cidsα ≍ sα ≈ xα xαcid deviation function see σα α ρyα α recursive aggregation operation “averaging” see theorem operations x ⊕ y x y x x x n ηx n theorem ky − yk y − y tpw badness rule equivalent empirical risk criterion lh s hxβi − yβi kskxi training set s β βm h explanation hypothesis proof πpwβi αi αi ψxβi hxβi ≍ deviation aligned pair αi βi δβi αi ρyαi βi yαi − yβi theorem shows recursive aggregation operation la lh s δβi αi m explanation criterion explanation criterion consists • series badness rules • regularization rule • combining operation ca deﬁned sequences real numbers maps outputs previous rules single number criterion value combining operation shall monotone components vector regularization rule necessary one badness rule combing operation used search best explanation pragmatism implies search best explanation want narrow concept search describe two basic search procedures let us consider learning algorithm minimizes explanation criterion lh s class hypotheses h ∈ f given training set s next procedures will considered standard basic training basic training takes f s lh s parameter q consists next steps • focusing optional transformation u s → sq • fitting generating hypothesis h ∈ f ′ ⊆ f evaluating lh sq • optimal selection output optimal explanation hq arg minf ′ lh sq focusing may nonlinear transformation training set yet typically used select observations features emphasize weights selection hypotheses may go whole class f ﬁnite subclass f ′ ⊆ f wrapper strategy wrapper strategy takes f s lh s empty set q consists next steps • wrapper loop generating parameters q q q ∪ q – basic training parameters q outputs hypothesis hq – calculating weight wq – stopping check evaluating conditions exit loop • output d ∆cidhq wqq∈qcid wrapper strategy repeats basic training diﬀerent parameters come single decision operation ∆ generates new decision d based hypotheses weights ob tained iterations basic training stopping check checks speciﬁed condition true procedure exits wrapper loop otherwise loop continues generating new parameters summarizing get deﬁnition abduction learner abduction learner abduction learner minimizes explanation criterion model data analysis logic using basic training without wrapper strategy main conjecture popular learning algorithm procedures knn naive bayes svm hierarchical clustering example formulated unique terms apparently solves type problem conjecture can explained abduction learners main conjecture ml every learning algorithm abduction learner popular learners support main conjecture show learner abduction learner need show given training set minimizes explanation criterion using basic training without wrapper loop linkagebased clustering algorithm also popularly known hierarchical clustering clustering can seen modeling nondeterministic dependence cluster number independent variable data vector feedback can said ﬁnding association data point cluster number explain training data set clusters general concept linkagebased clustering introduced way algorithms proceed sequence rounds start trivial clustering data point singlepoint cluster repeatedly algorithms merge “closest” clusters previous clustering h input clustering algorithm betweenpoint distance d many ways extending d measure distance domain subsets clusters book proposes three ways evaluate clusterdistance distances members minimum distance average distance maximum distance last option clearly contradicts declared goal “merge ‘closest’ clusters” will consider round training set sequence ﬁrst order formulas lde s ψci yi ≈ m ci ∈ n cluster number ith observation yi observed data point observation denote ci y ∃αα ∈ s y yα xα set data points cluster suppose k clusters purpose round identify two “closest” clusters notation hij j will indicate hypothesis clusters ci cj “belong together” best candidates merging hk hij j ≤ k denotes class hypotheses hypothetical instances hypothesis hij make set h ij ψci y ≍ y ∈ cj loss criterion lhij s ρci cj clusterdistance clusters ci cj now learning procedure can described rule round hierarchical custering • fitting generation hypotheses h ∈ hk evaluation loss criterion lh s • optimal selection select hypothesis h′ ∈ hk minimal value loss criterion lh s theorem linkage based clustering clusterdistances average minimum maximum abduction learner proof show learner abduction learner need show lhij s explanation criterion every suggested clusterdistance described procedure basic training explanation criterion single badness rule alignment relation πα α sα ≍sα ≈xα xα relation tpw badness rule deviation deﬁned rule δα α tρxαα ρyα α ρyα α means deviation function t case also coincides deviation function tpw aggregation operation identiﬁed type clustering minimum average maximum minimum maximum aggregations theorem averaging recursive aggregation proven theorem therefore criterion lhij s badness rule explanation criterion learning procedure basic training without focusing proves round linkagebased clustering works abduction learner learner agree main conjecture aggregation op erations mentioned book average minimum maximum also aggregation operation knn classiﬁcation method intended observations binary feedback y expectation small neighborhood underlying binary dependence mostly constant binary dependencies condition equivalent “explainable” discussed less diﬀerence f feedback training data points close x better explanation goal ﬁnd best explanation two hypotheses small neighborhood deﬁned parameter k denote dk distance point x kth closest x data point observations s dk x parameters learner alignment relation πα β xα x sα ≍ ρxα β ≤ dk sβ ≈ deviation function tr r r deviation δα β ρyα β aggregation operation averaging lf s k x la see theorem sequence deviations arbitrary order thus criterion lf s k x explanation criterion procedure learner can described steps knn • focusingdeﬁning parameter dk • fitting generating hypotheses f x f x evaluating error rate lf s k x • optimal selection selection hypothesis minimal error rate procedure basic training thus knn abduction learner two knn learners adaptive choice k knn may work presumed underlying dependence mostly small variations feedback close data points section binary underlying dependence mean mostly constant small neighborhoods optimally radius dk shall small enough majority points neigborhood class large enough random outliers ﬁnite sample sξ k play much role discuss two approaches select k optimally every new data point ﬁrst described second new algorithm learners ﬁnd prevalent class y focus sample calculate frequency pky error rate rky − pky knn authors propose given data point x start small k gradually increase calculating bias tky pky − prevalent class every k procedure stops bias reaches certain threshold threshold ever reached don’t output answer search smallest neighborhood prevalence one class threshold picked beforehand threshold propose use ∆n k δ c cs logn log k δ n size training sample δ c userselected parameters picked data analysis thus instead one parameter k proposed modiﬁcation require user pick parameters unclear meaning learner uses criterion knn procedure can described like ada knn • wrapper loop generating parameter k k – basic training parameter k ∗ focusing transformation s → sx k ∗ fitting generation two constant hypotheses evaluation loss lf s k x ∗ optimal selection outputs constant hypothesis f ′ min imal loss – stopping check cidlh′ sx k ∆s k δ ccid k n k n output f ′ decision otherwise refuse output • output decision thus learner performs basic training original knn wrapper parameter selection corroborating main conjecture learner developed within statistical learning paradigm training set expected arbitrary large n increases threshold ∆n k δ c therefore selected value k size focus training set will go inﬁnity n thus law large numbers solution will converge asymptotically expectation class given neighborhood time ratio k n expected decrease thus size kneighborhood will tend distribution continuous x leaner will likely ﬁnd solution n tends inﬁnity issue n going inﬁnity anywhere ﬁxed n learner favors smaller k evaluation prevalent class subject random ﬂuctuations caused small sample alleviate issue propose alternative approach uses hoeﬀding inequality see example select k hoeﬀding inequality can written p p − e t ≤ exp−k t p observed frequency event e expected frequency probability event t arbitrary threshold k sample size suppose p evaluates observed frequency class rate class among neighbors e probability class neighborhood given point p observations class prevail pick hypothesis two otherwise pick hypothesis let t − p p − e t expected prevalent class diﬀerent observed prevalent class case selected wrong hypothesis case right side inequality gives us upper limit probability picked prevalent class wrong selection k use weight calculated right part w y s k · exp− k p − obviously larger k frequency p lower weight weight will serve well selection parameters k need ﬁnd neighborhood p far uncertainty yet size neighborhood small description learner’s procedure given data point x hoeffding knn • generation parameter k k – basic training ∗ focusing select focus training set qkx k observations data points closest x ∗ fitting evaluate error rate rkc hypotheses c ∈ qkx ∗ optimal selection select hypothesis c′k minimal error rate rkc′k – calculating weight w x s k – stopping check k n − • output k′ arg min w x s k output c′k′ thus learner abduction learner well decision trees learner features expected “ordinal” every feature ﬁnite number ordered values operations feature values feedback observations binary assumption underlying dependence mostly constant small neighborhood size neighborhood set priory algo rithm ﬁnds maximal homogeneous neighborhoods best explanation observations learner starts whole domain splits two subdomains value feature procedure repeated every subdomains subdomain called ”leaf” reached decision selected subdomain navigation tree subdomains continues stopping criterion reached algorithm precise rule generating parameters next subdomain based previous trajectory obtained results two criteria leaf number observations subdomain threshold n percentage observations prevalent class subdomain thresh old q homogeneous subdomain procedure selects one two binary hypotheses minimal error rate easy see badness rule selection hypothesis tpw can denote ltwh s explanation criterion learner procedure may described basic training wrapper strategy decision tree • generating parameters g next subdomain – basic training ∗ focusing select subdomain gg subset training set sg parameters g ∗ fitting generate hypotheses h ∈ sg evaluate loss criteria lpwh sg ∗ optimal selection select hypothesis dg minimal value lpwh sg h ∈ – calculating weight gg leaf w g otherwise w g – stopping check end tree • output g w g output dg decision gg points outside leaf decision deﬁned therefore decision tree abduction learner well naive bayes underlying dependence n independent variables binary feedback learner works deals nominal data relationship data points equiv alence procedure deﬁnes decision function one data point time given data point z hz zni procedure selects n subsets training set subset sj includes observations jth variable equal zj subset sj learner evaluates error rate ec sj hypothesis c ∈ hypothesis calculates criterion ∆c s yj − ec sj learner selects hypothesis maximal value criterion explanation criterion learner n badness rules well functional ψ aggregate values n badness criteria badness rule hypothesis c πiα β sα ≍sβ ≈xiα xiβ tr r r δiα β ρyα β aggregation operation badness rule recursive aggregation averaging la theorem combining operation combine results badness rules ψa −yi − ai easy see operation ψ monotone order insensitive see thus loss criterion naive bayes explanation criterion now procedure learner given data point z hz zni may de scribed rule naive bayes • fitting generating hypotheses c ∈ calculating loss criterion ∆c s • optimal selection select hypothesis minimal ∆c s fitting step procedure calculates n badness values corresponding n badness rules hypotheses aggregates values abduction criterion given hypothesis interesting calculation individual badness way requires focusing however always process involved calculation criterion reﬂected scheme learner proves naive bayes naive bayes abduction learner product aggregation badness values chosen naive bayes sensitive low frequencies class value − ec sj close product will aﬀected much sum frequencies example feature value almost never happens given class c hypothesis c will chance selected regardless feature values z justiﬁes choice product aggregation loss criterion naive bayes traditionally interpreted evaluation posterior probabilities “naive” assumption features independent several issues narrative first works one learner nb learner based naive idea bayes rule learners need diﬀerent foundations another issue creates impression learner needs improvement sophisticated enough hope demonstrated interpretation learner “naive” “bayesian” misses point procedure driven speciﬁc data type explained performing agc inference majority learners logistic regression learner assumes features continuous feedback observations binary feedback decision continuous required rounding data point decision deﬁned domain χ procedure generating hypotheses speciﬁed class functions associated logistic regression f cid exp−hw xicid functions values interval learner minimizes criterion ∆f s logcidys − fcidxscidcid mxs∈s explanation criterion contains one badness rule see contradiction degree πα β πpw δα β logρyα β function isotone ρyα α depend ρxα α proper aggregation operation averaging la obvious badness rules deﬁnes criterion ∆f s logistic regression supports main conjecture well issue learner erm take account similarity feedback close identical data points therefore tendency overﬁtting linear svm classiﬁcation previous learners belong machine learning “folklore” authors known least famous svm one ﬁrst learners associated known author invented v vapnik earliest english publications subject appeared early nineties let us start linear svm binary classiﬁcation observations two class labels − data points x ∈ rn s βi m class hypotheses f consists linear functions f x n variables f ∈ f f x xt β β denote wf β bf β problem formulated minimization criterion linear svm st lf s ξ α kwf k m ξβ β ∈ s yβ · f xβ ≥ − ξβ ξβ ≥ m xβ∈s criterion looks intimidating may simpliﬁed though want switch narrower class functions shall contain decisions observations β ∈ s satisfying condition yβ·f xβ considered correctly classiﬁed function f denote s⊕f correctly classiﬁed observations function f s⊖f s s⊕f rest observations let us consider functions f ∈ f s⊕f ∅ min s⊕f f xβ denote class function f ′s class f ′s empty f f ≡ s⊕f ∅ s⊕−f s indeed q min s⊕f f xβ function f ′ q f satisﬁes condition f ′xβ min s⊕f last consideration implies f decision problem problem decision f ′ class f ′s set correctly recognized observations s⊕f ′ s⊕f therefore can restrict search decision class f ′s theorem linear svm classiﬁcation problem minimizes loss criterion lsvmf s αkwf k m xβ∈s⊖f yβ − f xβ f ∈ f ′s proof conditions can rewritten ∀β β ∈ s ξβ ≥ − yβ · f xβ ξβ ≥ ξβ ≥ maxcid − yβ · f xβ cid values ξβ β ∈ s depend minimum sum achieved every variable ξβ equals lowest possible value let us ﬁnd lowest values ξβ depending β ∈ s⊕f β ∈ s⊖f β ∈ s⊕f deﬁnition f ′s f xs ≥ yβ · f xβ f xs case lowest possible value ξβ ξβ ≥ maxcid − yβ · f xβ cid β ∈ s⊖f yβ · f xβ −f xβ case lowest possible value ξβ f xβ ξβ ≥ maxcid − yβ · f xβ cid f xβ min ξ mxs ξβ xβ∈s⊖f f xβ still need prove β ∈ s⊖f f xβ yβ − f xβ let us take β ∈ s⊖f yβ f xβ f xβ −f xβ f xβ − f xβ yβ − f xβ yβ − f xβ f xβ f xβ f xβ f xβ −yβ f xβ yβ − f xβ now need show prove lsvmf s explanation criterion distance feedback observations function value deﬁned rule ρyα α yα − yα yα · yα ≥ otherwise badness rule coincides pointwise rule tpw formula zf kwf k criterion regularization component wf gradient hypothesis f kwf k square norm functional ψ combines values two criteria ψx x αx x conﬁrms linear svm classiﬁcation minimizes explanation criterion linear support vector regression learner minimizes criterion m lsvrf s vǫcidyβi − f xβicid λkwf k xi vǫr r ǫ r − ǫ otherwise s β βm class hypothesis class linear functions second component loss criterion regularization svm distance feedback observation value hypothesis deﬁned function v α ≍ ϕx y α ≈ ϕx y ρyα α v y − y agc criterion scheme coincides scheme linear svm classiﬁcation linear support vector regression supports main conjecture well support vector regression kernels learner deﬁned suppose set basis functions h hix k looking hypotheses f x k xi wihix b loss criterion used m lf s vcidyβi − f xβicid λkwf k xi v r r ǫ r − ǫ otherwise transformation x → hhx hkxi ndimensional space rn kdimensional space hx may called focusing problem reduced solving linear svm regression transformed space thus svr kernel supports main conjecture well ridge regression learner ﬁnds solution class linear hyperplanes f f f wx b criterion lrrf s αkwf k f xβ − yβ mxβ∈s ﬁrst component loss criterion regularization component svm svr unlike svr case distance y ky −yk y −y svr count small errors ridge regression counts errors small errors low inﬂuence square norm explanation criterion ridge regression diﬀerent criterion svr diﬀerence learners interpretation data explanation logic distances y diﬀerent thus ridge regression corroborates main conjecture neural network nn let us consider single hidden layer nn two class classiﬁcation described first learner transforms n− dimensional metric space inputs r kdimensional space z using nonlinear transformation zix δgix k δr delta function gi linear functions denote zx vector coordinates hzx zkxi class c ∈ learner builds linear voting function fczx denote g g gk f f f x ∈ rn class selected cx g f arg maxc fczx learner uses loss criterion lnng f s xβ∈s yβ − cxβ g f obvious loss criterion pointwise badness rule tpw learner optimizes simultaneously parameters functions g f selection parameters functions learner uses gradient descent called “back propagation” case learner uses additional stopping criterion procedure focusing stage calculates loss given set parameters evaluates gradients parameter updates parameters based gradients stopping criterion achieved algorithm outputs decision lowest loss criterion procedure two types steps ﬁtting includes • generation cx g f hypothesis based previous value loss criterion gradients • evaluation loss criterion lnng f s current hypothesis optimal selection selection hypothesis lowest loss criterion thus nn also abduction learner k means clustering learner diﬀerent hierarchical clustering combine clusters rather observation chooses proper cluster assumed distance domain data points euclidean description learner given current set means clusters m m mk observation assigned cluster closest mean rounds assignment observations repeated clusters change learning happens search cluster given observed data point denote cx assignment cluster data point x given set observed data points sx x xm k clusters cluster centers m sizes l lk procedure assigns new class observed data point minimize sum pairwise distances within cluster k w c s kξ − ζk k xk xcξk xcζk lk xcξk xk kξ − xkk ξ ζ ∈ s xk mean kth cluster use form prove learner agrees main conjecture denote x data point x ∈ sx need assign cluster step case hierarchical clustering consider underlying dependence ϕ function cluster index k observed data point x k hypotheses hx h hk hypothesis hi single hypothetical case ψx ≍ assume current run learner clusters already assigned observed data point besides x run starts training set observations sx β ∈ s xβ x let us deﬁne explanation criterion equivalent one badness rule alignment relation πα α xα xα says evaluate contradiction degree pair formulas cluster regardless observation hypothetical instance alignment relation symmetrical therefore pair formulas α α satisﬁes condition pair α α satisﬁes condition well eﬀect every pair counted twice deviation function tr r r deviation calculated formula δα α tρxα α ρyα α ρyα α recursive aggregation use averaging la learner generates hypotheses hx evaluates explanation criterion selects hypothesis lowest value criterion basic training thus learner corroborates main conjecture well conclusions peirce considered learning experimental data “data explanation” pragmatism introduced informally describes logic behind process abduction infer ence ml automated learning data practical applications peirce right ml understood automated abduction inference indeed demonstrated case criterion data explanation formalized within data analysis logic de ﬁned statistical concepts used formalization process crite rion minimization described two standard procedures basic training wrapper combination explanation criterion procedure minimization makes abduction learner conjecture every ml learners abduction learner conjecture corroborated popular learners classiﬁcation clustering regression thus ml may called automated abduction indeed approach important advantages commonly accepted statistical learning paradigm allows understand real life learners variety diﬀerences common features conditions learning form ﬁnite data sets common structure classiﬁcation regression clustering problems al gorithms regularization aspect explanation criterion future development approach may include • logical understanding testing necessary part learning process • logical approach toward data analysis beyond learning • automated algorithm selection • understanding learning tasks survival analysis ranking optimal choice • development new algorithms based proposed ideas one example novel algorithm adaptive knn introduced references balsubramani s dasgupta y freund adaptive nearest neighbor rule classiﬁcation rd conference neural information processing systems neurips vancouver canada boser m guyon vn vapnik training algorithm optimal margin classiﬁers colt ’ proceedings ﬁfth annual workshop computational learning theory c cortes v vapnik support vector networks machine learning – thomas eiter georg gottlob complexity logicbased abduction tenth symposium theoretical aspects computing stacs lncs pages – springer t hastie r tibshirani j friedman elements statistical learning springer ulrike von luxburg bernhard scholkopf statistical learning theory models concepts results dov m gabbay stephan hartmann john woods editors handbook history logic volume inductive logic pages – elsevier bv sayan mukherjee partha niyogi tomaso poggio ryan rifkin statistical learning stability suﬃcient generalization necessary suﬃcient consistency advances computational mathematics – cs peirce essential peirce volume indiana university press cs pierce abduction induction philosophical writings pierce pages – routledge kegan paul ltd m sapir optimal choice new machine learning problem solution ternational journal computational science information technology – marina sapir bipartite ranking algorithm classiﬁcation survival analysis arxiv marina sapir papaya orchard comedy one act httpswwwacademiaedu papaya orchard comedy one act shai shalevshwartz shai bendavid understanding machine learning cam bridge university press ny v n vapnik nature statistical learning theory springer verlag ",36.15,"0.982"
10,"Deep Author Name Disambiguation using DBLP Data"," r m l d s c v v x r deep author name disambiguation using dblp data zeyd boukherscid nagaraj bahubali asundicid institute web science technologies west university koblenzlandau universitätsstraße koblenz germany department data science artiﬁcial intelligence fraunhofer institute applied information technology fit schloss birlinghoven sankt augustin germany corresponding authors emails zeydboukhersﬁtfraunhoferde contributing authors nagarajbahubaliunikoblenzde abstract academic world number scientists grows every year number authors sharing names consequently challenging assign newly published papers respective authors therefore author name ambiguity ana considered crit ical open problem digital libraries paper proposes author name disambiguation approach links author names realworld entities leveraging coauthors domain research end use data collected dblp reposi tory contains million bibliographic records authored around million coauthors approach ﬁrst groups authors share last names ﬁrst name initials author within group identiﬁed capturing relation hisher coauthors area research represented titles validated publications corresponding author end train neural network model learns representa tions coauthors titles validated eﬀectiveness approach conducting extensive experiments large dataset keywords author name disambiguation entity linkage bibliographic data neural networks classiﬁcation dblp dan using dblp introduction important task digital libraries aims properly link publication respective coauthors authorlevel metrics can accurately calculated authors’ publications can easily found however task extremely challenging due high number authors sharing names paper author name denotes sequence characters referring one several authors whereas author refers unique person authoring least one publication identiﬁed hisher author name rather support identiﬁers orcid researchgate id semantic scholar author id although relying identiﬁers almost eliminates chance mis linking publication appropriate author bibliographic sources include identiﬁers authors keen use identiﬁers procedure policy include identiﬁers cited therefore bibliographic data eg ref erences authors commonly referred names considering high number authors sharing names ie homonymy diﬃcult link names bibliographic sources realworld authors especially source reference available provide indicators author’s identity problem critical names substituted initials save space erroneous due wrong manual editing disciplines like social sciences humanities suﬀer problem publishers small midsized ensure continuous integrity bibliographic data table demonstrates real examples reference strings covering mentioned problems homonomy issue shows example two diﬀerent papers citing name j m lee refers two diﬀerent authors case possible disambiguate two authors without leveraging features synonymy issue shows example author jang myung leecid cited diﬀerently two diﬀerent papers jang myung lee j lee synonymy serious issue author name disambiguation requires awareness name variates given author moreover name variates might shared authors increases homonymy since problems known decades several studies – conducted using diﬀerent machine learning approaches problem estimated million people share common names dblp database exact matches ‘chen li’ reverse matches partial matches xu zhihao et al teleoperating formation carlike rovers time delays proceedings th chinese control conference ieee shi pu jianning hua yiwen zhao posturebased virtual force feedback control tele operated manipulator system th world congress intelligent control automation ieee xu zhihao lei ma klaus schilling passive bilateral teleoperation carlike mobile robot th mediterranean conference control automation ieee lu chinghsi hongyang hsu lei wang new contrast enhancement technique adaptively increasing value histogram ieee international workshop imaging systems techniques ieee dan using dblp table illustrative examples author name ambiguity incorrect author names issue type source synonyms homonyms see see see see citations t jin j lee h hashimoto “internetbased obstacle avoidance mobile robot using forcereﬂection” pro ceedings ieeersj international conference intelligent robots systems sendai japan pp – october tasseok jin jangmyung lee hideki hashimoto “internetbased obstacle avoidance mobile robot using forcereﬂection” ieeersj international conference intelligent robots systems pp ts jin jm lee h hashimoto internetbased obsta cle avoidance mobile robot using forcereﬂection proceedings ieeersj international conference intelligent robots systems pages – sendai japan october hj kim jm lee ja lee sg oh wy kim contrast enhancement using adaptively modiﬁed histogram equaliza tion lecture notes computer science vol pp dec often tackled using supervised approaches support vector machine svm bayesian classiﬁcation neural networks nn approaches rely matching publications authors veriﬁed either manually automatically unsupervised approaches – also used assess similarity pair papers unsupervised approaches also used estimate number coauthors sharing name decide whether new records can assigned existing author new one due continuous increase publica tions cites tens publications diﬃculty label streaming data semisupervised approaches also employed recent approaches leveraged outstanding eﬃciency deep learn ing diﬀerent domains exploit relationship among publications using network embedding approaches use available publication data authors titles venues year publication aﬃliation approaches currently integrated diﬀerent bibliographic sys tems however require exhausting manual correction reach acceptable accuracy addition approaches rely meta data extracted papers supposed correct complete real scenarios source paper always easy ﬁnd reference available paper builds upon earlier work aim employ bib liographic data consisting publication records link author’s name unseen records appropriate realworld authors ie dblp identiﬁers leveraging coauthors area research embedded publica tion title source note goal paper disambiguate author names newly published papers recorded bibliographic dan using dblp database therefore records considered unseen discarded bibliographic data used testing approach assumption author likely publish articles speciﬁc ﬁelds research therefore employ articles’ titles sources ie journal booktitle etc bring authors close ﬁelds research represented titles sources publications also assume authors already published together likely continue collaborating publishing papers goal mentioned proposed model whois trained bibliographic collection obtained dblp sample consists target author pair coauthors title source coauthors input vector representation obtained applying charvec returns character level embedding words title source bert model used capture semantic representations sequence words model trained tested challenging dataset thousands authors share atomic name variate main contributions paper • proposed novel approach author name disambiguation using semantic symbolic representations titles sources coauthors • provided statistical overview problem author name ambiguity • conducted experiments challenging datasets simulating critical scenario • obtained results comparison baseline approaches demonstrate eﬀectiveness model disambiguating author names rest paper organized follows section brieﬂy presents related work section describes proposed framework section presents dataset implementation details obtained results proposed model finally section concludes paper gives insights future work related work section discuss recent approaches softly categorized three categories namely unsupervised supervised graphbased unsupervisedbased studies treat problem author name ambiguity unsuper vised task using algorithms like dbscan agglomerative clustering liu et al kim et al rely similarity pair records name disambiguate author names pubmed dataset zhang et al used recurrent neural network rnn estimate number unique authors aminer dataset process followed manual annotation direction ferreira et al pro posed twophase approach applied dblp dataset ﬁrst one obtaining clusters authorship records disambiguation applied cluster wu et al fused features aﬃliation content papers using shannon’s entropy obtain matrix representing pairwise dan using dblp correlations papers return used hierarchical agglomera tive clustering hac disambiguate author names arnetminer dataset similar features employed approaches supervisedbased supervised approaches also widely used mainly applying block gathers authors sharing names together han et al present two supervised learning approaches disambiguate authors cited references given reference ﬁrst approach uses naive bayes model ﬁnd author class maximal posterior probability author cited reference second approach uses svm classify references dblp appropriate authors sun et al employ heuristic features like percentage citations gathered top name variations author disambiguate common author names neu ral networks also used verify two references close enough authored target author hourrane et al propose corpusbased approach uses word embeddings compute similar ity cited references entity resolution system called deeper proposed uses combination bidirectional recurrent neural networks brnn along long short term memory lstm hid den units generate distributed representation tuple capture similarities zhang et al proposed online bayesian approach identify authors ambiguous names case study bib liographic data temporal stream format used disambiguation resolved partitioning papers homogeneous groups graphbased bibliographic data can viewed graph citations several approaches leveraged property overcome problem author name ambigua tion hoﬀart et al present method collective disambiguation author names harnesses context knowl edge base uses new form coherence graph method generates weighted graph candidate entities mentions compute dense subgraph approximates best entitymention mapping xianpei et al aim improve traditional entity linking method proposing graphbased collective entity linking approach can model exploit global interdependence ie mutual dependence entities problem author name ambiguity overcome using relational information considering three graphs personperson persondocument documentdocument task becomes graph clustering task goal cluster contains documents authored unique realworld author ambiguous name xu et al build network papers multiple relationships networkembedding method proposed learn paper representations gap positive negative edges dan using dblp optimized hdbscan used cluster paper representations disjoint sets set contains papers unique realworld author m approach paper designed using bibliographic dataset d din consisting n bibliographic records record di refers unique publication di ti sicidaiu δiucidωi u ti si denote title source record respectively aiu δiu refer uth author corresponding name respectively among ωi coauthors di let ∆ δmm set m unique author names d shared set l unique authors coauthoring records d l m note author name δm might refer one authors author al might referred one two author names ∆ consider two variates author might occur diﬀerently diﬀerent papers example author “rachid deriche”cid assigned two elements ∆ namely “rachid deriche” “r deriche” given reference record d∗ ∈ d goal approach link u ∈ ∆ occurs d∗ appropriate author author name δ∗ leveraging t∗ s∗ δ∗ figure illustrates overview proposed ura approach first approach computes correspondence frequency δ∗ ura returns number authors corresponding δ∗ ura indicates indicates δ∗ corresponds one author al ∈ case directly δ∗ assign δ∗ al processing necessary note case might also refer new author anew ∈ name δ∗ existing author al ∈ however approach handle situation please refer section lists limitation proposed approach ura indi cates δ∗ can refer one author end approach extracts atomic name variate author name δ∗ example author name δ∗ u “l wang” let δ∗ correspond δµ denotes µth atomic name variate among k possible name variates afterwards corresponding neural net work model θµ ∈ θ θkk picked distinguish authors aµ alµlµ goal paper handle case δ∗ u “lei wang” atomic name variate δ∗ corresponds new author anew ∈ δ∗ share name variate δµ uω∗ u δ∗ u u u k u l u u u u lµ model architecture neural network nn model θµ takes input attributes d∗ namely ﬁrst name target author δ∗ﬁrstname full names two coauthors δ∗ δ∗ title t∗ source s∗ figure illustrates architecture θµ output layer length lk corresponding number unique authors aµ atomic name variate δk shown figure θµ takes two inputs xµ xµ u p j dan using dblp fig illustration task linking name mentioned reference string corresponding dblp author entity xµ charvecδ∗ﬁrstname u cid cidcharvecδ∗ j cid p charvecδ∗ bertt∗ berts∗ xµ charvecw returns vector representation length generated using charvec provides symbolic representation w bertw returns vector representation token w wrt context sen tence representation length generated using bert goal separating two inputs overcome sparseness content embedding force model emphasise target author representation hidden layers possess relu activation function whereas put softmax classiﬁer since model classify thousands classes represented samples units last hidden layers dropped training avoid overﬁtting fur thermore number publications signiﬁcantly diﬀers one author another therefore class ie author weighted according number samples ie publications model trained adam opti mizer sparse categorical crossentropy loss function empirical analysis showed best performance achieved architecture parameters obtained grid search author name representation names authors hold speciﬁc semantic nature simply speciﬁc sequence characters referring one persons therefore need model can encode words based order dis tribution characters author names similar name spellings encoded closely assuming possible manual editing errors cited papers filteringf wang y song z zhang w chen“structure analysis anddecoupling research novel flexibletactile sensor array” j sensorsvol may art fei wangfang wangmodelkf wangmodelkmodelfeng wangfei wangfeiluwangbibliographic record reference dan using dblp fig architecture model charsvec powerful nnbased language model preferred text consists abbreviations typos etc captures non vocabulary words places words similar spelling closer vector space model uses ﬁxed list characters word vectorization onehot encoding represents character source title embedding source eg journal names book titles reference can provide hint area research given reference addition title meaningful sentence embeds speciﬁc topic reference therefore used two features capture research area author contrary author’s name goal capture context sequences words forming title source therefore employed pretrained bert model obtain sentence embeddings title source training samples cidδµ δiµp δiµj tiµ siµcidωiµ model training given training set dµ ⊂ d corresponds subset bibliographic records authored authors atomic name variate δµ diµ ∈ dµ generates ωiµ δiµj ran dom coauthor diµ might also author name δiµp andor δµ note also consider one combination δiµp δµ order train model common name variate ﬁrst name substituted initial sample generate another version name variates cidδµ δiµp δiµj tiµ siµcid consequently bibliographic record fed model × ωiµ since third coauthor δiµp randomly assigned training sample among ωiµ randomly reassign y epochs addition lower training complexity shown conducted experiments slightly better result training model epoch samples possible coauthor pairs p j p times coauthors diµ output layer ∈ ℝinput layer ∈ ℝhidden layer ∈ ℝhidden layer ∈ ℝconcatenationdense relusoftmaxauthor author author mcharvec ∈ ℝaverage coauthor embeddingauthor first name embeddingsminput layer ∈ ℝhidden layer ∈ ℝdense relubert∈ ℝaverage title source embeddingscharvec ∈ ℝbatchnormalizationdropout dense relubatchnormalizationdropout dense relubatchnormalizationdropout dan using dblp model tuning training epoch whois model ﬁnetunes parameters predict appropriate target author performance model considerably inﬂuenced number epochs set train speciﬁcally low epoch count may lead underﬁtting whereas high epoch count may lead overﬁtting avoid enabled early stopping allows model specify arbitrarily large number epochs keras supports early stopping training via callback called earlystopping callback conﬁgured help monitor argu ment allows setting validation loss setup model receives trigger halt training observes improvement validation loss often ﬁrst indication improvement validation loss right epoch stop training model may start improving passing epochs overcome adding delay trigger terms consecutive epochs count can wait observe improvement delay added setting patience argument appropriate value patience whois set model halts validation loss stops getting better past consecutive epochs model checkpoint although whois stops training process achieves minimum val idation loss model obtained end training may give best accuracy validation data account keras provides addi tional callback called modelcheckpoint callback conﬁgured help another monitor argument set monitor monitor validation accuracy setup model updates weights observes better validation accuracy compared earlier epochs eventually end persisting best state model respect best validation accuracy prediction u goal dis ucidω∗ given new bibliographic record d∗ t∗ s∗cidδ∗ ambiguate author name δ∗ target shared one author targetra end y samples sy δ∗ generated pos sible pairs coauthor names p j cidδ∗ p δ∗ y cω∗ ie combination ω∗ authors taken time δ∗ can full abbreviated author name y samples fed corresponding model θµ target author atarget target name δ∗ target predicted follows j t∗ s∗cidω∗ω∗ pj y target δ∗ u atarget argmax ···lµ θµs ⊕ θµs ⊕ ··· ⊕ θµsy dan using dblp θµsy returns probability vector length lµ element lµ denotes probability author name δ∗ target author alµ experiments section presents experimental results proposed approach dblp dataset dataset following datasets widely used evaluate author name disambiguation approaches results datasets reﬂect results real scenario streaming data • orcid largest accurate dataset publication assigned author authorship claim another rigorous authorship conﬁrmation however accuracy comes cost number assignments investigation shows registered authors assigned publication important number authors even registered authors keen claim publications due several reasons • kdd cup large dataset consists m papers authored k authors author metadata available including aﬃliation • manually labelled eg penn qian aminer kisti datasets supposed accurate since manu ally labelled however process expensive timeconsuming therefore can cover small portion authors share names work collected dataset dblp bibliographic repos itory dblp version july contains million bibliographic records conference papers articles thesis etc various ﬁelds research stated maintainers dblp accuracy data guaranteed however lot eﬀort put manually disambiguat ing homonym cases reported users consequently aware possible homonym cases resolved yet repository collected records publications published journals proceedings record collection represents metadata information publica tion one authors title journal year publication attributes availability attributes diﬀers one reference httpsﬁgsharecomarticlesorcidpublicdatafile httpswwwkagglecomckddcupauthorpaperidentiﬁcationchallenge httpclgilesistpsuedudatanamesetauthordisambtarzip httpsgithubcomyayadblpnamedisambiguationdataset httparnetminerorglabdatasetsdisambiguationrichauthordisambiguationdatazip httpwwwlbddccufmgbrlbdcollectionsdisambiguationdblptargzatdownload ﬁle httpsdblpunitrierdexml july httpsdblporgfaqhowaccurateisthedataindblphtml dan using dblp table statistical details used dblp collection records unique authors unique author names unique atomic name variates another also authors dblp share name suﬃx number diﬀerentiate instance authors name ‘bing li’ given suﬃxes ‘bing li ’ ‘bing li ’ statistical details used dblp collection shown table figure indicates majority target authors subcollections subcollection includes records authors name distinct full names however considerable number share full names leading signiﬁcant challenge particularly multiple authors eg subcollections share full name unequal number publications cases becomes challenging diﬀerentiate authors dominant author name fig log frequency authors sharing full name top ﬁve sub collections figure illustrates log frequency bibliographic records full name top ﬁve subcollections used paper illustrated subcollections target authors around half records authored records less unique names although simple dis tinguish authors full names occur extremely challenging recognize among authors sharing atomic name variate due unbalance records authors figure shows frequency authors sharing names atomic name variates can seen problem critical authors cited atomic name variate ﬁve atomic name variates shared around k authors makes problem disambiguation critical targets authors might share dan using dblp fig log frequency records full name target author top ﬁve subcollections atomic name variate also coauthors instance observed publications authored pair coauthors atomic name vari ates y wang y zhang however refer diﬀerent y wang y zhang pairs realworld authors fig frequency authors sharing atomic name variate blue full name red since approach gathers authors name variates models required disambiguate author names collection therefore present paper experimental results models cor responding highest number authors sharing name variates table presents statistical details ﬁve subcollections demon strates challenges inherent author name disambiguation realworld scenarios ra instance publications two coauthors exact names makes disambiguation diﬃcult authors share names also coauthors papers ensure credible evaluation result reproducibility real scenar ios split records subcollection training set ∼ ≥ ≥ ≥ ≥ ≥ ≥ ≥ ≥ ≥ authors atomic name variate blueand name redfrequency dan using dblp table statistical details top subcollections authors sharing atomic name variates anv corresponding atomic name variate uta number unique target authors rcd number bibliographic records uca number unique coauthor full names uan number unique target author full names ra number records two coauthors record names atomic name variates ra number records three coauthors record names atomic name variates ra ra necessary authors name atomic name variate target author probably uta rcd uca uan ra ra ‘y wang’ ‘y zhang’ ‘y chen’ ‘y li’ ‘y liu’ validation set ∼ testing set ∼ terms recordstarget author speciﬁcally target author randomly split correspond ing records target author author enough publications split prioritize training set validation ﬁnally test set consequently number samples necessarily split according number coauthors diﬀers among publications moreover highly likely records unique target author completely diﬀerent among three sets consequently diﬃcult model recognize appropriate author hisher coauthors research area however believe realistic perfect simulation real scenario account possible name variates input sample full names duplicated duplicate sample full names coauthors atomic name variates note applied training validation test sets goal let model capture name variates author hisher coauthors none sets variates mixed single sample assume case less likely occur real world experiments conducted machine following speciﬁcations • processor amd ryzen threadripper x core • ram gb • graphics card nvidia titan v gv algorithm implemented python using tensorflow library results existing approaches use diﬀerent datasets design evalu ate models lead diﬀerent assumptions challenge disparity unfortunately codes reproduce results approaches available easily accessed therefore possible fairly compare whois baseline approaches future work code used datasets publicly available httpsdoiorgzenodo dan using dblp table detailed results whois subcollections corresponding top ﬁve authors sharing atomic name variates dblp repository results presented terms micro average precision miap macro average precision maap micro average recall miar macro average recall maar micro average fscore miaf macro average fscore maaf anv denotes atomic name variates used target authors coauthors maapanv maapall maaranv maarall maafanv maafall miafanv miafall ‘y wang’ ‘y zhang’ ‘y chen’ ‘y li’ ‘y liu’ table presents result whois subcollections presented table label table denotes samples predicted twice one full names target author coauthors another time atomic name variates whereas label anv denotes samples atomic names predicted obtained results show important number publications properly assigned appropriate authors due properties subcollections discussed statistically presented table example two authors common name authoring single publication one author common atomic name variate authoring single publication number authors full name uncertainty accuracy dataset etc although comparison diﬃcult completely fair com pare whois stateoftheart approaches whose results reported results obtained collection citeseerx con tains records authors name atomic name variate ‘y chen’ collection consists complete documents authored distinct authors picked name comparison two reasons number authors sharing name among top ﬁve shown table methods cited achieve good result applied whois collection randomly splitting records training validation testing results shown table note collection consider way records distinct authors see table use reference attributes ie coauthors title source results presented table show whois outperforms methods resolving disambiguation author name ‘y chen’ citeseerx dataset relatively small dataset really reﬂect per formance presented approaches real scenarios disparity results shown table table demonstrates existing bench mark datasets manually prepared sake accuracy however leads covering small portion records whose authors share similar httpclgilesistpsuedudata table comparison whois baseline methods citeseerx dataset terms macro f score reported anv denotes atomic name variates used target authors coauthors dan using dblp macro allanv micro allanv na na na na na na na na na na whois ndag gf deepwalk line nodevec pte gl rand authorlist authorlistnnmf names disparity conﬁrms author name disambiguation still open problem digital libraries far solved obtained results whois illustrate importance relying research area target authors coauthors disambiguate names however trigger need encourage authors use dif ferent author identiﬁers orcid publications automatic approaches able provide perfect result mainly due complexity problem limitations obstacles whois whois demonstrated satisfactory result outperformed stateoftheart approaches challenging dataset however approach faces several obstacles will addressed future works following list limitations proposed approach • new authors properly handled approach conﬁ dence threshold set decide whether input corresponds new author existing one knowledge none existing supervised approaches capable handle situation • commonly authors found new collaborations lead new co authorship approach beneﬁt occurrence new cocombinations coauthors never seen training planned solution will train independent model embed author’s discipline using hisher known publications assume authors working area research will put close even publish paper together model able capture potential coauthorship pair authors terms area research • authors continuously extend research expertise coauthoring new publications relatively diﬀerent disciplines means titles journals discriminative anymore consequently hard approach disambiguate authors holding common names dan using dblp planned solution plan determine author’s areas research mining domainspeciﬁc keywords entire paper instead title assuming author uses similar keywordswriting styles even diﬀerent research areas gradual changes can captured model • lot models trained disambiguate authors dblp repository • commonly number samples small compared number classes ie authors sharing atomic name variate leads overﬁtting model planned solution plan follow reverse strategy disambiguation instead employing coauthors target author will employ coauthors aiming ﬁnd target author among aim also learn coauthor representation employing coauthors help resolve disambiguation target author’s name • mentioned earlier stated maintainers platform accuracy dblp repository guaranteed conclusion presented paper comprehensive overview problem overcome problem proposed novel framework consists lot supervised models models dedicated distinguishing among authors share atomic name variate ie ﬁrst name initial last name leveraging coauthors titles sources known publications experiments challenging realscenario datasets shown promising satisfactory results also demonstrated limitations challenges inherent process overcome limitations challenges plan future work exploit citation graphs author names can linked real world entities employing coauthors coauthors assume using reverse process identity target author can found among coauthors hisher coauthors plan also learn research area coauthors order overcome issue new coauthorships references müller mc semantic author name disambiguation word embed dings international conference theory practice digital libraries pp – springer kim k seﬁd weinberg ba giles cl web service author name disambiguation scholarly databases ieee international conference web services icws pp – ieee httpsdblporgfaqhowaccurateisthedataindblphtml dan using dblp foxcroft j d’alessandro antonie l namevec personal names embeddings canadian conference artiﬁcial intelligence pp – springer hussain asghar s survey author name disambiguation tech niques knowledge eng review ferreira aa gonçalves ma laender ah brief survey auto matic methods author name disambiguation acm sigmod record – qian y zheng q sakai t ye j liu j dynamic author name disambiguation growing digital libraries information retrieval journal – zhang b dundar m al hasan m bayesian nonexhaustive classiﬁ cation case study online name disambiguation using temporal record streams proceedings th acm international conference information knowledge management pp – khabsa m treeratpituk p giles cl large scale author name dis ambiguation digital libraries ieee international conference big data big data pp – ieee khabsa m treeratpituk p giles cl online person name disam biguation constraints proceedings th acmieeecs joint conference digital libraries pp – han h giles l zha h li c tsioutsiouliklis k two supervised learning approaches name disambiguation author citations pro ceedings joint acmieee conference digital libraries pp – ieee tran hn huynh t t author name disambiguation using deep neural network asian conference intelligent information database systems pp – springer liu w islamaj doğan r kim s comeau dc kim w yeganova l lu z wilbur wj author name disambiguation p ub m ed journal association information science technology – kim k seﬁd giles cl learning cnf blocking largescale author name disambiguation proceedings first workshop scholarly document processing pp – dan using dblp fan x wang j pu x zhou l lv b graphbased name dis ambiguation journal data information quality jdiq – zhang y zhang f yao p tang j name disambiguation aminer clustering maintenance human loop proceedings th acm sigkdd international conference knowledge discovery data mining pp – louppe g alnatsheh ht susik m maguire ej ethnicity sensitive author disambiguation using semisupervised learning inter national conference knowledge engineering semantic web pp – springer zhao j wang p huang k semisupervised approach author disambiguation kdd cup proceedings kdd cup workshop pp – zhang b al hasan m name disambiguation anonymized graphs using network embedding proceedings acm conference information knowledge management pp – xu j shen s li d fu y networkembedding based method author disambiguation proceedings th acm international conference information knowledge management pp – boukhers z asundi nb whois deep author name disambiguation using bibliographic data linking theory practice digital libraries th international conference theory practice digital libraries tpdl padua italy september – proceedings pp – springer wu h li b pei y j unsupervised author disambiguation using dempster–shafer theory scientometrics – ferreira aa veloso gonçalves ma laender ah eﬀective self training author name disambiguation scholarly digital libraries proceedings th annual joint conference digital libraries pp – yang kh wu yh author name disambiguation citations ieeewicacm international conferences web intelligence intelligent agent technology vol pp – ieee arif t ali r asger m author name disambiguation using vector dan using dblp space model hybrid similarity measures seventh inter national conference contemporary computing ic pp – ieee qian y hu y cui j zheng q nie z combining machine learn ing human judgment author disambiguation proceedings th acm international conference information knowledge management pp – sun x kaur j possamai l menczer f detecting ambiguous author names crowdsourced scholarly data ieee third international conference privacy security risk trust ieee third international conference social computing pp – ieee hourrane o mifrah s bouhriz n rachdi m et al using deep learning word embeddings citations similarity academic papers international conference big data cloud applications pp – springer ebraheem m thirumuruganathan s joty s ouzzani m tang n distributed representations tuples entity resolution proceedings vldb endowment – hoﬀart j yosef ma bordino fürstenau h pinkal m spaniol m taneva b thater s weikum g robust disambiguation named entities text proceedings conference empirical methods natural language processing pp – han x sun l zhao j collective entity linking web text graph based method proceedings th international acm sigir conference research development information retrieval pp – cao k rei m joint model word embedding word morphology arxiv preprint arxiv devlin j chang mw lee k toutanova k bert pretraining deep bidirectional transformers language understanding arxiv preprint arxiv kuang d ding c park h symmetric nonnegative matrix factoriza tion graph clustering proceedings siam international conference data mining pp – siam perozzi b alrfou r skiena s deepwalk online learning social representations proceedings th acm sigkdd interna tional conference knowledge discovery data mining pp – dan using dblp tang j qu m wang m zhang m yan j mei q line largescale information network embedding proceedings th international conference world wide web pp – grover leskovec j nodevec scalable feature learning networks proceedings nd acm sigkdd international conference knowledge discovery data mining pp – tang j qu m mei q pte predictive text embedding large scale heterogeneous text networks proceedings th acm sigkdd international conference knowledge discovery data mining pp – hermansson l kerola t johansson f jethava v dubhashi d entity disambiguation anonymized graphs using graph kernels pro ceedings nd acm international conference information knowledge management pp – baglioni m manghi p mannocci bardi can make better use orcid ﬁve observed misapplications data science journal ",43.55,"1"
11,"Which Invariance Should We Transfer? A Causal Minimax Learning Approach"," invariance transfer causal minimax learning approach mingzhou liu xiangyu zheng xinwei sun ",1.4,"0"
12,"Active Fairness Auditing","active fairness auditing tom yan chicheng zhang n u j g l s c v v x r abstract fast spreading adoption machine learn ing ml companies across industries poses signiﬁcant regulatory challenges one chal lenge scalability can regulatory bodies efﬁciently audit ml models ensuring fair paper initiate study querybased auditing algorithms can es timate demographic parity ml models queryefﬁcient manner propose opti mal deterministic algorithm well practi cal randomized oracleefﬁcient algorithm comparable guarantees furthermore make inroads understanding optimal query com plexity randomized active fairness estimation algorithms ﬁrst exploration active fairness estimation aims put ai governance ﬁrmer theoretical foundations introduction growing usage artiﬁcial intelligence ai across industries governance efforts increasingly ramping key challenge regulatory efforts problem scalability even wellresourced countries like norway pioneering efforts ai governance regulators able monitor engage “small fraction companies” mccarthy growing issue calls better understanding efﬁcient approaches auditing machine learning ml models now formalize problem formulation regulatory institution inter ested auditing model h∗ x → − held company eg lending company ﬁnance sector x feature space eg information sup plied users assume regulatory institution knowledge hypothesis class h h∗ carnegie mellon university tom zhang university arizona yan chichengzcsarizonaedu tyanandrewcmuedu contribution equal correspondence chicheng proceedings th international conference machine learning baltimore maryland usa pmlr copy right authors comes eg family linear classiﬁers like estimate µh∗ function µ measures model property interest end institution allowed send blackbox queries model h∗ ie send company query example x receive h∗x regulatory institution’s goal efﬁciently estimate µh∗ within error  measure algorithm’s efﬁciency terms query complexity computational complexity auditing algorithm low query computational com plexity naturally helps address scalability challenge greater efﬁciency means audit may processed faster audits may processed time property interest properties µ assess still heavily debated regulators initiate study auditing algorithms focusing fairness mainstay regulatory focuses particular µ will consider will demographic parity dp given distribution dx x × feature x sensitive attribute xa jointly drawn µdx h prxxa∼dx hx xa − prxxa∼dx hx xa brevity clear context abbreviate prdx µdx pr µ respectively dp measures degree disparate treatment model h two subpopulations x xa x xa assume nonnegligible p minprxa prxa ω achiev ing small demographic parity may thought stronger version us equal employment opportunity commission’s “fourﬁfths rule” focus query complexity will abstract away dif ﬁculty evaluating µ assuming dx known thus h may evaluate µh arbitrary precision instance may achieved availability arbitrarily large number unlabeled samples randomly drawn x xa x xa main chal lenge know h∗ want query h∗ insofar able accurately estimate µh∗ guarantees audit paper investigate fairness focus work algorithm may adapted µ function x h∗ “selection rate race sex ethnic group must least fourﬁfths eighty percent rate group highest rate” active fairness auditing s v h s cidhcid ∈ v hcids hscid au algorithms can provide two types guarantees ﬁrst natural direct estimation accuracy estimate returned algorithm within  µh∗ second manipulationproof mp estima tion audits can consequential companies may subject hefty penalties caught viola tions surprisingly effortful attempts past avoid caught violations eg hotten “gaming” audit formulate notion manipulationproofness light one way audit may gamed now describe note auditor knows model used company consistent queried labels audit algorithm may estimated µh∗ accurately audittime nothing stops company changing model postaudit h∗ different model hnew ∈ h eg improve proﬁt long hnew still consistent queries seen audit also look understand given posthoc possibility manipulation can devise algorithm nonetheless ensures algorithm’s estimate within  µhnew indeed robust set audit queries serve cer tiﬁcate matter model company changes audit µestimation remain accurate given set classiﬁers v classiﬁer h unlabeled dataset s deﬁne version space mitchell induced diting algorithm manipulationproof h∗ outputs set queries s estimate ˆµ guarantees maxh∈hh∗s baseline iid sampling one natural baseline comes mind direct estimation iid sampling sam ple o examples iid distribution x xa ∈ query h∗ examples take average obtain estimate prh∗x xa finally take difference two estimates ﬁnal dp estimate hoeffding’s inequality high probability estimate accurate estimation procedure makes o queries however iid sampling necessarily mp see example let n points group xa n  shattered h dx uniform points suppose points group xa labeled prdx hx xa ∀h ∈ h µestimation reduces estimating proportion positives group xa iid sampling will randomly choose n data points see will produce accurate estimate µh∗ however see n points since n points shattered h queried points determined see company can increase decrease dp switching different model cidcidµh − ˆµcidcid ≤  cidcidcid ≤  see cidcidcidµˆh − µh∗ obtain direct mp estimation seems promis ing examine algorithms make use noniid sampling moreover mp observe auditing algorithm leverage knowledge hypothesis class well iid sampling agnostic baseline active learning algorithm achieves direct mp estimation accuracy pac active learn ing hanneke pac stands probably ap proximately correct valiant pac active learning algorithms guarantee high probability ˆh resultant version space pˆhx cid h∗x ≤ p o lemma c appendix c formal proof mention setting learning favored iid sampling learning homogeneous linear classiﬁers certain wellbehaved unlabeled data distributions requires od log  queries eg dasgupta b balcan long thus far efﬁcient o lowdimensional learning settings high auditing precision requirements still goal estimate µ values induced version space unclear need go far learn model paper investigate whether may possible design adap tive approaches efﬁciently directly andor mp estimate µh∗ using knowledge h best knowledge ﬁrst theoretically investigate active approaches direct mp estimation µh∗ ﬁrst exploration active fairness estimation seeks provide complete picture theory auditing machine learning models hope theoretical results can pave way subsequent development practical algorithms contributions main contributions two fronts mp estimation direct estimation µh∗ • newly introduced notion manipulation proofness identify statistically optimal com putationally intractable deterministic algorithm gain insights query complexity com parisons two baselines iid sampling pac active learning • light computational intractability opti mal deterministic algorithm design randomized algorithm enjoys oracle efﬁciency eg dasgupta et al efﬁcient implementation given access mistakebounded online learning oracle constrained empirical risk minimization ora cle hypothesis class h furthermore query performance matches optimal deterministic algorithm polylogh factors active fairness auditing • finally direct estimation front obtain bounds informationtheoretic query complexity establish mp estimation may expensive direct estimation thus highlighting need develop separate algorithms two guarantees establish usefulness randomization algorithm design develop optimal randomized algorithm linear classiﬁcation gaussian sub populations finally shed insight general settings develop distributionfree lower bounds direc tion estimation general vc classes lower bound charts query complexity optimal randomized auditing algorithms must attain additional notations now introduce additional useful notation used throughout paper let m denote m unlabeled dataset s two classiﬁers h hcid say hs hcids x ∈ s hx hcidx given set classiﬁers v labeled dataset t deﬁne v t cidh ∈ v ∀x y ∈ t hx ycid furthermore cidcidx ycidcid x v denote v y notational simplicity given set classiﬁers v fairness measure µ denote diamµv maxhhcid∈v µh − µhcid µdiameter v given set labeled examples t denote prt · probability uniform distribution t given classiﬁer h denote errh t prt hx cid y empirical error h t throughout paper will consider active fairness au diting membership query model similar mem bership querybased active learning angluin specif ically deterministic active auditing algorithm label budget n formally deﬁned collection n com putable functions f f fn g every ∈ n fi x × yi− → x label querying function used step ﬁrst − labeled exam takes input ples cidx y xi− yi−cid obtained far chooses ith example xi label query g x × yn → r estimator func tion takes input n labeled examples cidx y xn yn cid obtained throughout teraction process outputs ˆµ estimate µh∗ interacts target classiﬁer h let resultant queried unlabeled dataset sah cidx xncid ﬁnal µ estimate ˆµah similar deterministic algorithms randomized active auditing algorithm label budget n b bits random seed formally deﬁned collection n x × computable functions f fn g fi yi− × b → x g x × yn × b → r note function now take input bbit random seed result interacts ﬁxed h∗ output ˆµ now random variable note also deﬁnition randomized active auditing algorithm uses ﬁxed seed b may viewed deterministic active auditing algorithm ab will comparing algorithms’ query com plexities disagreementbased active learn cid ing algorithms cohn et al hanneke given classiﬁer h r deﬁne bh r hcid ∈ h prdx disagree cidx ∈ x ∃h hcid ∈ v hx cid hcidxcid hypothesis ment ball centered h radius r given set classiﬁers v deﬁne disagreement region disv class h unlabeled data distribution dx im portant quantity characterizes query complexity disagreementbased active learning algorithm dis agreement coefﬁcient θr deﬁned cidhcidx cid hxcid ≤ r cid θr sup h∈hrcid≥r prdx x ∈ disbh rcid rcid √ related work work related following two lines work concerned estimating property model without learn model sampleefﬁcient optimal loss estimation dicker kong valiant propose ustatisticsbased estimators estimate optimal population mean square error ddimensional linear regression sample com plexity o d much lower od sample com plexity learning optimal linear regressor kong valiant also extend results wellspeciﬁed logisitic regression setting goal estimate optimal zeroone loss work similar focusing ques tion efﬁcient µh∗ estimation without learn h∗ work differs focusing fairness property instead optimal mse zeroone loss moreover results apply arbitrary h just linear models interactive veriﬁcation goldwasser et al studies veriﬁcation whether model h’s loss nearoptimal respect hypothesis class h looks understand veriﬁcation cheaper learning prove veriﬁcation cheaper learning speciﬁc hypothesis classes just expensive hypothesis classes work differs focusing different property model fairness algorithm also utilizes tools active learning machine teaching review active fairness auditing active learning teaching task learning h∗ approximately membership queries well studied eg angluin heged˝us dasgupta hanneke computationally ef ﬁcient algorithm active fairness auditing built upon connection active learning machine teach ing goldman kearns ﬁrst noted heged˝us hanneke achieve computational efﬁ ciency work builds recent work blackbox teach ing dasgupta et al implicitly gives efﬁ cient procedure computing approximateminimum specifying set adapt dasgupta et al ’s algorithm give similar procedure approximating minimum specifying set speciﬁes µ value interest space please see discussion additional related work appendix manipulationproof algorithms optimal deterministic algorithm begin study mp estimation µh∗ identi fying optimal deterministic algorithm based dynamic programming inspired minimax analysis exact ac tive learning membership queries hanneke recursively deﬁne following value function version space v ⊆ h cid costv diamµv ≤  minx maxy costv x y otherwise note costv similar minimax query com plexity exact active learning hanneke except induction base case different – base case diamµv ≤  implies subject h∗ ∈ v identiﬁed µh∗ error  contrast exact active learning hanneke ’s induction base case v identify h∗ v value function cost also gametheoretic inter pretation imagine learner plays multiround game adversary learner makes sequential queries examples obtain labels adversary reveals labels examples subject constraint la beled examples shown agree classiﬁer h version space v encodes state game set classiﬁers agrees labeled examples shown far game interaction learner adversary ends classiﬁers v µ values close learner like minimize total cost number rounds costv can viewed minimaxoptimal future cost subject game’s current state represented version space v based notion cost design algorithm al algorithm minimax optimal deterministic auditing require finite hypothesis class h target error  fairness measure µ ensure ˆµ estimate µh∗ let v ← h diamµv  x obtain label query x ∈ argminx maxy cost v y h∗x v ← v h∗x cidmaxh∈v µh minh∈v µhcid return gorithm worstcase label complexity costh speciﬁcally maintains version space v ⊂ h initialized h line every iteration µdiameter v diamµv maxhhcid∈v µh − µhcid  since µh∗ ∈ minh∈v µh maxh∈v µh returning midpoint gives us accurate estimate µh∗ line otherwise algorithm makes query choosing x minimizes worstcase future value functions line receiving h∗x updates version space v line construction interaction learner labeler lasts costv rounds gives following theorem theorem algorithm interacts h∗ ∈ h costh labels outputs ˆµ thatcidcidˆµ − µh∗cidcid ≤  queries minimax nature cost also show among deterministic algorithms algorithm optimal worstcase query complexity theorem deterministic algorithm query budget n ≤ costh − exists h∗ ∈ h ˆµ output querying h∗ satisﬁes cidcidˆµ − µh∗cidcid  proofs theorems deferred ap pendix d comparison baselines gain better understanding costh relate label complexity algorithm two baselines iid sampling active learning establish comparison prove can derandomize existing iid samplingbased active learningbased auditing al gorithms small overhead label complexity comparison follows algorithm optimal determin istic algorithm ﬁrst result label complexity algorithm within factor olnh label complexity iid sampling proposition costh ≤ ocid  lnhcid active fairness auditing second result label complexity algorithm always worse distributiondependent label complexity cal cohn et al hanneke wellknown pac active learning algorithm believe similar bounds costh compared generic ac tive learning algorithms can shown splitting algorithm dasgupta b conﬁdencebased al gorithm zhang chaudhuri suitable derandomization procedures proposition costh ≤ ocidθ · lnh · ln θ disagreement coefﬁcient h respect dx recall section deﬁnition cid  cid ln h ulation proportion p fairness measure µ algorithm derandomized phased cal auditing require hypothesis class h target error  minority pop ensure ˆµ estimate µh∗ let n cidlog let v ← h n n let mn n find lexicographically smallest sn ∈ x mn prsn x ∈ disvn ≤ prdx x ∈ disvn p ln mn ∀h hcid ∈ h prsn hx cid hcidx ⇒ prdx hx cid hcidx ≤ lnh mn query h∗ labels examples tn sn ∩ disvn vn ← vnh∗ tn return µh arbitrary h ∈ vn finally upper bound algorithm ’s label complexity tn mn · prdx x ∈ disvn ln mn ncid ncid ≤ ncid cid n n ≤o proof sketch present algorithm deran domized version phased cal algorithm hsu chapter prove proposition using theorem sufﬁces show algorithm deterministic la bel complexity bound ocidθ · lnh · ln cid present main idea defer precise version proof appendix d ﬁrst show every n optimization problem line always feasible see observe draw sn sample size mn drawn iid dx  bernstein’s inequality probability − prsn x ∈ disvn ≤ prdx x ∈ disvn ln mn n bernstein’s inequality union bound h hcid ∈ h probability − mn · θ θ · lnh · ln  ln mn lnh mn cid ∀h hcid ∈ h prshx cid hcidx ⇒ prdx hx cid hcidx ≤ lnh mn union bound nonzero probability two condition hold simultaneously showing feasibility optimization problem argue n vn ⊆ bh∗ ln h h ∈ vn h h∗ vn therefore agree sn tn hand h h∗ agree tn deﬁnition vn consequence prsnhx cid h∗x implies prdx hx cid h∗x ≤ ln h consequence h ∈ vn prhx cid h∗x ≤ p combined lemma c implies thatcidcidµh − µh∗cidcid ≤  mn mn computational hardness implementing algorithm although algorithm optimal label complexity guarantees among deterministic algorithms show following proposition standard complexity theoretic assumptions np cid⊆ timenolog log n even approximating costh computationally intractable proposition algorithm can approximate costh within lnh factor polyhx  time np ⊆ timenolog log n remark constant can improved con stant arbitrarily smaller main insight behind proposition connection costh optimal depth decision trees see theorem d using hard ness computing approximatelyoptimaldepth decision active fairness auditing tree laber nogueira taking account structure µ establish intractability approximat ing costh owing intractability algorithm next sec tion turn design computationally efﬁcient algorithm whose label complexity nears algorithm ie costh efﬁcient randomized algorithm competitive guarantees present efﬁcient algorithm section also serves ﬁrst upper bound statistical com plexity computationally tractable algorithms algo rithm algorithm inspired exact active learning literature heged˝us hanneke based connection machine teaching goldman kearns active learning algorithm takes input two oracles mistakebounded online learning oracle o constrained empirical risk minimization erm oracle cerm deﬁned deﬁnition onlinelearning oracle o said mistake bound m hypothesis class h classiﬁer h∗ ∈ h sequence exam ples x x every round t ∈ n given historical examples xs h∗xst− s outputs classiﬁer ˆht cid∞ t iˆhtxt cid h∗xt ≤ m wellknown implementations mistake bounded online learning oracle include halving algorithm efﬁcient samplingbased approximations bertsimas vempala well perceptron winnow algorithm little stone bendavid et al instance o halving algorithm mistake bound m log h may achieved next deﬁne constrained erm oracle previously used number works oracleefﬁcient ac tive learning dasgupta et al hanneke huang et al deﬁnition constrained erm oracle hypoth esis class h cerm one takes input la beled datasets b outputs classiﬁer ˆh ∈ argminciderrh h ∈ h errh b cid highlevel idea algorithm follows ev ery iteration uses mistakebounded online learn ing oracle generate classiﬁer ˆh line aims construct dataset t small size af ter querying h∗ labels examples t one following two happens ˆh disagrees h∗ example t classiﬁers version diamµv ≤  case found counterex space v cidh ∈ h ∀x ∈ t hx h∗xcid universe u iscidh hcid ∈ h µh − µhcid cid ample ˆh can fed online learning oracle learn new model can happen m times case done queried labeled examples ensure auditing estimate accurate satis ﬁes manipulationproofness dataset t property called µ specifying set ˆh formally deﬁned deﬁntion d appendix d another view µspecifying set set t h hcid µh − µhcid  exists x ∈ t hx cid ˆhx hcidx cid ˆhx requirements t can viewed set cover problem set system c cx x ∈ x h hcid cx hx cid ˆhx hcidx cid ˆhx motivates us design efﬁcient set cover algorithms context key challenge applying standard ofﬂine set cover algorithms greedy set cover algorithm construct approximate minimum µ specifying set afford enumerate elements universe u u can exponential size face challenge draw inspiration line set cover literature alon et al dasgupta et al design oracleefﬁcient algorithm computes olog h log xapproximate minimum µ  specifying sets avoids enumeration u key idea simulate online set cover process build cover set t iteratively starting t ∅ line every inner iteration ﬁrst try ﬁnd pair h h u yet covered current t shall see next step line can implemented efﬁciently given constrained erm oracle cerm pair h h can found use online set cover algorithm implicit dasgupta et al ﬁnd new example covers pair add t move onto next iteration lines otherwise t successfully covered elements u case break inner loop line see line ﬁnds uncovered pair u note can also written cid cid µh − µhcid ht hcidt ˆht h h argmax hhcid∈h thus µh− µh  returned pair h h corresponds pair universe u covered t otherwise optimality h h t covers elements u furthermore note optimization problems can implemented access cerm show program reasoning program clear context slightly abuse notations say “x covers h hcid” h hcid ∈ cx active fairness auditing cidx x ∈ x xa cid∪cidx− x ∈ x xa cid analogous observe maximizing µh h ∈ h subject constraint ht ˆht equivalent mini mizing weighted empirical error h ∈ h dataset subject h zero error x ˆhx x ∈ t now ready present label complexity guarantee algorithm algorithm oracleefﬁcient active fairness auditing require hypothesis class h online learning oracle o mistake bound m constrained erm oracle cerm target error  fairness measure µ ensure ˆµ estimate µh∗ initialize s ← ∅ true ˆh ← os let t ← ∅ computing approximate minimum µ  specifying set ˆh initialize weights wx x threshold τx ∼ exponentiallnhmδ random initialization thresholds true use cerm solve separate programs h∈h µh st ht ˆht h ← ﬁnd max h ← ﬁnd min h∈h µh st ht ˆht t µ specifying set ˆh µh − µh ≤  break else add examples t cover h h using online set cover algorithm implicit dasgupta et al determine ∆h h x ∈ x hx cid ˆhx hx cid ˆhx whilecid update t ←cidx ∈ x wx ≥ τx double weights wx x ∆h h x∈∆hh wx ≤ cid query h∗ t s ← s ∪ t ˆht h∗t return µh µh theorem online learning oracle o makes total m mistakes probability − δ algorithm outputs ˆµ thatcidcidˆµ − µh∗cidcid ≤  number label queries bounded cid costhm log hm δ log x o cid cid proof theorem deferred appendix d nutshell combines following observations first algorithm m outer iterations us ing mistake bound guarantee oracle o second ˆh inner iteration minimum µ  specifying set size costh based nontrivial connection optimal deterministic query complexity µ extended teaching dimension see deﬁnition d present lemma d third o approximation guar antee online set cover algorithm implicit das gupta et al outer iteration makes o remark seen algorithm implicitly per forms online set cover connection inherits ˜ωlog h log x inapproximability factor online set cover alon et al proposition log xcid log xcid costh log label queries hm hm log δ cid δ finally appendix f empirically explore perfor mance algorithm active learning compare iid sampling expected experiments conﬁrm ﬁxed budget algorithm ef fective inducing version space small µdiameter can thus provide strongest manipulationproofness guarantee statistical limits estimation section turn direct estimation second two main guarantees wish auditing algorithm particular focus statistical limits direct estimation goal design auditing algorithm can output ˆµ thatcidcidˆµ − µh∗cidcid ≤  small number queries separation estimation without manipulationproofness start natural contrast guarantee  manipulationproofness estimation accuracy deed two guarantees one may just apply auditing algorithms developed achieve mp direct estimation well look answer question whether achieving mp strictly harder answer question afﬁrmative speciﬁcally following simple example sug gests mp estimation can sometimes require much higher label complexity direct estimation active fairness auditing x example let  n x xa ∼ uniform x xa ∼ uniform n let h cidh x → − h −cid n cid first  iid sampling baseline makes o queries ensures estimates µh∗ error  probability ≥ however manipulationproof estimation least ωn labels needed ensure queried dataset s sat isﬁes diamµhh∗ s ≤  indeed let h∗ ≡ − unlabeled dataset s size ≤ n deﬁnition h always exist h hcid ∈ hh∗ s x ∈ n s hx − hcidx re ≥ sult µh implies diamµhh∗ s ≥ µhcid ns  n − − n randomized algorithms direct estimation separation result suggests different algo rithms may needed interested efﬁcient direct estimation motivated previous exploration ﬁrst question answer whether randomization key ingredient algorithm design can ran domized auditing algorithm lower query complexity optimal deterministic algorithm using example answer question afﬁrmative example setting example recall iid sampling randomized algorithm estimates µh∗ probability ≥ error  query complexity o contrast consider deterministic algorithm label budget n ≤ n consider interaction history classiﬁer h ≡ − can summarized sequence unlabeled examples s cidx xncid now consider alternative classiﬁer h hx − s ∪ hx n s inductive argument can shown interaction history h also s implies underlying hypotheses h∗ h h∗ h must output estimate ˆµ see lemma b appendix b formal proof however µh− µh ≥ implying least one two hypotheses must cidcidˆµ − µh∗cidcid ≥  summary setting randomized algorithm query complexity o much smaller ωn optimal query complexity deterministic algorithms case study nonhomogeneous linear classiﬁers gaussian populations subsection identify practicallymotivated set ting able comprehensively characterize bound shows possibly randomized algorithm must  cid well matching lower minimax randomized active fairness auditing query com plexity logarithmic factors speciﬁcally present positive result form algorithm query complexity ˜ocidmind query complexity ωcidmind  cid cidhabx signcida xcid b ∈ rd b ∈ rcid class cid recall iid sampling label complexity ocid example let d ≥ x rd x xa ∼ nm σ whereas x xa let hypothesis class hlin ∼ nm σ nonhomogenenous linear classiﬁers hand membership querybased active learning algorithm algorithm appendix e can approximately estimate µh∗ scaling d binary searches using active label queries approach incurs total label complexity ˜od choosing better two algorithms gives active fairness auditing strategy label complexity ˜ocidmind  cid  cid cida∗cid cid b∗ cid cidd present main idea algorithm full analysis deferred appendix e core com ponent algorithm labelefﬁciently es timates γh∗ px∼nidh∗x black box label queries h∗x signcida∗ xcid b∗ algo rithm based following insights first observe γh∗ φ φsr φ stan dard normal cdf s signb∗ r − mi − b∗ one hand s can easily obtained ∗ querying h∗ cid line hand estimating r can reduced estimating mi however mi’s can unbounded makes estimation challeng ing get around challenge prove following lemma shows sufﬁces accurately estimate mi’s unreasonably large ie mi’s ∈ s deﬁned lemma let α  d ln suppose r ≤ α s ⊂ d d ln  cid β  m ∈ smi ≥ β ∈ s ˆmi − mi ≤  − r cidcidcidcid ≤  cidcidcidcidcid cid mi ≥ α r ≥ cid ∈s ˆm − algorithm carefully utilizes lemma estimate r first tests whether h∗αei h∗−αei yes  γh∗ close depending value s line otherwise ln active fairness auditing must case r ≤ α case go coordinate ﬁrst testing whethermi ≤ β line skip coordinate add s otherwise include s estimate mi precision  using binary search line guarantees lemma havesˆr − sr ≤  lipschitzness φ implies thatcidcidˆγ − γh∗cidcid ≤  total query complexity √ π algorithm d d d log β  ˜od cid algorithm estimatepositive label efﬁcient esti mation algorithm γh∗ nonhomogenoeus linear classiﬁers require query access h∗ ∈ hlin target error  ensure ˆγ thatcidcidˆγ − γh∗cidcid ≤  query h∗ oncidραei ρ ∈ ± ∈ dcid let α d ln s ← query h∗ cid ∈ d h∗αei h∗−αei return s s −  β d ln   cid d ln  otherwise r ≤ α s ← ∅ d query h∗ βei −βei h∗βei cid h∗−βei s ← s ∪ use binary search obtain ˆmi estimate mi − b∗ ∗ ˆmi ← binarysearchi β  algorithm precision  ˆr estimate r − ˆr ←cid cid ∈s ˆm return φsˆr ai algorithm binarysearch require β h∗βei cid h∗−βei precision  ensure m accurate estimate mi − b u ← β l ← −β u − l ≥  m ← ul query h∗ mei h∗mei h∗lei return m l ← m u ← m else lower bound formulate hypothesis testing problem hypotheses h h µh∗ values approximately separated used show active learning algorithm label query budget  cid effectively distinguish h ≤ ωcidmind h construction requires delicate analysis kl divergence observation distributions two hypotheses refer readers theorem e details general distributionfree lower bounds d finally subsection move beyond gaussian population setting derive general query complexity lower bounds randomized estimation algorithms audit general hypothesis classes ﬁnite vc dimension d result suggests d cid  equivalently  cid √ exists hard data distribution target classiﬁer h active fairness auditing query complexity lower bound ω  iid sampling nearoptimal theorem lower bound randomized auditing fix  ∈ hypothesis class h vc dimension d ≥ possibly randomized algorithm label budget n ≤ omind  exists distribution dx x h∗ ∈ h ’s output ˆµ interacting h∗ satisﬁes cid pcidcidcidˆµ − µh∗cidcid  proof theorem can found appendix e lower bound construction follows similar set ting example except now choose h∗ randomized fashion conclusion paper initiate study theory query efﬁcient algorithms auditing model properties interest focus auditing demographic parity one canon ical fairness notions investigate natural auditing guarantee estimation accuracy introduce new guar antee based possibility postaudit manipulation manipulationproofness identify optimal determin istic algorithm matching randomized algorithm de velop upper lower bounds mark performance optimal auditing algorithm must meet ﬁrst exploration active fairness estimation seeks provide complete picture theory auditing natural next direction explore guarantees fairness notions equalized odds indeed one construct queryefﬁcient algorithms µ function h∗x y another natural question motivated connection disagreementbased active learning design active fairness auditing algorithms based notion disagreement respect µ acknowledgments thank stefanos poulis sharing implementation blackbox teaching algorithm dasgupta et al special thanks steve han neke sanjoy dasgupta helpful discussions also thank anonymous icml reviewers feedback active fairness auditing references agarwal beygelzimer dud´ık m langford j wallach h reductions approach fair classiﬁcation international conference machine learning pp – pmlr alon n awerbuch b azar y buchbinder n naor j online set cover problem siam journal computing – angluin d queries concept learning machine learn ing – balcan mf long p active passive learning linear separators logconcave distributions conference learning theory pp – pmlr balcan mf blais e blum yang l active property testing ieee rd annual symposium foundations computer science pp – ieee bendavid s p´al d shalevshwartz s agnostic online learning colt volume pp bertsimas d vempala s solving convex programs random walks journal acm jacm – blais e ferreira pinto jr r harms n vc dimension distributionfree samplebased testing proceed ings rd annual acm sigact symposium theory computing pp – blanc g gupta n lange j tan ly estimating decision tree learnability polylogarithmic sample complexity advances neural information processing systems blum hu l active tolerant testing conference learning theory pp – pmlr cohn d atlas l ladner r improving general ization active learning machine learning – dasgupta s analysis greedy active learning strategy advances neural information processing systems – dasgupta s coarse sample complexity bounds active learning nips volume pp – b dasgupta s hsu d poulis s zhu x teaching blackbox learner international conference ma chine learning pp – pmlr dicker l h variance estimation highdimensional linear models biometrika – feige u threshold ln n approximating set cover journal acm jacm – goldman s kearns m j complexity teaching journal computer system sciences – goldreich o goldwasser s ron d property testing connection learning approximation journal acm jacm – goldwasser s rothblum g n shafer j yehuday interactive proofs verifying machine learning th innovations theoretical computer science con ference itcs schloss dagstuhlleibnizzentrum f¨ur informatik hanneke s cost complexity interactive learning unpublished manuscript hanneke s teaching dimension complexity active learning international conference compu tational learning theory pp – springer hanneke s rates convergence active learning annals statistics pp – hanneke s theory active learning foundations trends machine learning heged˝us t generalized teaching dimensions query proceedings eighth complexity learning annual conference computational learning theory pp – hotten r volkswagen scandal explained bbc news url httpswwwbbccomnews business hsu d j algorithms active learning phd thesis uc san diego huang tk agarwal hsu d j langford j schapire r e efﬁcient parsimonious agnostic active learning advances neural information processing systems dasgupta s hsu d j monteleoni c general agnostic active learning algorithm advances neural information processing systems – kong w valiant g estimating learnability sublinear data regime advances neural information processing systems – active fairness auditing laber e s nogueira l t hardness minimum height decision tree problem discrete applied mathematics – larson j mattu s kirchner l angwin j analyzed compas recidivism algorithm propublica – littlestone n learning quickly irrelevant attributes abound new linearthreshold algorithm machine learning – regulate ai emerging tech brew mccarthy d sandbox try playing httpswwwmorningbrewcom url emergingtechstories regulateaijustplaysandbox mitchell t m generalization search artiﬁcial intelli gence – rastegarpanah b gummadi k crovella m audit ing blackbox prediction models data minimization compliance advances neural information processing systems ron d property testing learning theory perspective now publishers inc sabato s sarwate d srebro n auditing active learning outcomedependent query costs pro ceedings th international conference neural information processing systemsvolume pp – valiant l g theory learnable communications acm – xu z yu t sra s towards efﬁcient evaluation risk via herding negative dependence theory applications machine learning zhang c chaudhuri k beyond disagreementbased agnostic active learning advances neural information processing systems active fairness auditing additional related works property testing notion auditing leverages knowledge h similar theme topic property testing goldreich et al ron balcan et al blum hu blanc et al blais et al tests whether h∗ h h∗ far away classiﬁer h given query access h∗ works provide algorithms testing query complexity lower order sample complexity learning respect h speciﬁc hypothesis classes monomials dnfs decision trees linear classiﬁers etc problem can reduced property testing testing whether h∗ incidh ∈ h µh ∈  cid ∈cid cid cidcid however best knowledge result known context property testing feature minimization audits rastegarpanah et al study another notion auditing focusing assessing whether model trained inline gdpr’s data minimization principle speciﬁcally work evaluates necessity individual feature used ml model done imputing feature constant values checking extent variation predictions one commonality work indeed across auditing works concern minimizing number queries needed conduct audit herding sampleefﬁcient mean estimation additionally estimation dp may viewed estimating difference two means viewed light herding xu et al offers way use noniid sampling efﬁciently estimate means however key difference needed herding h∗ whose output − may wellapproximated cidw φxcid mapping φ known apriori comparison sabato et al lastly sabato et al also uses term “auditing” context active learning outcomedependent query costs although term “auditing” shared problem settings completely different sabato et al focuses active learning model h∗ opposed just estimating µh∗ b general lemma deterministic query learning section present general lemma inspired hanneke used proofs establishing lower bounds deterministic active fairness auditing algorithms lemma b deterministic active auditing algorithm label budget n interacts labeling oracle uses classiﬁer h generates following interaction history cidx hx x hx xn hxn cid exists classiﬁer h hx hx x ∈ x xn interacting h generates interaction history outputs auditing estimate formally sah sah ˆµah ˆµah proof recall section deterministic active auditing algorithm can viewed sequence n functions f f fn g fin label query function used iteration g ﬁnal estimator function show induction steps n interaction histories h h agree ﬁrst elements base case step interaction histories empty agree trivially inductive case suppose statement holds step ie interacting h h generates set labeled examples si cidx y xi yicid step now step applies query function fi queries example xi fisi assumption lemma hxi hxi implies st labeled example obtained interacts h xi hxi identical xi hxi st example interacts h combined inductive hypotheses two histories agree ﬁrst examples shown interacting h h generates set labeled examples si cidx y xi yi xi yicid step completes induction active fairness auditing interaction histories h h identical unlabeled data part history identical formally sah sah addition interactive processes applies deterministic function g interaction history length n obtain estimate ˆµ ˆµah ˆµah c deferred materials section following lemma formalizes idea pac learning o error sufﬁcient fairness auditing given p mincidprdx xa prdx xa cid ω lemma c h phx cid h∗x ≤ α thencidcidµh − µh∗cidcid ≤ α p proof first observe cidcidprhx xa − prh∗x xa cidcid ≤ prhx cid h∗x xa prhx cid h∗x xa ≤ prhx cid h∗x xa prxa p ﬁrst inequality triangle inequality second inequality deﬁnition p symmetrically adding two inequalities cidcidprhx xa − prh∗x xa cidcid ≤ prhxcidh∗xxa cidcidµh − µh∗cidcid ≤cidcidprhx xa − prh∗x xa cidcid cidcidprhx xa − prh∗x xa cidcid ≤ prhx cid h∗x xa prhx cid h∗x prhx cid h∗x xa p p p p ≤ α p d deferred materials section d proof theorems proof theorem suppose algorithm denoted throughout proof interacts target classiﬁer h∗ ∈ h will show following claim stage set labeled examples l shown far induces version v hl will subsequently query costv labels exiting loop note theorem follows claim taking l ∅ v h costh label queries exits loop implies queried unlabeled examples sah∗ induces version space v cid hh∗ sah∗ also note h∗ ∈ v cid implies µh∗ ∈ minh∈v cid µh maxh∈v cid µh combining two observations max h∈v cid µh diamµv cid ≤  h∈v cid µh − min cid cid cidcidˆµ − µh∗cidcid ≤ h∈v cid µh − min h∈v cid µh max ≤  now come back proving claim induction costv base case costv immediately exits loop without label queries inductive case suppose claim holds v costv ≤ n now consider version space v costv n case ﬁrst recall active fairness auditing ie minx∈x maxy∈− cost v y version space v next query example x chosen solution following minimax optimization problem x costv − n also recall deﬁnition algorithm facing costv min x∈x max y∈− cost v y x implies maxy∈− cost v y v cidh∗xcid v h∗x satisﬁes costv cidh∗xcid ≤ n combining inductive hypothesis seen total costv cidh∗xcid ≤ n costv number label queries will exit loop x n speciﬁcally implies version space next iteration x x argmin x∈x max y∈− cost v y x completes inductive proof claim proof theorem fix deterministic active fairness auditing algorithm will show following claim already obtained ordered sequence labeled examples l remaining label budget n ≤ costhl − exists h ∈ hl interacting h target classiﬁer obtains sequence labeled examples l ﬁrst l rounds ﬁnal version space hh sah µdiameter  theorem follow claim taking l ∅ see let h ∈ h∅ h classiﬁer described claim first note exists classiﬁer hcid cid h ﬁnal version space hh sah subsequently h hcid exact labeling s ˆµah ˆµahcid implies least one following must true cidcidµhcid − µhcidcid  hcid hcidsah hsah therefore lemma b sah sahcid denote s cidcidˆµah − µhcidcid  orcidcidˆµahcid − µhcidcidcid  showing guarantee estimation error ≤  target h ∈ h now turn proving claim induction ’s remaining label budget n following denote v hl max base case make queries case sah l hsah h v costv ≥ know n costv ≥ point zero label budget means allowed cidcidµh − µhcidcid max cidcidµh − µhcidcid  hh∈v hh∈hhsah completes proof base case inductive case suppose claim holds n ≤ n now suppose learning process remaining label budget n n obtained labeled examples l v hl satisﬁes costv ≥ n let x next example queries deﬁnition cost exists y ∈ − making query learner remaining label budget n − n l ∪cidx ycidcidcid hcid inductive hypothesis exists h ∈ hcid obtained labeled examples l ∪cidx ycid label budget n ﬁnal unlabeled dataset sah satisﬁes cidhh sahcid cidcidµh − µhcidcid  interacts h subsequently l ∪cidx ycidcid x ≥ costv − ≥ n cost v y diamµ cid cost addition interacting h obtains example sequence cidl x ycid ﬁrst l rounds interaction implies obtains example sequence l ﬁrst l rounds interaction h completes induction max hh∈hhsah d proof sketch proposition proof sketch let s s ocid  lnhcid iid samples dx xa dx xa respectively deﬁne active fairness auditing ˆµh s s prx∼shx − prx∼s hx hoeffding’s inequality union bound guarantees probability least now consider following deterministic algorithm • let n ocid  lnhcid • find lexicographically smallest s s x n ∀h ∈ h cidcidˆµh s s − µhcidcid ≤  ∀h ∈ h ˆµh s s − µh ≤  optimization problem feasible seen random choice s s makes equation happen nonzero probability • return ˆµh∗ s s n label queries examples s ∪ s  lnhcid labels returns ˆµ close µh∗ construction queries n ocid d proof proposition prove proposition ﬁrst recall wellknown bernstein’s inequality lemma d bernstein’s inequality given set iid random variables z zn mean µ variance σ addition zi ≤ b almost surely probability − δ cidcidcidcidcidcid cid complexity bound ocidθ · lnh · ln n  ncid cid cidcidcidcidcidcid ≤ zi − µ σ ln δ n b ln δ n proof proposition will analyze algorithm derandomized version phased cal algorithm hsu chapter prove proposition using theorem sufﬁces show algorithm deterministic label ﬁrst show every n optimization problem line always feasible see observe draw sn x xmn sample size mn drawn iid dx bernstein’s inequality zi ixi ∈ disvn probability − cid prsn x ∈ disvn ≤ prdx x ∈ disvn prdx x ∈ disvn ln mn ln mn ≤ prdx x ∈ disvn ln mn second inequality uses arithmetic meangeometric mean amgm inequality bernstein’s inequality union bound h hcid ∈ h probability − cid prdx hx cid hcidx lnh mn lnh mn ∀h hcid ∈ h prdx hx cid hcidx ≤ prsn hx cid hcidx ∀h hcid ∈ h prsn hx cid hcidx ⇒ prdx hx cid hcidx ≤ lnh mn active fairness auditing union bound nonzero probability two condition hold simultaneously showing feasibility optimization problem argue n vn ⊆ bh∗ ln h h ∈ vn h∗ vn therefore agree sn tn hand h h∗ agree tn deﬁnition vn consequence prsnhx cid h∗x implies prdx hx cid h∗x ≤ ln h consequence h ∈ vn prhx cid h∗x ≤ ln h now turn upper bounding algorithm ’s label complexity ≤ p implying thatcidcidµh − µh∗cidcid ≤  recall lemma c ncid mn mn mn tn n ncid ≤ ncid cid n n ≤o ln mn mn · prdx x ∈ disvn mn · θ · lnh cid · p ln mn mn θ · lnh · ln  cid inequality uses observation every n ∈ n lnh prdx x ∈ disvn ≤ prdx x ∈ disbh∗ cid mn ≤ θ p · lnh mn ≤ θ · lnh mn · p second inequality deﬁnition disagreement coefﬁcient recall section last inequality basic property disagreement coefﬁcient hanneke corollary d proof proposition ﬁrst prove following theorem gives decision treebased characterization cost· function connections active learning optimal decision trees observed prior works eg laber nogueira balcan et al deﬁnition d examplebased decision tree t instance domain hypothesis set pair x v t ’s internal nodes examples x every internal node two branches left branch labeled right labeled − every leaf l t corresponds set classiﬁers vl ⊂ v h ∈ vl agree examples appear roottoleaf path l formally suppose path root leaf l alternating sequence examples labels cidx y xn yncid every ∈ n hxi yi deﬁnition d fix dx examplebased decision tree t said µ separate hypothesis set v every leaf l t vl satisﬁes diamµvl ≤  theorem d given version space v costv minimum depth decision trees µ separates v proof prove theorem induction costv base case µ separates v also smallest depth possible costv diamµv ≤  exists trivial decision tree leaf depth inductive case suppose statement holds v costv n now consider v costv n ﬁrst show exists decision tree depth n µ separates v argminx∈x maxy costv y x indeed pick x active fairness auditing v x choice x costv − x equal n therefore inductive hypothesis x can construct decision trees t − t depths n µ separate two hypothesis classes v − respectively now deﬁne t root node x left subtree t right subtree t − see t depth n µ separates v x costv next show decision tree depth n µ separate v indeed assume sake contradiction tree t exists consider example x root tree deﬁnition cost one costv − x costv cid ≥ n therefore costv must exists subset v cidcid ⊂ v cid costv cidcid n applying inductive hypothesis v cidcid decision tree depth n − can µ separate v cidcid contradicts observation left subtree t depth n − µ separates v cid x must ≥ n without loss generality assume v cid v x now restate precise version proposition first deﬁne computational task computing lnh approximation costh following problem problem minimaxcost mc input instance space x hypothesis class h data distribution dx precision parameter  output number l costh ≤ l ≤ lnhcosth proposition d proposition restated algorithm solves minimaxcost polyhx  time np ⊆ timenolog log n proof proposition d proof takes laber nogueira ’s reduction set cover sc decision tree problem dtp reduce sc minimaxcost problem mc ie computing costh given hypothesis class h taking account unique structure active fairness auditing speciﬁcally following gap version sc’s decision problem shown computationally hard problem gapsetcover gapsc input universe u u un size n n ≥ family subsets c c cm integer k either following happens • case optsc ≤ k • case optsc ≥ k ln n optsc denotes minimum set cover size uc output case instance speciﬁcally wellknown obtaining polynomial time algorithm decision problem minimum set cover imply np ⊆ timenolog log n feige believed false start recall instance gapsc problem isc uc k instance mc problem imc hx dx  deﬁne coarse reduction β constructs mcinstance gapsc instance universe u u un sets c c cm will reﬁned shortly let h h h hn hx ≡ − always j ∈ n hj corresponds uj deﬁnitions hj’s will given shortly create example x h ∈ h hx − every ∈ m create basis example xi correspond ci every j ∈ n hjxi iff uj ∈ ci deﬁnition gapsc requires n ≥ without loss generality gapsc instances n solvable constant time constant can changed constant feige active fairness auditing set ci create ci − auxiliary x’s follows given set ci ci si corresponds hi hisi create balanced binary tree ti leaf corresponding hij create auxiliary example associated internal node ti follows internal node tree deﬁne corresponding auxiliary sample x label classiﬁers leaves subtree rooted left child label − remaining classiﬁers h total number auxiliary x’s ≤ m · n − deﬁne x union example sets constructed three items n ≤ mn ex amples deﬁne dx x xa ∼ uniformx x x xa ∼ uniformx set  n setting  every h ∈ h h cid h cidcidµh − µhcidcid cidcidprhx xa − prhx xa cidcid ≥ n−  recall optsc deﬁned size optimal solution sc instance uc let optmc denote height tree corresponding optimal query strategy mc instance imc obtained reduction β following result lemma d optsc ≤ optmc ≤ optsc max c∈c log c proof let k optsc show two inequalities respectively theorem d sufﬁces show examplebased decision tree t µ separates h must depth least k see ﬁrst note item reduction β deﬁnition µ separation leaf t contains h must contain hypotheses h addition h ≡ − h must lie rightmost leaf t now prove statement know examples along rightmost path t corresponds collection sets form set cover c sufﬁces show set cover size greater set cover isc examples along rightmost path either xi’s correspond set c auxiliary examples correspond subset set c set cover instance u ccid ccid comprises sets c subsets sets c will smaller set cover therefore length path root rightmost leaf least k size smallest set cover original sc instance isc let optimal solution isc g ik construct examplebased decision tree t depth c∈c log c µ separates h o mincidl hxil cid k max let rightmost path t contain nodes corresponding xi xik order important level l k left subtree xil deﬁned til deﬁned step reduction β note may result t potentially empty leaves h covered multiple xil’s appears xio will prove construction t µ separates h every leaf corresponds version space v singleton set thus diamµv ≤  rightmost leaf holds construction til’s b rightmost leaf will show h version space since g set cover ∪k lcil u therefore ∀j ∈ n ∃l ∈ k uj ∈ cil ⇔ hjxil construction implies zero labeling xi xik can correspond h therefore version space rightmost leaf v satisﬁes v h recall theorem d depth t upper bounds optmc t ’s maximum root leaf path length k maxc∈c log c built β now construct improved gap preserving reduction βcid deﬁned follows given gapsc instance isc uc k universe u u un sets c c cm take constant z log n construct gapsc instance iscz u zcz kz containing z copies original set n cz c czm cp−mi covering instance u z u uz p ∈ z ∈ m note optscz koptsc isi u n uz active fairness auditing apply reduction β obtain imcz iscz now will argue βcid gappreserving reduction suppose original gapsc instance isc uc k case ie optsc ≤ k optscz ≤ kz lemma d optmcz ≤ kz maxc∈cz log c ≤ kz log n ≤ zk ≤ zk suppose original gapsc instance isc uc k case ie opt ≥ k ln n optscz ≥ zk ln n lemma d yields optmcz ≥ zk ln n now suppose exists algorithm solves mc problem polyhx  time propose following algorithm acid solves gapsc problem polynomial time mentioned implies np ⊆ timenolog log n input isc uc k • apply βcid isc obtain instance mc imcz • let l ← aimcz output l ≤ zk ln n otherwise correctness seen isc case optmcz ≤ zk n ≥ guarantee l ≤ lnh· optmcz ≤ lnn log n· zk ≤ zk ln n acid outputs otherwise isc case optmcz ≥ zk ln n guarantee l ≥ zk ln n zk ln n acid outputs time complexity ω mn log n runs time opolyxh  acid runs time opolym n imcz x ≤ mz · nz omn log n h nz n log n  n mz·nz d deferred materials section d µ specifying set µ teaching dimension properties following deﬁnitions inspired teaching exact active learning literature heged˝us hanneke deﬁnition d µ specifying set fix hypothesis class h function h x → y set unlabeled examples s said µ specifying set h h ∀h h ∈ hh s cid µh − µh ≤  deﬁnition d µ extended teaching dimension fix hypothesis class h function h x → y deﬁne thh µ  size minimum µ specifying set h h ie optimal solution following optimization problem oph mins st∀h h ∈ hh s cid µh − µh ≤  deﬁnition d deﬁne µextended teaching dimension xtdh µ  maxhx→y thh µ  improper teaching dimension related costh lemma d xtdh µ  ≤ costh proof let h argmaxhx→y thh µ  let k denote thh µ  − sufﬁces show costh ≥ k see ﬁrst note x max costh min ≥ min ≥ min costhx y x∈x costhx hx x∈x min y x∈x costhx hx x hx note h allowed outside h active fairness auditing can repeatedly unroll expression long diamµhx hx xi hxi least  unrolling k − times uk− cidx xk−cid costh ≥ k − min uk− costhh uk− deﬁnition thh µ  u u ≤ k − exists hcid hcidcid ∈ hh u µhcid − µhcidcid  ⇒ diamµhh u  thus unlabeled dataset uk− size k − costhh uk− ≥ therefore costh ≥ k d proof theorem proof prove theorem follows correctness observe right algorithm returns must execute lines since condition line also satisﬁed dataset t must ˆht h∗t combined deﬁnitions optimization problems implies h h used line right return satisfy µh min h∈hh∗t µh µh max h∈hh∗t µh therefore µh∗ ∈ minh∈hh∗t µh maxh∈hh∗t µh µh µh furthermore line µh− µh ≤  hence ˆµ output algorithm satisﬁes cidcidˆµ − µh∗cidcid cidµh µhcid − µh∗ cidcidcidcid label complexity now bound label complexity algorithm speciﬁcally terms xtdh µ  first end tth iteration outer loop newly collected dataset tt must ∃x ∈ tt ˆhx cid h∗x o mistake bound m total number outer loop iterations denoted n must m addition lemma d given probability − δm tt ≤ o therefore union bound probability − δ total number label queries made algorithm log xcid xtdh µ  · log hm δ cidcidcidcid ≤  cid cid tt ≤ o m · xtdh µ  · log hm δ log x lemma d every outer iteration algorithm probability ≥ − δ satisﬁest ≤ o xtdh µ  · log hm δ cid ncid t cid log xcid m t dataset end iteration proof inner loop similar “blackbox teaching” algorithm dasgupta et al except teaching µˆh opposed ˆh although dasgupta et al ’s algorithm originally designed exact interactive teaching implicitly gives oracleefﬁcient algorithm approximately computing minimum set cover will use insight throughout proof analysis dasgupta et al expected number teaching examples use different ﬁltration obtain high probability bound number teaching examples first setup useful notations proof let x x xm recall λ ln let wix denote weight point x ∈ x denoted wx algorithm end round inner loop let τxj exponentiallydistributed threshold associated xj deﬁne random variable uij τxj wixj let mi denotes j∈m uij also deﬁne j cid icid jcid iff j precedes icid jcid lexicographically deﬁne two ﬁltrations let fij sigmaﬁeld indicator events uicidjcid icid jcid cid j convention fi fi−m number teaching examples selected ith round doubling can seen mi cid hm δ pr ijcidnm cid ijcidnm ∃n m ynm ≥ s cid cid ≤ cid cid ncid cid ≤ λ ijcidnm ijcidnm ijcidnm efi− mi wnx x∈x ez ijfij− eu ijfij− − euijfij− eu ijfij− euijfij− lemma d lemma d active fairness auditing let fi sigmaﬁeld indicator events uicidjcid jcid ∈ m ≤ icid ≤ ﬁltration used dasgupta deﬁne yij cid et al can easily seen fi fim icidjcidcidij zicidjcid zij uij − eciduij fij− cid ∈ − yij martingale eyijfij− ezijfij− eyij−fij− yij− let n total number rounds item lemma d oxtdh µ  lnx lemma dasgupta et al probability may apply freedman’s inequality lemma d since yij − yij− zij ≤ almost surely s σ  ≤ exp cid cid ez ijfij− ≤ σ − s σ s next let σ λ xtdh µ  lnx n m ≤ λ xtdh µ  lnx σ cidcid cid cid ln δ σ ln δ ensures right hand δ o δ δ log log σ log meanwhile choose s side eq δ thus equation probability − δ n m uicidjcid − ncid also using lemma d d probability cidn icidjcidcidnm cid ynm efi− mi ≤ o cidcid cid δ ln δ σ ln therefore yn m particular yn m ≤ o efi− mi ≤ λ xtdh µ  lnx cid cid λ xtdh µ  lnx cidλ xtdh µ  lnx lnδ lnδ cid cid ocidxtdh µ  lnx lnhm δcid λ xtdh µ  lnx ln o δ lemma d freedman’s inequality let martingale yk∞ k y let wk cidk ej−x j j t ≥ σ − t σ rt cid cid pr∃k ≥ yk ≥ t ∧ wk ≤ σ ≤ exp k difference sequence xk∞ k xk ≤ r lemma d outer iteration algorithm active fairness auditing number inner loop iterations xtdh µ  · logx point inner loop thatcid x∈x wx ≤ xtdh µ  · logx proof proof similar dasgupta et al lemma differences completeness include proof ﬁrst prove second item first note point algorithm x wx ≤ let s∗ˆh optimal solution optimization problem opˆh s∗ˆh tˆhh µ  ≤ xtdh µ  note every time line called feasibility s∗ˆh respect opˆh ∆h h ∩ s∗ˆh cid ∅ therefore weight element x ∈ s∗ˆh gets doubled implies total number times line executed s∗ˆh · logx otherwise number time line executed ≥ s∗ˆh · logx pigeonhole principle must exist element x ∈ s∗ˆh whose weight exceeds contradiction finally note weight doubling increases total weight ≤ ﬁnal total weight · s∗ˆh · logx ≤ xtdh µ  · logx ﬁrst item follows since number inner iterations number weight doublings lemma d every inner iteration emifi− ≤cid x∈x λwix − wi−x proof proof almost verbatim copy dasgupta et al lemma include emifi− x∈x cid cid cid ≤ cid x∈x x∈x x∈x prx chosen round ix chosen round ifi− − prτx wixτx wi−x − exp−λwix − wi−x λwix − wi−x e deferred materials section e distributionfree query complexity lower bounds auditing vc classes theorem e lower bound randomized auditing hypothesis class h vc dimension d ≥  ∈ possibly randomized algorithm exists distribution d realizable h∗ ∈ h given querying budget n ≤ ωmind  output ˆµ pcidcidcidˆµ − µh∗cidcid  cid proof will using le cam’s method several subtle modiﬁcations first will reduce estimation problem hypothesis testing problem different hypotheses µh∗ will centered around two ωseparated values high probability second will upper bound distribution divergence interaction history two hypotheses requires delicate handling label queried example depends identity example also historical labeled examples active fairness auditing step construction vch d exists set examples z z z zd− ⊂ x shattered h let z z zd− let dx follows x xa uniform z whereas x xa delta mass z let ˜ max √ conditions d ≥  ≤ let label budget n ˜ ≤ ˜ d ωcidmind  cid consider two hypotheses choose h∗ randomly − z subject h∗z • h choose h∗ every ∈ d − independently h∗zi • h choose h∗ every ∈ d − independently h∗zi cid cid probability − probability probability − probability − ˜ ˜ ˜ − ˜ cidµh∗ ≤ following simple claim shows separation µh∗ two hypotheses proof deferred end main proof claim e ph∗∼h step upper bounding statistical distance next show h h hard distinguish label budget n end upper bound kl divergence joint distributions cidx y xn yncid x y≤n h h denoted p p respectively applying lemma e cidµh∗ ≥ ˜cid ≥ ˜cid ≥ ph∗∼h − ecid klcidpyi · x y≤− xi pyi · x y≤− xicidcid klp p ncid p claim every x y≤− xi ∈ x × yi− × x support p klcidpyi · x y≤− xi pyi · x y≤− xicid ≤ ˜ independent conditioned x y≤− cidh∗x x y≤− first observe cidx y≤− xicid support p must exists h∗ z → − h∗xj yj j ∈ − particular means must exist j cid j − xj xj yj cid yj next note h conditioned x y≤− posterior distribution h∗ supported set cidh h z → − ∀j ∈ − hxj yj cid speciﬁcally x ∈ z cidxj j ∈ − cid h∗x’s statement holds h except x ∈ z cidxj j ∈ − cid now ph∗x xi ∈cidxj j ∈ − cid h h distributions h∗xi x y≤− equal equal delta mass supported element singleton setcidyj j ∈ − xj xi cid case klcidpyi · x y≤− xi pyi · x y≤− xicid ≤ ˜ otherwise xi ∈ cidxj j ∈ − cid h h∗xi x y≤− takes value probability klcidpyi · x y≤− xi pyi · x y≤− xicid klcid − ˜ ˜ similarly h h∗xi x y≤− takes value case fact e ˜ ≤ − ˜ takes value − probability probability x y≤− tion h∗xi x y≤− h h now perform case analysis ˜ addition conditional distribution yi x y≤− xi equals conditional distribu ˜ takes value − probability ˜cid ≤ ˜ pinsker’s inequality lemma e dtvp p ≤cid summary cases equation holds plugging back equation n klp p ≤ n˜ ≤ ˜ le cam’s klp p ≤ cid − ˜ − ˜ lemma lemma e hypothesis tester ˆb active fairness auditing cidˆb cid p p cid ≥ cid − dtvp pcid ≥ step concluding proof given ’s output auditing estimate ˆµ consider following hypothesis test plugging equation cidˆb cid ˆb cid p ˆµ ≥ cid ˆµ ˆµ ≥ cid cid p ˆµ ≥ ˜ cid ≥ p ˆµ ≥ cid cid cidcidcidˆµ − µh∗cidcid ≥ µh∗ ≥ cid ˜ now recall claim e using fact pa ∩ b ≥ pa − pbc pa pb − ˆµ ≥ cidcidcidˆµ − µh∗cidcid ≥ µh∗ ≤ − ≥ p − cid cid p ˜ cid − symmetrically also cid cid ≥ p ˆµ − p ≥ p ˆµ ≥ cid cid cidcidcidˆµ − µh∗cidcid ≥ cid cidcidcidˆµ − µh∗cidcid ≥ cid cidcidcidˆµ − µh∗cidcid  p ˜ cid cid ≥ p ˆµ combining equations ˜  left hand side can viewed total probability ofcidcidˆµ − µh∗cidcid  h∗ drawn uniform mixture distribution h∗ distributions h h probabilistic method exists h∗ ph∗ ˜ ˜ ≥ − p proof claim e without loss generality show ﬁrst inequality second inequality can shown symmetri cally note h random h∗’s dp value satisﬁes µh∗ prh∗x xa − prh∗x xa d − h∗zi second equality follows prh∗x xa h∗z − always true h d − µh∗ sum d − iid bernoulli random variables mean parameter hoeffding’s inequality − ˜ therefore d−cid cid cid cid p µh∗ − ˜ ≤ exp −d − · cid cid ≥ √ d  √ d cidcid cid ˜ ≤ second inequality uses fact ˜ max e query complexity auditing nonhomogeneous halfspaces gaussian subpopulations theorem e lower bound let d ≥  ∈ dx x xa ∼ nd id whereas x xa ∼ nd d×d ie deltamass supported d possibly randomized algorithm exists h∗ hlin class nonhomogeneous linear classiﬁers given query budget n ≤ ωcidmind  cid output ˆµ cid cidcidcidˆµ − µh∗cidcid  pah∗ active fairness auditing proof similar proof theorem e will use le cam’s method addition challenges proof theorem e active fairness auditing halfspaces setting faced extra challenge posterior distributions h∗xi x y≤− deviates signiﬁcantly prior distribution h∗xi easily calculated closed form get around difﬁculty using chain rule kl divergence along posterior formula noiseless bayesian linear regression gaussian prior calculate tight upper bound kl divergence two carefully constructed wellseparated hypotheses ˜ ωcidmind  cid step construction let ˜ max √ label budget n consider two hypotheses choose h∗ ha∗b∗ b∗ − ∗ chosen randomly different distributions d ≥ ˜ ≤ assumption  ≤ let d • h ∗ ∼ n • h ∗ ∼ n d ˜id d − ˜id cid cid e− z cid z cid ≥ µh∗ φ− ˜ dz standard normal cdf following claim shows separation µh∗ two hypotheses proof deferred end main proof claim e ph∗∼h −∞ √ step upper bounding statistical distance next show h h hard distinguish making n ≤ n label queries end upper bound kl divergence joint distributions x y≤n h h denoted p p respectively end deﬁne ˜yi cida∗ xicid − ∈ n yi sign˜yi deﬁne ˜p ˜p resp q q joint distributions x ˜y≤n resp x y ˜y≤n h h respectively chain rule kl divergence lemma e z x y≤n w ˜y≤n z x ˜y≤n w y≤n respectively get cid ≥ µh∗ φ− − ˜ ph∗∼h φz π klqx y ˜y≤n qx y ˜y≤n klqx y≤n qx y≤n klq˜y≤n x y≤n q˜y≤n x y≤n klqx ˜y≤n qx ˜y≤n klqy≤n x ˜y≤n qy≤n x ˜y≤n cid cid cidcid cidcid klpp kl˜p˜p cid cid cid cid cidcid cidcid ≥ cid cid last term q q y≤n x ˜y≤n delta mass supported sign˜y≤n consequence klp p ≤ kl˜p ˜p also note can viewed query learning algorithm round receives x ˜y≤− input choose next example query ie elects use thresholded value yj’s opposed ˜yj’s applying lemma e ncid ecidklp˜yi · x ˜y≤− xi p˜yi · x ˜y≤− xicid kl˜p ˜p claim every x ˜y≤− xi ∈ x × yi− × x support ˜p klp˜yi · x ˜y≤− xi p˜yi · x ˜y≤− xi ≤ ˜ cida ∈ rd cida xlcid ˜yl∀l ∈ − cid denote xi− xcid first lemma e deferred end proof h conditioned x ˜y≤− support ˜p posterior distribution ∗ ∗ ∼ n d ˜id conditioned afﬁne set s − ∈ ri−×d ˜yi− ˜y ˜yi− † − ˜yi− − − ∈ s − always x ˜y≤− support ˜p must case s cid ∅ result ˆa x also denote x⊥ welldeﬁned − ≤ n − ≤ d − applying lemma e − matrix whose columns orthonormal basis spanx xi− x⊥ xcid xcid active fairness auditing cid ∗ x ˜y≤− ∼ n ˆa ˜x⊥ −cid rankdeﬁcient d −x⊥ −cidcid d ˜x⊥ covariance matrix now observe ˜yi x⊥ d ˜xcid ncidcidˆa xicid similarly h ˜yi x ˜y≤− xi distribution ncidcidˆa xicid x ˜y≤− xi distribution cida∗ xicid −x⊥ −x⊥ −cidxi cid d − ˜xcid x ˜y≤− cid now x⊥ −x⊥ −cidxi prove case analysis xi ∈ spanx xi− x⊥ −cidxi h h posterior distributions ˜yi x ˜y≤− xi delta mass cidˆa xicid therefore klp˜yi · x ˜y≤− xi p˜yi · x ˜y≤− xi ≤ ˜ xi ∈ spanx xi− x⊥ x ˜y≤− xi nˆµi ˜σ d xcid ˜yi σ −x⊥ −cidxi case fact e −cidxi nˆµi − ˜σ x⊥ cid h h posterior distributions respectively ˆµi cidˆa xicid cid klcidp˜yi · x ˜y≤− xi p˜yi · x ˜y≤− xicid cid cid ˜ cid ˜ nˆµi − ˜σ − ˜ ˜ nˆµi ˜σ − ln cid − ˜ cid kl ≤ ≤˜ − ˜ algebra ﬁrst inequality fact ln x ≥ x − x x ≥ taking x ˜ ˜ ≤ summary cases equation holds plugging back equation n ≤ klp p ≤ n˜ ≤ lemma e implies hypothesis tester ˆb pinsker’s inequality lemma e dtvp p ≤cid klp p ≤ ˜ le cam’s lemma −˜ second inequality pˆb pˆb − dtvp p ≥ step concluding proof given ’s output auditing estimate ˆµ consider following hypothesis tester cid ˆb ˆµ φ− ˆµ ≤ φ− plugging equation cidˆµ ≤ φ−cid p p now recall claim e using fact pa ∩ b ≥ pa − pbc pa pb − − ≥ p cidˆµ ≤ φ−cid cidcidcidˆµ − µh∗cidcid ≥ ˆµ ≤ φ− µh∗ φ − ≥ p cid cid p ˜ cidˆµ ≤ φ−cid− symmetrically also p cidcidcidˆµ − µh∗cidcid ≥ cid ≥ p ˜ cidˆµ φ−cid ≥ cid cidˆµ φ−cid − ≥ p ˜ active fairness auditing cid cidcidcidˆµ − µh∗cidcid ≥ cid p combining equations p cidcidcidˆµ − µh∗cidcid ≥ cid cidcidcidˆµ − µh∗cidcid  ˜ ˜ ≥  left hand side can viewed total probability ofcidcidˆµ − µh∗cidcid ≥  h∗ drawn ∗ ∗ ∈ u u cida ∀j ∈ cidxj acid ˜yj uniform mixture distribution h∗ distributions h h probabilistic method exists h∗ ∈ h ph∗ lemma e given setting ﬁxed ∈ n x ˜y≤ posterior distribution ∗ x ˜y≤ cid ˜ ≥ − proof use bayes formula expand posterior ∝ denotes equality multiplicative factor independent ∗ pa∗ x ˜y≤ ∝pa∗ x ˜y≤ j icid icid icid j ∝pa∗ ∝pa∗ ∝pa∗ pxj ∗ x ˜y≤j−p˜yj xj ∗ x ˜y≤j− pxj x ˜y≤j−cid˜yj cidxj ∗cid cid cid˜yj cidxj ∗cid cid j second equality uses deﬁnition conditional probability third equality uses fact ﬁxed query learning algorithm xj independent ∗ conditioned x ˜y≤j− observation given xj ∗ ˜yj cidxj ∗cid deterministically concludes proof proof claim e h∗x signcida∗ xcid b∗ b∗ − can seen ph∗x xa hand ph∗x xa pz∼nidcida∗ zcid ≥ pz∼nid cidcid ∗ cida∗cid z cid cid ≥ cida∗cid cid cida∗cid cid − φ ˜ ∼ χd therefore fact e probability ≥ dcida∗cid ˜ ≥ cid also note h dcida∗cid d · − d implies therefore every b ∈ cidcidcidcid cid ˜ − cid cida∗cid ≤ cidcidφa − φbcidcid ≥ minξ∈ cid cid cid d ≥ − φ − ˜ cida∗cid − φ cid ˜ − ˜ ≤ − ˜ ≤ φcidξa − b ≥ ≥ − φ − ˜ − b ≥ φ− ˜ concludes proof ﬁrst inequality second inequality proved symmetrically active fairness auditing now present deterministic active fairness auditing algorithm algorithm guarantees algorithm works setting two subpopulations gaussian whose mean covariance parameters m σ m σ aims estimate µh∗ within precision  recall known also assumes access blackbox queries h∗ ∈ hlin cidhabx signcida xcid b ∈ rd b ∈ rcid cidh∗x xa cid − prx∼dx cidh∗x xa cid within precision  b ∈ end note cid cidh∗x cid pr˜x∼nid cidh∗x xa cid cid sufﬁces estimate γb prx∼dx µh∗ prx∼dx h∗mb σ b ˜x γb prx∼nmbσb deﬁne ˜hb rd → − ˜hb˜x h∗mb σ cidh˜x cid probability positive prediction h standard b ˜x γb equals γ˜hb γh p˜x∼nid gaussian distribution importantly h∗ linear classiﬁer ˜hb also linear classiﬁer lies hlin recall procedure estimatepositive algorithm labelefﬁciently estimates γh h ∈ hlin using query access h algorithm uses subprocedure estimate γb γ˜hb line simulate label queries ˜hb using query access h∗ according equation sufﬁces apply afﬁne transformation input ˜x obtaining transformed input mb σ finally ˆγ ˆγ accurate estimators γ γ obtained algorithm takes difference estimator ˆµ µh∗ line b ˜x query h∗ transformed input ensure ˆµ thatcidcidˆµ − µh∗cidcid ≤  algorithm active fairness auditing nonhomogeneous linear classiﬁers gaussian subpopulations require subpopulation parameters m σ m σ query access h∗ ∈ hlin target error  b ∈ b ˜x ˜hb ∈ hlin query ˜hb can simulated deﬁne ˜hb rd → − ˜hb˜x h∗mb σ one query h∗ ˆγb ← estimatepositive˜hb  return ˆγ − ˆγ algorithm outputs ˆµ probability cidcidˆµ − µh∗cidcid ≤  moreover algorithm makes od ln d theorem e upper bound h∗ ∈ hlin dx x xa ∼ nm σ x xa ∼ nm σ  label queries h∗ proof will see lemma e b ∈ respective calls estimatepositive ensures therefore ˆγb − γb ≤  cidcidˆµ − µh∗cidcid ≤ ˆγ − γ ˆγ − γ ≤  moreover every b lemma e ensures call estimatepositive makes od ln d  label queries ˜hb simulating query ˜hb takes one query h∗ every b also makes od ln d  label queries h∗ summing number label queries b ∈ total number label queries algorithm od ln d  now turn presenting guarantee key subprocedure estimatepositive proof expands analysis sketch section active fairness auditing cidcidˆγ − γh∗cidcid ≤  lemma e guarantees estimatepositive recall γh prx∼nidhx estimatepositive algorithm receives inputs query access h∗ ∈ hlin target error  outputs ˆγ furthermore makes od ln d  queries h∗ proof let h∗x signcida∗ xcid b∗ target classiﬁer first observe γh∗ φ standard normal cdf s signb∗ r correctly obtains s s h∗cid signcida∗cidcid b signb mi − b∗ ∗ − m cid cidd cid cid cid b∗ cida∗cid φsr φ note line estimatepositive   ln  β d consider two cases depending line recall α d ln estimatepositive returns estimatepositive returns line must case ∈ d h∗αei h∗−αei case lemma e every imi ≥ α implies r  ln case s − γh∗ φsr ≤  use standard fact φx ≤ exp− x x ≤ case ˆγ ensures equation holds symmetric case s γh∗ φsr ≥ −  ˆγ also ensures equation ≥cid dα− ≥cid cid cidd − m cid cidd ≤cid hand estimatepositive returns line must case exists ∈ d mi ≤ α implies r now estimatepositive must execute lines ﬁnal s computes following properties every ∈ s added guarantee procedure binarysearch algorithm ˆmi − mi ≤  otherwise ∈ s must case h∗βei cid h∗−βei lemma e implies thatmi ≥ β therefore conditions lemma satisﬁed thusˆr − r ≤  also yields thatsˆr − sr ≤  finally note φ mi ≤ α lipschitz m − − m √ π cidcidˆγ − γh∗cidcid cidcidφsˆr − φsrcidcid ≤ √ ·sˆr − sr ≤  π summary cases estimatepositive outputs ˆγ equation satisﬁed now calculate total query complexity estimatepositive line makes label query line makes d label queries ∈ d line makes label queries binarysearch makes log β  label queries summary total label query complexity estimatepositive d d log β  o d ln cid cid d  now present proof lemma key proof lemma e proof lemma first lemma e assumption ∈ s ˆmi − mi ≤  remains prove cidcidcidcidcidcid cid cidcidcidcidcidcid cid cid ∈s ˆm− cid ∈s m− − − cid cid cid ∈s m− cidd m− cidcidcidcidcidcid ≤  cidcidcidcidcidcid ≤  combined inequality will conclude proof zs cid m− active fairness auditing see let z cidd also note thatcid cidd lagrange mean value theorem cidcidcidcidcid √ zs m − cidcidcidcidcid ≤ max zcid∈zs z − √ z since ∈ smi ≥ β implies ∈s m− z − zs ≤ d β ≤  d ln  therefore zs ≥ z − r ≤ α implies z ≥ α d ln   d ln  ≥ d ln  now zcid− ·zs − z ≤ zs− ·zs − z ≤ d ln ·   d ln  ≤  concludes proof lemma e let l ∈ n f m ml cid cidl m − f lipschitz respect cid · cid∞ proof first show f lipschitz respect cid · cid∞ orthants rl without loss generality focus positive orthant r cidm ∈ rl mi ≥ ∀icid now check two points cidm cidn r cidcidf cidm − f cidncidcid ≤ cid cidm − cidncid∞ lagrange mean value theorem exists θ ∈cidt cidm − tcidn t ∈ cid cidcidf cidm − f cidncidcid cidcidcid∇f θ cidm − cidncidcidcid ≤ cid∇f θcidcid cidm − cidncid∞ cid cidm ∈ rl mi ∀icid interior r cid∇f m mlcid ≤ see note cid observe thatcidl implies every ∈ lgi ≤ therefore second inequality h¨older’s inequalty therefore sufﬁces check cidm r ∇f m md m− m− cidl cidl cid g l igi m− m− lcid cidgcid gi ≤ now consider cidm cidn ∈ rl cidt cidm − tcidn t ∈ cid consists k pieces piece iscidt cidm − tcidn t ∈ ti− ticid t necessarily lie orthant suppose line segment t tk piece contained orthant cidcidf cidm − f cidncidcid ≤ kcid ≤ kcid kcid cidcidf ti− cidm − ti−cidn − f ti cidm − ticidncidcid cidti− cidm − ti−cidn − ti cidm − ticidncid∞ ti− − ticid cidm − cidncid∞ cid cidm − cidncid∞ second inequality uses lipchitzness f within orthant contains piece k lemma e given ∈ d ξ h∗ξei h∗−ξei thenmi ≥ ξ proof suppose h∗ξei h∗−ξei case −bi ≤ ξa∗ mi ≥ ξ case h∗ξei h∗−ξei can proved symmetrically ≤ bi thereforecidcidξa∗ cidcid ≤ bi implies e auxiliary lemmas query learning lower bounds active fairness auditing subsection collect standard useful lemmas establishing lower bounds general adaptive sampling query learning algorithms including active fairness auditing algorithms throughout denote p distribution interaction transcript sequence n labeled examples cidx y xn yn cid obtained query learning algorithm interacting environment use shorthand x y≤ denote cidx y xi yicid lemma e le cam’s lemma given two distributions p p observation space z ∈ z let ˆb z → hypothesis tester cidˆbz cid − dtvp pcid lemma e pinsker’s inequality two distributions p q dtvp p ≤cid dtvp p denotes total variation distance p p cidˆbz cid ≥ cid p p lemma e chain rule kl divergence two distributions qz w qz w z × w klp q cid klq q klq z q z e z∼q z klq wz· z q wz· z fact e let kl·· denote binary relative entropy function b ∈ kla b ≤ b − following lemma wellknown lemma e divergence decomposition possibly randomized query learning algorithm label budget n two hypotheses h h represented distributions target concept h∗ ecidklpyi · x y≤− xi pyi · x y≤− xicid klp p ncid proof simplify klp p follows cid cid klp p xy≤n xy≤n cid cid ncid ncid ncid px y≤n ln px y≤n px y≤n ncid pyi x y≤− xi pyi x y≤− xi ln ln px y≤n px y≤ ln paxi x y≤− paxi x y≤− pyi x y≤− xi pyi x y≤− xi cid cid ecidklpyi · x y≤− xi pyi · x y≤− xicid px y≤− xi ·cid pyi x y≤− xi ln xy≤−xi xy≤ yi pyi x y≤− xi pyi x y≤− xi ﬁrst equality deﬁnition kl divergence second equality chain rule conditional probability third equality canceling conditional probabilities unlabeled examples given history run algorithm two environments fourth equality law total probability ﬁfth equality deﬁnition kl divergence fact e kl divergence gaussians mean µ ∈ r σ σ cid kl nµ σ nµ σ σ σ − ln σ σ fact e concentration χ random variables d ≥ z ∼ χd δ p speciﬁcally active fairness auditing cid cid z − d ≤ d ln ln δ pcidz − d ≤ √ d ≥ − δ δ cid cid ≥ lemma standard fact normal distribution conditioned afﬁne subspaces include proof ﬁnd reference lemma e suppose u cidθ ∈ rd xθ ycid nonempty afﬁne subspace rd x ∈ rm×d rows x xm ∈ rd let dimspanx xm l let w ∈ rd×d−l matrix whose columns form orthonormal basis spanx xm⊥ consider z ∼ n id proof denote ˆθ x†y least norm solution equation xθ y wellknown ˆθ ∈ spanx xm u cid ∅ x ˆθ y now claim u can equivalently written cidˆθ w α α ∈ rd−lcid z z ∈ u ∼ nx†y w w cid one hand θ ˆθ w α xθ x ˆθ xw α y y hand every θ ∈ u xθ y xθ− ˆθ cid implies θ− ˆθ ∈ spanx xm⊥ therefore exists α ∈ rd−l θ ˆθ w α deﬁne v ∈ rd×l matrix whose columns form orthonormal basis spanx xm also claim given vector z ∈ rd z ∈ u ⇔ v cidz v cid ˆθ z ∈ u previous claim z ˆθ w α therefore v cidz v cid ˆθ v cidw α v cid ˆθ v cidz v cid ˆθ note z v v cidz w w cidz v v cid ˆθ w w cidz ˆθ w w cidz last equality follows ˆθ ∈ spanx xm taking αz w cidz ∈ rd−l z ˆθ w αz implying z ∈ u rest proof let d denote equality distribution consider random variable z d n id let v v cidz w w cidz now note matrix t ∈ rd×d orthonormal matrix cid cid cid cid w cid v cid v cid w cid cid cid v w z t z d n id therefore v w two independent standard normal random variables distributions n il n id−l respectively note second claim event z ∈ u equivalent v v cid ˆθ therefore w z ∈ u d n id−l result z z ∈ u d v v w w z ∈ u d ˆθ w w z ∈ u d nx†y w w cid f experiments section empirically explore shrinkage version space various baseline methods algorithm two baseline methods sampling will consider iid sampling without replacement active learning cal active fairness auditing procedure train logistic regression model ﬁnd h∗ two datasets commonly used fairness literature ﬁrst compas larson et al two groups deﬁned caucasian noncaucasian second student performance dataset two groups deﬁned female male run three methods alloted label budget small fraction total dataset size much smaller compas student performance evaluation evaluation will version space induced labels requested three methods will evaluate version space two ways given hs will compute µdiameter maxhhcid∈hs µh− µhcid µdiameter version space captures largest extent algorithm’s µ estimate may changed posthoc manipulation smaller higher degree manipulationproofness compute maxhhcid∈hs µh − µhcid will evaluate maxh∈hs µh minh∈hs µh let g x ∈ x xa g x ∈ x xa implement maximization program may move constraint objective lagrangian cid x∈g cid x∈g cid x∈s max h g hx − g hx λ hx h∗x equivalently max h g cid x∈g hx g cid x∈g hx − λ cid x∈s hx h∗x mentioned earlier observe objective may framed costsensitive classiﬁcation problem commonly used fairness literature agarwal et al particular cost predicting x ∈ g − g ow cost predicting x ∈ g − g ow cost predicting h∗x x ∈ s −λ ow using iterative doubling grid search look smallest λ may enforce hx h∗x ∀x ∈ s since hard constraints ﬁnd maximizing h version space given λ procedure applied minimizing h version space since may choose µh h ∈ hs return estimate µh∗ will evaluate eh∼unifhsµh− µh∗ – corresponds average error proportional estimation accuracy sampling version space will use classic hitandrun algorithm sample models version space budget average error results terms µdiameter version space may interpreted maximum possible degree postaudit manipulation µ see figure algorithm best three methods budgets expected since algorithm designed make use maxh∈hs µh minh∈hs µh estimates query selection “shrink” version space µspace behind algorithm cal looks generally better onpar iid sampling terms estimation error going average µ estimation error version space see figure general one active approaches outperforms iid sampling two active approaches budgets setting one better vice versa active fairness auditing figure left comparison three methods student performance dataset µdiameters ﬁnal version spaces function label query budget right comparison three methods compas dataset error bars percent conﬁdence interval constructed using repeats budget figure left comparison three methods student performance dataset average µestimation errors ﬁnal version spaces function label query budget right comparison three methods compas dataset error bars percent conﬁdence interval constructed using repeats budget ",55.3,"1"
