doi,abstract,title,journal,description,cover_date,first_name,last_name,date
10.1001/archneur.1984.04050230026012,"We compared local cerebral glucose metabolism, as determined by positron emission tomography following administration of fluorodeoxyglucose F 18, with Wechsler Adult Intelligence Scale (WAIS) scores in 22 right-handed persons. Five study subjects were normal volunteers and the remainder had Alzheimer's disease. Subtest scores within the verbal IQ group or the performance IQ group were generally highly interrelated; there was usually no correlation between the verbal and performance subtests. The cortical distribution of regions in which glucose metabolism was most closely associated with verbal subtest scores generally centered in the left parasylvian area. In contrast, scores on the performance subtests mainly localized to the right posterior parietal region. Most WAIS subtests thus appeared to evaluate primarily either verbal or visuospatial cognitive functions and either left parasylvian or right posteroparietal cortical activity. © 1984, American Medical Association. All rights reserved.",Wechsler Adult Intelligence Scale Performance: Cortical Localization by Fluorodeoxyglucose F18-Positron Emission Tomography,Archives of Neurology,Article,1/1/1984,Rodney,Brooks,1984
10.1001/jamainternmed.2015.3043,"Copyright 2015 American Medical Association. All rights reserved.IMPORTANCE: Screening mammography rates vary considerably by location in the United States, providing a natural opportunity to investigate the associations of screening with breast cancer incidence and mortality, which are subjects of debate. OBJECTIVE: To examine the associations between rates of modern screening mammography and the incidence of breast cancer, mortality from breast cancer, and tumor size. DESIGN, SETTING, AND PARTICIPANTS: An ecological study of 16 million women 40 years or older who resided in 547 counties reporting to the Surveillance, Epidemiology, and End Results cancer registries during the year 2000. Of these women, 53 207 were diagnosed with breast cancer that year and followed up for the next 10 years. The study covered the period January 1, 2000, to December 31,2010, and the analysis was performed between April 2013 and March 2015. EXPOSURES: Extent of screening in each county, assessed as the percentage of included women who received a screening mammogram in the prior 2 years. MAIN OUTCOMES AND MEASURES: Breast cancer incidence in 2000 and incidence-based breast cancer mortality during the 10-year follow-up. Incidence and mortality were calculated for each county and age adjusted to the US population. RESULTS: A cross US counties, there was a positive correlation between the extent of screening and breast cancer incidence (weighted r = 0.54; P <.001) but not with breast cancer mortality (weighted r = 0.00; P =.98). An absolute increase of 10 percentage points in the extent of screening was accompanied by 16% more breast cancer diagnoses (relative rate [RR], 1.16; 95% CI, 1.13-1.19) but no significant change in breast cancer deaths (RR, 1.01; 95% CI, 0.96-1.06). In an analysis stratified by tumor size, we found that more screening was strongly associated with an increased incidence of small breast cancers (<2 cm) but not with a decreased incidence of larger breast cancers (<2 cm). An increase of 10 percentage points in screening was associated with a 25% increase in the incidence of small breast cancers (RR, 1.25; 95% CI, 1.18-1.32) and a 7% increase in the incidence of larger breast cancers (RR, 1.07; 95% CI, 1.02-1.12). CONCLUSIONS AND RELEVANCE: When analyzed at the county level, the clearest result of mammography screening is the diagnosis of additional small cancers. Furthermore, there is no concomitant decline in the detection of larger cancers, which might explain the absence of any significant difference in the overall rate of death from the disease. Together, these findings suggest widespread overdiagnosis.","Breast cancer screening, incidence, and mortality across US counties",JAMA Internal Medicine,Article,9/1/2015,Rediet,Abebe,2015
10.1001/jamanetworkopen.2019.6700,"© 2019 Feldman S et al. JAMA Network Open.Importance: Analyses of female representation in clinical studies have been limited in scope and scale. Objective: To perform a large-scale analysis of global enrollment sex bias in clinical studies. Design, Setting, and Participants: In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and female participants in 43135 articles (792004915 participants) and 13165 records (12977103 participants) were included. Main Outcomes and Measures: Sex bias was defined as the difference between the fraction of female participants in study participants minus prevalence fraction of female participants for each disease category. A total of 1000 bootstrap estimates of sex bias were computed by resampling individual studies with replacement. Sex bias was reported as mean and 95% bootstrap confidence intervals from articles and records in each disease category over time (before or during 1993 to 2018), with studies or participants as the measurement unit. Results: There were 792004915 participants, including 390470834 female participants (49%), in articles and 12977103 participants, including 6351619 female participants (49%), in records. With studies as measurement unit, substantial female underrepresentation (sex bias = -0.05) was observed in 7 of 11 disease categories, especially HIV/AIDS (mean for articles, -0.17 [95% CI, -0.18 to -0.16]), chronic kidney diseases (mean, -0.17 [95% CI, -0.17 to -0.16]), and cardiovascular diseases (mean, -0.14 [95% CI, -0.14 to -0.13]). Sex bias in articles for all categories combined was unchanged over time with studies as measurement unit (range, -0.15 [95% CI, -0.16 to -0.13] to -0.10 [95% CI, -0.14 to -0.06]), but improved from before or during 1993 (mean, -0.11 [95% CI, -0.16 to -0.05]) to 2014 to 2018 (mean, -0.05 [95% CI, -0.09 to -0.02]) with participants as the measurement unit. Larger study size was associated with greater female representation. Conclusions and Relevance: Automated extraction of the number of participants in clinical reports provides an effective alternative to manual analysis of demographic bias. Despite legal and policy initiatives to increase female representation, sex bias against female participants in clinical studies persists. Studies with more participants have greater female representation. Differences between sex bias estimates with studies vs participants as measurement unit, and between articles vs records, suggest that sex bias with both measures and data sources should be reported.",Quantifying Sex Bias in Clinical Studies at Scale with Automated Data Extraction,JAMA Network Open,Article,7/5/2019,Oren,Etzioni,2019
10.1001/jamaneurol.2023.0265,"© 2023 American Medical Association. All rights reserved.IMPORTANCE The benefit of reperfusion therapies for acute ischemic stroke decreases over time. This decreasing benefit is presumably due to the disappearance of salvageable ischemic brain tissue (ie, the penumbra). OBJECTIVE To study the association between stroke onset-to-imaging time and penumbral volume in patients with acute ischemic stroke with a large vessel occlusion. DESIGN, SETTING, AND PARTICIPANTS A retrospective, multicenter, cross-sectional study was conducted from January 1, 2015, to June 30, 2022. To limit selection bias, patients were selected from (1) the prospective registries of 2 comprehensive centers with systematic use of magnetic resonance imaging (MRI) with perfusion, including both thrombectomy-treated and untreated patients, and (2) 1 prospective thrombectomy study in which MRI with perfusion was acquired per protocol but treatment decisions were made with clinicians blinded to the results. Consecutive patients with acute stroke with intracranial internal carotid artery or first segment of middle cerebral artery occlusion and adequate quality MRI, including perfusion, performed within 24 hours from known symptoms onset were included in the analysis. EXPOSURES Time from stroke symptom onset to baseline MRI. MAIN OUTCOMES AND MEASURES Penumbral volume, measured using automated software, was defined as the volume of tissue with critical hypoperfusion (time to maximum >6 seconds) minus the volume of the ischemic core. Substantial penumbra was defined as greater than or equal to 15 mL and a mismatch ratio (time to maximum >6-second volume/core volume) greater than or equal to 1.8. RESULTS Of 940 patients screened, 516 were excluded (no MRI, n = 19; no perfusion imaging, n = 59; technically inadequate perfusion imaging, n = 75; second segment of the middle cerebral artery occlusion, n = 156; unwitnessed stroke onset, n = 207). Of 424 included patients, 226 (53.3%) were men, and mean (SD) age was 68.9 (15.1) years. Median onset-to-imaging time was 3.8 (IQR, 2.4-5.5) hours. Only 16 patients were admitted beyond 10 hours from symptom onset. Median core volume was 24 (IQR, 8-76) mL and median penumbral volume was 58 (IQR, 29-91) mL. An increment in onset-to-imaging time by 1 hour resulted in a decrease of 3.1 mL of penumbral volume (ß coefficient = -3.1; 95% CI, -4.6 to -1.5; P < .001) and an increase of 3.0 mL of core volume (ß coefficient = 3.0; 95% CI, 1.3-4.7; P < .001) after adjustment for confounders. The presence of a substantial penumbra ranged from approximately 80% in patients imaged at 1 hour to 70% at 5 hours, 60% at 10 hours, and 40% at 15 hours. CONCLUSIONS AND RELEVANCE Time is associated with increasing core and decreasing penumbral volumes. Despite this, a substantial percentage of patients have notable penumbra in extended time windows; the findings of this study suggest that a large proportion of patients with large vessel occlusion may benefit from therapeutic interventions.",Quantification of Penumbral Volume in Association With Time From Stroke Onset in Acute Ischemic Stroke With Large Vessel Occlusion,JAMA Neurology,Article,5/8/2023,François,Chollet,2023
10.1002/9781119527183,"© 2020 by The Institute of Electrical and Electronics Engineers, Inc.Simulation and Computational Red Teaming for Problem Solving offers a review of computer simulation that is grounded in a multi-disciplinary approach. The authors present the theoretical foundations of simulation and modeling paradigms from the perspective of an analyst. The book provides the fundamental background information needed for designing and developing consistent and useful simulations. In addition to this basic information, the authors explore several advanced topics. The book's advanced topics demonstrate how modern artificial intelligence and computational intelligence concepts and techniques can be combined with various simulation paradigms for solving complex and critical problems. Authors examine the concept of Computational Red Teaming to reveal how the combined fundamentals and advanced techniques are used successfully for solving and testing complex real-world problems. This important book: Demonstrates how computer simulation and Computational Red Teaming support each other for solving complex problems. Describes the main approaches to modeling real-world phenomena and embedding these models into computer simulations. Explores how a number of advanced artificial intelligence and computational intelligence concepts are used in conjunction with the fundamental aspects of simulation Written for researchers and students in the computational modelling and data analysis fields, Simulation and Computational Red Teaming for Problem Solving covers the foundation and the standard elements of the process of building a simulation and explores the simulation topic with a modern research approach.",Simulation and computational red teaming for problem solving,Simulation and Computational Red Teaming for Problem Solving,Book,10/25/2019,Hussein,Abbass,2019
10.1002/9781119698821.ch4,"© 2021 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved.As the world embraces the evolution of cognitive cyber-physical systems, there is an emergence of systems with the ability to comprehend and act on information efficiently while also supporting human operators to achieve ethically aligned mission objectives consistently. This human-autonomy teaming (HAT) relationship will evolve with technology and increasing levels of autonomy. It is essential to identify the role of trust and systematically develop to that role to minimize challenges in HAT relationships. Life-learning processes are needed to guide system developers to assess meaningful interactions and to refine the critical contexts that require trust continuously. By assessing the presence of trust, cognitive competence, and meaningful interactions, human factors specialists and human systems engineers will also be able to assess the sociotechnical system evolution. This chapter first defines trust, both as a concept and for use in HAT. It then proposes processes for how to assess trust, especially to meet recently proposed IEEE standards and United Nations articles concerning meaningful human control. The chapter then examines how our research program on the herding of sheep using unmanned aerial vehicles (UAVs) is validating HAT concepts and the proposed processes. The program is concerned with teaming farmers with smart autonomous capabilities to assure meaningful farmers’ control. Meaningfulness needs to account for public obligations and ethical responsibilities. This application area potentially solves extant ethical challenges in shepherding in Australia concerning the use of dogs and timely monitoring and action in vast sheep stations while also allowing the investigation of ethical challenges around continually evolving artificial intelligence in the system. Ethically designed HAT systems are about pragmatic trade-offs with prior processes, yet in this future context of life learning of intelligent systems, can now be revised through-life with changing expectations and improvements.",Life learning of smart autonomous systems for meaningful human-autonomy teaming,A Framework of Human Systems Engineering: Applications and Case Studies,Book Chapter,1/1/2020,Hussein,Abbass,2020
10.1002/cpe.1409,"We describe the results of the RDF(S) activity within the Open Grid Forum (http://www.ogf.org) (OGF) Database Access and Integration Services (DAIS) Working Group (http://forge.gridforum.org/projects/ dais-wg) whose objective is to develop standard service-based grid access mechanisms for data expressed in RDF and RDF Schema. We produce two specifications, focused on the provision of SPARQL querying capabilities for accessing RDF data and a set of RDF Schema ontology handling primitives for creating, retrieving, updating, and deleting RDF data. In this paper we present a set of use cases that justify this work and an overview of these specifications, which will enter in editorial process at OGF25. We conclude by outlining the future work that will be made in the context of this standardization process. © 2009 John Wiley & Sons, Ltd.",Accessing RDF(S) data resources in service-based Grid infrastructures,Concurrency and Computation: Practice and Experience,Article,6/10/2009,Asunción Gómez,Pérez,2009
10.1002/jcc.540070508,"SYNCHEM2 is a large knowledge-based domain-specific heuristic problem solving system that applies the methodology of artificial intelligence to the problem domain of synthetic organic chemistry. The program may be used for both synthetic and retrosynthetic exploration of the reaction transform space. We describe some experiments that might lead to the development of a special purpose version of the program for the investigation and prediction of environmental and metabolic processes undergone by chemicals that are released into the biosphere or directly ingested. The test system was used to predict the metabolites of three molecules of considerable biochemical interest, benzo[a]pyrene (known to be metabolically transformed into an active carcinogen), paramethyl substituted phenylalanine (which undergoes the NIH-shift when metabolized), and haloperidol (an important and widely used neuroleptic). In the case of the benzo[a]pyrene, the program predicted all of the known metabolites, as well as several others. All of the 11 compounds predicted as metabolites of 4-methylphenylalanine, which included the major known metabolite that exhibits the NIH-shift, were reasonable. Contained among the predicted metabolic pathways for haloperidol was a credible explanation for the occurrence of a known haloperidol metabolite derived from a substituted phenylacetic acid, but for which no intermediates have been isolated. The program was able to bridge the problem space between haloperidol and the derived hippuric acid by using an unsuspected, but metabolically sound pinacol rearrangement. Copyright © 1986 John Wiley & Sons, Inc.",Computer Simulation of Metabolic Transformation,Journal of Computational Chemistry,Article,1/1/1986,Herbert,Gelernter,1986
10.1002/lno.12152,"© 2022 The Authors. Limnology and Oceanography published by Wiley Periodicals LLC on behalf of Association for the Sciences of Limnology and Oceanography.Ocean warming endangers coastal ecosystems through increased risk of infectious disease, yet detection, surveillance, and forecasting of marine diseases remain limited. Eelgrass (Zostera marina) meadows provide essential coastal habitat and are vulnerable to a temperature-sensitive wasting disease caused by the protist Labyrinthula zosterae. We assessed wasting disease sensitivity to warming temperatures across a 3500 km study range by combining long-term satellite remote sensing of ocean temperature with field surveys from 32 meadows along the Pacific coast of North America in 2019. Between 11% and 99% of plants were infected in individual meadows, with up to 35% of plant tissue damaged. Disease prevalence was 3× higher in locations with warm temperature anomalies in summer, indicating that the risk of wasting disease will increase with climate warming throughout the geographic range for eelgrass. Large-scale surveys were made possible for the first time by the Eelgrass Lesion Image Segmentation Application, an artificial intelligence (AI) system that quantifies eelgrass wasting disease 5000× faster and with comparable accuracy to a human expert. This study highlights the value of AI in marine biological observing specifically for detecting widespread climate-driven disease outbreaks.",Disease surveillance by artificial intelligence links eelgrass wasting disease to ocean warming across latitudes,Limnology and Oceanography,Article,7/1/2022,Carla,Gomes,2022
10.1002/wcms.1608,"© 2022 The Authors. WIREs Computational Molecular Science published by Wiley Periodicals LLC.Development of new products often relies on the discovery of novel molecules. While conventional molecular design involves using human expertise to propose, synthesize, and test new molecules, this process can be cost and time intensive, limiting the number of molecules that can be reasonably tested. Generative modeling provides an alternative approach to molecular discovery by reformulating molecular design as an inverse design problem. Here, we review the recent advances in the state-of-the-art of generative molecular design and discusses the considerations for integrating these models into real molecular discovery campaigns. We first review the model design choices required to develop and train a generative model including common 1D, 2D, and 3D representations of molecules and typical generative modeling neural network architectures. We then describe different problem statements for molecular discovery applications and explore the benchmarks used to evaluate models based on those problem statements. Finally, we discuss the important factors that play a role in integrating generative models into experimental workflows. Our aim is that this review will equip the reader with the information and context necessary to utilize generative modeling within their domain. This article is categorized under: Data Science > Artificial Intelligence/Machine Learning.",Generative models for molecular discovery: Recent advances and challenges,Wiley Interdisciplinary Reviews: Computational Molecular Science,Review,9/1/2022,Regina,Barzilay,2022
10.1002/wcs.105,"Analogical mapping is a core process in human cognition. A number of valuable computational models of analogy have been created, capturing aspects of how people compare representations, retrieve potential analogs from memory, and learn from the results. In the past 25 years, this area has progressed rapidly, fueled by strong collaboration between psychologists and Artificial Intelligence (AI) scientists, with contributions from linguists and philosophers as well. There is now considerable consensus regarding the constraints governing the mapping process. However, computational models still differ in their focus, with some aimed at capturing the range of analogical phenomena at the cognitive level and others aimed at modeling how analogical processes might be implemented in neural systems. Some recent work has focused on modeling interactions between analogy and other processes, and on modeling analogy as a part of larger cognitive systems. © 2010 John Wiley & Sons, Ltd.",Computational models of analogy,Wiley Interdisciplinary Reviews: Cognitive Science,Review,5/1/2011,Ken,Forbus,2011
10.1006/ccog.1999.0391,"Baars (1988, 1997) has proposed a psychological theory of consciousness, called global workspace theory. The present study describes a software agent implementation of that theory, called ""Conscious"" Mattie (CMattie). CMattie operates in a clerical domain from within a UNIX operating system, sending messages and interpreting messages in natural language that organize seminars at a university. CMattie fleshes out global workspace theory with a detailed computational model that integrates contemporary architectures in cognitive science and artificial intelligence. Baars (1997) lists the psychological ""facts that any complete theory of consciousness must explain"" in his appendix to In the Theater of Consciousness; global workspace theory was designed to explain these ""facts."" The present article discusses how the design of CMattie accounts for these facts and thereby the extent to which it implements global workspace theory. © 1999 Academic Press.",A Software Agent Model of Consciousness,Consciousness and Cognition,Article,9/1/1999,Stan,Franklin,1999
10.1006/ijhc.1994.1046,"Dealing with uncertainty problems in intelligent systems has attracted a lot of attention in the AI community. Quite a few techniques have been proposed. Among them, the Dempster-Shafer theory of evidence (DS theory) has been widely appreciated. In DS theory, Dempster’s combination rule plays a major role. However, it has been pointed out that the application domains of the rule are rather limited and the application of the theory sometimes gives unexpected results. We have previously explored the problem with Dempster’s combination rule and proposed an alternative combination mechanism in generalized incidence calculus. In this paper we give a comprehensive comparison between generalized incidence calculus and the Dempster-Shafer theory of evidence. We first prove that these two theories have the same ability in representing evidence and combining DS-independent evidence. We then show that the new approach can deal with some dependent situations while Dempster’s combination rule cannot. Various examples in the paper show the ways of using generalized incidence calculus in expert systems. © 1994 Academic Press, Inc.",A comprehensive comparison between generalized incidence calculus and the dempster-shafer theory of evidence,International Journal of Human Computer Studies,Article,1/1/1994,Alan,Bundy,1994
10.1006/ssre.2000.0687,"Using differential item validity (DIV) methodology, this study investigated whether the items of the AIDS-Related Social Skills (ASAS) questionnaire had the same interpretation by gender and country. Polytomous logistic regression was used to test if the coefficients for individual items were the same by gender and country. The subjects (n = 1133) were Black Anglophone African 10th-grade students from Nigeria (n = 396), Kenya (n = 280), Zimbabwe (n = 319), and Sierra Leone (n = 138). The analyses showed that many items had significant DIV. Because 12 of the 30 test items (i.e., 40%) showed significant DIV, it suggested that the ASAS may not be internally valid by gender and across countries. Further qualitative work is needed to understand the extensive DIV that was statistically found in this study. © 2001 Academic Press.",Assessing differential item validity of the AIDS-related social skills questionnaire among african adolescents,Social Science Research,Article,1/1/2001,W,Ross,2001
10.1007/1-4020-8151-0_32,"Ontologies implemented in RDF(S), DAML+OIL, and OWL should be evaluated from the point of view of knowledge representation before using them in Semantic Web applications. Several language-dependent ontology validation tools and ontology platforms, such as OilEd with FaCT, can be used in order to evaluate RDF(S), DAML+OIL and OWL ontologies. This paper offers two main contributions. The first of these exams whether previous ontology tools detect knowledge representation problems in RDF(S), DAML+OIL, and OWL concept taxonomies. Indeed, such tools do not focus on detecting inconsistencies and redundancies in concept taxonomies. The second contribution is ODEval, a language-dependent tool for evaluating, from the point of view of knowledge representation, concept taxonomies in ontologies implemented in such languages. ODEval complements previous ontology tools when we want to evaluate RDF(S), DAML+OIL, and OWL concept taxonomies. © 2004 Springer Science + Business Media, Inc.","ODEval: A tool for evaluating RDF(S), DAML+OIL, and OWL concept taxonomies",IFIP Advances in Information and Communication Technology,Conference Paper,1/1/2004,Asunción Gómez,Pérez,2004
10.1007/11424550_12,"Exploiting the structure of a document allows for more powerful information retrieval techniques. In this article a basic approach is discussed for the retrieval of XML document fragments. Based on a vector-space model for text retrieval we aim at investigating various strategies that influence the retrieval performance of an XML-based IR system. The first extension of the system uses a schema-based approach that assumes that authors tag their text to emphasise on particular pieces of content that are of importance. Based on the schema used by the document collection, the system can easily derive the children of mixed content nodes. Our hypothesis is that those child nodes are more important than other nodes. The second approach discussed here is based on a horizontal fragmentation of the inverse document frequencies, used by the vector space model. The underlying assumption states that the distribution of terms is related to the semantical structure of the document. However, we observed that the IEEE collection is not a good example of semantic tagging. The third approach investigates how the performance of the retrieval system can improve for the 'Content Only' task by using a set of a-priori defined cut-off nodes that define 'logical' document fragments that are of interest to a user. © Springer-Verlag Berlin Heidelberg 2005.",The utrecht blend: Basic ingredients for an XML retrieval system,Lecture Notes in Computer Science,Conference Paper,1/1/2005,Virginia,Dignum,2005
10.1007/11564096_37,"In this paper we consider latent variable models and introduce a new u-likelihood concept for estimating the distribution over hidden variables. One can derive an estimate of parameters from this distribution. Our approach differs from the Bayesian and Maximum Likelihood (ML) approaches. It gives an alternative to Bayesian inference when we don't want to define a prior over parameters and gives an alternative to the ML method when we want a better estimate of the distribution over hidden variables. As a practical implementation, we present a u-updating algorithm based on the mean field theory to approximate the distribution over hidden variables from the u-likelihood. This algorithm captures some of the correlations among hidden variables by estimating reaction terms. Those reaction terms are found to penalize the likelihood. We show that the u-updating algorithm becomes the EM algorithm as a special case in the large sample limit. The useful behavior of our method is confirmed for the case of mixture of Gaussians by comparing to the EM algorithm. © Springer-Verlag Berlin Heidelberg 2005.",u-likelihood and u-updating algorithms: Statistical inference in latent variable models,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2005,Zoubin,Ghahramani,2005
10.1007/11564096_73,"Processes involving change over time, uncertainty, and rich relational structure are common in the real world, but no general algorithms exist for learning models of them. In this paper we show how Markov logic networks (MLNs), a recently developed approach to combining logic and probability, can be applied to time-changing domains. We then show how existing algorithms for parameter and structure learning in MLNs can be extended to this setting. We apply this approach in two domains: modeling the spread of research topics in scientific communities, and modeling faults in factory assembly processes. Our experiments show that it greatly outperforms purely logical (ILP) and purely probabilistic (DBN) learners. © Springer-Verlag Berlin Heidelberg 2005.",Learning models of relational stochastic processes,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,12/1/2005,Pedro,Domingos,2005
10.1007/11801603_1,"AI systems must be able to learn, reason logically, and handle uncertainty. While much research has focused on each of these goals individually, only recently have we begun to attempt to achieve all three at once. In this talk, I describe Markov logic, a representation that combines first-order logic and probabilistic graphical models, and algorithms for learning and inference in it. Syntactically, Markov logic is first-order logic augmented with a weight for each formula. Semantically, a set of Markov logic formulas represents a probability distribution over possible worlds, in the form of a Markov network with one feature per grounding of a formula in the set, with the corresponding weight. Formulas are learned from relational databases using inductive logic programming techniques. Weights can be learned either generatively (using pseudo-likelihood optimization) or discriminatively (using a voted perceptron algorithm). Inference is performed by a weighted satisfiability solver or by Markov chain Monte Carlo, operating on the minimal subset of the ground network required for answering the query. Experiments in link prediction, entity resolution and other problems illustrate the promise of this approach. This work, joint with Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla, is described in further detail in Domingos et al. [1]. An' open-source implementation of Markov logic and the algorithms described in this talk is available in the Alchemy package [2]. © Springer-Verlag Berlin Heidelberg 2006.","Learning, logic, and probability: A unified view",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2006,Pedro,Domingos,2006
10.1007/11874850_2,"AI systems must be able to learn, reason logically, and handle uncertainty. While much research has focused on each of these goals individually, only recently have we begun to attempt to achieve all three at once. In this talk, I describe Markov logic, a representation that combines first-order logic and probabilistic graphical models, and algorithms for learning and inference in it. Syntactically, Markov logic is first-order logic augmented with a weight for each formula. Semantically, a set of Markov logic formulas represents a probability distribution over possible worlds, in the form of a Markov network with one feature per grounding of a formula in the set, with the corresponding weight. Formulas are learned from relational databases using inductive logic programming techniques. Weights can be learned either generatively (using pseudo-likelihood optimization) or discriminatively (using a voted perceptron algorithm). Inference is performed by a weighted satisfiability solver or by Markov chain Monte Carlo, operating on the minimal subset of the ground network required for answering the query. Experiments in link prediction, entity resolution and other problems illustrate the promise of this approach. This work, joint with Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla, is described in further detail in Domingos et al. [1]. An opensource implementation of Markov logic and the algorithms described in this talk is available in the Alchemy package [2]. © Springer-Verlag Berlin Heidelberg 2006.","Learning, logic, and probability: A unified view",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2006,Pedro,Domingos,2006
10.1007/11903697_7,"Evolutionary Learning Classifier Systems (LCSs) are rule based systems that have been used effectively in concept learning. XCS is a prominent LCS that uses genetic algorithms and reinforcement learning techniques. In traditional machine learning (ML), early stopping has been investigated extensively to the extent that it is now a default mechanism in many systems. However, there has been a belief that EC methods are more resilient to overfitting. Therefore, this topic is under-investigated in the evolutionary computation literature and has not been investigated in LCS. In this paper, we show that it is necessary to stop evolution in LCS using a stopping criteria other than a maximum number of generations and that evolution may suffer from overfitting similar to other ML methods. © Springer-Verlag Berlin Heidelberg 2006.",The role of early stopping and population size in XCS for intrusion detection,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2006,Hussein,Abbass,2006
10.1007/3-540-34518-3_1,"In 1991, the DARPA Knowledge Sharing Effort ([88], p. 37) envisioned a new way to build intelligent systems. It proposed the following: Building knowledge-based systems today usually entails constructing new knowledge bases from scratch. It could be instead done by assembling reusable components. System developers would then only need to worry about creating the specialized knowledge and reasoners new to the specific task of their system. This new system would interoperate with existing systems, using them to perform some of its reasoning. In this way, declarative knowledge, problem-solving techniques and reasoning services would all be shared among systems. This approach would facilitate building bigger and better systems and cheaply. Static knowledge is modeled by means of ontologies while problem solving methods specify generic reasoning mechanisms. Both types of components can be viewed as complementary entities that can be used to configure new knowledge-based systems from existing reusable components. Since DARPA's idea, considerable progress has been made in developing the conceptual bases to build technology that allows reusing and sharing knowledge components. Ontologies and problem solving methods (PSMs) have been created to share and reuse knowledge and reasoning behavior across domains and tasks. In this evolution, the most important fact has been the emergence of the Semantic Web. According to [10], the Semantic Web is an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. This cooperation can be achieved by using shared knowledge components, and so ontologies and PSMs have become key instruments in developing the Semantic Web. Currently, ontologies are widely used in knowledge engineering, artificial intelligence and computer science, in applications related to knowledge management, natural language processing, e-commerce, intelligent integration information, information retrieval, database design and integration, bio-informatics, education, etc. In this chapter, we present the basics about ontologies, and show what activities should be carried out during the ontology development process, what principles should be followed in ontology design, and what methods, methodologies, software tools and languages are available to give support to each one of these activities. First, in Sect. 1.2, we define the word 'ontology' and we briefly explaining its roots in philosophy. Section 1.3 is devoted to explain which are the main components that can be used to model ontologies. In Sect. 1.4, we present the main ontology design principles. In Sect. 1.5, we describe the ontology development process in the context of the Semantic Web, where ontologies can be highly distributed and present many links among each other (hence the notion of networked ontologies). In Sect. 1.6, we describe the development of ontologies and the life cycle. In Sect. 1.7, we describe the methods, methodologies and tools commonly used for the whole ontology development process or only for specific activities. Among them we pay attention to those aimed at ontology learning, which reduce the effort needed during the knowledge acquisition process; at ontology merging, which generates a unique target ontology from several source ontologies; at ontology alignment, which establishes different types of mappings between ontologies (hence preserving the original ones); and at ontology evaluation, which evaluates ontology content. In the implementation activity description, we present ontology languages that can be used to implement ontologies. Finally, conclusions and future lines of research are presented in Sect. 1.8. © 2006 Springer-Verlag Berlin Heidelberg.","Ontological engineering: Principles, methods, tools and languages",Ontologies for Software Engineering and Software Technology,Book Chapter,12/1/2006,Asunción Gómez,Pérez,2006
10.1007/3-540-36189-8_20,"© Springer-Verlag Berlin Heidelberg 2002.Increasing computational capabilities and decreasing costs of commonly used devices, and the concomitant development of short range, ad-hoc networking technologies, will help realize the pervasive computing paradigm. In this paper, we present the use of semantically rich descriptions for devices to discover, and cooperate with, others in their vicinity. In particular, we describe our ongoing projects that have usedDAMLfor service discovery, service composition, data management, and trust based security in pervasive computing environments.","Me-services: A framework for secure and personalized discovery, composition and management of services in pervasive environments",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2002,Tim,Finin,2002
10.1007/3-540-36285-1_24,"Interest in XML databases has been growing over the last few years. In this paper, we study the problem of incorporating probabilistic information into XML databases. We propose the Probabilistic Interval XML (PIXml for short) data model in this paper. Using this data model, users can express probabilistic information within XML markups. In addition, we provide two alternative formal model-theoretic semantics for PIXml data. The first semantics is a ""global"" semantics which is relatively intuitive, but is not directly amenable to computation. The second semantics is a ""local"" semantics which is more amenable to computation. We prove several results linking the two semantics together. To our knowledge, this is the first formal model theoretic semantics for probabilistic interval XML. We then provide an operational semantics that may be used to compute answers to queries and that is correct for a large class of probabilistic instances. © Springer-Verlag Berlin Heidelberg 2003.",Probabilistic interval XML,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2003,Lise,Getoor,2003
10.1007/3-540-39967-4_7,"© Springer-Verlag Berlin Heidelberg 2000.The interchange of ontologies across the World Wide Web (WWW) and the cooperation among heterogeneous agents placed on it is the main reason for the development of a new set of ontology specification languages, based on new web standards such as XML or RDF. These languages (SHOE, XOL, RDF, OIL, etc) aim to represent the knowledge contained in an ontology in a simple and human-readable way, as well as allow for the interchange of ontologies across the web. In this paper, we establish a common framework to compare the expressiveness and reasoning capabilities of „traditional“ ontology languages (Ontolingua, OKBC, OCML, FLogic, LOOM) and „web-based“ ontology languages, and conclude with the results of applying this framework to the selected languages.",A roadmap to ontology specification languages,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2000,Asunción Gómez,Pérez,2000
10.1007/3-540-45584-1_12,"© 2001 Springer-Verlag Berlin Heidelberg.Effective use of the vast quantity of information now available on the web will require the use of ""Semantic Web"" markup languages such as the DARPA Agent Markup Language (DAML). Such languages will enable the automated gathering and processing of much information that is currently available but insufficiently utilized. Effectively, such languages will facilitate the integration of multi-agent systems with the existing information infrastructure. As part of our exploration of SemanticWeb technology, and DAML in particular, we have constructed ITTALKS, a web-based system for automatic and intelligent notifi-cation of information technology talks. In this paper, we describe the ITTALKS system, and discuss the numerous ways in which the use of Semantic Web concepts and DAML extend its ability to provide an intelligent online service to both the human community and the agents assisting them.",ITTALKS: An application of agents in the semantic web,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2001,Tim,Finin,2001
10.1007/3-540-48775-1_29,© Springer-Verlag Berlin Heidelberg 1999.This paper emphasizes the interest of XML meta-language for corporate knowledge management and presents an experiment of enterprise-ontologyguided search in XML documents constituting a part of a corporate memory.,Exploitation of XML for corporate knowledge management,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1999,Rose,Dieng-Kuntz,1999
10.1007/3-540-57322-4_19,"© Springer-Verlag Berlin Heidelberg 1993.Reasoning about time is essential for applications in artificial intelligence and in many other disciplines. Given certain explicit relationships between a set of events, we would like to have the ability to infer additional relationships which are implicit in those given. For example, the transitivity of “before” and “contains” may allow us to infer information regarding the sequence of events. Such inferences are essential in story understanding, planning and causal reasoning. There are a great number of practical problems in which one is interested in constructing a time line where each particular event or phenomenon corresponds to an interval representing its duration. These include seriation in archeology, behavioral psychology, temporal reasoning, scheduling, and combinatorics. Other applications arise in non-temporal context, for example, in molecular biology, arrangement of DNA segments along a linear DNA chain involves similar problems. Interval consistency problems deal with events, each of which is assumed to be an interval on the real line or on any other linearly ordered set, and reasoning about such intervals when the precise topological relationships between them is unknown or only partially specified. In Golumbic and Shamir (1991), we relate the two notions of interval algebra from the temporal reasoning community and interval graphs from the combinatorics community, obtaining new algorithmic complexity results of interest to both disciplines. Several versions of the satisfiability, minimum labeling and all consistent solutions problems for temporal (interval) data are investigated. The satisfiability question is shown to be NP-Complete even when restricting the possible interval relationships to subsets of the relations intersection and precedence only. On the other hand, we give efficient algorithm for several other restrictions of the problem.",Reasoning about time,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1993,Martin Charles,Golumbic,1993
10.1007/3-540-60343-3_25,"© Springer-Verlag Berlin Heidelberg 1995.This paper explores and reasons about the interplay between symbolic and continuous representations. We first provide some historical perspective on the signal and symbol integration as viewed by the Artificial Intelligence (AI), Robotics and Computer Vision communities. The domain of autonomous robotic agents residing in the dynamically changing environments anchors well different aspects of this integration and allows us to look at the problem in its entirety. Models of reasoning, sensing and control actions of such agents determine three different dimensions for discretization of the agent-world behavioral state space. The design and modeling of robotic agents, where these three aspects have to be closely tied together, provide a good experimental platform for addressing the signal-to-symbol-to-signal transformation problem. We present some experimental results from the domain of cooperating mobile agents involved in tasks of navigation and manipulation.",The problem of signal and symbol integration: A study of cooperative mobile autonomous agent behaviors,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1995,Ruzena,Bajcsy,1995
10.1007/3540608052_77,"© Springer-Verlag Berlin Heidelberg 1996.This chapter discusses the desirable features of languages and protocols for communication among intelligent information agents. These desiderata are divided into seven categories: form, content, semantics, implementation, networking, environment, and reliability. The Knowledge Query and Manipulation Language (KQML) is a new language and protocol for exchanging information and knowledge. This work is part of a larger effort, the ARPA Knowledge Sharing Effort, which is aimed at developing techniques and methodologies for building large-scale knowledge bases that are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML is described and evaluated as an agent communication language relative to the desiderata.",Evaluation of KQML as an agent communication language,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1996,Tim,Finin,1996
10.1007/978-0-387-76872-4,"This book describes Probabilistic Logic Networks (PLN), a novel conceptual, mathematical and computational approach to uncertain inference. Going beyond prior probabilistic approaches to uncertain inference, PLN encompasses such ideas as induction, abduction, analogy, fuzziness and speculation, and reasoning about time and causality. The book provides an overview of PLN in the context of other approaches to uncertain inference. Topics addressed in the text include: •the basic formalism of PLN knowledge representation •the conceptual interpretation of the terms used in PLN •an indefinite probability approach to quantifying uncertainty, providing a general method for calculating the ""weight-of-evidence"" underlying the conclusions of uncertain inference •specific PLN inference rules and the corresponding truth-value formulas used to determine the strength of the conclusion of an inference rule from the strengths of the premises •large-scale inference strategies •inference using variables •indefinite probabilities involving quantifiers •inheritance based on properties or patterns •the Novamente Cognition Engine, an application of PLN •temporal and causal logic in PLN Researchers and graduate students in artificial intelligence, computer science, mathematics and cognitive sciences will find this novel perspective on uncertain inference a thought-provoking integration of ideas from a variety of other lines of inquiry. © 2009 Springer Science+Business Media, LLC. All rights reserved.",Probabilistic logic networks: A comprehensive framework for uncertain inference,Probabilistic Logic Networks: A Comprehensive Framework for Uncertain Inference,Book,12/1/2009,Ben,Goertzel,2009
10.1007/978-1-4020-6710-5_29,"Twenty-first-century technologies will allow the creation of massively intelligent machines, many trillions of times as smart, fast, and durable as humans. Issues concerning industrial, consumer, and military applications of mobile autonomous robots, cyborgs, and computer-based AI systems could divisively split humanity into ideological camps regarding whether artilects (artificial intellects) should be built or not. The artilect debate, unlike any before it, could dominate the 21st-century political landscape, and has the potential to cause conflict on a global scale. Research is needed to inform policy and individual decisions; and healthy debate should be initiated now to prepare institutions and individuals alike for the impact of AI. © 2009 Springer Science+Business Media B.V.","The artilect debate: Why build superhuman machines, and why not?",Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer,Book Chapter,12/1/2009,Hugo de,Garis,2009
10.1007/978-1-4939-9442-7_14,"© 2019, Springer Science+Business Media, LLC, part of Springer Nature.The current situation in microarray data analysis and prospects for the future are briefly discussed in this chapter, in which the competition between microarray technologies and high-throughput technologies is considered under a data analysis view. The up-to-date limitations of DNA microarrays are important to forecast challenges and future trends in microarray data analysis; these include data analysis techniques associated with an increasing sample sizes, new feature selection methods, deep learning techniques, covariate significance testing as well as false discovery rate methods, among other procedures for a better interpretability of the results.",Challenges and Future Trends for Microarray Analysis,Methods in Molecular Biology,Book Chapter,1/1/2019,Amparo Alonso,Betanzos,2019
10.1007/978-3-030-00794-2_29,"© Springer Nature Switzerland AG 2018.Machine comprehension of various forms of semantically similar questions with same or similar answers has been an ongoing challenge. Especially in many industrial domains with limited set of questions, it is hard to identify proper semantic match for a newly asked question having the same answer but presented in different lexical form. This paper proposes a linguistically motivated taxonomy for English questions and an effective approach for question matching by combining deep learning models for question representations with general taxonomy based features. Experiments performed on short datasets demonstrate the effectiveness of the proposed approach as better matching classification was observed by coupling the standard distributional features with knowledge-based methods.",Semantic question matching in data constrained environment,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-030-03062-9_9,"© Springer Nature Switzerland AG 2018.The European Initiative on Smart Cities [2] is an effort by the European Commission [4] to improve quality of life throughout Europe, while progressing toward energy and climate objectives. Many of its goals are relevant to and desirable in the world at large. We propose that it is essential that artificial agents in a Smart City have theories of the minds of its inhabitants. We describe a scenario in which such theories are indispensable, and cannot be adequately and usefully captured by current forms of ambient intelligence. Then, we show how a new form of distributed, multi-agent artificial intelligence, Tentacular AI, which among other things entails a capacity for reasoning and planning based in highly expressive cognitive calculi (logics), is able to intelligently address this situation.",Toward a smart city using tentacular AI,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Selmer,Bringsjord,2018
10.1007/978-3-030-04212-7_57,"© Springer Nature Switzerland AG 2018.An important component of every dialog system is understanding the language popularly known as Spoken Language Understanding (SLU). Intent detection (ID) and slot filling (SF) are the two very important and inter-related tasks of SLU. In this paper, we propose a deep learning based multi-task ensemble model that can perform both intent detection and slot filling tasks together. We use a deep bi-directional recurrent neural network (RNN) with long short term memory (LSTM) and gated recurrent unit (GRU) as the base-level classifiers. A multi-layer perceptron (MLP) framework is used to combine the outputs together. A combined word embedding representation is used to train the model obtained from both Glove and word2vec. This is further augmented with the syntactic Part-of-Speech (PoS) information. On the benchmark ATIS dataset, our experiments show that the proposed ensemble multi-task model (MTM) achieves better results than the individual models and the existing state-of-the-art systems. Experiments on the another dataset, TRAINS also proves that the proposed multi-task ensemble model is more effective compared to the individual models.",A deep learning based multi-task ensemble model for intent detection and slot filling in spoken language understanding,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-030-04221-9_15,"© 2018, Springer Nature Switzerland AG.In this paper, we propose a novel neural network based architecture which incorporates character, word and lexicon level information to predict the degree of intensity for sentiment and emotion. At first we develop two deep learning models based on Long Short Term Memory (LSTM) & Convolutional Neural Network (CNN), and a feature based model. Each of these models takes as input a fusion of various representations obtained from the characters, words and lexicons. A Multi-Layer Perceptron (MLP) network based ensemble model is then constructed by combining the outputs of these three models. Evaluation on the benchmark datasets related to sentiment and emotion shows that our proposed model attains the state-of-the-art performance.","Deep ensemble model with the fusion of character, word and lexicon level information for emotion and sentiment prediction",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-030-21074-8_17,"© 2019, Springer Nature Switzerland AG.The assessment of retinal and choroidal thickness derived from spectral domain optical coherence tomography (SD-OCT) images is an important clinical and research task. Current OCT instruments allow the capture of densely sampled, high-resolution cross-sectional images of ocular tissues. The extensive nature of such datasets makes the manual delineation of tissue boundaries time-consuming and impractical, especially for large datasets of images. Therefore, the development of reliable and accurate methods to automatically segment tissue boundaries in OCT images is fundamental. In this work, two different deep learning methods; convolutional neural networks (CNN) and recurrent neural networks (RNN) are evaluated to calculate the probability of the retinal and choroidal boundaries of interest to be located in a specific position within the SD-OCT images. The method is initially trained using small image patches centred around the three boundaries of interest. After that, the method can be used to provide a per-layer probability map that marks the most likely location of the boundaries. To convert each layer-probability map into a boundary position, the map is subsequently traced using a graph-search method. The effect of the network architecture (CNN vs RNN), patch size, and image intensity compensation on the performance and subsequent boundary segmentation is presented. The results are compared with manual boundary segmentation as well as a previously proposed method based on standard image analysis techniques.",Automatic Retinal and Choroidal Boundary Segmentation in OCT Images Using Patch-Based Supervised Machine Learning Methods,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2019,Michael,Collins,2019
10.1007/978-3-030-23281-8_30,"© 2019, Springer Nature Switzerland AG.The phenomenal growth in web information has nourished research endeavours for automatic fact checking, or fake news and/or misinformation detection. This is one of the very emerging and challenging problems in Natural Language Processing (NLP), Machine Learning (ML) and Data Science. One such problem relates to estimating the veracity of a news story, which is a complex and deep problem. The very recently released Fake News Challenge Stage 1 (FNC-1) dataset introduced the benchmark FNC stage-1: stance detection task. This task could be an effective first step towards building a robust fact checking system. In this paper, we correlate this stance detection problem with Textual Entailment (TE). We present the systems which are based on statistical machine learning (ML), Deep Learning (DL), and a combination of both. Empirical evaluation shows encouraging performance, outperforming the state-of-the-art system.",A Novel Approach Towards Fake News Detection: Deep Learning Augmented with Textual Entailment Features,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2019,Pushpak,Bhattacharyya,2019
10.1007/978-3-030-24074-5_5,"© 2020, Springer Nature Switzerland AG.Many discussions about robotics and Artificial Intelligence (AI) focus on far futurescenarios such as superintelligence, but for the near future ethics of robotics AI it is necessary to think about more concrete ethical issues that pervade the daily use of the technologies. This talks gives a very brief overview of ethical and societal issues raised by robotics and AI, with a focus on inclusive robotics. It also offers some remarks on robot and AI policy.",Inclusive Robotics and AI – Some Urgent Ethical and Societal Issues,Biosystems and Biorobotics,Book Chapter,1/1/2020,Mark,Coeckelbergh,2020
10.1007/978-3-030-28619-4_3,"© 2020, Springer Nature Switzerland AG.In the last decade, deep learning has revolutionized various components of the conventional robot autonomy stack including aspects of perception, navigation and manipulation. There have been numerous advances in perfecting individual tasks such as scene understanding, visual localization, end-to-end navigation and grasping, which has given us a critical understanding on how to create individual architectures for a specific task. This now brings us to the question, as to whether this disjoint learning of models for robotic tasks, effective in the real-world and whether it is scalable? And more generally, is training task specific models on task specific datasets beneficial to architecting robot intelligence as a whole? In this paper, we argue that multimodel learning or joint multi-task learning is an effective strategy for enabling robots to excel across multiple domains. We describe how multimodel learning can facilitate generalization to unseen scenarios by utilizing domain-specific cues from auxiliary tasks and discuss some of the current mechanisms that can be employed to design multimodel frameworks for robot autonomy.",Perspectives on Deep Multimodel Robot Learning,Springer Proceedings in Advanced Robotics,Book Chapter,1/1/2020,Wolfram,Burgard,2020
10.1007/978-3-030-32226-7_82,"© 2019, Springer Nature Switzerland AG.The scarcity of richly annotated medical images is limiting supervised deep learning based solutions to medical image analysis tasks, such as localizing discriminatory radiomic disease signatures. Therefore, it is desirable to leverage unsupervised and weakly supervised models. Most recent weakly supervised localization methods apply attention maps or region proposals in a multiple instance learning formulation. While attention maps can be noisy, leading to erroneously highlighted regions, it is not simple to decide on an optimal window/bag size for multiple instance learning approaches. In this paper, we propose a learned spatial masking mechanism to filter out irrelevant background signals from attention maps. The proposed method minimizes mutual information between a masked variational representation and the input while maximizing the information between the masked representation and class labels. This results in more accurate localization of discriminatory regions. We tested the proposed model on the ChestX-ray8 dataset to localize pneumonia from chest X-ray images without using any pixel-level or bounding-box annotations.",InfoMask: Masked Variational Latent Representation to Localize Chest Disease,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2019,Yoshua,Bengio,2019
10.1007/978-3-030-34784-0_1,"© 2020, Springer Nature Switzerland AG.The field of Neuroergonomics sits at the interface between humans’ brains and humans’ working environment with an aim to improve the work humans do. In this chapter, I will review the scientific journey that led to the birth of Neuroergonomics. The journey starts from the Biocybernetics and Brain–Computer Interfaces projects in 1960s, followed by work on adaptive aiding in 1970s, and all the way to early 2000s with work on Augmented Cognition and Neuroergonomics. An extension to Neuroergonomics gave birth to Cognitive-Cyber Symbiosis, whereby Neuroergonomics is augmented with artificial intelligence agents that act as relationship managers between the human brain and the information-centric work environment. Some challenges facing these fields today are then discussed using a human-swarm teaming lens.",An Introduction to Neuroergonomics: From Brains at Work to Human-Swarm Teaming,Cognitive Science and Technology,Book Chapter,1/1/2020,Hussein,Abbass,2020
10.1007/978-3-030-34869-4_11,"© 2019, Springer Nature Switzerland AG.The self-organizing map (SOM), which is a type of neural network, helps in the exploratory phase of data mining by projecting the input data into a lower-dimensional map consisting of a grid of neurons. In recent years, SOM has also been applied for classification of data points. The prominent utility of SOM based classification is evident from the use of no labeled data during training. In this paper, a self-organizing map based algorithm is proposed to solve the multi-label classification problem, named as ML-SOM. SOM follows an unsupervised training process to learn the topological structure of the training points. At testing-phase, a testing instance can be mapped to a specific neuron in the network and it’s label can be determined using the training instances mapped to that specific neuron and nearby neurons. Thus in this paper, we have considered the neighborhood information of SOM to determine the label vector of testing instances. Experiments were performed on five multi-labeled datasets and performance of the proposed system is compared with various state-of-the-art methods showing competitive performance. Results are also validated using statistical significance t-test.",Incorporation of Neighborhood Concept in Enhancing SOM Based Multi-label Classification,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2019,Pushpak,Bhattacharyya,2019
10.1007/978-3-030-36802-9_71,"© Springer Nature Switzerland AG 2019.In the current paper, a system for multi-document summarization (MLDS) is developed which simultaneously optimized different quality measures to obtain a good summary. These measures include anti-redundancy, coverage, and, readability. For optimization, multi-objective binary differential evolution (MBDE) is utilized which is an evolutionary algorithm. MBDE consists of a set of solutions and each solution represents a subset of sentences to be selected in the summary. Generally, MBDE uses a single DE variant, but, here, an ensemble of two different DE variants measuring diversity among solutions and convergence towards the global optimal solution, respectively, is employed for efficient search. Three versions of the proposed model are developed varying the syntactic/semantic similarity between sentences and DE parameters selection strategy. Results are evaluated on the standard DUC datasets using ROUGE-2 measure and significant improvements of (formula presented) and (formula presented) are attained by the proposed approach over the two exiting techniques.",Multi-document Summarization Using Adaptive Composite Differential Evolution,Communications in Computer and Information Science,Conference Paper,1/1/2019,Pushpak,Bhattacharyya,2019
10.1007/978-3-030-36808-1_72,"© Springer Nature Switzerland AG 2019.Multi-modal sentiment and emotion analysis have been an emerging and prominent field nowadays at the intersection of natural language processing, deep learning, machine learning, computer vision, and speech processing. Sentiment and emotion prediction model finds the attitude of a speaker or writer towards any discussion, debate, event, document or topic. It can be expressed in different ways like the words spoken, energy and tone while delivering words, accompanying facial expressions, gestures, etc. Moreover related and similar tasks generally depend on each other and are predicted better if solved through a joint framework. In this paper, we present a multi-task gated contextual cross-modal attention framework which considers all the three modalities (viz. text, acoustic and visual) and multiple utterances for sentiment and emotion prediction together. We evaluate our proposed approach on CMU-MOSEI dataset for sentiment and emotion prediction. Evaluation results depict that our proposed approach extracts co-relation among the three modalities and attains an improvement over the previous state-of-the-art models.",Multi-task gated contextual cross-modal attention framework for sentiment and emotion analysis,Communications in Computer and Information Science,Conference Paper,1/1/2019,Pushpak,Bhattacharyya,2019
10.1007/978-3-030-54173-6_17,"© The Author(s) 2021. All rights reserved.The field of social robotics is fast developing and will have wide implications especially within health care, where much progress has been made towards the development of ""companion robots."" Such robots provide therapeutic or monitoring assistance to patients with a range of disabilities over a long timeframe. Preliminary results show that such robots may be particularly beneficial for use with individuals who suffer from neurodegenerative pathologies. Treatment can be accorded around the clock and with a level of patience rarely found among human healthcare workers. Several elements are requisite for the effective deployment of companion robots. They must be able to detect human emotions and in turn mimic human emotional reactions as well as having an outward appearance that corresponds to human expectations about their caregiving role. This chapter presents laboratory findings on AI-systems that enable robots to recognize specific emotions and to adapt their behavior accordingly. Emotional perception by humans (how language and gestures are interpreted by us to grasp the emotional states of others) is being studied as a guide to programming robots so they can simulate emotions in their interactions with humans.",Human-robot interactions and affective computing: The ethical implications,"Robotics, AI, and Humanity: Science, Ethics, and Policy",Book Chapter,4/17/2021,Laurence,Devillers,2021
10.1007/978-3-030-57977-7_12,"© 2020, Springer Nature Switzerland AG.The right to contest a decision with consequences on individuals or the society is a well-established democratic right. Despite this right also being explicitly included in GDPR in reference to automated decision-making, its study seems to have received much less attention in the AI literature compared, for example, to the right for explanation. This paper investigates the type of assurances that are needed in the contesting process when algorithmic black boxes are involved, opening new questions about the interplay of contestability and explainability. We argue that specialised complementary methodologies to evaluate automated decision-making in the case of a particular decision being contested need to be developed. Further, we propose a combination of well-established software engineering and rule-based approaches as a possible socio-technical solution to the issue of contestability, one of the new democratic challenges posed by the automation of decision making.",Contestable Black Boxes,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2020,Virginia,Dignum,2020
10.1007/978-3-030-58598-3_35,"© 2020, Springer Nature Switzerland AG.Visual similarity plays an important role in many computer vision applications. Deep metric learning (DML) is a powerful framework for learning such similarities which not only generalize from training data to identically distributed test distributions, but in particular also translate to unknown test classes. However, its prevailing learning paradigm is class-discriminative supervised training, which typically results in representations specialized in separating training classes. For effective generalization, however, such an image representation needs to capture a diverse range of data characteristics. To this end, we propose and study multiple complementary learning tasks, targeting conceptually different data relationships by only resorting to the available training samples and labels of a standard DML setting. Through simultaneous optimization of our tasks we learn a single model to aggregate their training signals, resulting in strong generalization and state-of-the-art performance on multiple established DML benchmark datasets.",DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2020,Yoshua,Bengio,2020
10.1007/978-3-030-60898-9_1,"© 2021, Springer Nature Switzerland AG.The aim of this chapter is to uncover the beauty and complexity in the world of shepherding as we view it through the lens of Artificial Intelligence (AI) and Autonomous Systems (AS). In the pursuit of imitating human intelligence, AI researchers have made significant and vast contributions over decades. Yet even with such interest and activity from within industry and the academic community, general AI remains out of our reach. By comparison, this book aims for a less ambitious goal in trying to recreate the intelligence of a sheepdog. As our efforts display, even with this seemingly modest goal, there is a plethora of research opportunities where AI and AS still have a long way to go. Let us start this journey by asking the basic questions: what is shepherding and what makes shepherding an interesting problem? How does one design a smart shepherd for swarm guidance? What AI algorithms are required and how are they organised in a cognitive architecture to enable a smart shepherd? How does one design transparent AI for smart shepherding?",Smart Shepherding: Towards Transparent Artificial Intelligence Enabled Human-Swarm Teams,Unmanned System Technologies,Book Chapter,1/1/2021,Hussein,Abbass,2021
10.1007/978-3-030-60898-9_10,"© 2021, Springer Nature Switzerland AG.The coordination of unmanned air–ground vehicles has been an active area due to the significant advantages of this coordination wherein unmanned air vehicles (UAVs) have a wide field of view, enabling them to effectively guide a swarm of unmanned ground vehicles (UGVs). Due to significant recent advances in artificial intelligence (AI), autonomous agents are being used to design more robust coordination of air–ground systems, reducing the intervention load of human operators and increasing the autonomy of unmanned air–ground systems. A guidance and control shepherding system design allows for single learning agent to influence and manage a larger swarm of rule-based entities. In this chapter, we present a learning algorithm for a sky shepherd-guiding rule-based AI-driven UGVs. The apprenticeship bootstrapping learning algorithm is introduced and is applied to the aerial shepherding task.",Apprenticeship Bootstrapping Reinforcement Learning for Sky Shepherding of a Ground Swarm in Gazebo,Unmanned System Technologies,Book Chapter,1/1/2021,Hussein,Abbass,2021
10.1007/978-3-030-69128-8_13,"© 2021, Springer Nature Switzerland AG.Capability and expected potential of AI-based computer solutions increased significantly in the recent years, mainly due to progress in machine learning technologies and available data. Growing effectiveness in reasoning, knowledge representation, automatic training via machine learning, and especially in computer vision and speech technology result in AI systems becoming an ever-better communication and work partner of their human counterparts. Deeply embedded in the every-day context of work and leisure, AI systems can act as competent dialog partners and powerful work assistants. Furthermore, they increasingly help humans to better acquire new insights, process and apply situation-specific instructions, receive improved training and learn new knowledge more effectively. Ultimately, intelligent systems exhibit the potential to become inseparable partners of humans or – in case e.g. of prosthesis solutions and innovative sensor technology – even become part of the human body. Such close mental and physical interconnection between human and AI system raises new concerns and ethical questions which need to be considered not only by computer scientists, but ask for interdisciplinary work and social discourse. This paper outlines the different levels of human-computer integration, gives examples of the innovative potential in work assistance and learning support, and sketches ethical and moral issues conjoined with such progress.",Augmented Human and Human-Machine Co-evolution: Efficiency and Ethics,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Book Chapter,1/1/2021,Laurence,Devillers,2021
10.1007/978-3-030-69128-8_2,"© 2021, Springer Nature Switzerland AG.Modern AI systems have become of widespread use in almost all sectors with a strong impact on our society. However, the very methods on which they rely, based on Machine Learning techniques for processing data to predict outcomes and to make decisions, are opaque, prone to bias and may produce wrong answers. Objective functions optimized in learning systems are not guaranteed to align with the values that motivated their definition. Properties such as transparency, verifiability, explainability, security, technical robustness and safety, are key to build operational governance frameworks, so that to make AI systems justifiably trustworthy and to align their development and use with human rights and values.",Trustworthy AI,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Book Chapter,1/1/2021,Virginia,Dignum,2021
10.1007/978-3-030-69128-8_6,"© 2021, Springer Nature Switzerland AG.This chapter summarizes contributions made by Ricardo Baeza-Yates, Francesco Bonchi, Kate Crawford, Laurence Devillers and Eric Salobir in the session chaired by Françoise Fogelman-Soulié on AI & Human values at the Global Forum on AI for Humanity. It provides an overview of key concepts and definitions relevant for the study of inequalities and Artificial Intelligence. It then presents and discusses concrete examples of inequalities produced by AI systems, highlighting their variety and potential harmfulness. Finally, we conclude by discussing how putting human values at the core of AI requires answering many questions, still open for further research.","AI &amp; Human Values: Inequalities, Biases, Fairness, Nudge, and Feedback Loops",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Book Chapter,1/1/2021,Laurence,Devillers,2021
10.1007/978-3-030-69128-8_7,"© 2021, Springer Nature Switzerland AG.The field of AI is rich in scientific and technical challenges. Progress needs to be made in machine learning paradigms to make them more efficient and less data intensive. Bridges between data-based and model-based AI are needed in order to benefit from the best of both approaches. Many real-life situations cannot yet be addressed by current robots, demanding progress in perception, scene interpretation or group coordination. This chapter addresses some of the major scientific and technological challenges in core AI technology.",Next Big Challenges in Core AI Technology,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Book Chapter,1/1/2021,Oren,Etzioni,2021
10.1007/978-3-030-77211-6_1,"© 2021, Springer Nature Switzerland AG.Just recently, IBM invited me to participate in a panel titled “Will AI ever be completely fair?” My first reaction was that it surely would be a very short panel, as the only possible answer is ‘no’. In this short paper, I wish to further motivate my position in that debate: “I will never be completely fair. Nothing ever is. The point is not complete fairness, but the need to establish metrics and thresholds for fairness that ensure trust in AI systems”.",The Myth of Complete AI-Fairness,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2021,Virginia,Dignum,2021
10.1007/978-3-030-81907-1_1,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.This is the introduction to the volume. It highlights the various “seasons” through which the development of AI has gone, and how the failures and successes of AI raise ethical questions, and require an ethical approach.",Introduction – The Importance of an Ethics-First Approach to the Development of AI,Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-81907-1_10,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.The debate about the ethical implications of Artificial Intelligence dates from the 1960s (Samuel in Science, 132(3429):741–742, 1960. https://doi.org/10.1126/scien ce.132.3429.741 ; Wiener in Cybernetics: or control and communication in the animal and the machine, MIT Press, New York, 1961). However, in recent years symbolic AI has been complemented and sometimes replaced by (Deep) Neural Networks and Machine Learning (ML) techniques. This has vastly increased its potential utility and impact on society, with the consequence that the ethical debate has gone mainstream. Such a debate has primarily focused on principles—the ‘what’ of AI ethics (beneficence, non-maleficence, autonomy, justice and explicability)—rather than on practices, the ‘how.’ Awareness of the potential issues is increasing at a fast rate, but the AI community’s ability to take action to mitigate the associated risks is still at its infancy. Our intention in presenting this research is to contribute to closing the gap between principles and practices by constructing a typology that may help practically-minded developers apply ethics at each stage of the Machine Learning development pipeline, and to signal to researchers where further work is needed. The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI. The article outlines the research method for creating this typology, the initial findings, and provides a summary of future research needs.","From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-81907-1_2,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of ‘principle proliferation’ be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes ‘ethical AI.’ Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question ‘how does it work?’) and in the ethical sense of accountability (as an answer to the question: ‘who is responsible for the way it works?’). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.",A Unified Framework of Five Principles for AI in Society,Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-81907-1_20,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.An increasing number of financial services (FS) companies are adopting solutions driven by artificial intelligence (AI) to gain operational efficiencies, derive strategic insights, and improve customer engagement. However, the rate of adoption has been low, in part due to the apprehension around its complexity and self-learning capability, which makes auditability a challenge in a highly regulated industry. There is limited literature on how FS companies can implement the governance and controls specific to AI-driven solutions. AI auditing cannot be performed in a vacuum; the risks are not confined to the algorithm itself, but rather permeates the entire organization. Using the risk of unfairness as an example, this paper will introduce the overarching governance strategy and control framework to address the practical challenges in mitigating risks AI introduces. With regulatory implications and industry use cases, this framework will enable leaders to innovate with confidence.",Innovating with Confidence: Embedding AI Governance and Fairness in a Financial Services Risk Management Framework,Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-81907-1_22,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.In this chapter, I look into the possible developments of Artificial Intelligence (AI) in the near future and identify two likely trends: (a) a shift from historical to synthetic data; and (b) a translation of difficult tasks (in terms of abilities) into complex ones (in terms of computation). I then argue that (a) and (b) will be pursued as development strategies of AI solutions whenever and as far as they are feasible.",What the Near Future of Artificial Intelligence Could Be,Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-81907-1_4,"© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.In this chapter, I argue that the European commission’s report, ‘Ethics guidelines for trustworthy AI’, provides a clear benchmark to evaluate the responsible development of AI systems, and to facilitate international support for AI solutions that are good for humanity and the environment.",Establishing the Rules for Building Trustworthy AI,Philosophical Studies Series,Book Chapter,1/1/2021,Luciano,Floridi,2021
10.1007/978-3-030-95593-9_3,"© 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.With numerous applications in distinct domains, especially healthcare, human activity detection is of utmost significance. The objective of this study is to monitor activities of daily living using the publicly available dataset recorded in nine different geometrical locations for ninety-nine volunteers including young and older adults (65+) using 5.8 GHz Frequency Modulated Continuous Wave (FMCW) radar. In this work, we experimented with discrete human activities, for instance, walking, sitting, standing, bending, and drinking, recorded for 10 s and 5 s. To detect the list of activities mentioned above, we obtained the Micro-Doppler signatures through Short-time Fourier transform using MATLAB tool and procured the spectrograms as images. The acquired data of the spectrograms are trained, validated, and tested exploiting a state-of-the-art deep learning approach known as Residual Neural Network (ResNet). Moreover, the confusion matrix, model loss, and classification accuracy are used as performance evaluation metrics for the trained ResNet model. The unique skip connection technique of ResNet minimises the overfitting and underfitting issue, consequently resulting accuracy rate up to 91 %.",Monitoring Discrete Activities of Daily Living of Young and Older Adults Using 5.8 GHz Frequency Modulated Continuous Wave Radar and ResNet Algorithm,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",Conference Paper,1/1/2022,Fatmah,Baothman,2022
10.1007/978-3-031-10960-7_12,"© 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.This chapter describes musebots and their specific use as collaborators within systems designed by an artist, rather than a computer scientist, as well as a brief personal history describing the trajectory leading to their use. The musebots are not proof of concept, but producers of genuine art: their work has been performed in a variety of concerts and festivals throughout the world. While AI systems have been developed that offer themselves as compositional assistants (Agres et al., Computers in Entertainment (CIE) 14:1–33, 2016), I view collaboration as an equal partner in the creative process and describe the unique relationship between composer and artificial agents in the creation of artworks that exist as artworks, rather than examples of computational creativity.",Musebots and I: Collaborating with Creative Systems,Springer Series on Cultural Computing,Book Chapter,1/1/2022,Arne,Eigenfeldt,2022
10.1007/978-3-031-17982-2,"© The Author(s), under exclusive licence to Springer Nature Switzerland AG 2022.Our digital existence is hurried and fast. We are tied to the present, or perhaps we are not present enough: immersed in digital social media and processes by artificial intelligence, we are hardly present to ourselves and to others, and feel alienated from nature. We are also made to fear climate change and the end of humanity. How can we live a good life and give meaning to our lives under these conditions? How can and should we co-exist today? Using process philosophy, narrative theory, and the concept of technoperformances, this book analyzes how digital technologies shape our relation to time and our existence, and discusses what this means in the light of climate change and new technologies such as AI. In dialogue with contemporary philosophy of technology and media theory and asking original questions about finding common times in what it calls the “Anthropochrone”, it proposes a conceptual framework that helps us to understand how we (should) exist and relate to time today.","Digital Technologies, Temporality, and the Politics of Co-Existence","Digital Technologies, Temporality, and the Politics of Co-Existence",Book,1/1/2023,Mark,Coeckelbergh,2023
10.1007/978-3-031-19907-3_17,"© 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.After introductory remarks, we share our two-part theoretical position, viz. that: (P1) The best overarching approach to suitably defining GI, and obtaining AGI, is via formal logic, including specifically via logic-based learning that is academic in nature; and (P2) AI/AGI is best pursued by seeking artificial agents that pass determinate cognitive tests. We note that in striking harmony with this position is work on AGI by Goertzel et al. that has inspired us; this is work in which PreSchool for would-be AGIs provides an attractive route toward AGI itself. While Goertzel et al. envisage a virtual academic environment, we have in mind physical classrooms, for physical robots. We describe the robot PERI.2, which we have started to send to school.","PERI.2 Goes to PreSchool and Beyond, in Search of AGI",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2023,Selmer,Bringsjord,2023
10.1007/978-3-031-21147-8_1,"© 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.Artificial intelligence (AI) as a general-purpose technology has great potential for advancing the United Nations Sustainable Development Goals (SDGs). However, the AI×SDGs phenomenon is still in its infancy in terms of diffusion, analysis, and empirical evidence. Moreover, a scalable adoption of AI solutions to advance the achievement of the SDGs requires private and public actors to engage in coordinated actions that have been analysed only partially so far. This volume provides the first overview of the AI×SDGs phenomenon and its related challenges and opportunities. The first part of the book adopts a programmatic approach, discussing AI×SDGs at a theoretical level and from the perspectives of different stakeholders. The second part illustrates existing projects and potential new applications.",Introduction: Understanding the Ethics of Artificial Intelligence for the Sustainable Development Goals,Philosophical Studies Series,Book Chapter,1/1/2023,Luciano,Floridi,2023
10.1007/978-3-031-21333-5_97,"© 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.The continuous expansion of cyberattacks and their high degree of professionalization generates a context of extreme risk in all sectors of society, both public and private. In this scenario, emergency management centers become a target that could lead to a high impact for citizens and for emergency coordination. This work focuses on the application of Machine Learning techniques for the early detection of possible Telephone Denial of Service or TDoS attacks, regardless of whether they use traditional telephone exchanges or VoIP. To do this, an extensive dataset with real data is here used for the training of the proposed models. In addition, the implementation in Jetson Nano type devices is proposed for the integration in the real context of emergency centers.",Detection of Anomalies in the Call Flow of an Emergency Management Center,Lecture Notes in Networks and Systems,Conference Paper,1/1/2023,Pino Caballero,Gil,2023
10.1007/978-3-031-21606-0_5,"© 2023, Springer Nature Switzerland AG.There is an emerging body of research that demonstrates people interact with humanlike forms of AI (such as holograms and robots) socially in some situations and furthermore may develop meaningful emotional attachment to them. Therefore, as the first generations of AI designed specifically to enhance human sexual pleasure are introduced it is important to contemplate the ethical questions because these new technological entities introduce new social actors into the complicated mix of morals and emotional attachment of romantic and sexual relationships with their unique characteristics of being both (a) technology and (b) socially meaningful. Exploring the dynamic of human–robot social interactions in this way also acknowledges that robots can be incorporated as meaningful social actors in human relationships, forming new dynamics between humans and technology. This chapter explores some of the cultural expectations that people have around human–human romantic commitment and explains how the introduction of AI-based sexual partners like sex robots will introduce new ideas and definitions about the ethics of emotional cheating when they are applied to a technological and sociological Other.",Emotional Intimacy and the Idea of Cheating on Committed Human–Human Relationships with a Robot,Intelligent Systems Reference Library,Book Chapter,1/1/2023,Julie,Carpenter,2023
10.1007/978-3-031-23793-5_1,"© 2023, Springer Nature Switzerland AG.Answer triggering is the task of selecting the best-suited answer for a given question from a set of candidate answers if it exists. This paper presents a hybrid deep learning model for answer triggering, which combines several dependency graph-based alignment features, namely graph edit distance, graph-based similarity, and dependency graph coverage, with dense vector embeddings from a Convolutional Neural Network (CNN). Our experiments on the WikiQA dataset show that such a combination can more accurately trigger a candidate answer compared to the previous state-of-the-art models. Comparative study on WikiQA data set shows 5.86 % absolute F-score improvement at the question level.",Combining Graph-Based Dependency Features with Convolutional Neural Network for Answer Triggering,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2023,Pushpak,Bhattacharyya,2023
10.1007/978-3-031-24340-0_35,"© 2023, Springer Nature Switzerland AG.News Title (NT) and News Body (NB) consistency detection is a demanding problem in Fake News Detection. In this paper, we formulate consistency detection between NT and NB from the perspective of Textual Entailment (TE), and propose various deep learning based methods for solving this problem. Inconsistency between NT and NB can affect the purpose of the news and alter the view of the reader towards the news contents. We develop various models based on Multilayer Perceptron (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and a combination of CNN and LSTM. Evaluation of the proposed approaches on a recently released benchmark dataset demonstrate the effectiveness of our approaches.",“News Title Can Be Deceptive” Title Body Consistency Detection for News Articles Using Text Entailment,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2023,Pushpak,Bhattacharyya,2023
10.1007/978-3-031-24349-3_1,"© 2023, Springer Nature Switzerland AG.Human-centered AI mobilizes several disciplines such as AI, human-machine interaction, philosophy, ethics, law and social sciences. In such a context, being introduced to the basic concepts of Human-centered AI is challenging. In this chapter, we describe the learning objectives of the Advanced Course on AI organized in 2021 with a focus on Human-centered AI.",The Advanced Course on Human-Centered AI: Learning Objectives,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2023,Virginia,Dignum,2023
10.1007/978-3-319-00395-5_52,"© Springer International Publishing Switzerland 2013.The successful implementation of policies in complex social environments, require a deep understanding of interdependencies between many actors with different perspectives. In order to understand, analyse and design such complex systems, advanced modelling tools are required. In this paper, we describe the MAIA modeling tool, based on the IAD framework. MAIA supports the development of agent-based models for policy making by providing (1) a methodology that provides guidelines on how to produce executable code from a conceptualized model, (2) a web-based application that supports the conceptualization process, and (3) a (semi) automatic transformation to generate executable simulations.",Agent-based simulation for complex social systems: Support for the developer,Springer Proceedings in Complexity,Book Chapter,1/1/2013,Virginia,Dignum,2013
10.1007/978-3-319-06614-1_2,"When AI was still a glimmer in Alan Turing's eye, and when (soon afterwards) it was the new kid on the block at MIT and elsewhere, it wasn't regarded primarily as a source of technical gizmos for public use or commercial exploitation (Boden 2006, p. 10.i-ii). To the contrary, it was aimed at illuminating the powers of the human mind. © Springer International Publishing Switzerland 2014.",Aaron Sloman: A Bright Tile in AI's Mosaic,Cognitive Systems Monographs,Article,1/1/2014,Margaret,Boden,2014
10.1007/978-3-319-09274-4_24,"In order to explore the practical manifestations of the ""cognitive synergy"" between the PLN (Probabilistic Logic Networks) and ECAN (Economic Attention Network) components of the OpenCog AGI architecture, we explore the behavior of PLN and ECAN operating together on two standard test problems commonly used with Markov Logic Networks (MLN). Our preliminary results suggest that, while PLN can address these problems adequately, ECAN offers little added value for the problems in their standard form. However, we outline modified versions of the problem that we hypothesize would demonstrate the value of ECAN more effectively, via inclusion of confounding information that needs to be heuristically sifted through. © 2014 Springer International Publishing.",Guiding probabilistic logical inference with nonlinear dynamical attention allocation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2014,Ben,Goertzel,2014
10.1007/978-3-319-09274-4_25,"An Application Programming Interface for human-level AGI systems is proposed, aimed at bridging the gap between proto-AGI R&D systems and practical AI application development. The API contains simply formalized queries corresponding to the various key aspects of human-like intelligence, organized so as to be independent of the algorithms used under the hood for query resolution and associated supporting cognitive processes. A novel, qualitative (and in principle quantifiable) measure of software general intelligence is proposed (the APIQ), measuring the degree to which a system succeeds at fulfilling the various API functions using a compact set of representations and algorithms. © 2014 Springer International Publishing.",A cognitive API and its application to AGI intelligence assessment,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2014,Ben,Goertzel,2014
10.1007/978-3-319-17091-6_23,© Springer International Publishing Switzerland 2015.The paper investigates the problem of anomaly detection in the maritime trajectory surveillance domain. Conformal predictors in this paper are used as a basis for anomaly detection. A multi-class hierarchy framework is presented for different class representations. Experiments are conducted with data taken from shipping vessel trajectories using data obtained through AIS (Automatic Identification System) broadcasts and the results are discussed.,Conformal anomaly detection of trajectories with a multi-class hierarchy,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2015,Alexander,Gammerman,2015
10.1007/978-3-319-21365-1_8,"© Springer International Publishing Switzerland 2015.Some currently popular and successful deep learning architectures display certain pathological behaviors (e.g. confidently classifying random data as belonging to a familiar category of nonrandom images; and misclassifying miniscule perturbations of correctly classified images). It is hypothesized that these behaviors are tied with limitations in the internal representations learned by these architectures, and that these same limitations would inhibit integration of these architectures into heterogeneous multi-component AGI architectures. It is suggested that these issues can be worked around by developing deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events.",Are there deep reasons underlying the pathologies of today’s deep learning algorithms?,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2015,Ben,Goertzel,2015
10.1007/978-3-319-21548-8_5,"© Springer International Publishing Switzerland 2015.The authors argue that unless computational deontic logics (or, for that matter, any other class of systems for mechanizing moral and/or legal principles) or achieving ethical control of future AIs and robots are woven into the operatingsystem level of such artifacts, such control will be at best dangerously brittle.",Ethical regulation of robots must be embedded in their operating systems,Cognitive Technologies,Book Chapter,1/1/2015,Selmer,Bringsjord,2015
10.1007/978-3-319-23528-8_31,"© Springer International Publishing Switzerland 2015.Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.",Difference target propagation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2015,Yoshua,Bengio,2015
10.1007/978-3-319-26485-1_12,"© 2016, Springer International Publishing Switzerland.When IBM’s Deep Blue beat Kasparov in 1997, Bringsjord (Technol Rev 101(2):23–28, 1998) complained that despite the impressive engineering that made this victory possible, chess is simply too easy a challenge for AI, given the full range of what the rational side of the human mind can muster. However, arguably everything changed in 2011. For in that year, playing not a simple board game, but rather an open-ended game based in natural language, IBM’s Watson trounced the best human Jeopardy! players on the planet. And what does Watson’s prowess tell us about the philosophy, theory, and future of AI? We present and defend snyoptic answers to these questions, ones based upon Leibniz’s seminal writings on a universal logic, on a Leibnizian “three-ray” space of computational formal logics that, inspired by those writings, we have invented, and on a “scorecard” approach to assessing real AI systems based in turn on that three-ray space.","Leibniz’s Art of Infallibility, Watson, and the Philosophy, Theory, and Future of AI",Synthese Library,Book Chapter,1/1/2016,Selmer,Bringsjord,2016
10.1007/978-3-319-27543-711,"© Springer-Verlag Berlin Heidelberg 2015.The dual-process theory of human cognition proposes the existence of two systems for decision-making: a slower, deliberative, problem-solving system and a quicker, reactive, pattern-recognition system. We alter the balance of these systems in a number of computational simulations using three types of agent equipped with a novel, hybrid, human-like cognitive architecture. These agents are situated in the stochastic, multi-agent Tileworld domain, whose complexity can be precisely controlled and widely varied. We explore how agent performance is affected by different balances of problem-solving and pattern-recognition, and conduct a sensitivity analysis upon key pattern-recognition system variables. Results indicate that pattern-recognition improves agent performance by as much as 36.5% and, if a balance is struck with particular pattern-recognition components to promote pattern-recognition use, performance can be further improved by up to 3.6%. This research is of interest for studies of expert behaviour in particular, and AI in general.",A question of balance the benefits of pattern-recognition when solving problems in a complex domain,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2015,Fernand,Gobet,2015
10.1007/978-3-319-41649-6_35,"© Springer International Publishing Switzerland 2016.A new conceptual framing of the notion of the general intelligence is outlined, in the form of a universal learning meta-algorithm called Probabilistic Growth and Mining of Combinations (PGMC). Incorporating ideas from logical inference systems, Solomonoff induction and probabilistic programming, PGMC is a probabilistic inference based framework which reflects processes broadly occurring in the natural world, is theoretically capable of arbitrarily powerful generally intelligent reasoning, and encompasses a variety of existing practical AI algorithms as special cases. Several ways of manifesting PGMC using the OpenCog AI framework are described. It is proposed that PGMC can be viewed as a core learning process serving as the central dynamic of real-world general intelligence; but that to achieve high levels of general intelligence using limited computational resources, it may be necessary for cognitive systems to incorporate multiple distinct structures and dynamics, each of which realizes this core PGMC process in a different way (optimized for some particular sort of sub-problem).",Probabilistic growth and mining of combinations: A unifying meta-algorithm for practical general intelligence,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2016,Ben,Goertzel,2016
10.1007/978-3-319-46331-5_3,"© Springer International Publishing Switzerland 2017.With the increasing trend in exploring the use of agent-based models in empirical contexts, this paper reflects on the use of decision trees learned from questionnaire data as behavioral models for the agents. Decision trees are machine learning algorithms most commonly used in the data mining literature, especially for smaller datasets where other techniques such as Bayesian Networks cannot be applied. In agent-based modelling contexts, decision trees have the advantage over some other machine learning techniques in that the results are more transparent, and can be critiqued by domain experts without a background in computing or artificial intelligence. However, decision trees are sensitive to the way in which they are constructed, particularly with respect to preprocessing. We describe the processes by which the decision trees were derived in the context of a model of everyday pro-environmental behavior at work, comparing various preprocessing methods and exploring their differences.",Empirically-derived behavioral rules in agent-based models using decision trees learned from questionnaire data,Understanding Complex Systems,Article,1/1/2017,Amparo Alonso,Betanzos,2017
10.1007/978-3-319-46723-8_54,"© Springer International Publishing AG 2016.We introduce a deep learning image segmentation framework that is extremely robust to missing imaging modalities. Instead of attempting to impute or synthesize missing data,the proposed approach learns,for each modality,an embedding of the input image into a single latent vector space for which arithmetic operations (such as taking the mean) are well defined. Points in that space,which are averaged over modalities available at inference time,can then be further processed to yield the desired segmentation. As such,any combinatorial subset of available modalities can be provided as input,without having to learn a combinatorial number of imputation models. Evaluated on two neurological MRI datasets (brain tumors and MS lesions),the approach yields state-of-the-art segmentation results when provided with all modalities; moreover,its performance degrades remarkably gracefully when modalities are removed,significantly more so than alternative mean-filling or other synthesis approaches.",HeMIS: Hetero-modal image segmentation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2016,Yoshua,Bengio,2016
10.1007/978-3-319-50115-4_43,"© 2017, Springer International Publishing AG.Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames and that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers.",Towards Learning to Perceive and Reason About Liquids,Springer Proceedings in Advanced Robotics,Book Chapter,1/1/2017,Dieter,Fox,2017
10.1007/978-3-319-61043-6_6,"© Springer International Publishing AG 2017.This is an essay on the Singularity business. Contrary to what many might expect upon parsing our title, we don’t use ‘the Singularity business’ to refer to the general and multi-faceted discussion and debate surrounding the Singularity, that mythical future point in time when AI exceeds today’s technology beyond what we can see from the present. Rather, we’re concerned with business and economic questions relating to what we dub ‘The MiniMaxularity’, that forseeable future time when the AI of today simply matures.","The Singularity Business: Toward a Realistic, Fine-Grained Economics for an AI-Infused World",Philosophical Studies Series,Book Chapter,1/1/2017,Selmer,Bringsjord,2017
10.1007/978-3-319-75477-2_30,"© Springer International Publishing AG, part of Springer Nature 2018.Identifying named entities is vital for many Natural Language Processing (NLP) applications. Much of the earlier work for identifying named entities focused on using handcrafted features and knowledge resources (feature engineering). This is a barrier for resource-scarce languages as many resources are not readily available. Recently, Deep Learning techniques have been proposed for various NLP tasks requiring little/no hand-crafted features and knowledge resources, instead the features are learned from the data. Many proposed deep learning solutions for Named Entity Recognition (NER) still rely on feature engineering as opposed to feature learning. However, it is not clear whether the deep learning system or the engineered features are responsible for the positive results reported. This is in contrast with the goal of deep learning systems i.e., to learn the features from the data itself. In this study, we show that a feature learned deep learning system is a viable solution to NER task. We test our deep learning systems on CoNLL English and Spanish NER datasets. Our system is able to give comparable results with the existing state-of-the-art feature engineered systems for English. We report the best performance of 89.27 F-Score for English when comparing with systems which do not use any handcrafted features or knowledge resources. Evaluation of our trained system on out-of-domain data indicate that the results are promising with the reported results. Our system when tested on Spanish NER achieves the best reported F-Score of 82.59 indicating its applicability to other languages.",A deep learning solution to named entity recognition,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-319-75487-1_41,"© Springer International Publishing AG, part of Springer Nature 2018.The task of end-to-end relation extraction consists of two sub-tasks: (i) identifying entity mentions along with their types and (ii) recognizing semantic relations among the entity mention pairs. It has been shown that for better performance, it is necessary to address these two sub-tasks jointly [13, 22]. We propose an approach for simultaneous extraction of entity mentions and relations in a sentence, by using inference in Markov Logic Networks (MLN) [21]. We learn three different classifiers: (i) local entity classifier, (ii) local relation classifier and (iii) “pipeline” relation classifier which uses predictions of the local entity classifier. Predictions of these classifiers may be inconsistent with each other. We represent these predictions along with some domain knowledge using weighted first-order logic rules in an MLN and perform joint inference over the MLN to obtain a global output with minimum inconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 dataset demonstrate that our approach of joint extraction using MLNs outperforms the baselines of individual classifiers. Our end-to-end relation extraction performance is better than 2 out of 3 previous results reported on the ACE 2004 dataset.",End-to-end relation extraction using markov logic networks,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-319-90080-3_3,"© Springer International Publishing AG, part of Springer Nature 2018.This chapter describes the basic ideas under the ensemble approach, together with the classical methods that have being used in the field of Machine Learning. Section 3.1 states the rationale under the approach, while in Sect. 3.2 the most popular methods are briefly described. Finally, Sect. 3.3 summarizes and discusses the contents of this chapter.",Foundations of ensemble learning,Intelligent Systems Reference Library,Book Chapter,1/1/2018,Amparo Alonso,Betanzos,2018
10.1007/978-3-319-92285-0_10,"© 2018, Springer Verlag. All rights reserved.The Spatial Intelligence and Learning Center (SILC) pioneered the idea of spatial learning: improving learning about spatial concepts and using spatial concepts to facilitate learning about other domains. Spatial concepts are of significant importance in Science, Technology, Engineering and Mathematics (STEM) education including physics, chemistry, geoscience, and biology, as well as most branches of engineering. Spatial concepts are also important in the military. For example, sand tables are used to understand the terrain, since calculating Line of Sight (LOS) for cover and concealment on the battlefield can mean the difference between life or death. In addition, land navigation using topographic maps is a critical skill that all members of the military are required to obtain. For this work in progress, the Army Research Laboratory (ARL) and Northwestern University, a member of SILC, are exploring sketching technologies to support spatial learning, as part of on-going research in adaptive training technologies. ARL’s Generalized Intelligent Framework for Tutoring (GIFT) provides a software platform for developing intelligent tutoring systems. Adaptive learning, where the state of the learner is used to help select their path through activities, has been shown to improve learning. As part of our on-going research, we are in the process of integrating sketch worksheets into GIFT as a new type of instructional media. Sketch worksheets were designed to be general-purpose and use artificial intelligence to provide immediate feedback to students performing sketching assignments. To facilitate dissemination, an authoring environment was created for domain experts and instructors, to enable them to create new worksheets. This poster describes a work in progress to bring together the two lines of research to support the use of sketching in an adaptive learning environment.",Sketching as a modality in intelligent tutoring systems,Communications in Computer and Information Science,Conference Paper,1/1/2018,Ken,Forbus,2018
10.1007/978-3-319-96448-5_14,"© Springer Nature Switzerland AG 2018.We answer the present paper’s title in the negative. We begin by introducing and characterizing “real learning” (RL) in the formal sciences, a phenomenon that has been firmly in place in homes and schools since at least Euclid. The defense of our negative answer pivots on an integration of reductio and proof by cases, and constitutes a general method for showing that any contemporary form of machine learning (ML) isn’t real learning. Along the way, we canvass the many different conceptions of “learning” in not only AI, but psychology and its allied disciplines; none of these conceptions (with one exception arising from the view of cognitive development espoused by Piaget), aligns with real learning. We explain in this context by four steps how to broadly characterize and arrive at a focus on RL.",Do Machine-Learning Machines Learn?,"Studies in Applied Philosophy, Epistemology and Rational Ethics",Book Chapter,1/1/2018,Selmer,Bringsjord,2018
10.1007/978-3-319-97304-3_48,"© Springer Nature Switzerland AG 2018.One of the significant task in spoken language understanding (SLU) is intent detection. In this paper, we propose a deep learning based ensemble model for intent detection. The outputs of different deep learning architectures such as convolutional neural network (CNN) and variants of recurrent neural networks (RNN) like long short term memory (LSTM) and gated recurrent units (GRU) are combined together using a multi-layer perceptron (MLP). The classifiers are trained using a combined word embedding representation obtained from both Word2Vec and Glove. Our experiments on the benchmark ATIS dataset show state-of-the-art performance for intent detection.",Intent detection for spoken language understanding using a deep ensemble model,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.1007/978-3-319-99316-4_3,"© 2019, Springer Nature Switzerland AG.AI models of the mind rarely discuss the so called “hard problem” of consciousness. Here, I will sketch informally a possible functional explanation for phenomenal consciousness: the conductor theory of consciousness (CTC). Unlike IIT, CTC is a functionalist model of consciousness, with similarity to other functionalist approaches, such as the ones suggested by Dennett and Graziano.",The Cortical Conductor Theory: Towards Addressing Consciousness in AI Models,Advances in Intelligent Systems and Computing,Conference Paper,1/1/2019,Joscha,Bach,2019
10.1007/978-3-540-24612-1_19,"The semantic web is becoming a realizable technology due to the efforts of researchers to develop semantic markup languages such as the DARPA Agent Markup Language (DAML). A major problem that faces the semantic web community is that most information sources on the web today lack semantic markup. To fully realize the potential of the semantic web, we must find a way to automatically upgrade information sources with semantic markup. We have developed a system based on the STALKER algorithm that automatically generates DAML markup for a set of documents based on previously seen labeled training documents. Our rule-learning approach to semantic markup is highly effective when dealing with semistructured documents. © Springer-Verlag Berlin Heidelberg 2003.",Automatically generated DAML markup for semistructured documents,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2004,Tim,Finin,2004
10.1007/978-3-540-24694-7_12,"Both ontology content and ontology building tools evaluations play an important role before using ontologies in Semantic Web applications. In this paper we try to assess ontology evaluation functionalities of the following ontology platforms: OilEd, OntoEdit, Protégé-2000, and WebODE. The goal of this paper is to analyze whether such ontology platforms prevent the ontologist from making knowledge representation mistakes in concept taxonomies during RDF(S) and DAML+OIL ontology import, during ontology building and during ontology export to RDF(S) and DAML+OIL. Our study reveals that most of these ontology platforms only detect a few mistakes in concept taxonomies when importing RDF(S) and DAML+OIL ontologies. It also reveals that most of these ontology platforms only detect some mistakes in concept taxonomies during building ontologies. Our study also reveals that these platforms do not detect any taxonomic mistake when exporting ontologies to such languages.",Evaluation of RDF(S) and DAML+OIL import/export services within ontology platforms,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2004,Asunción Gómez,Pérez,2004
10.1007/978-3-540-25940-4_10,"© Springer-Verlag Berlin Heidelberg 2004.Specific behavior description languages prove to be suitable replacements to native programming language like C++ when the number and complexity of behavior patterns of an agent increases. The XML based Extensible Agent Behavior Specification Language (XABSL) also simplifies the process of specifying complex behaviors and supports the design of both very reactive and long term oriented behaviors. XABSL uses hierarchies of behavior modules called options that contain state machines for decision making. In this paper we introduce the architecture behind XABSL, the formalization of that architecture in XML and the software library XabslEngine that runs the formalized behavior on an agent platform. The GermanTeam [9] employed XABSL in the RoboCup Sony Four Legged League competitions in Fukuoka.",Designing agent behavior with the extensible agent behavior specification language XABSL,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2004,Joscha,Bach,2004
10.1007/978-3-540-30109-7_26,"AI systems must be able to learn, reason logically, and handle uncertainty. While much research has focused on each of these goals individually, only recently have we begun to attempt to achieve all three at once. In this talk I will describe Markov logic, a representation that combines the full power of first-order logic and probabilistic graphical models, and algorithms for learning and inference in it. Syntactically, Markov logic is first-order logic augmented with a weight for each formula. Semantically, a set of Markov logic formulas represents a probability distribution over possible worlds, in the form of a Markov network with one feature per grounding of a formula in the set, with the corresponding weight. Formulas and weights are learned from relational databases using inductive logic programming and iterative optimization of a pseudo-likelihood measure. Inference is performed by Markov chain Monte Carlo over the minimal subset of the ground network required for answering the query. Experiments in a real-world university domain illustrate the promise of this approach. © Springer-Verlag Berlin Heidelberg 2004.","Learning, logic, and probability: A unified view",Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2004,Pedro,Domingos,2004
10.1007/978-3-540-30110-3_86,"Blind deconvolution is considered as a problem of quasi maximum likelihood (QML) estimation of the restoration kernel. Simple closed-form expressions for the asymptotic estimation error are derived. The asymptotic performance bounds coincide with the Cramér-Rao bounds, when the true ML estimator is used. Conditions for asymptotic stability of the QML estimator are derived. Special cases when the estimator is super-efficient are discussed. © Springer-Verlag 2004.",QML blind deconvolution: Asymptotic analysis,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2004,Michael,Bronstein,2004
10.1007/978-3-540-30209-4_6,"Web Services are interfaces to a collection of operations that are network-accessible through standardized XML messaging, and whose features are described using standard XML-based languages. Semantic Web Services (SWS) describe semantically the internal structure and the functional/nonfunctional capabilities of the services, facilitating the design and evaluation of SWSs based on that semantic description of the features of the services. To enable users to design and compose SWSs at the knowledge level, the ODE SWS framework has been proposed. That framework uses problem-solving methods to describe the functional and structural features of the SWSs. In this work, we present a description of the ODE SWS environment as an implementation of the ODE SWS framework. Specially, we focus on the description of the capabilities of the SWSDesigner, the tool of the ODE SWS environment that enables users to design graphically SWSs through different but complementary views of the services. © Springer-Verlag 2004.",Development of semantic web services at the knowledge level,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2004,Asunción Gómez,Pérez,2004
10.1007/978-3-540-30475-3_4,"A central theme of the Semantic Web is that programs should be able to easily aggregate data from different sources. Unfortunately, even if two sites provide their data using the same data model and vocabulary, subtle differences in their use of terms and in the assumptions they make pose challenges for aggregation. Experiences with the TAP project reveal some of the phenomena that pose obstacles to a simplistic model of aggregation. Similar experiences have been reported by AI projects such as Cyc, which has led to the development and use of various context mechanisms. In this paper we report on some of the problems with aggregating independently published data and propose a context mechanism to handle some of these problems. We briefly survey the context mechanisms developed in AI and contrast them with the requirements of a context mechanism for the Semantic Web. Finally, we present a context mechanism for the Semantic Web that is adequate to handle the aggregation tasks, yet simple from both computational and model theoretic perspectives. © Springer-Verlag 2004.",Contexts for the semantic web,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2004,Richard,Fikes,2004
10.1007/978-3-540-32256-6_2,"In this paper we propose an approach for tracking a moving target using Rao-Blackwellised particle filters. Such filters represent posteriors over the target location by a mixture of Kalman filters, where each filter is conditioned on the discrete states of a particle filter. The discrete states represent the non-linear parts of the state estimation problem. In the context of target tracking, these are the non-linear motion of the observing platform and the different motion models for the target. Using this representation, we show how to reason about physical interactions between the observing platform and the tracked object, as well as between the tracked object and the environment. The approach is implemented on a four-legged AIBO robot and tested in the context of ball tracking in the RoboCup domain. © Springer-Verlag Berlin Heidelberg 2005.",Map-based multiple model tracking of a moving object,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),Conference Paper,1/1/2005,Dieter,Fox,2005
10.1007/978-3-540-39718-2_48,"Despite numerous efforts, the semantic web has yet to achieve widespread adoption. Recently, some researchers have argued that participation in the semantic web is too difficult for ""ordinary"" people, limiting its growth and popularity. In response, this paper introduces MANGROVE, a system whose goal is to entice non-technical people to semantically annotate their existing HTML data. MANGROVE seeks to alter the cost-benefit equation of authoring semantic content. To increase the benefit, MANGROVE is designed to make semantic content instantly available to services that consume the content and yield immediate, tangible benefit to authors. To reduce the cost, MANGROVE makes semantic authoring as painless as possible by transferring some of the burden of schema design, data cleaning, and data structuring from content authors to the programmers who create semantic services. We have designed and implemented a MANGROVE prototype, built several semantic services for the system, and deployed those services in our department. This paper describes MANGROVE'S goals, presents the system architecture, and reports on our implementation and deployment experience. Overall, MANGROVE demonstrates a concrete path for enabling and enticing non-technical people to enter the semantic web. © Springer-Verlag Berlin Heidelberg 2003.",Mangrove: Enticing ordinary people onto the semantic web via instant gratification,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2003,Oren,Etzioni,2003
10.1007/978-3-540-45173-0_26,Darpa Agent Markup Language (DAML) [7] is the newest effort for Semantic Web [5]. It can be used to create ontologies and markup information resource like web pages. The information resource can be read by human and understood by agent programs. We believed DAML could be used to markup agent communication content and promote knowledge sharing and exchanging between agents. This paper also suggested an alternative model to connect web and agent together. We defined the necessary ontologies for agent communication in DAML language and described the agent communication scenario occurred in the ITTalks Project. © Springer-Verlag Berlin Heidelberg 2003.,Agent communication in DAML world,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2003,Tim,Finin,2003
10.1007/978-3-540-45173-0_32,"Effective use of the vast quantity of available information and services on the Internet will require multi-agent systems to be tightly integrated with existing web infrastructure. This however will be impossible unless the information on the web is presented in a semantic language, such as the DARPA Agent Markup Language (DAML), which is one aim of the ""Semantic Web"". As part of our exploration of Semantic Web technology, and DAML in particular, we have constructed ITTALKS, a web-based system for automatic and intelligent notification of information technology talks. In this paper, we describe the ITTALKS system, and discuss the numerous ways in which the use of Semantic Web concepts and DAML extend its ability to provide an intelligent online service to both the human community and, more interestingly, the agents assisting them. © Springer-Verlag Berlin Heidelberg 2003.",Agents making sense of the semantic web,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2003,Tim,Finin,2003
10.1007/978-3-540-45173-0_8,"Service matching is one of the crucial elements in the success of large, open agent systems. While finding ""perfect"" matches is always desirable, it is not always possible. The capabilities of an agent may change over time; some agents may be unwilling to, or unable to communicate their capabilities at the right level of details. The solution we propose is to have the broker agent dynamically refine the agent's capability model and to conduct performance rating. The agent capability model will be updated using the information from the consumer agent feedback, capability querying, etc. The update process is based on a concept of ""dynamic weight sum system"", as well as based on the local distribution of the agent services. We assume that the agents in the system share a common domain ontology that will be represented in DAML+OIL, and the agent capabilities will be described using DAML-S. © Springer-Verlag Berlin Heidelberg 2003.",Learning in the broker agent,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Article,1/1/2003,Tim,Finin,2003
10.1007/978-3-540-68677-4_3,"The Novamente AI Engine, a novel AI software system, is briefly reviewed. Novamente is an integrative artificial general intelligence design, which integrates aspects of many prior AI projects and paradigms, including symbolic, probabilistic, evolutionary programming and reinforcement learning approaches; but its overall architecture is unique, drawing on system-theoretic ideas regarding complex mental dynamics and associated emergent patterns. The chapter reviews both the conceptual models of mind and intelligence which inspired the system design, and the concrete architecture of Novamente as a software system.",The Novamente Artificial Intelligence Engine,Cognitive Technologies,Review,12/1/2007,Ben,Goertzel,2007
10.1007/978-3-540-69912-5_2,"The Psi theory of human action regulation is a candidate for a cognitive architecture that tackles the problem of the interrelation of motivation and emotion with cognitive processes. We have transferred this theory into a cognitive modeling framework, implemented as an AI architecture, called MicroPsi. Here, we describe the main assumptions of the Psi theory and summarize a neural prototyping algorithm that matches perceptual input to hierarchical declarative representations. © Springer-Verlag Berlin Heidelberg 2007.",MicroPsi: Contributions to a broad architecture of cognition,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2007,Joscha,Bach,2007
10.1007/978-3-540-74565-5_2,"With this talk we want to pay tribute to the late Professor Gerd Veenker who deserves the historic credit of initiating the formation of the German AI community. We present a summary of his scientific contributions in the context of the early approaches to theorem proving and, against this background, we point out future perspectives of Automated Deduction. © Springer-Verlag Berlin Heidelberg 2007.",Early history and perspectives of automated deduction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2007,Wolfgang,Bibel,2007
10.1007/978-3-540-74889-2_43,"The HUMAINE project is concerned with developing interfaces that will register and respond to emotion, particularly pervasive emotion (forms of feeling, expression and action that colour most of human life). The HUMAINE Database provides naturalistic clips which record that kind of material, in multiple modalities, and labelling techniques that are suited to describing it. © Springer-Verlag Berlin Heidelberg 2007.",The HUMAINE database: Addressing the collection and annotation of naturalistic and induced emotional data,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2007,Laurence,Devillers,2007
10.1007/978-3-540-74976-9_21,"Markov logic networks (MLNs) combine Markov networks and first-order logic, and are a powerful and increasingly popular representation for statistical relational learning. The state-of-the-art method for discriminative learning of MLN weights is the voted perceptron algorithm, which is essentially gradient descent with an MPE approximation to the expected sufficient statistics (true clause counts). Unfortunately, these can vary widely between clauses, causing the learning problem to be highly ill-conditioned, and making gradient descent very slow. In this paper, we explore several alternatives, from per-weight learning rates to second-order methods. In particular, we focus on two approaches that avoid computing the partition function: diagonal Newton and scaled conjugate gradient. In experiments on standard SRL datasets, we obtain order-of-magnitude speedups, or more accurate models given comparable learning times. © Springer-Verlag Berlin Heidelberg 2007.",Efficient weight learning for Markov logic networks,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2007,Pedro,Domingos,2007
10.1007/978-3-540-74997-4_10,"Since the beginning of the SAIBA effort to unify key interfaces in the multi-modal behavior generation process, the Behavior Markup Language (BML) has both gained ground as an important component in many projects worldwide, and continues to undergo further refinement. This paper reports on the progress made in the last year in further developing BML. It discusses some of the key challenges identified that the effort is facing, and reviews a number of projects that already are making use of BML or support its use. © Springer-Verlag Berlin Heidelberg 2007.",The behavior markup language: Recent developments and challenges,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2007,Justine,Cassell,2007
10.1007/978-3-540-87479-9_59,"Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks. © 2008 Springer-Verlag Berlin Heidelberg.",Extracting semantic networks from text via relational clustering,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,11/19/2008,Pedro,Domingos,2008
10.1007/978-3-540-89197-0_6,"Predicting and explaining the behavior of others in terms of mental states is indispensable for everyday life. It will be equally important for artificial agents. We present an inference system for representing and reasoning about certain types of mental states, and use it to provide a formal analysis of the false-belief task. The system allows for the representation of information about events, causation, and perceptual, doxastic, and epistemic states (vision, belief, and knowledge), incorporating ideas from the event calculus and multi-agent epistemic logic. Unlike previous AI formalisms, our focus here is on mechanized proofs and proof programmability, not on metamathematical results. Reasoning is performed via cognitively plausible inference rules, and automation is achieved by general-purpose inference methods. The system has been implemented as an interactive theorem prover and is available for experimentation. © 2008 Springer Berlin Heidelberg.",Toward formalizing common-sense psychology: An analysis of the false-belief task,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,12/1/2008,Selmer,Bringsjord,2008
10.1007/978-3-540-89694-4_13,"This paper presents a novel ensemble construction approach based on Artificial Immune Systems (AIS) to solve regression problems. Over the last few years AIS have increasingly attracted interest from researchers due to their ability to balance the exploration and exploitation of the search space. Nevertheless, there have been just a few applications of those algorithms in the construction of committee machines. In this paper, a population of feed-forward neural networks is evolved using the Clonal Selection Algorithm and then ensembles are automatically composed of a subset of this neural network population. Results show that the proposed algorithm can achieve good generalization performance on some hard benchmark regression problems. © 2008 Springer Berlin Heidelberg.",Evolving an ensemble of neural networks using artificial immune systems,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,12/1/2008,Hussein,Abbass,2008
10.1007/978-3-642-12337-5,"Style is a fundamental and ubiquitous aspect of the human experience: Everyone instantly and constantly assesses people and things according to their individual styles, academics establish careers by researching musical, artistic, or architectural styles, and entire industries maintain themselves by continuously creating and marketing new styles. Yet what exactly style is and how it works are elusive: We certainly know it when we see it, but there is no shared and clear understanding of the diverse phenomena that we call style. The Structure of Style explores this issue from a computational viewpoint, in terms of how information is represented, organized, and transformed in the production and perception of different styles. New computational techniques are now making it possible to model the role of style in the creation of and response to human artifacts-and therefore to develop software systems that directly make use of style in useful ways. Argamon, Burns, and Dubnov organize the research they have collected in this book according to the three roles that computation can play in stylistics. The first section of the book, Production, provides conceptual foundations by describing computer systems that create artifacts-musical pieces, texts, artworks-in different styles. The second section, Perception, explains methods for analyzing different styles and gleaning useful information, viewing style as a form of communication. The final section, Interaction, deals with reciprocal interaction between style producers and perceivers, in areas such as interactive media, improvised musical accompaniment, and game playing. The Structure of Style is written for researchers and practitioners in areas including information retrieval, computer art and music, digital humanities, computational linguistics, and artificial intelligence, who can all benefit from this comprehensive overview and in-depth description of current research in this active interdisciplinary field. © Springer-Verlag Berlin Heidelberg 2010. All rights are reserved.",The structure of style: Algorithmic approaches to understanding manner and meaning,The Structure of Style: Algorithmic Approaches to Understanding Manner and Meaning,Book,12/1/2010,Shlomo,Argamon,2010
10.1007/978-3-642-12337-5,"Style is a fundamental and ubiquitous aspect of the human experience: Everyone instantly and constantly assesses people and things according to their individual styles, academics establish careers by researching musical, artistic, or architectural styles, and entire industries maintain themselves by continuously creating and marketing new styles. Yet what exactly style is and how it works are elusive: We certainly know it when we see it, but there is no shared and clear understanding of the diverse phenomena that we call style. The Structure of Style explores this issue from a computational viewpoint, in terms of how information is represented, organized, and transformed in the production and perception of different styles. New computational techniques are now making it possible to model the role of style in the creation of and response to human artifacts-and therefore to develop software systems that directly make use of style in useful ways. Argamon, Burns, and Dubnov organize the research they have collected in this book according to the three roles that computation can play in stylistics. The first section of the book, Production, provides conceptual foundations by describing computer systems that create artifacts-musical pieces, texts, artworks-in different styles. The second section, Perception, explains methods for analyzing different styles and gleaning useful information, viewing style as a form of communication. The final section, Interaction, deals with reciprocal interaction between style producers and perceivers, in areas such as interactive media, improvised musical accompaniment, and game playing. The Structure of Style is written for researchers and practitioners in areas including information retrieval, computer art and music, digital humanities, computational linguistics, and artificial intelligence, who can all benefit from this comprehensive overview and in-depth description of current research in this active interdisciplinary field. © Springer-Verlag Berlin Heidelberg 2010. All rights are reserved.",Preface,The Structure of Style: Algorithmic Approaches to Understanding Manner and Meaning,Editorial,12/1/2010,Shlomo,Argamon,2010
10.1007/978-3-642-14749-4_2,"Sketching is a powerful means of working out and communicating ideas. Sketch understanding involves a combination of visual, spatial, and conceptual knowledge and reasoning, which makes it both challenging and potentially illuminating. This talk will describe how a team of AI researchers, cognitive psychologists, learning scientists, and educators is attempting to build the intellectual and software infrastructure needed to achieve more human-like sketch understanding software. We are creating CogSketch, an open-domain sketch understanding system that will serve as both a cognitive science research instrument and as a platform for sketch-based educational software. These missions interact: Our cognitive simulation work leads to improvements which can be exploited in creating educational software, and our prototype efforts to create educational software expose where we need further basic research. CogSketch incorporates a model of visual processing and qualitative spatial representations, facilities for analogical reasoning and learning, and a large common-sense knowledge base. Our vision is that sketch-based intelligent educational software will ultimately be as widely available to students as graphing calculators are today. © 2010 Springer-Verlag Berlin Heidelberg.",CogSketch: Sketch understanding for cognitive science research and for education,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,9/30/2010,Ken,Forbus,2010
10.1007/978-3-642-15184-2_14,"© 2011, Springer-Verlag Berlin Heidelberg.The HUMAINE Database is grounded in HUMAINE’s core emphasis on considering emotion in a broad sense – ‘pervasive emotion’ – and engaging with the way it colours action and interaction. The aim of the database is to provide a resource to which the community can go to see and hear the forms that emotion takes in everyday action and interaction, and to look at the tools that might be relevant to describing it. Earlier chapters in this handbook describe the techniques and models underpinning the collection and labelling of such data. This chapter focuses on conveying the range of forms that emotion takes in the database, the ways that they can be labelled and the issues that the data raises. The HUMAINE Database provides naturalistic clips which record that kind of material, in multiple modalities, and labelling techniques that are suited to describing it. It was clear when the HUMAINE project began that work on databases should form part of it. However there were very different directions that the work might have taken. They were encapsulated early on in the contrast between ‘supportive’ and ‘provocative’ approaches, introduced in an earlier chapter in this handbook. The supportive option was to assemble a body of data whose size and structure allowed it to be used directly to build systems for recognition and/or synthesis. The provocative option was to assemble a body of data that encapsulated the challenges that the field faces.",The HUMAINE database,Cognitive Technologies,Book Chapter,1/1/2011,Laurence,Devillers,2011
10.1007/978-3-642-15387-7_21,"In this work, the design and implementation of a log analyzer agent is described. This agent is conceived to act as a part of a multi-agent Intrusion Detection System. The agent analyzes log files of services, applications or operating systems contrasting every log line with a set of security rules defined by experts. These rules can be created using a new easy to use XML-based format founded on an object-oriented model. Whenever a security match is found, the agent sends a security report to the next level of the multi-agent system using the IDMEF (Intrusion Detection Message Exchange Format) and the IDXP (Intrusion Detection Exchange Protocol). © 2010 Springer-Verlag.",A log analyzer agent for intrusion detection in a multi-agent system,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,11/23/2010,Amparo Alonso,Betanzos,2010
10.1007/978-3-642-18272-3_2,"The field of embodied artificial intelligence is maturing, and as such has progressed from what questions (""what is embodiment?"") to how questions: how should the body plan of an autonomous robot be designed to maximize the chance that it will exhibit a desired set of behaviors. In order to stand on its own however, rather than a reaction to classical AI, the field of embodied AI must address why questions as well: why should body and brain both be considered when creating intelligent machines? This report provides three new lines of evidence for why the body plays an important role in cognition: (1) an autonomous robot must be able to adapt behavior in the face of drastic, unanticipated change to its body; (2) under-explored body plans raise new research questions related to cognition; and (3) optimizing body plans accelerates the automated design of intelligent machines, compared to leaving them fixed. © 2011 Springer-Verlag Berlin Heidelberg.","The 'what', 'how' and the 'why' of evolutionary robotics",Studies in Computational Intelligence,Conference Paper,3/7/2011,Josh,Bongard,2011
10.1007/978-3-642-22887-2_41,"A novel approach to computer vision is outlined, involving the use of imprecise probabilities to connect a deep learning based hierarchical vision system with both local feature detection based preprocessing and symbolic cognition based guidance. The core notion is to cause the deep learning vision system to utilize imprecise rather than single-point probabilities, and use local feature detection and symbolic cognition to affect the confidence associated with particular imprecise probabilities, thus modulating the amount of credence the deep learning system places on various observations and guiding its pattern recognition/formation activity. The potential application to the hybridization of the DeSTIN, SIFT and OpenCog systems is described in moderate detail. The underlying ideas are even more broadly applicable, to any computer vision approach with a significant probabilistic component which satisfies certain broad criteria. © 2011 Springer-Verlag Berlin Heidelberg.","Imprecise probability as a linking mechanism between deep learning, symbolic cognition and local feature detection in vision processing",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,8/11/2011,Ben,Goertzel,2011
10.1007/978-3-642-23786-7_39,"Motivated by an important and challenging task encountered in material discovery, we consider the problem of finding K basis patterns of numbers that jointly compose N observed patterns while enforcing additional spatial and scaling constraints. We propose a Constraint Programming (CP) model which captures the exact problem structure yet fails to scale in the presence of noisy data about the patterns. We alleviate this issue by employing Machine Learning (ML) techniques, namely kernel methods and clustering, to decompose the problem into smaller ones based on a global data-driven view, and then stitch the partial solutions together using a global CP model. Combining the complementary strengths of CP and ML techniques yields a more accurate and scalable method than the few found in the literature for this complex problem. © 2011 Springer-Verlag.",Constraint reasoning and kernel clustering for pattern decomposition with scaling,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,9/26/2011,Carla,Gomes,2011
10.1007/978-3-642-27216-5_8,"Agent-based modeling is one of the popular tools for analyzing complex social systems. To model such systems, social attributes such as culture, law and institutions need to implemented as part of the context of a MAS, independently of individual agents. In this paper, we present MAIA; a framework for modeling agent-based systems based on the Institutional Analysis and Development Framework (IAD). The IAD is a well established comprehensive framework which addresses many social attributes. To make this framework applicable to agent-based software implementation, we inspire from some of the detailed definitions in the OperA methodology. The framework covers the different types of structures affecting agents at the operational level; physical, collective and constitutional. Moreover, this framework includes the conceptualization and design of evaluation. An agent-based methodology has also been developed from the MAIA framework which consists of two layers. A conceptualization layer for analyzing and decomposing the system and a detailed design layer which leads to the implementation of social models. MAIA allows the balance of global institutional requirements with the autonomy of individual agents thus enabling system evolution and reflecting more of reality in artificial societies. © 2012 Springer-Verlag Berlin Heidelberg.",An analysis and design framework for agent-based social simulation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/23/2012,Virginia,Dignum,2012
10.1007/978-3-642-31674-6_11,"© Springer-Verlag Berlin Heidelberg 2013.We herein report on a project devoted to charting some of the most salient points in a modern “geography” of minds, machines, and mathematics; the project is funded by the John Templeton Foundation, and is being carried out in Bringsjord's AI and Reasoning Laboratory. We are profoundly grateful to the John Templeton Foundation for their generous support of the project reported upon herein.","Toward a modern geography of minds, machines, and math","Studies in Applied Philosophy, Epistemology and Rational Ethics",Book Chapter,1/1/2013,Selmer,Bringsjord,2013
10.1007/978-3-642-31753-8_9,"Security of dynamic web applications is a serious issue. While Model Driven Architecture (MDA) techniques can be used to generate applications with given access control security properties, analysis of existing web applications is more problematic. In this paper we present a model transformation technique to automatically construct a role-based access control (RBAC) security model of dynamic web applications from previously recovered structural and behavioral models. The SecureUML model generated by this technique can be used to check for security properties of the original application. We demonstrate our approach by constructing an RBAC security model of PhpBB, a popular internet bulletin board system. © 2012 Springer-Verlag.",Recovering role-based access control security models from dynamic web applications,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,8/23/2012,Thomas,Dean,2012
10.1007/978-3-642-33347-7_13,"Cognitive models of blackjack playing are presented and investigated. Blackjack playing is considered a useful test case for theories on human learning. Curiously, despite the existence of a relatively simple, well-known and optimal strategy for blackjack, empirical studies have found that casino players play quite differently from that strategy. The computational models presented here attempt to explain this result by modelling blackjack playing using the cognitive architecture CHREST. Two approaches to modeling are investigated and compared; (i) the combination of classical and operant conditioning, as studied in psychology, and (ii) SARSA, as studied in AI. © 2012 Springer-Verlag.",A comparison between cognitive and AI models of blackjack strategy learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,11/6/2012,Fernand,Gobet,2012
10.1007/978-3-642-33783-3_58,"We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%. © 2012 Springer-Verlag.",Disentangling factors of variation for facial expression recognition,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,10/30/2012,Yoshua,Bengio,2012
10.1007/978-3-642-34274-5_1,"Machine consciousness has emerged from the confusion of an oxymoron into an evolving set of principles which, by leaning on information integration theories, define and distinguish what is meant by 'a conscious machine'. This paper reviews this process of emergence by indicating how it is possible to break away from the Chalmers 'hardness' of a computational consciousness by a general concept of A becoming conscious of B where both are formally described. We highlight how this differs from classical AI approaches, by following through a simple example using the specific methodology of weightless neural nets as an instance of a system that owes its competence to something that can be naturally described as 'being conscious' rather depending on the use AI algorithms structured by a programmer. © 2013 Springer-Verlag.",Back to basics and forward to novelty in machine consciousness,Advances in Intelligent Systems and Computing,Conference Paper,1/1/2013,Igor,Aleksander,2013
10.1007/978-3-642-35506-6_9,"Bridging the gap between symbolic and subsymbolic representations is a - perhaps the - key obstacle along the path from the present state of AI achievement to human-level artificial general intelligence. One approach to bridging this gap is hybridization - for instance, incorporation of a subsymbolic system and a symbolic system into a integrative cognitive architecture. Here we present a detailed design for an implementation of this approach, via integrating a version of the DeSTIN deep learning system into OpenCog, an integrative cognitive architecture including rich symbolic capabilities. This is a ""tight"" integration, in which the symbolic and subsymbolic aspects exert detailed real-time influence on each others' operations. An earlier technical report has described in detail the revisions to DeSTIN needed to support this integration, which are mainly along the lines of making it more ""representationally transparent,"" so that its internal states are easier for OpenCog to understand. © 2012 Springer-Verlag.",Perception processing for general intelligence: Bridging the symbolic/subsymbolic gap,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,12/26/2012,Ben,Goertzel,2012
10.1007/978-3-642-36657-4_1,"Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many unlabeled examples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build a deep architecture, either as initialization for a supervised predictor, or as a generative model. Deep learning algorithms can yield representations that are more abstract and better disentangle the hidden factors of variation underlying the unknown generating distribution, i.e., to capture invariances and discover non-local structure in that distribution. This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead, focusing on the questions of invariance and disentangling. © Springer-Verlag Berlin Heidelberg 2013.",Deep Learning of Representations,Intelligent Systems Reference Library,Review,10/21/2013,Yoshua,Bengio,2013
10.1007/978-3-642-37256-8_4,"Recently there has been a lot of interest in Cross Language Sentiment Analysis (CLSA) using Machine Translation (MT) to facilitate Sentiment Analysis in resource deprived languages. The idea is to use the annotated resources of one language (say, L1) for performing Sentiment Analysis in another language (say, L2) which does not have annotated resources. The success of such a scheme crucially depends on the availability of a MT system between L1 and L2. We argue that such a strategy ignores the fact that a Machine Translation system is much more demanding in terms of resources than a Sentiment Analysis engine. Moreover, these approaches fail to take into account the divergence in the expression of sentiments across languages. We provide strong experimental evidence to prove that even the best of such systems do not outperform a system trained using only a few polarity annotated documents in the target language. Having a very large number of documents in L1 also does not help because most Machine Learning approaches converge (or reach a plateau) after a certain training size (as demonstrated by our results). Based on our study, we take the stand that languages which have a genuine need for a Sentiment Analysis engine should focus on collecting a few polarity annotated documents in their language instead of relying on CLSA. © 2013 Springer-Verlag.",Lost in translation: Viability of machine translation for cross language sentiment analysis,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,4/3/2013,Pushpak,Bhattacharyya,2013
10.1007/978-3-642-37374-9_18,"Various threats and security issues exist in the cyber environment. For information assurance, we need to fully understand the concept of Cyber Intelligence (CI) which includes the identification, tracking, analysis and countering of security threats in cyberspace. In order to achieve this, we focus on educating and training CI for organizations and individuals via effective smart systems design and implementation using artificial intelligence (AI) techniques to build an interactive and adaptive learning environment. Based on investigation of basic theories and CI concepts, a simulation model is proposed. Interaction and adaptation are then integrated or designed upon simulation. Such a learning environment aims to impart a thorough and comprehensive understanding of cyber intelligence and provide enhanced learning experience. © 2013 Springer-Verlag.",Robo-teacher: A computational simulation based educational system to improve cyber security,Advances in Intelligent Systems and Computing,Conference Paper,1/1/2013,Hussein,Abbass,2013
10.1007/978-3-642-39521-5_5,"The bridging of the gap between 1) subsymbolic pattern recognition and learning algorithms and 2) symbolic reasoning algorithms, has been a major issue for AI since the early days of the field. One class of approaches involves integrating subsymbolic and symbolic systems, but this raises the question of how to effectively translate between the very different languages involved. In the approach described here, a frequent subtree mining algorithm is used to identify recurrent patterns in the state of a hierarchical deep learning system (DeSTIN) that is exposed to visual stimuli. The relationships between state-subtrees and percepts are then input to a probabilistic logic system (OpenCog's Probabilistic Logic Networks), which conducts uncertain inferences using them as axioms. The core conceptual idea is to use patterns in the states inferred by a perceptual hierarchy, as inputs to an uncertain logic system. Simple illustrative examples are presented based on the presentation of images of typed letters to DeSTIN. This work forms a component of a larger project to integrate perceptual, motoric and cognitive processing within the integrative OpenCog cognitive architecture. © 2013 Springer-Verlag.",Integrating deep learning based perception with probabilistic logic via frequent pattern mining,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,7/15/2013,Ben,Goertzel,2013
10.1007/978-3-642-39593-2_1,"Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges. © 2013 Springer-Verlag.",Deep learning of representations: Looking forward,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,9/3/2013,Yoshua,Bengio,2013
10.1007/978-3-662-44722-2_29,© IFIP International Federation for Information Processing 2014.This paper describes conformal prediction techniques for detecting anomalous trajectories in the maritime domain. The data used in experiments were obtained from Automatic Identification System (AIS) broadcasts-a system for tracking vessel locations. A dimensionality reduction package is used and a kernel density estimation function as a nonconformity measure has been applied to detect anomalies. We propose average p-value as an efficiency criteria for conformal anomaly detection. A comparison with a k-nearest neighbours non-conformity measure is presented and the results are discussed.,Anomaly detection of trajectories with kernel density estimation by conformal prediction,IFIP Advances in Information and Communication Technology,Article,1/1/2014,Alexander,Gammerman,2014
10.1007/978-3-662-44848-9_34,"In this paper we propose and investigate a novel nonlinear unit, called Lp unit, for deep neural networks. The proposed L p unit receives signals from several projections of a subset of units in the layer below and computes a normalized L p norm. We notice two interesting interpretations of the Lp unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the L p unit is, to a certain degree, similar to the recently proposed maxout unit [13] which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the L p unit is more efficient at representing complex, nonlinear separating boundaries. Each L p unit defines a superelliptic boundary, with its exact shape defined by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few L p units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed L p units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the L p units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed L p unit on the recently proposed deep recurrent neural networks (RNN). © 2014 Springer-Verlag.",Learned-norm pooling for deep feedforward and recurrent neural networks,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2014,Yoshua,Bengio,2014
10.1007/b106715,"Conformal prediction is a valuable new method of machine learning. Conformal predictors are among the most accurate methods of machine learning, and unlike other state-of-the-art methods, they provide information about their own accuracy and reliability. This new monograph integrates mathematical theory and revealing experimental work. It demonstrates mathematically the validity of the reliability claimed by conformal predictors when they are applied to independent and identically distributed data, and it confirms experimentally that the accuracy is sufficient for many practical problems. Later chapters generalize these results to models called repetitive structures, which originate in the algorithmic theory of randomness and statistical physics. The approach is flexible enough to incorporate most existing methods of machine learning, including newer methods such as boosting and support vector machines and older methods such as nearest neighbors and the bootstrap. Topics and Features: * Describes how conformal predictors yield accurate and reliable predictions, complemented with quantitative measures of their accuracy and reliability * Handles both classification and regression problems * Explains how to apply the new algorithms to real-world data sets * Demonstrates the infeasibility of some standard prediction tasks * Explains connections with Kolmogorov's algorithmic randomness, recent work in machine learning, and older work in statistics * Develops new methods of probability forecasting and shows how to use them for prediction in causal networks Researchers in computer science, statistics, and artificial intelligence will find the book an authoritative and rigorous treatment of some of the most promising new developments in machine learning. Practitioners and students in all areas of research that use quantitative prediction or machine learning will learn about important new methods. © 2005 Springer Science+Business Media, Inc.",Algorithmic learning in a random world,Algorithmic Learning in a Random World,Book,12/1/2005,Alexander,Gammerman,2005
10.1007/BF00119551,"The expectation-based 4D approach to dynamic machine vision exploiting integral spatiotemporal models of objects in the real world is discussed in the application domains of unmanned ground and air vehicles. The method has demonstrated superior performance over the last half decade in autonomous road vehicle guidance with three different vans and busses, with an AGV on the factory floor and with completely autonomous relative state estimation for a twin turboprop aircraft in the landing approach to a runway without any external support; in all application areas only a small set of conventional microcomputers was sufficient for realizing the system. This shows the computational efficiency of the method combining both conventional engineering type algorithms and artificial intelligence components in a well balanced way. The modularity of the approach is demonstrated in a simulation set-up serving both the ground- and the air vehicle applications. Expermental results in both areas are discussed. © 1992 Kluwer Academic Publishers.",A general dynamic vision architecture for UGV and UAV,Applied Intelligence,Article,9/1/1992,Ernst,Dickmanns,1992
10.1007/BF00128779,"Abductive reasoning involves generating an explanation for a given set of observations about the world. Abduction provides a good reasoning framework for many AI problems, including diagnosis, plan recognition and learning. This paper focuses on the use of abductive reasoning in diagnostic systems in which there may be more than one underlying cause for the observed symptoms. In exploring this topic, we will review and compare several different approaches, including Binary Choice Bayesian, Sequential Bayesian, Causal Model Based Abduction, Parsimonious Set Covering, and the use of First Order Logic. Throughout the paper we will use as an example a simple diagnostic problem involving automotive troubleshooting. © 1989 Intellect Ltd.",Abductive reasoning in multiple fault diagnosis,Artificial Intelligence Review,Article,6/1/1989,Tim,Finin,1989
10.1007/BF00139196,"Some connectionists (e.g. Smolensky, 1988) argue that classical AI models offer at best a good approximation to the fine-grained pychological truths revealed by connectionism. Likewise, some conventional AI theorists suggest that connectionism at best displays a new way of implementing the insight embodied in classical models. But the terms of the debate, I suggest, are by no means as cut and dried as such polarizations suggest. Instead, the mind may require explanation in terms of a multiplicity of virtual architectures. Both different tasks, and different aspects of that same tasks, may call for computational explanations quantifying over the operations and data-structures of different virtual machines. © 1989 Intellect Ltd.",Connectionism and the multiplicity of mind,Artificial Intelligence Review,Article,3/1/1989,Andy,Clark,1989
10.1007/BF00974171,"What's computation? The received answer is that computation is a computer at work, and a computer at work is that which can be modelled as a Turing machine at work. Unfortunately, as John Searle has recently argued, and as others have agreed, the received answer appears to imply that AI and Cog Sci are a royal waste of time. The argument here is alarmingly simple: AI and Cog Sci (of the ""Strong"" sort, anyway) are committed to the view that cognition is computation (or brains are computers); but all processes are computations (or all physical things are computers); so AI and Cog Sci are positively silly. I refute this argument herein, in part by defining the locutions 'x is a computer' and 'c is a computation' in a way that blocks Searle's argument but exploits the hard-to-deny link between What's Computation? and the theory of computation. However, I also provide, at the end of this essay, an argument which, it seems to me, implies not that AI and Cog Sci are silly, but that they're based on a form of computation that is well ""beneath"" human persons. © 1995 Kluwer Academic Publishers.","computation, among other things, is beneath us",Minds and Machines,Article,11/1/1994,Selmer,Bringsjord,1994
10.1007/BF01531061,The algorithmic probability measure is a probabilistic measure reflecting the structural complexity of strings of symbols. It has been formulated as a result of work into information theory and algorithmic complexity theory. This paper gives a study of this measure and how it can be approximated and used as the basis for solving problems in the field of artificial intelligence. © 1991 J.C. Baltzer A.G. Scientific Publishing Company.,The representation and manipulation of the algorithmic probability measure for problem solving,Annals of Mathematics and Artificial Intelligence,Article,9/1/1991,Alexander,Gammerman,1991
10.1007/BF01905886,"Some of the concerns people have about AI are: its misuses, effect on unemployment, and its potential for dehumanising. Contrary to what most people believe and fear, AI can lead to respect for the enormous power and complexity of the human mind. It is potentially very dangerous for users in the public domain to impute much more inferential power to computer systems, which look common-sensical, than they actually have. No matter how impressive AI programs may be, we must be aware of their limitations and should not abrogate human responsibility to such programs. © 1987 Springer-Verlag.",Artificial intelligence: Cannibal or missionary?,AI &amp; Society,Article,7/1/1987,Margaret,Boden,1987
10.1007/BF01905893,"This paper is a modified version of my acceptance lecture for the 1986 SPL-Insight Award. It turned into something of a personal credo - describing my view of the nature of AI the potential social benefit of applied AI the importance of basic AI research the role of logic and the methodology of rational construction the interplay of applied and basic AI research, and the importance of funding basic AI. These points are knitted together by an analogy between AI and structural engineering: in particular, between building expert systems and building bridges. © 1987 Springer-Verlag.",AI Bridges and dreams,AI &amp; Society,Article,7/1/1987,Alan,Bundy,1987
10.1007/bfb0026760,"© Springer-Verlag Berlin Heidelberg 1998.We address the issue of semantics for an agent communication language. In particular, the semantics of Knowledge Query Manipulation Language (KQML) is investigated. KQML is a language and protocol to support communication between software agents. We present a semantic description for KQML that associates states of the agent with the use of the language's primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Our research offers a method for a speech act theory-based semantic description of a language of communication acts.",Semantics for an agent communication language,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1998,Tim,Finin,1998
10.1007/bfb0026770,"© Springer-Verlag Berlin Heidelberg 1998.Tcl/Tk is an attractive language for the design of intelligent agents because it allows the quick construction of prototypes and user interfaces; new scripts can easily be bound at runtime to respond to events; and execution state is encapsulated by the interpreter, which helps in agent migration. However, a system of intelligent agents must share a common language for communicating requests and knowledge. We have integrated KQML (Knowledge Query Manipulation Language), one such standard language, into Tcl/Tk. The resulting system, called TKQML, provides several benefits to those building intelligent agent systems. First, TKQML allows easy integration of existing tools which have Tcl/Tk interfaces with an agent system by using TO to move information between KQML and the application. Second, TKQML is an excellent language with which to build agents, allowing on-the-fly specification of message handlers and construction of graphical interfaces. This paper describes the implementation of TKQML, and discusses its use in our intelligent agent system for information retrieval.",TKQML: A scripting tool for building agents,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1998,Tim,Finin,1998
10.1007/bfb0033857,"© Springer International Publishing AG, Part of Springer Science+Business Media.This paper describes a novel editor intended as an aid in the learning of the functional programming language Standard ML. A common technique used by novices is programming by analogy whereby students refer to similar programs that they have written before or have seen in the course literature and use these programs as a basis to write a new program. We present a novel editor for ML which supports programming by analogy by providing a collection of editing commands that transform old programs into new ones. Each command makes changes to an isolated part of the program. These changes are propagated to the rest of the program using analogical techniques. We observed a group of novice ML students to determine the most common programming errors in learning ML and restrict our editor such that it is impossible to commit these errors. In this way, students encounter fewer bugs and so their rate of learning increases. Our editor, C YNTHIA, has been implemented and is due to be tested on students of ML from September, 1997.",An editor for helping novices to learn standard ML,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/2015,Alan,Bundy,2015
10.1007/BFb0095437,"© Springer-Verlag Berlin Heidelberg 1998.The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operation appropriately. We present two different real-word applications in which GOLEX successfully operated a mobile robot in dynamic and even unstructured environments. These results suggest that the time is ripe for using symbolic action languages for mobile robot applications.",GOLEX m bridging the gap between logic (GOLOG) and a real robot,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Conference Paper,1/1/1998,Wolfram,Burgard,1998
10.1007/s00146-007-0090-9,"Bill Joy's deep pessimism is now famous. ""Why the Future Doesn't Need Us,"" his defense of that pessimism, has been read by, it seems, everyone - and many of these readers, apparently, have been converted to the dark side, or rather more accurately, to the future-is-dark side. Fortunately (for us; unfortunately for Joy), the defense, at least the part of it that pertains to AI and robotics, fails. Ours may be a dark future, but we cannot know that on the basis of Joy's reasoning. On the other hand, we ought to fear a good deal more than fear itself: we ought to fear not robots, but what some of us may do with robots. © Springer-Verlag London Limited 2007.",Ethical robots: The future can heed us,AI and Society,Article,4/1/2008,Selmer,Bringsjord,2008
10.1007/s00146-018-0810-3,"© 2018, The Author(s).Dreyfus’s work is widely known for its critique of artificial intelligence and still stands as an example of how to do excellent philosophical work that is at the same time relevant to contemporary technological and scientific developments. But for philosophers of technology, especially for those sympathetic to using Heidegger, Merleau-Ponty, and Wittgenstein as sources of inspiration, it has much more to offer. This paper outlines Dreyfus’s account of skillful coping and critically evaluates its potential for thinking about technology. First, it is argued that his account of skillful coping can be developed into a general view about handling technology which gives due attention to know-how/implicit knowledge and embodiment. Then a number of outstanding challenges are identified that are difficult to cope with if one remains entirely within the world of Dreyfus’s writings. They concern (1) questions regarding other conceptualizations of technology and human–technology relations, (2) issues concerning how to conceptualize the social and the relation between skill, meaning, and practices, and (3) the question about the ethical and political implications of his view, including how virtue and skill are related. Acknowledging some known discussions about Dreyfus’s work, but also drawing on other material and on the author’s previous writings, the paper suggests that to address these challenges and develop the account of skillful coping into a wider scoped, Dreyfus-inspired philosophy of technology, it could take more distance from Heidegger’s conceptions of technology and benefit from (more) engagement with work in postphenomenology (Ihde), pragmatism (Dewey), the later Wittgenstein, and virtue ethics.",Skillful coping with and through technologies: Some challenges and avenues for a Dreyfus-inspired philosophy of technology,AI and Society,Article,6/1/2019,Mark,Coeckelbergh,2019
10.1007/s00146-020-01104-w,"© 2020, The Author(s).A humanoid robot named ‘Sophia’ has sparked controversy since it has been given citizenship and has done media performances all over the world. The company that made the robot, Hanson Robotics, has touted Sophia as the future of artificial intelligence (AI). Robot scientists and philosophers have been more pessimistic about its capabilities, describing Sophia as a sophisticated puppet or chatbot. Looking behind the rhetoric about Sophia’s citizenship and intelligence and going beyond recent discussions on the moral status or legal personhood of AI robots, we analyse the performativity of Sophia from the perspective of what we call ‘political choreography’: drawing on phenomenological approaches to performance-oriented philosophy of technology. This paper proposes to interpret and discuss the world tour of Sophia as a political choreography that boosts the rise of the social robot market, rather than a statement about robot citizenship or artificial intelligence. We argue that the media performances of the Sophia robot were choreographed to advance specific political interests. We illustrate our philosophical discussion with media material of the Sophia performance, which helps us to explore the mechanisms through which the media spectacle functions hand in hand with advancing the economic interests of technology industries and their governmental promotors. Using a phenomenological approach and attending to the movement of robots, we also criticize the notion of ‘embodied intelligence’ used in the context of social robotics and AI. In this way, we put the discussions about the robot’s rights or citizenship in the context of AI politics and economics.",The political choreography of the Sophia robot: beyond robot rights and citizenship to political performances for the social robotics market,AI and Society,Article,9/1/2021,Mark,Coeckelbergh,2021
10.1007/s00146-021-01308-8,"© 2021, The Author(s).By mid-2019 there were more than 80 AI ethics guides available in the public domain. Despite this, 2020 saw numerous news stories break related to ethically questionable uses of AI. In part, this is because AI ethics theory remains highly abstract, and of limited practical applicability to those actually responsible for designing algorithms and AI systems. Our previous research sought to start closing this gap between the ‘what’ and the ‘how’ of AI ethics through the creation of a searchable typology of tools and methods designed to translate between the five most common AI ethics principles and implementable design practices. Whilst a useful starting point, that research rested on the assumption that all AI practitioners are aware of the ethical implications of AI, understand their importance, and are actively seeking to respond to them. In reality, it is unclear whether this is the case. It is this limitation that we seek to overcome here by conducting a mixed-methods qualitative analysis to answer the following four questions: what do AI practitioners understand about the need to translate ethical principles into practice? What motivates AI practitioners to embed ethical principles into design practices? What barriers do AI practitioners face when attempting to translate ethical principles into practice? And finally, what assistance do AI practitioners want and need when translating ethical principles into practice?","Operationalising AI ethics: barriers, enablers and next steps",AI and Society,Article,2/1/2023,Luciano,Floridi,2023
10.1007/s00146-021-01375-x,"© 2021, The Author(s).Most accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.",Narrative responsibility and artificial intelligence: How AI challenges human responsibility and sense-making,AI and Society,Article,1/1/2021,Mark,Coeckelbergh,2021
10.1007/s00146-022-01499-8,"© 2022, The Author(s).As China and the United States strive to be the primary global leader in AI, their visions are coming into conflict. This is frequently painted as a fundamental clash of civilisations, with evidence based primarily around each country’s current political system and present geopolitical tensions. However, such a narrow view claims to extrapolate into the future from an analysis of a momentary situation, ignoring a wealth of historical factors that influence each country’s prevailing philosophy of technology and thus their overarching AI strategies. In this article, we build a philosophy-of-technology-grounded framework to analyse what differences in Chinese and American AI policies exist and, on a fundamental level, why they exist. We support this with Natural Language Processing methods to provide an evidentiary basis for our analysis of policy differences. By looking at documents from three different American presidential administrations––Barack Obama, Donald Trump, and Joe Biden––as well as both national and local policy documents (many available only in Chinese) from China, we provide a thorough comparative analysis of policy differences. This article fills a gap in US–China AI policy comparison and constructs a framework for understanding the origin and trajectory of policy differences. By investigating what factors are informing each country’s philosophy of technology and thus their overall approach to AI policy, we argue that while significant obstacles to cooperation remain, there is room for dialogue and mutual growth.",Artificial intelligence with American values and Chinese characteristics: a comparative analysis of American and Chinese governmental AI policies,AI and Society,Article,1/1/2022,Luciano,Floridi,2022
10.1007/s001650200019,"CYNTHIA is a transformation-based editor for a functional subset of ML that lies somewhere between a structure editor and a framework for formal program development. Users construct programs from existing code by applying editing commands that make a semantic analysis of the program's behaviour, e.g., whether it is terminating. All analysis is done using the Oyster system, which is an implementation of proofs-as-programs. We concentrate on identifying analyses that can be done fully automatically (e.g., using a decision procedure) and hence can be hidden from the user. As a result, CYNTHIA represents progress towards a goal of program editors that make an intelligent analysis of their code, but in a way that requires no extra input from the programmer. © 2002 BCS.",Proofs-as-programs as a framework for the design of an analogy-based ML editor,Formal Aspects of Computing,Article,12/1/2002,Alan,Bundy,2002
10.1007/s00500-010-0613-z,"This paper presents two new approaches for constructing an ensemble of neural networks (NN) using coevolution and the artificial immune system (AIS). These approaches are extensions of the CLONal Selection Algorithm for building ENSembles (CLONENS) algorithm. An explicit diversity promotion technique was added to CLONENS and a novel coevolutionary approach to build neural ensembles is introduced, whereby two populations representing the gates and the individual NN are coevolved. The former population is responsible for defining the ensemble size and selecting the members of the ensemble. This population is evolved using the differential evolution algorithm. The latter population supplies the best individuals for building the ensemble, which is evolved by AIS. Results show that it is possible to automatically define the ensemble size being also possible to find smaller ensembles with good generalization performance on the tested benchmark regression problems. More interestingly, the use of the diversity measure during the evolutionary process did not necessarily improve generalization. In this case, diverse ensembles may be found using only implicit diversity promotion techniques. © 2010 Springer-Verlag.",The use of coevolution and the artificial immune system for ensemble learning,Soft Computing,Article,9/1/2011,Hussein,Abbass,2011
10.1007/s10506-005-6742-5,"The paper identifies some of the problems with legal systems and outlines the potential of AI technology for overcoming them. For expository purposes, this outline is based on a simplified epistemology of the primary functions of law. Social and philosophical impediments from the side of the legal community to taking advantage of the potential of this technology are discussed and strategic recommendations are given. © Springer 2004.",AI and the conquest of complexity in law,Artificial Intelligence and Law,Review,9/1/2004,Wolfgang,Bibel,2004
10.1007/s10508-009-9507-5,"The present study explored the relationship between compulsive sexual behavior (CSB) and unprotected anal intercourse (UAI) for men who have sex with men (MSM) across a number of ethnic/racial groups and who used the Internet to seek sexual partners. A sample of 2,716 MSM (512 Asian, 445 Black, 683 Latino, 348 Other, 728 White) completed on online survey that collected information about their sexual behaviors with partners met online and offline. The survey also included the Compulsive Sexual Behavior Inventory (CSBI). Consistent with the notion that CSB is a stable trait, higher scores on the CSBI were associated with greater odds for engaging in UAI, regardless of the context in which sex partners were met (online or offline). Differences in median CSB scores were generally similar across racial and ethnic groups. The median CSB score was significantly higher for HIV-positive participants than for HIV-negative participants. HIV-prevention interventions are needed among MSM, but should take into account that some may be resistant to risk reduction strategies because of CSB. © 2009 Springer Science+Business Media, LLC.",Compulsive sexual behavior and risk for unsafe sex among internet using men who have sex with men,Archives of Sexual Behavior,Article,10/1/2010,W,Ross,2010
10.1007/s10508-010-9653-9,"The AIDS epidemic in the United States continues to disproportionately affect minorities of color, especially African Americans. The purpose of this study was to explore the sexual behaviors of a sample of African American HIV positive crack smokers aware of their serostatus. Participants (100 men, 37 women) were included in this study based on the following criteria: a minimum age of 18 years, HIV positive serostatus, treatment with HIV antiretroviral medications for a minimum of 3 months prior to interview, crack cocaine use at least once in the 7 days prior to being interviewed, willingness to provide a urine sample to confirm recent drug use, and vaginal or anal sex at least once in the past 7 days. The questionnaire was a compilation of other reliable surveys and was designed to collect sociodemographic data, drug use, sexual behavior, condom use intentions and motivators, STD and HIV infection history, HIV medications, and adherence requirements. Participants reported having 1,266 different partners in the 30 days prior to the interview and had traded sex for money or drugs with 68%. A total of 79 participants had multiple partners and accounted for 1,247 partnerships. Rates of consistent condom use across partnerships were low, indicating that more interventions in this at-risk population are needed. © Springer Science+Business Media, LLC 2010.",Sexual activity in HIV-positive African American crack cocaine smokers,Archives of Sexual Behavior,Article,12/1/2010,W,Ross,2010
10.1007/s10514-017-9615-3,"© 2017, The Author(s).Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.",Revisiting active perception,Autonomous Robots,Article,2/1/2018,Ruzena,Bajcsy,2018
10.1007/s10676-015-9377-6,"© 2015, Springer Science+Business Media Dordrecht.Responding to long-standing warnings that robots and AI will enslave humans, I argue that the main problem we face is not that automation might turn us into slaves but, rather, that we remain masters. First I construct an argument concerning what I call ‘the tragedy of the master’: using the master–slave dialectic, I argue that automation technologies threaten to make us vulnerable, alienated, and automated masters. I elaborate the implications for power, knowledge, and experience. Then I critically discuss and question this argument but also the very thinking in terms of masters and slaves, which fuels both arguments. I question the discourse about slavery and object to the assumptions made about human–technology relations. However, I also show that the discussion about masters and slaves attends us to issues with human–human relations, in particular to the social consequences of automation such as power issues and the problem of the relation between automation and (un)employment. Finally, I reflect on how we can respond to our predicament, to ‘the tragedy of the master’.","The tragedy of the master: automation, vulnerability, and distance",Ethics and Information Technology,Article,9/3/2015,Mark,Coeckelbergh,2015
10.1007/s10676-016-9400-6,"© 2016, Springer Science+Business Media Dordrecht.The growing number of ‘smart’ instruments, those equipped with AI, has raised concerns because these instruments make autonomous decisions; that is, they act beyond the guidelines provided them by programmers. Hence, the question the makers and users of smart instrument (e.g., driver-less cars) face is how to ensure that these instruments will not engage in unethical conduct (not to be conflated with illegal conduct). The article suggests that to proceed we need a new kind of AI program—oversight programs—that will monitor, audit, and hold operational AI programs accountable.",AI assisted ethics,Ethics and Information Technology,Article,6/1/2016,Oren,Etzioni,2016
10.1007/s10677-009-9186-2,"Scenarios involving the introduction of artificially intelligent (AI) assistive technologies in health care practices raise several ethical issues. In this paper, I discuss four objections to introducing AI assistive technologies in health care practices as replacements of human care. I analyse them as demands for felt care, good care, private care, and real care. I argue that although these objections cannot stand as good reasons for a general and a priori rejection of AI assistive technologies as such or as replacements of human care, they demand us to clarify what is at stake, to develop more comprehensive criteria for good care, and to rethink existing practices of care. In response to these challenges, I propose a (modified) capabilities approach to care and emphasize the inherent social dimension of care. I also discuss the demand for real care by introducing the 'Care Experience Machine' thought experiment. I conclude that if we set the standards of care too high when evaluating the introduction of AI assistive technologies in health care, we have to reject many of our existing, low-tech health care practices. © Springer Science+Business Media B.V. 2009.","Health care, capabilities, and AI assistive technologies",Ethical Theory and Moral Practice,Article,4/1/2010,Mark,Coeckelbergh,2010
10.1007/s10892-017-9252-2,"© 2017, Springer Science+Business Media Dordrecht.This article reviews the reasons scholars hold that driverless cars and many other AI equipped machines must be able to make ethical decisions, and the difficulties this approach faces. It then shows that cars have no moral agency, and that the term ‘autonomous’, commonly applied to these machines, is misleading, and leads to invalid conclusions about the ways these machines can be kept ethical. The article’s most important claim is that a significant part of the challenge posed by AI-equipped machines can be addressed by the kind of ethical choices made by human beings for millennia. Ergo, there is little need to teach machines ethics even if this could be done in the first place. Finally, the article points out that it is a grievous error to draw on extreme outlier scenarios—such as the Trolley narratives—as a basis for conceptualizing the ethical issues at hand.",Incorporating Ethics into Artificial Intelligence,Journal of Ethics,Article,12/1/2017,Oren,Etzioni,2017
10.1007/s10994-006-5833-1,"We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a firstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.",Markov logic networks,Machine Learning,Conference Paper,2/1/2006,Pedro,Domingos,2006
10.1007/s10994-018-5754-9,"© 2018, The Author(s).It is well known that ensembling predictions from different Machine Learning (ML) algorithms can improve accuracy. This paper proposes a approach to combine Conformal Predictors (CPs) with different underlying ML algorithms in a way that preserves their key property, i.e. validity. Different combination methods are discussed and their performance is evaluated on a chemoinformatics problem. In order to deal with the size, high-dimensionality, and strong imbalance of the data set, the paper applies a special type of CP: an Inductive Mondrian Conformal Predictor. We propose and evaluate, alongside methods from Statistical Hypothesis Testing, a heuristically motivated method for learning to combine CPs to improve the quality of prediction. We also explore a general nonparametric method for recovering validity after combination using a calibration set. On a real-world data set, several of the combined predictors consistently outperform the base CPs.",Combination of inductive mondrian conformal predictors,Machine Learning,Article,3/15/2019,Alexander,Gammerman,2019
10.1007/s11023-008-9131-5,"Several authors have hailed intuition as one of the defining features of expertise. In particular, while disagreeing on almost anything that touches on human cognition and artificial intelligence, Hubert Dreyfus and Herbert Simon agreed on this point. However, the highly influential theories of intuition they proposed differed in major ways, especially with respect to the role given to search and as to whether intuition is holistic or analytic. Both theories suffer from empirical weaknesses. In this paper, we show how, with some additions, a recent theory of expert memory (the template theory) offers a coherent and wide-ranging explanation of intuition in expert behaviour. It is shown that the theory accounts for the key features of intuition: it explains the rapid onset of intuition and its perceptual nature, provides mechanisms for learning, incorporates processes showing how perception is linked to action and emotion, and how experts capture the entirety of a situation. In doing so, the new theory addresses the issues problematic for Dreyfus's and Simon's theories. Implications for research and practice are discussed. © 2008 Springer Science+Business Media B.V.",Expertise and intuition: A tale of three theories,Minds and Machines,Article,5/1/2009,Fernand,Gobet,2009
10.1007/s11023-017-9445-2,"© 2017, The Author(s).Online technologies enable vast amounts of data to outlive their producers online, thereby giving rise to a new, digital form of afterlife presence. Although researchers have begun investigating the nature of such presence, academic literature has until now failed to acknowledge the role of commercial interests in shaping it. The goal of this paper is to analyse what those interests are and what ethical consequences they may have. This goal is pursued in three steps. First, we introduce the concept of the Digital Afterlife Industry (DAI), and define it as an object of study. Second, we identify the politico-economic interests of the DAI. For this purpose, we develop an analytical approach based on an informational interpretation of Marxian economics. Third, we explain the practical manifestations of the interests using four real life cases. The findings expose the incentives of the DAI to alter what is referred to as the “informational bodies” of the dead, which in turn is to be seen as a violation of the principle of human dignity. To prevent such consequences, we argue that the ethical conventions that guide trade with remains of organic bodies may serve as a good model for future regulation of DAI.",The Political Economy of Death in the Age of Information: A Critical Approach to the Digital Afterlife Industry,Minds and Machines,Article,12/1/2017,Luciano,Floridi,2017
10.1007/s11023-020-09548-1,"© 2020, The Author(s).In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.","GPT-3: Its Nature, Scope, Limits, and Consequences",Minds and Machines,Note,12/1/2020,Luciano,Floridi,2020
10.1007/s11023-021-09557-8,"© 2021, The Author(s).A series of recent developments points towards auditing as a promising mechanism to bridge the gap between principles and practice in AI ethics. Building on ongoing discussions concerning ethics-based auditing, we offer three contributions. First, we argue that ethics-based auditing can improve the quality of decision making, increase user satisfaction, unlock growth potential, enable law-making, and relieve human suffering. Second, we highlight current best practices to support the design and implementation of ethics-based auditing: To be feasible and effective, ethics-based auditing should take the form of a continuous and constructive process, approach ethical alignment from a system perspective, and be aligned with public policies and incentives for ethically desirable behaviour. Third, we identify and discuss the constraints associated with ethics-based auditing. Only by understanding and accounting for these constraints can ethics-based auditing facilitate ethical alignment of AI, while enabling society to reap the full economic and social benefits of automation.",Ethics-Based Auditing to Develop Trustworthy AI,Minds and Machines,Note,6/1/2021,Luciano,Floridi,2021
10.1007/s11023-021-09563-w,"© 2021, The Author(s).As the range of potential uses for Artificial Intelligence (AI), in particular machine learning (ML), has increased, so has awareness of the associated ethical issues. This increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals, groups, society, and the environment from AI harms. In response to this realisation, there has been a proliferation of principle-based ethics codes, guidelines and frameworks. However, it has become increasingly clear that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems. In previous work, we analysed whether it is possible to close this gap between the ‘what’ and the ‘how’ of AI ethics through the use of tools and methods designed to help AI developers, engineers, and designers translate principles into practice. We concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible (and thus vulnerable to ethics washing) or too strict (unresponsive to context). This raised the question: if, even with technical guidance, AI ethics is challenging to embed in the process of algorithmic design, is the entire pro-ethical design endeavour rendered futile? And, if no, then how can AI ethics be made useful for AI practitioners? This is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited, and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed ‘Ethics as a Service.’",Ethics as a Service: A Pragmatic Operationalisation of AI Ethics,Minds and Machines,Article,6/1/2021,Luciano,Floridi,2021
10.1007/s11023-021-09577-4,"© 2021, The Author(s).The proposed European Artificial Intelligence Act (AIA) is the first attempt to elaborate a general legal framework for AI carried out by any major global economy. As such, the AIA is likely to become a point of reference in the larger discourse on how AI systems can (and should) be regulated. In this article, we describe and discuss the two primary enforcement mechanisms proposed in the AIA: the conformity assessments that providers of high-risk AI systems are expected to conduct, and the post-market monitoring plans that providers must establish to document the performance of high-risk AI systems throughout their lifetimes. We argue that the AIA can be interpreted as a proposal to establish a Europe-wide ecosystem for conducting AI auditing, albeit in other words. Our analysis offers two main contributions. First, by describing the enforcement mechanisms included in the AIA in terminology borrowed from existing literature on AI auditing, we help providers of AI systems understand how they can prove adherence to the requirements set out in the AIA in practice. Second, by examining the AIA from an auditing perspective, we seek to provide transferable lessons from previous research about how to refine further the regulatory approach outlined in the AIA. We conclude by highlighting seven aspects of the AIA where amendments (or simply clarifications) would be helpful. These include, above all, the need to translate vague concepts into verifiable criteria and to strengthen the institutional safeguards concerning conformity assessments based on internal checks.",Conformity Assessments and Post-market Monitoring: A Guide to the Role of Auditing in the Proposed European AI Regulation,Minds and Machines,Article,6/1/2022,Luciano,Floridi,2022
10.1007/s11023-022-09598-7,"© 2022, The Author(s).Necessity and sufficiency are the building blocks of all successful explanations. Yet despite their importance, these notions have been conceptually underdeveloped and inconsistently applied in explainable artificial intelligence (XAI), a fast-growing research area that is so far lacking in firm theoretical foundations. In this article, an expanded version of a paper originally presented at the 37th Conference on Uncertainty in Artificial Intelligence (Watson et al., 2021), we attempt to fill this gap. Building on work in logic, probability, and causality, we establish the central role of necessity and sufficiency in XAI, unifying seemingly disparate methods in a single formal framework. We propose a novel formulation of these concepts, and demonstrate its advantages over leading alternatives. We present a sound and complete algorithm for computing explanatory factors with respect to a given context and set of agentive preferences, allowing users to identify necessary and sufficient conditions for desired outcomes at minimal cost. Experiments on real and simulated data confirm our method’s competitive performance against state of the art XAI tools on a diverse array of tasks.",Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice,Minds and Machines,Article,3/1/2022,Luciano,Floridi,2022
10.1007/s11023-022-09612-y,"© 2022, The Author(s).On the whole, the US Algorithmic Accountability Act of 2022 (US AAA) is a pragmatic approach to balancing the benefits and risks of automated decision systems. Yet there is still room for improvement. This commentary highlights how the US AAA can both inform and learn from the European Artificial Intelligence Act (EU AIA).",The US Algorithmic Accountability Act of 2022 vs. The EU Artificial Intelligence Act: what can they learn from each other?,Minds and Machines,Article,12/1/2022,Luciano,Floridi,2022
10.1007/s11023-022-09620-y,"© 2023, The Author(s).Organisations that design and deploy artificial intelligence (AI) systems increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. One major obstacle organisations face when attempting to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the purpose of implementing AI governance in practice. We find that attempts to classify AI systems proposed in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, input data, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the vocabulary needed to demarcate the material scope of their AI governance frameworks.","The Switch, the Ladder, and the Matrix: Models for Classifying AI Systems",Minds and Machines,Article,3/1/2023,Luciano,Floridi,2023
10.1007/s11023-023-09625-1,"© 2023, The Author(s).The US is promoting a new vision of a “Good AI Society” through its recent AI Bill of Rights. This offers a promising vision of community-oriented equity unique amongst peer countries. However, it leaves the door open for potential rights violations. Furthermore, it may have some federal impact, but it is non-binding, and without concrete legislation, the private sector is likely to ignore it.","The Blueprint for an AI Bill of Rights: In Search of Enaction, at Risk of Inaction",Minds and Machines,Article,1/1/2023,Luciano,Floridi,2023
10.1007/s11229-014-0424-3,"© 2014, Springer Science+Business Media Dordrecht.The vision of machines autonomously carrying out substantive conjecture generation, theorem discovery, proof discovery, and proof verification in mathematics and the natural sciences has a long history that reaches back before the development of automatic systems designed for such processes. While there has been considerable progress in proof verification in the formal sciences, for instance the Mizar project’ and the four-color theorem, now machine verified, there has been scant such work carried out in the realm of the natural sciences—until recently. The delay in the case of the natural sciences can be attributed to both a lack of formal analysis of the so-called “theories” in such sciences, and the lack of sufficient progress in automated theorem proving. While the lack of analysis is probably due to an inclination toward informality and empiricism on the part of nearly all of the relevant scientists, the lack of progress is to be expected, given the computational hardness of automated theorem proving; after all, theoremhood in even first-order logic is Turing-undecidable. We give in the present short paper a compressed report on our building upon these formal theories using logic-based AI in order to achieve, in relativity, both machine proof discovery and proof verification, for theorems previously established by humans. Our report is intended to serve as a springboard to machine-produced results in the future that have not been obtained by humans.",Proof verification and proof discovery for relativity,Synthese,Article,7/29/2015,Selmer,Bringsjord,2015
10.1007/s11263-021-01494-4,"© 2021, The Author(s).Standard registration algorithms need to be independently applied to each surface to register, following careful pre-processing and hand-tuning. Recently, learning-based approaches have emerged that reduce the registration of new scans to running inference with a previously-trained model. The potential benefits are multifold: inference is typically orders of magnitude faster than solving a new instance of a difficult optimization problem, deep learning models can be made robust to noise and corruption, and the trained model may be re-used for other tasks, e.g. through transfer learning. In this paper, we cast the registration task as a surface-to-surface translation problem, and design a model to reliably capture the latent geometric information directly from raw 3D face scans. We introduce Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an improved point cloud encoder, a novel visual attention mechanism, graph convolutional decoders with skip connections, and a specialized mouth model that we smoothly integrate with the mesh convolutions. Compared to the previous state-of-the-art learning algorithms for non-rigid registration of face scans, SMF only requires the raw data to be rigidly aligned (with scaling) with a pre-defined face template. Additionally, our model provides topologically-sound meshes with minimal supervision, offers faster training time, has orders of magnitude fewer trainable parameters, is more robust to noise, and can generalize to previously unseen datasets. We extensively evaluate the quality of our registrations on diverse data. We demonstrate the robustness and generalizability of our model with in-the-wild face scans across different modalities, sensor types, and resolutions. Finally, we show that, by learning to register scans, SMF produces a hybrid linear and non-linear morphable model. Manipulation of the latent space of SMF allows for shape generation, and morphing applications such as expression transfer in-the-wild. We train SMF on a dataset of human faces comprising 9 large-scale databases on commodity hardware.",Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation,International Journal of Computer Vision,Article,9/1/2021,Michael,Bronstein,2021
10.1007/s11412-021-09357-3,"© 2021, The Author(s).In order to promote the practice of co-creation, a real-time collaboration (RTC) version of the popular block-based programming (BBP) learning environment, MIT App Inventor (MAI), was proposed and implemented. RTC overcomes challenges related to non-collocated group work, thus lowering barriers to cross-region and multi-user collaborative software development. An empirical study probed into the differential impact on self-efficacy and collaborative behavior of learners in the environment depending upon their disciplinary background. The study serves as an example of the use of learning analytics to explore the frequent behavior patterns of adult learners, in this case specifically while performing BBP in MAI integrated with RTC. This study compares behavior patterns that are collaborative or individual that occurred on the platform, and investigates the effects of collaboration on learners working within the RTC depending on whether they were CS-majors or not. We highlight advantages of the new MAI design during multi-user programming in the online RTC based on the connections between the interface design and BBP as illustrated by two significant behavior patterns found in this instructional experiment. First, the multi-user programming in the RTC allowed multiple tasks to happen at the same time, which promoted engagement in joint behavior. For example, one user arranged components in the interface design while another dragged blocks to complete the program. Second, this study confirmed that the Computer Programming Self-Efficacy (CPSE) was similar for individual and multi-user programming overall. The CPSE of the homogeneous CS-major groups engaged in programming within the RTC was higher than that of the homogeneous non-CS-major groups and heterogeneous groups. There was no significant difference between the CPSE of the homogenous non-CS group and the CPSE of the heterogeneous groups, regardless of whether they were engaged in individual programming or collaborative programming within their groups. The results of the study support the value of engaging with MAI collaboratively, especially for CS-majors, and suggest directions for future work in RTC design.",Self-efficacy and behavior patterns of learners using a real-time collaboration system developed for group programming,International Journal of Computer-Supported Collaborative Learning,Article,12/1/2021,Hal,Abelson,2021
10.1007/s11721-021-00206-5,"© 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.In swarm decision making, hand-crafting agents’ rules that use local information to achieve desirable swarm-level behaviours is a non-trivial design problem. Instead of relying entirely on swarm experts for designing these local rules, machine learning (ML) algorithms can be utilised for learning some of the local rules by mapping an agent’s perception to an appropriate action. To facilitate this process, we propose the use of Machine Education (ME) as a systematic approach for designing a curriculum for teaching the agents the required skills to autonomously select appropriate behaviours. We study the use of ME in the context of decision-making in best-of-n problems. The proposed approach draws on swarm robotics expertise for identifying agents’ capabilities and limitations, the skills required for generating the desirable behaviours, and the corresponding performance measures. In addition, ME utilises ML expertise for the selection and development of the ML algorithms suitable for each skill. The results of the experimental evaluations demonstrate the superior efficacy of the ME-based approach over the state-of-the-art approaches with respect to speed and accuracy. In addition, our approach shows considerable robustness to changes in swarm size and to changes in sensing and communication noise. Our findings promote the use of ME for teaching swarm members the required skills for achieving complex swarm tasks.",A machine education approach to swarm decision-making in best-of-n problems,Swarm Intelligence,Article,3/1/2022,Hussein,Abbass,2022
10.1007/s11948-019-00146-8,"© 2019, The Author(s).This paper discusses the problem of responsibility attribution raised by the use of artificial intelligence (AI) technologies. It is assumed that only humans can be responsible agents; yet this alone already raises many issues, which are discussed starting from two Aristotelian conditions for responsibility. Next to the well-known problem of many hands, the issue of “many things” is identified and the temporal dimension is emphasized when it comes to the control condition. Special attention is given to the epistemic condition, which draws attention to the issues of transparency and explainability. In contrast to standard discussions, however, it is then argued that this knowledge problem regarding agents of responsibility is linked to the other side of the responsibility relation: the addressees or “patients” of responsibility, who may demand reasons for actions and decisions made by using AI. Inspired by a relational approach, responsibility as answerability thus offers an important additional, if not primary, justification for explainability based, not on agency, but on patiency.","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",Science and Engineering Ethics,Article,8/1/2020,Mark,Coeckelbergh,2020
10.1007/s11948-019-00165-5,"© 2019, The Author(s).The debate about the ethical implications of Artificial Intelligence dates from the 1960s (Samuel in Science, 132(3429):741–742, 1960. https://doi.org/10.1126/science.132.3429.741; Wiener in Cybernetics: or control and communication in the animal and the machine, MIT Press, New York, 1961). However, in recent years symbolic AI has been complemented and sometimes replaced by (Deep) Neural Networks and Machine Learning (ML) techniques. This has vastly increased its potential utility and impact on society, with the consequence that the ethical debate has gone mainstream. Such a debate has primarily focused on principles—the ‘what’ of AI ethics (beneficence, non-maleficence, autonomy, justice and explicability)—rather than on practices, the ‘how.’ Awareness of the potential issues is increasing at a fast rate, but the AI community’s ability to take action to mitigate the associated risks is still at its infancy. Our intention in presenting this research is to contribute to closing the gap between principles and practices by constructing a typology that may help practically-minded developers apply ethics at each stage of the Machine Learning development pipeline, and to signal to researchers where further work is needed. The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI. The article outlines the research method for creating this typology, the initial findings, and provides a summary of future research needs.","From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",Science and Engineering Ethics,Article,8/1/2020,Luciano,Floridi,2020
10.1007/s12193-015-0195-2,"© 2015, OpenInterface Association.The task of the Emotion Recognition in the Wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based “bag-of-mouths” model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67 % on the 2014 dataset.",EmoNets: Multimodal deep learning approaches for emotion recognition in video,Journal on Multimodal User Interfaces,Article,6/1/2016,Yoshua,Bengio,2016
10.1007/s12559-018-9619-0,"© 2019, The Author(s).Artificial intelligence (AI) is finding more uses in the human society resulting in a need to scrutinise the relationship between humans and AI. Technology itself has advanced from the mere encoding of human knowledge into a machine to designing machines that “know how” to autonomously acquire the knowledge they need, learn from it and act independently in the environment. Fortunately, this need is not new; it has scientific grounds that could be traced back to the inception of computers. This paper uses a multi-disciplinary lens to explore how the natural cognitive intelligence in a human could interface with the artificial cognitive intelligence of a machine. The scientific journey over the last 50 years will be examined to understand the Human-AI relationship, and to present the nature of, and the role of trust in, this relationship. Risks and opportunities sitting at the human-AI interface will be studied to reveal some of the fundamental technical challenges for a trustworthy human-AI relationship. The critical assessment of the literature leads to the conclusion that any social integration of AI into the human social system would necessitate a form of a relationship on one level or another in society, meaning that humans will “always” actively participate in certain decision-making loops—either in-the-loop or on-the-loop—that will influence the operations of AI, regardless of how sophisticated it is.","Social Integration of Artificial Intelligence: Functions, Automation Allocation Logic and Human-Autonomy Trust",Cognitive Computation,Review,4/15/2019,Hussein,Abbass,2019
10.1007/s12559-019-09704-5,"© 2020, Springer Science+Business Media, LLC, part of Springer Nature.Dialogue act classification (DAC) gives a significant insight into understanding the communicative intention of the user. Numerous machine learning (ML) and deep learning (DL) approaches have been proposed over the years in these regards for task-oriented/independent conversations in the form of texts. However, the affect of emotional state in determining the dialogue acts (DAs) has not been studied in depth in a multi-modal framework involving text, audio, and visual features. Conversations are intrinsically determined and regulated by direct, exquisite, and subtle emotions. The emotional state of a speaker has a considerable affect on its intentional or its pragmatic content. This paper thoroughly investigates the role of emotions in automatic identification of the DAs in task-independent conversations in a multi-modal framework (specifically audio and texts). A DL-based multi-tasking network for DAC and emotion recognition (ER) has been developed incorporating attention to facilitate the fusion of different modalities. An open source, benchmarked ER multi-modal dataset IEMOCAP has been manually annotated for its corresponding DAs to make it suitable for multi-task learning and further advance the research in multi-modal DAC. The proposed multi-task framework attains an improvement of 2.5% against its single-task DAC counterpart for manually annotated IEMOCAP dataset. Results as compared with several baselines establish the efficacy of the proposed approach and the importance of incorporating emotion while identifying the DAs.",Emotion Aided Dialogue Act Classification for Task-Independent Conversations in a Multi-modal Framework,Cognitive Computation,Article,3/1/2021,Pushpak,Bhattacharyya,2021
10.1007/s12559-020-09718-4,"© 2020, Springer Science+Business Media, LLC, part of Springer Nature.An essential component of any dialogue system is understanding the language which is known as spoken language understanding (SLU). Dialogue act classification (DAC), intent detection (ID) and slot filling (SF) are significant aspects of every dialogue system. In this paper, we propose a deep learning-based multi-task model that can perform DAC, ID and SF tasks together. We use a deep bi-directional recurrent neural network (RNN) with long short-term memory (LSTM) and gated recurrent unit (GRU) as the frameworks in our multi-task model. We use attention on the LSTM/GRU output for DAC and ID. The attention outputs are fed to individual task-specific dense layers for DAC and ID. The output of LSTM/GRU is fed to softmax layer for slot filling as well. Experiments on three datasets, i.e. ATIS, TRAINS and FRAMES, show that our proposed multi-task model performs better than the individual models as well as all the pipeline models. The experimental results prove that our attention-based multi-task model outperforms the state-of-the-art approaches for the SLU tasks. For DAC, in relation to the individual model, we achieve an improvement of more than 2% for all the datasets. Similarly, for ID, we get an improvement of 1% on the ATIS dataset, while for TRAINS and FRAMES dataset, there is a significant improvement of more than 3% compared to individual models. We also get a 0.8% enhancement for ATIS and a 4% enhancement for TRAINS and FRAMES dataset for SF with respect to individual models. Results obtained clearly show that our approach is better than existing methods. The validation of the obtained results is also demonstrated using statistical significance t tests.","A Deep Multi-task Model for Dialogue Act Classification, Intent Detection and Slot Filling",Cognitive Computation,Article,5/1/2021,Pushpak,Bhattacharyya,2021
10.1007/s13218-020-00637-y,"© 2020, The Author(s).The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.",One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency,KI - Kunstliche Intelligenz,Article,6/1/2020,Peter,Flach,2020
10.1007/s13347-021-00479-y,"© 2021, The Author(s).While today there is much discussion about the ethics of artificial intelligence (AI), less work has been done on the philosophical nature of AI. Drawing on Bergson and Ricoeur, this paper proposes to use the concepts of time, process, and narrative to conceptualize AI and its normatively relevant impact on human lives and society. Distinguishing between a number of different ways in which AI and time are related, the paper explores what it means to understand AI as narrative, as process, or as the emergent outcome of processes and narratives. It pays particular attention to what it calls the “narrator” and “time machine” roles of AI and the normative implications of these roles. It argues that AI processes and narratives shape our time and link past, present, and future in particular ways that are ethically and politically significant.","Time Machines: Artificial Intelligence, Process, and Narrative",Philosophy and Technology,Article,12/1/2021,Mark,Coeckelbergh,2021
10.1016/0004-3702(90)90099-L,"Reasoning about shapes and their interaction is an important unsolved problem in artificial intelligence. In this paper, we present a theory of qualitative reasoning about kinematic interactions in fixed-axis mechanisms. A key idea of the approach is that a mechanism is represented not by the set of its parts, but by the pairwise interactions between them. Using such symbolic representations of kinematic pairs as the basis for reasoning bypasses the difficult problem of finding a symbolic shape representation which is powerful enough for inferring kinematic properties. We introduce a qualitative representation of the kinematic function of pairs of parts, called a place vocabulary, and show how it can serve as a spatial substrate for envisionments and causal explanations. By composition of place vocabularies, complex mechanisms such as mechanical clocks can be analyzed. © 1990.",Qualitative kinematics in mechanisms,Artificial Intelligence,Article,1/1/1990,Boi,Faltings,1990
10.1016/0004-3702(94)00011-O,"Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory. © 1995 Elsevier Science B.V. All rights reserved.",Learning to act using real-time dynamic programming,Artificial Intelligence,Article,1/1/1995,Andrew,Barto,1995
10.1016/0004-3702(94)00054-5,"Recent research in artificial intelligence has developed computational theories of agents' involvements in their environments. Although inspired by a great diversity of formalisms and architectures, these research projects are unified by a common concern: using principled characterizations of agents' interactions with their environments to guide analysis of living agents and design of artificial ones. This article offers a conceptual framework for such theories, surveys several other fields of research that hold the potential for dialogue with these new computational projects, and summarizes the principal contributions of the articles in this special double volume. It also briefly describes a case study in these ideas-a computer program called Toast that acts as a short-order breakfast cook. Because its designers have discovered useful structures in the world it inhabits, Toast can employ an extremely simple mechanism to decide what to do next. © 1995 Elsevier Science B.V. All rights reserved.",Computational research on interaction and agency,Artificial Intelligence,Article,1/1/1995,Philip,Agre,1995
10.1016/0004-3702(95)00074-7,The reviewers of The Creative Mind (henceforth TCM) have raised a host of interesting points. Most fall into seven groups: the definition of creativity; the distinction between H-creativity and P-creativity; the role of the social context; the role of evaluation; the four Lovelace questions; specific computational mechanisms used for modelling creativity; AI-models as aids to creativity; and the treatment of music in TCM. I shall group my replies accordingly. © 1995.,Modelling creativity: reply to reviewers,Artificial Intelligence,Article,1/1/1995,Margaret,Boden,1995
10.1016/0016-3287(84)90007-7,"Progress in artificial intelligence (AI) is expected in a variety of areas over the next decade. This article discusses amongst others, robotics, low-level vision, natural language processing and knowledge-based 'expert' systems and their uses for education, science and technology. What should be closely and critically monitored is the ignorance and sensationalism that sometimes attends the notion of AI. © 1984.",Impacts of artificial intelligence,Futures,Article,1/1/1984,Margaret,Boden,1984
10.1016/0146-6402(89)90015-5,"To date, there has been only one study (Bouton et al., 1987) that aims specifically at developing a measure for assessing personal concern about acquiring AIDS. In the present study, it was argued that the suitability and/or validity of the Bouton et al. Fear of AIDS Scale should be seriously questioned on conceptual and psychometric grounds. In addition, there are no published studies in which the dimensionality of AIDS fear has been examined. The present investigation was carried out to examine whether fear of AIDS can be shown to emerge as a consistent fear composite in factor analysis of specific AIDS-related fears and whether it can be distinguished from Blood/Injury fears, the latter of which has been established as a complex which possesses both cross-sample and cross-national invariance qualities. The dimensional structure of a 38-item Fear of AIDS Schedule (the acronym FAIDSS being used for describing it) was explored with a sample of 684 American students. Principal components analysis with VARIMAX rotation revealed two separate but related, internally consistent and replicable dimensions of AIDS fear: (I) Fear of AIDS contraction associated with risky sexual behavior, and the fear of the psychological and somatic consequences of having caught the disease, and (II) Fear of exposure to the AIDS virus and other associated viruses through (a) interpersonal, not necessarily sexual, contact with members of risk groups and (b) the subjection to medical procedures. Both components were shown to be invariant across sex. Further analyses pointed to the possibility of using a general (i.e. overall) measure of AIDS fear next to the factorially-derived subscales. On the basis of the patterns of correlations of the fear of AIDS constructs with the conventional Fear Survey Schedule-III and background factors such as sex, age, ethnicity/race, students' major in college and religious preference, it was concluded that, if unjustified overgeneralizations or misleading undergeneralizations are to be avoided, the researcher/clinician should use both the subscales and the general scale conjointly. Evidence in favor of discriminant validity of the fear of AIDS constructs in relation to Blood/Injury fears was obtained. Some implications and recommendations for further study were given. © 1989.","Fear of aids: Are there replicable, invariant questionnaire dimensions?",Advances in Behaviour Research and Therapy,Article,1/1/1989,W,Ross,1989
10.1016/0167-6393(90)90041-7,"The vowel sub-component of a speaker-independent phoneme classification system will be described. The architecture of the vowel classifier is based on an ear model followed by a set of Multi-Layered Neural Networks (MLNN). MLNNs are trained to learn how to recognize articulatory features like the place of articulation and the manner of articulation related to tongue position. Experiments are performed on 10 English vowels showing a recognition rate higher than 95% on new speakers. When features are used for recognition, comparable results are obtained for vowels and diphthongs not used for training and pronounced by new speakers. This suggests that MLNNs suitably fed by the data computed by an ear model have good generalization capabilities over new speakers and new sounds. © 1990.",Phonetically-based multi-layered neural networks for vowel classification,Speech Communication,Article,1/1/1990,Yoshua,Bengio,1990
10.1016/0167-8191(85)90009-2,"The parallel approaches to AI are divided into three broad categories, though the boundaries between them are often fuzzy: the general programming approach, applications of parallelism to the processing of specialized programming languages, and massively parallel active memory systems. The general programming approach attempts to detect and exploit any opportunities for concurrent execution that may exist in free-form heuristic AI programs written in some general purpose language such as LISP. The specialized programming language approach requires the programmer to write his code in some language that, while it may be fully general and Turing-equivalent, imposes some constraints on the programmer's style. The active memory approach attempts to apply massive amounts of parallelism to a group of closely-related problems that occupy a central position in most AI systems.",PARALLEL PROCESSING IN ARTIFICIAL INTELLIGENCE.,Parallel Computing,Conference Paper,1/1/1985,Scott,Fahlman,1985
10.1016/0263-2373(88)90018-7,"Neural computing is on everyone's lips. Major manufacturers from the world over are busy announcing that they are planning to enter the field. This is greeted by a considerable rise in their share values. Several machines have already appeared on the market and their sales brochures suggest that they offer a radical improvement in ""intelligence"" over conventional artificial intelligence programs. Although much of this is pure hype, I argue in this article that there is much technological benefit to be derived from the technique, but not to the extent that it will overthrow all we know about computing, but rather, it will work hand-in-hand with conventional methods to produce an improvement in the usability of computers. © 1988.",Neural computing: Hype or reality?,European Management Journal,Article,1/1/1988,Igor,Aleksander,1988
10.1016/0277-9536(88)90360-7,"Knowledge of, and attitudes toward, AIDS were assessed in a random sample of over 2600 individuals aged 16 and over in all states and territories of Australia. Those with lower knowledge of AIDS were more likely to be separated, divorced or widowed, older, and more personally concerned about AIDS. There were no differences in knowledge of AIDS across states, or between sexes. Individuals with lower knowledge of AIDS had greater fear of homosexuals, more unrealistic concerns about AIDS, blamed those infected more, were more afraid of the unknown aspects of AIDS, and were more conservative. Individuals who had used intravenous drugs ever and in the past year had significantly lower knowledge of AIDS; for other risk behaviours, there were no significant differences. Individuals who personally knew homosexual people had higher knowledge of AIDS. These data indicate that the determinants of knowledge of AIDS are related more strongly to attitudinal variables than to demographic ones, and that there are few differences in knowledge across those practising at risk behaviours, compared with the general population, with the exception of intravenous drug users. © 1988.",Distribution of knowledge of AIDS: A national study,Social Science and Medicine,Article,1/1/1988,W,Ross,1988
10.1016/0277-9536(94)90403-0,A study based on a sample of 2500 individuals aged 18 years and over in six west African cities was undertaken for the purpose of determining the cross-cultural consistency and replicability of fears about AIDS. A factor analysis of the data obtained confirmed a similar factor structure to that reported in Australia. Implications for the prevention of HIV transmission in Nigeria and other west African countries is discussed. Our findings showed that the fear of AIDS Scale (FAIDSS) is a reliable index of fear of aids and is readily scaleable. Other implications for health education are considered. © 1994.,Fears of aids in Nigerian students: Dimensions of the Fear of Aids Scale (FAIDSS) in West Africa,Social Science and Medicine,Article,1/1/1994,W,Ross,1994
10.1016/0376-8716(92)90038-E,"Attitudes toward injecting drug users were assessed using a 53-item questionnaire which was administered to 143 workers at a drug and alcohol research unit and an AIDS treatment facility. Factor analysis revealed three interpretable dimensions: intravenous drug use as a matter of both public concern and personal inadequacy; intravenous drug users as criminals who should be removed from society; and social avoidance of, and personal distaste for, intravenous drug users. The scale and subscales had good test-retest reliability and internal consistency. Its potential use as a measure of attitudes toward injecting drug users for both treatment research and AIDS research is discussed. © 1992.","Mad, bad and dangerous to know: dimensions and measurement of attitudes toward injecting drug users",Drug and Alcohol Dependence,Article,1/1/1992,W,Ross,1992
10.1016/0738-3991(88)90073-0,"A review of the counselling issues in AIDS-related syndromes is presented. Using a risk continuum model, psychological issues differentiating high risk and low risk behavior are outlined. Differentiation by risk behaviour is held to most clearly identify differing psychological reactions to a diagnosis of HIV infection. The psychological concomitants of AIDS, ARC and HIV asymptomatic infection, together with organic mental syndrome complications, are summarised and the appropriate response noted. Special concerns affecting individuals who are participants in high risk activity, have AIDS-related anxiety, or AIDS phobias who are not HIV antibody positive are addressed. © 1988.",Psychological issues in AIDS-related syndromes,Patient Education and Counseling,Review,1/1/1988,W,Ross,1988
10.1016/0893-6080(88)90242-0,"Although neural net models show promise in areas where traditional AI approaches falter, such as pattern recognition, pattern completion and content addressable memory, their success is constrained by slow learning rates and the difficulty of physical implementation; learning strategies such as error-back-propagation are also implausible as biological models. The Probabilistic Logic Neuron (PLN) represents an attempt to address these issues while retaining the emergent properties of the traditional connectionist models. It is implemented as a variable logic device, with the consequences that it can perform any Boolean function of its inputs, while its use in nets allows drastic reduction in quantity and specificity of connection requirements.",Learning algorithms for probabilistic neural nets,Neural Networks,Article,1/1/1988,Igor,Aleksander,1988
10.1016/B978-0-12-815367-3.00004-9,"© 2020 Elsevier Inc. All rights reserved.Peoples' roles with cars will evolve from driver to passenger as consumers turn increasingly to autonomous vehicles (AVs), technology that will become common in new automobiles in the next few years. For AVs to gain rapid acceptance and adoption across a broad consumer market, understanding how users will respond to the technology and their experience as nondriving passengers becomes important to examine. This chapter frames a discussion about understanding what might cause negatively stressful experiences that lead to aggressive behaviors by users in an autonomous car environment where artificial intelligence is in some or all control of the vehicle. Adopting a view based in territoriality and via the lens of attachment theory, this work is an exploration of the emerging design concepts for autonomous cars as an extension of the home, living, or workspace, and the related idea of these spaces then being perceived by users as primary territory weighted with emotional attachment, and what happens when our territorial borders are invaded or our territorial defenses are piqued.",Kill switch: The evolution of road rage in an increasingly AI car culture,Living with Robots: Emerging Issues on the Psychological and Social Implications of Robotics,Book Chapter,1/1/2020,Julie,Carpenter,2020
10.1016/B978-0-934613-64-4.50047-5,"© 1988 Proceedings of the 5th International Conference on Machine Learning, ICML 1988. All rights reserved.This paper shows how to take virtually any learning algorithm and turn it into one which is accurate and reliable to an arbitrary degree. The transformation is accomplished by appending a filter that performs a statistical test to the learning algorithm. The filter only outputs hypotheses that pass the test. The test takes time which is polynomial in the desired accuracy and reliability levels and is independent of the learning algorithm and the complexity of its domain. Distribution-free statistical theory is used to prove that the filter works. We describe the application of the filter to concept learning, and to SE, a system which learns control knowledge by clustering its data. The significance of hypothesis filtering is two fold. First, filters may be used to evaluate and compare the performance of a wide range of learning algorithms. Second, applying hypothesis filtering to inductive learning algorithms demonstrates that the algorithms can be made arbitrarily reliable, and provably so.",Hypothesis Filtering: A Practical Approach to Reliable Learning,"Proceedings of the 5th International Conference on Machine Learning, ICML 1988",Conference Paper,1/1/1988,Oren,Etzioni,1988
10.1016/B978-1-4832-1447-4.50070-5,"© 1990 Morgan Kaufmann Publishers, Inc. Published by Elsevier Inc. All rights reserved.Causality plays an important role in human thinking. Yet we are far from having a complete account of causal reasoning. This paper presents an analysis of causal reasoning about changes in quantities. We abstract from AI theories of qualitative physics three dimensions along which causal reasoning about quantities may be decomposed. We then use this framework to make some psychological predictions.",Causal reasoning about quantities,Readings in Qualitative Reasoning About Physical Systems,Book Chapter,9/17/2013,Ken,Forbus,2013
10.1016/B978-1-55860-141-3.50019-5,"© 1990 Proceedings of the 7th International Conference on Machine Learning, ICML 1990. All rights reserved.This paper introduces a new programming methodology, called Genetic Programming, which is the application of the Genetic Algorithm to the evolution of the signs and weights of fully (self) connected neural network modules which perform some time (in)dependent function (e.g. walking, oscillating etc.) in an ""optimal"" manner. Genetically Programmed Neural Net (GenNet) modules are of two types, functional and control. A series of functional GenNets can be evolved, and their weights frozen. Control GenNets are then evolved whose outputs are the inputs of the functional GenNets. The size and timing of these control signals are evolved such that the combination of control and functional GenNets performs as desired. This combination can then be frozen and used as a module in a more complex structure. This procedure can be repeated indefinitely, thus allowing the construction of hierarchical neural networks. Genetic Programming has recently proven to be so successful that the building of artificial nervous systems becomes a real possibility. This paper illustrates both the conceptual simplicity and the power of Genetic Programming by showing how a GenNet can be evolved which teaches a pair of stick legs to walk. This is followed by a description of work in progress on the next major phase of Genetic Programming research, namely the building of artificial nervous systems (""brain building""), and on the tools which will be needed to evolve them, called Darwin Machines.",Building Artificial Nervous Systems Using Genetically Programmed Neural Network Modules,"Proceedings of the 7th International Conference on Machine Learning, ICML 1990",Conference Paper,1/1/1990,Hugo de,Garis,1990
10.1016/B978-1-55860-247-2.50022-X,"© 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.Based on a simple asymptotic analysis, this paper presents two observations regarding the state-of-the-art in speedup learning. First, reducing the match cost of control rules to a polynomial or even linear function of rule length does not guarantee polynomial-time problem solving or any speedup for that matter. Hence, the elimination expensive control rules is not guaranteed to solve the utility problem. Second, augmenting a problem solver's operator set with macro-operators can increase the branching factor of the problem solver's search. The overhead of this increase, even when it occurs only in a vanishingly small fraction of the problems encountered, dominates any search reduction on the remaining problems. Thus, acquiring macro-operators is guaranteed to slow down a problem solver, in the limit, unless the macros modify the topology of the search space so that sufficient search-depth reduction accompanies branching-factor increase, everywhere.",An Asymptotic Analysis of Speedup Learning,"Proceedings of the 9th International Workshop on Machine Learning, ICML 1992",Conference Paper,1/1/1992,Oren,Etzioni,1992
10.1016/B978-1-55860-247-2.50023-1,"© 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.There are many different ways to prove that a training example is subsumed by a target concept. Each proof gives rise to a different sufficient condition for the concept, some of which are considerably more general than others. Since EBL merely computes the weakest precondition of a particular proof, it is by no means guaranteed to find a sufficient condition that is maximally general. In practice, EBL frequently derives overly-specific control knowledge, retaining extraneous features of its training examples. In this paper we formally define the notion of a maximally general sufficient condition. We identify common pitfalls that prevent EBL from deriving such conditions, and critique the array of heuristic mechanisms used to improve EBL's generalizations in the PRODIGY system (including logical simplification, abstraction, and static analysis). Finally, we advocate the design of domain-independent, meta-level theories as a direction for future work.",Why EBL Produces Overly-Specific Knowledge: A Critique of the PRODIGY Approaches,"Proceedings of the 9th International Workshop on Machine Learning, ICML 1992",Conference Paper,1/1/1992,Oren,Etzioni,1992
10.1016/B978-1-55860-247-2.50052-8,"© 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.Most Explanation-Based Learning (EBL) systems construct explanations by directly translating a trace of their problem solver's search, on training problems, into proofs. This approach makes proof derivation tractable, but can focus EBL on incidental aspects of its training problems, yielding overly-specific control knowledge. Previous work has described the other extreme: STATIC, a system that generates more general control knowledge by statically analyzing problem-space definitions. However, since STATIC does not utilize training problems, it has a number ofpotential disadvantages compared with EBL. This paper advocates an intermediate approach in which training problems pinpoint learning opportunities but do not determine EBL's explanations. Based on this design principle, we developed DYNAMIC, a module that learns control rules for the PRODIGY problem solver. In DYNAMIC, choosing what to explain and how to explain it are independent, DYNAMIC utilizes the analysis algorithms introduced by STATIC, but relies on training problems to achieve the distribution-sensitivity of EBL. On a highly skewed problem distribution, DYNAMIC was almost four times as effective as STATIC in speeding up PRODIGY. When tested in PRODIGY/EBL'S benchmark problem spaces, DYNAMIC ran considerably faster than PRODIGY/EBL and produced control rules that were close to three times as effective. In addition, DYNAMIC required only a fraction of the training problems used by PRODIGY/EBL.",DYNAMIC: A new role for training problems in EBL,"Proceedings of the 9th International Workshop on Machine Learning, ICML 1992",Conference Paper,1/1/1992,Oren,Etzioni,1992
10.1016/j.acra.2019.12.012,"© 2020 The Association of University RadiologistsRationale and Objectives: Federal legislation requires patient notification of dense mammographic breast tissue because increased density is a marker of breast cancer risk and can limit the sensitivity of mammography. As previously described, we clinically implemented our deep learning model at the academic breast imaging practice where the model was developed with high clinical acceptance. Our objective was to externally validate our deep learning model on radiologist breast density assessments in a community breast imaging practice. Materials and Methods: Our deep learning model was implemented at a dedicated breast imaging practice staffed by both academic and community breast imaging radiologists in October 2018. Deep learning model assessment of mammographic breast density was presented to the radiologist during routine clinical practice at the time of mammogram interpretation. We identified 2174 consecutive screening mammograms after implementation of the deep learning model. Radiologist agreement with the model's assessment was measured and compared across radiologist groups. Results: Both academic and community radiologists had high clinical acceptance of the deep learning model's density prediction, with 94.9% (academic) and 90.7% (community) acceptance for dense versus nondense categories (p < 0.001). The proportion of mammograms assessed as dense by all radiologists decreased from 47.0% before deep learning model implementation to 41.0% after deep learning model implementation (p < 0.001). Conclusion: Our deep learning model had a high clinical acceptance rate among both academic and community radiologists and reduced the proportion of mammograms assessed as dense. This is an important step to validating our deep learning model prior to potential widespread implementation.",External Validation of a Deep Learning Model for Predicting Mammographic Breast Density in Routine Clinical Practice,Academic Radiology,Article,4/1/2021,Regina,Barzilay,2021
10.1016/j.aej.2022.05.029,"© 2022With the increased popularity of social media platforms, people are increasingly depending on them for news and updates. Even official media channels post news on social media platforms such as Twitter and Facebook. However, with the vast amount of user-generated content, the credibility of shared information must be verified, and this process should be performed automatically and efficiently to accommodate the huge rate of generated posts. Current technology provides powerful methods and tools to solve the issue of rumor spreading on social networks. In this study, the aim is to investigate the use of state-of-the-art machine learning and deep learning models to detect rumors in a collection of Arabic tweets using the ArCOV19-Rumors dataset. A comprehensive comparison of the performance of the models was conducted. In deep learning experiments, the performances of seven optimizers were compared. The results demonstrated that using over-sampled data did not enhance classical and deep learning models. By contrast, using stacking classifiers increased the predictive model's performance. As a result, the model became more logical and realistic in predicting rumors, non-rumors, and other classes than using classical machine learning without the stacking technique. Additionally, both long short-term memory (LSTM) and bidirectional-LSTM (Bi-LSTM) with the Root mean square propagation (RMSprop) optimizer obtained the best results. Finally, the results were analyzed to explain and interpret the low performance.",Arabic rumor detection: A comparative study,Alexandria Engineering Journal,Article,12/1/2022,Fatmah,Baothman,2022
10.1016/j.apsusc.2021.149059,"© 2021 Elsevier B.V.We have addressed the issue of improper and unreliable analysis of materials characterization data by developing an artificial intelligence based methodology that can reliably and more efficiently analyze experimental results from extended X-ray absorption fine structure (EXAFS) measurements. Such methods help address growing reproducibility problems that are slowing research progress, discouraging the quest for research excellence, and inhibiting effective technology transfer and manufacturing innovation. We have developed a machine learning system for automated analysis of EXAFS spectroscopy measurements and demonstrated its effectiveness on measurements collected at powerful, third generation synchrotron radiation facilities. Specifically, the system uses a genetic algorithm to efficiently find sets of structural parameters that lead to high quality fits of the experimental spectra. A human analyst suggests a set of chemical compounds potentially present in the sample, used as theoretical standards. The algorithm then searches the large multidimensional space of combinations of these materials to determine the set of structural paths using the theoretical standards that best reproduces the experimental data. The algorithm further calculates a goodness of fit value from the suggested standards that can be used to identify the chemical moieties present in the measured sample.",Analysis of extended X-ray absorption fine structure (EXAFS) data using artificial intelligence techniques,Applied Surface Science,Article,5/1/2021,Shlomo,Argamon,2021
10.1016/j.artint.2007.10.011,"An analysis of Ray Kurzweil's recent book The Singularity Is Near is given, along with Drew McDermott's recent critique. The conclusion is that Kurzweil does an excellent job of fleshing out one particular plausible scenario regarding the future of AI, in which human-level AI first arrives via human-brain emulation. McDermott's arguments against the notion of Singularity via iteratively self-improving AI, as described by Kurzweil, are considered and found wanting. However, it is pointed out that the scenario focused on by Kurzweil is not the only plausible one; and an alternative is discussed, in which human-level AI arrives first via non-human-like AI's operating virtual worlds. © 2007 Elsevier B.V. All rights reserved.","Human-level artificial general intelligence and the possibility of a technological singularity. A reaction to Ray Kurzweil's The Singularity Is Near, and McDermott's critique of Kurzweil",Artificial Intelligence,Article,12/1/2007,Ben,Goertzel,2007
10.1016/j.artint.2010.04.024,"Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without the use of hand-labeled training examples. Because UIE systems do not require human intervention, they can recursively discover new relations, attributes, and instances in a scalable manner. When applied to massive corpora such as the Web, UIE systems present an approach to a primary challenge in artificial intelligence: The automatic accumulation of massive bodies of knowledge. A fundamental problem for a UIE system is assessing the probability that its extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness? We present a combinatorial ""balls-and-urns"" model, called Urns, that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating Urns's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are 15 times better, on average, than those obtained by methods used in previous work. We illustrate the generality of the redundancy model by detailing multiple applications beyond UIE in which Urns has been effective. We also provide a theoretical foundation for Urns's performance, including a theorem showing that PAC Learnability in Urns is guaranteed without hand-labeled data, under certain assumptions. © 2010 Elsevier B.V. All rights reserved.",Analysis of a probabilistic model of redundancy in unsupervised information extraction,Artificial Intelligence,Article,1/1/2010,Oren,Etzioni,2010
10.1016/j.caeai.2021.100012,"© 2021 The Author(s)This study aimed at developing an instructional tool for the artificial intelligence education of young students, and used learning analytics to identify the sequential learning behavioral patterns of students during the process of learning with the instructional tool. The instructional experiment took 9 weeks. The first stage of the course was 5 weeks spent on individual learning of MIT App Inventor and Personal Image Classifier. The second stage was 4 weeks spent on cooperative learning to make a robot car and play a computational thinking board game. In the second stage, the students worked in pairs to make the robot car. Finally, they played the computational thinking board game with the personal image classification application they developed in the first stage and the robot car they made in the second stage. The innovative studies found meaningful behavioral patterns when the young students learned the application of artificial intelligence with the instructional tool developed and proposed in the study.",Behavioral-pattern exploration and development of an instructional tool for young children to learn AI,Computers and Education: Artificial Intelligence,Article,1/1/2021,Hal,Abelson,2021
10.1016/j.cageo.2016.12.013,"© 2017 Elsevier LtdFast and effective oil spill detection systems are crucial to ensure a proper response to environmental emergencies caused by hydrocarbon pollution on the ocean's surface. Typically, these systems uncover not only oil spills, but also a high number of look-alikes. The feature extraction is a critical and computationally intensive phase where each detected dark spot is independently examined. Traditionally, detection systems use an arbitrary set of features to discriminate between oil spills and look-alikes phenomena. However, Feature Selection (FS) methods based on Machine Learning (ML) have proved to be very useful in real domains for enhancing the generalization capabilities of the classifiers, while discarding the existing irrelevant features. In this work, we present a generic and systematic approach, based on FS methods, for choosing a concise and relevant set of features to improve the oil spill detection systems. We have compared five FS methods: Correlation-based feature selection (CFS), Consistency-based filter, Information Gain, ReliefF and Recursive Feature Elimination for Support Vector Machine (SVM-RFE). They were applied on a 141-input vector composed of features from a collection of outstanding studies. Selected features were validated via a Support Vector Machine (SVM) classifier and the results were compared with previous works. Test experiments revealed that the classifier trained with the 6-input feature vector proposed by SVM-RFE achieved the best accuracy and Cohen's kappa coefficient (87.1% and 74.06% respectively). This is a smaller feature combination with similar or even better classification accuracy than previous works. The presented finding allows to speed up the feature extraction phase without reducing the classifier accuracy. Experiments also confirmed the significance of the geometrical features since 75.0% of the different features selected by the applied FS methods as well as 66.67% of the proposed 6-input feature vector belong to this category.",On the use of feature selection to improve the detection of sea oil spills in SAR images,Computers and Geosciences,Article,3/1/2017,Amparo Alonso,Betanzos,2017
10.1016/j.cell.2020.01.021,"© 2020 Elsevier Inc.Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub—halicin—that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from >107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules. A trained deep neural network predicts antibiotic activity in molecules that are structurally different from known antibiotics, among which Halicin exhibits efficacy against broad-spectrum bacterial infections in mice.",A Deep Learning Approach to Antibiotic Discovery,Cell,Article,2/20/2020,Regina,Barzilay,2020
10.1016/j.cell.2020.04.001,"© 2020 Elsevier Inc.(Cell 180, 688–702.e1–e13; February 20, 2020) Our paper reported the use of a machine learning approach to discover new antibacterial molecules. Since publication, we have become aware of the following errors in our paper that we are now correcting. (1) The structure of ZINC000100032716, shown in the Graphical Abstract and Figure 6D, mistakenly displayed a carbonyl carbon making five bonds. We have redrawn the molecules to display the correct structures. (2) In Figure S3A and the accompanying legend, the concentrations of halicin used were 20 µg/mL (10x MIC) and 40 µg/mL (20x MIC), not 10 µg/mL and 20 µg/mL. (3) In Figure S5K, the x axis labels should range from 10-6 to 103, in accordance with the axis tick marks, not 10-5 to 103. (4) In Table S2B, BRD-K57502136-345-03-4, BRD-K90177246-001-05-5, BRD-K15514357-001-05-6, and BRD-A56621826-001-02-1 were listed as being unavailable for empirical validation. However, the four molecules that were not available for testing were BRD-K76819217-001-01-4, BRD-A41063939-001-01-0, BRD-M10279501-065-05-9, and BRD-A40472231-304-02-5. This resulted from a transposition error in converting the original training data file into the Table S2B spreadsheet. This transposition error was not present in model training. (5) In the “Initial model training and the identification of halicin” section of the Results, the halicin prediction ranks noted in parentheses should read “positions ranging from 273 to 2579,” rather than “positions raging from 273 to 1987.” (6) In the “Bacterial cell killing assays” subsection of the STAR Methods, the M. tuberculosis strain used was H37Rv, not “M37Rv.” (7) In the “Mutant generation” section, <U+0394>nfsA::kan was mistakenly written as “<U+0394>nsfA::kan.” (8) The section title “baumannii mouse infection model” should have been “A. baumannii mouse infection model.” (9) It was brought to our attention that SU3327 (which we renamed halicin) had been reported as an active compound in an unpublished screen, deposited to PubChem, for growth inhibition of M. tuberculosis. The following sentence has been added to the last paragraph of the Results section “Halicin is a broad-spectrum bactericidal antibiotic” to acknowledge this: “The molecule we have named halicin was reported to have growth inhibitory activity against M. tuberculosis in a high-throughput screening setting (unpublished data; PubChem AID 1259343).” (10) In preparing the final version of the manuscript, we inadvertently misspelled the last name of author Zohar Bloom-Ackermann as ‘‘Zohar Bloom-Ackerman.” These errors have now been corrected in the online version of the paper. We apologize for any inconvenience they may have caused the readers. [Figure presented] [Figure presented] [Figure presented] [Figure presented] [Figure presented] [Figure presented] [Figure presented] [Figure presented]","Erratum: A Deep Learning Approach to Antibiotic Discovery (Cell (2020) 180(4) (688–702.e13), (S0092867420301021), (10.1016/j.cell.2020.01.021))",Cell,Erratum,4/16/2020,Regina,Barzilay,2020
10.1016/j.cognition.2015.05.027,"© 2015 Elsevier B.V.One of the most striking features of children's early multi-word speech is their tendency to produce non-finite verb forms in contexts in which a finite verb form is required (Optional Infinitive [OI] errors, Wexler, 1994). MOSAIC is a computational model of language learning that simulates developmental changes in the rate of OI errors across several different languages by learning compound finite constructions from the right edge of the utterance (Freudenthal, Pine, Aguado-Orea, & Gobet, 2007; Freudenthal, Pine, & Gobet, 2006a, 2009). However, MOSAIC currently only simulates the pattern of OI errors in declaratives, and there are important differences in the cross-linguistic patterning of OI errors in declaratives and Wh- questions. In the present study, we describe a new version of MOSAIC that learns from both the right and left edges of the utterance. Our simulations demonstrate that this new version of the model is able to capture the cross-linguistic patterning of OI errors in declaratives in English, Dutch, German and Spanish by learning from declarative input, and the cross-linguistic patterning of OI errors in Wh- questions in English, German and Spanish by learning from interrogative input. These results show that MOSAIC is able to provide an integrated account of the cross-linguistic patterning of OI errors in declaratives and Wh- questions, and provide further support for the view, instantiated in MOSAIC, that OI errors are compound-finite utterances with missing modals or auxiliaries.",Simulating the cross-linguistic pattern of Optional Infinitive errors in children's declaratives and Wh- questions,Cognition,Article,10/1/2015,Fernand,Gobet,2015
10.1016/j.cogsys.2004.09.003,"In recent years, several authors have investigated how co-occurrence statistics in natural language can act as a cue that children may use to extract syntactic categories for the language they are learning. While some authors have reported encouraging results, it is difficult to evaluate the quality of the syntactic categories derived. It is argued in this paper that traditional measures of accuracy are inherently flawed. A valid evaluation metric needs to consider the well-formedness of utterances generated through a production end. This paper attempts to evaluate the quality of the categories derived from co-occurrence statistics through the use of MOSAIC, a computational model of syntax acquisition that has already been used to simulate several phenomena in child language. It is shown that derived syntactic categories that may appear to be of high quality quickly give rise to errors that are not typical of child speech. A solution to this problem is suggested in the form of a chunking mechanism that serves to differentiate between alternative grammatical functions of identical word forms. Results are evaluated in terms of the error rates in utterances produced by the system as well as the quantitative fit to the phenomenon of subject omission. © 2004 Elsevier B.V. All rights reserved.",On the resolution of ambiguities in the extraction of syntactic categories through chunking,Cognitive Systems Research,Article,1/1/2005,Fernand,Gobet,2005
10.1016/j.compbiomed.2022.105471,"© 2022Background: Anterior segment optical coherence tomography (AS-OCT) constitutes an important imaging modality to examine the anterior eye, which is commonly used in research and clinical practice. Since its introduction, a range of image analysis methods have been developed to quantify these images using different analysis techniques for various applications. This systematic review aims to provide an in-depth summary and to classify image analysis techniques found in the literature applied to AS-OCT images. Methods: Scopus and Engineering Village databases were searched to retrieve relevant studies up to and including January 2022. Customized search statements were used along with cross reference and hand search techniques to ensure a complete coverage. Performance metrics were extracted, analyzed, and compared (when possible). Results: Three main application categories were identified: glaucoma assessment, corneal segmentation, and anterior segment biometry. These three categories constitute 66% of the total studies reported in this review. Studies were also analyzed by year of publication, and since 2019 deep learning methods were favored over traditional programming or machine learning methodologies. Overall, the AS-OCT image analysis field is less developed compared to posterior segment OCT imaging. Conclusion: This review presents the state of the art in the field of AS-OCT image analysis. It highlights the opportunities for future areas of research, such as the expansion of DL methods and the extension to specific clinical areas that have received limited attention including surgical monitoring, contact lenses, and specific clinical conditions such as keratoconus and corneal lesions.",Anterior segment optical coherence tomography (AS-OCT) image analysis methods and applications: A systematic review,Computers in Biology and Medicine,Review,7/1/2022,Michael,Collins,2022
10.1016/j.compbiomed.2022.106342,"© 2022 Elsevier LtdAnterior segment optical coherence tomography (AS-OCT) is a fundamental ophthalmic imaging technique. AS-OCT images can be examined by experts and segmented to provide quantitative metrics that inform clinical decision making. Manual segmentation of these images is time-consuming and subjective, encouraging software developers in the field to automate segmentation procedures. Traditional programing segmentation approaches are being replaced by deep learning methods, which have shown state-of-the-art performance in AS-OCT image analysis. In this study, a method based on patch-based convolutional neural networks (CNN) was used to segment the three main boundaries of the cornea: the epithelium, Bowman's layer, and the endothelium. To assess the effect of the number of classes on performance, the model was designed as a patch-based boundary classifier using 4 and 8 classes. The effect of image quality was also assessed using different data distributions during the training process. While the Dice coefficient and probability revealed greater precision for the 8 class models, the boundary error metric indicated comparable performance. Additionally, for 8 class models, the image quality test had only a small negative effect on performance, which may be an indication of the robustness of the model and could also suggest that the data augmentation methods did not show significant improvement. These findings contribute to the development of automatic segmentation techniques for AS-OCT images, since patch-based methods have been largely unexplored in favor of other deep learning techniques. The overall performance of the proposed method is comparable to other well-established segmentation methods.",Patch-based CNN for corneal segmentation of AS-OCT images: Effect of the number of classes and image quality upon performance,Computers in Biology and Medicine,Article,1/1/2023,Michael,Collins,2023
10.1016/j.drugalcdep.2021.108986,"© 2021 The AuthorsBackground: The COVID-19 pandemic disrupted access to treatment for substance use disorders (SUDs), while alcohol and cannabis retail sales increased. During the pandemic, we tested a tailored digital health solution, Woebot-SUDs (W-SUDs), for reducing substance misuse. Methods: In a randomized controlled trial, we compared W-SUDs for 8 weeks to a waitlist control. U.S. adults (N = 180) who screened positive for substance misuse (CAGE-AID>1) were enrolled June–August 2020. The primary outcome was the change in past-month substance use occasions from baseline to end-of-treatment (EOT). Study retention was 84%. General linear models tested group differences in baseline-to-EOT change scores, adjusting for baseline differences and attrition. Results: At baseline, the sample (age M = 40, SD = 12, 65% female, 68% non-Hispanic white) averaged 30.2 (SD = 18.6) substance occasions in the past month. Most (77%) reported alcohol problems, 28% cannabis, and 45% multiple substances; 46% reported moderate-to-severe depressive symptoms. Treatment participants averaged 920 in-app text messages (SD = 892, Median = 701); 96% of completed lessons were rated positively; and 88% would recommend W-SUDs. Relative to waitlist, W-SUDs participants significantly reduced past-month substance use occasions (M = -9.1, SE = 2.0 vs. M = -3.3, SE = 1.8; p = .039). Secondary substance use and mood outcomes did not change significantly by group; however, reductions in substance use occasions correlated significantly with increased confidence and fewer substance use problems, cravings, depression and anxiety symptoms, and pandemic-related mental health effects (p-value<.05). Conclusions: W-SUDs was associated with significant reductions in substance use occasions. Reduction in substance use occasions was associated with better outcomes, including improved mental health. W-SUDs satisfaction was high.",A randomized controlled trial of a therapeutic relational agent for reducing substance misuse during the COVID-19 pandemic,Drug and Alcohol Dependence,Article,10/1/2021,Alison,Darcy,2021
10.1016/j.eswa.2020.114101,"© 2020 Elsevier LtdThe current paper investigates the problem of multimodal named entity recognition from Twitter data. Named entity recognition (NER) is an important task in natural language processing and has been carefully studied in recent decades. NER from tweets is particularly challenging because of 1) tweets are limited in length, 2) contains noisy text, and 3) contains hashtags. Moreover often tweets are associated with images and hyperlinks. Existing works on tweet-NER mostly concentrate on multimodal deep learning based models neglecting the use of hand-crafted features and usage of hyperlinks. The current paper investigates the incorporation of hand-crafted features extracted from different modalities like images, hyperlinks while extracting named entities from tweet-text. A large set of hand-crafted features are extracted from different modalities (images, hyperlinks) and those are added with the features extracted by a hybrid deep-neural model, bi-directional LSTM and CNN, followed by a conditional random field to perform this task. Several variants of these models in association with different hand-crafted feature sets are designed. Extensive experimentations on a multimodal Twitter data (containing text, images and urls) illustrate that character level hand-crafted features significantly improve the performance of the systems. In a part of the paper, results of the proposed models are also shown on a standard NER dataset, CoNLL 2003 dataset.",Why pay more? A simple and efficient named entity recognition system for tweets,Expert Systems with Applications,Article,4/1/2021,Pushpak,Bhattacharyya,2021
10.1016/j.ijhcs.2023.103003,"© 2023 The Author(s)Type 1 Diabetes (T1D) self-management requires hundreds of daily decisions. Diabetes technologies that use machine learning have significant potential to simplify this process and provide better decision support, but often rely on cumbersome data logging and cognitively demanding reflection on collected data. We set out to use co-design to identify opportunities for machine learning to support diabetes self-management in everyday settings. However, over nine months of interviews and design workshops with 15 people with T1D, we had to re-assess our assumptions about user needs. Our participants reported confidence in their personal knowledge and rejected machine learning based decision support when coping with routine situations, but highlighted the need for technological support in the context of unfamiliar or unexpected situations (holidays, illness, etc.). However, these are the situations where prior data are often lacking and drawing data-driven conclusions is challenging. Reflecting this challenge, we provide suggestions on how machine learning and other artificial intelligence approaches, e.g., expert systems, could enable decision-making support in both routine and unexpected situations.",Co-designing opportunities for Human-Centred Machine Learning in supporting Type 1 diabetes decision-making,International Journal of Human Computer Studies,Article,5/1/2023,Peter,Flach,2023
10.1016/j.inffus.2018.11.008,"© 2018 Elsevier B.V.Ensemble learning is a prolific field in Machine Learning since it is based on the assumption that combining the output of multiple models is better than using a single model, and it usually provides good results. Normally, it has been commonly employed for classification, but it can be used to improve other disciplines such as feature selection. Feature selection consists of selecting the relevant features for a problem and discard those irrelevant or redundant, with the main goal of improving classification accuracy. In this work, we provide the reader with the basic concepts necessary to build an ensemble for feature selection, as well as reviewing the up-to-date advances and commenting on the future trends that are still to be faced.",Ensembles for feature selection: A review and future trends,Information Fusion,Article,12/1/2019,Amparo Alonso,Betanzos,2019
10.1016/j.jal.2008.09.001,"This paper is a sustained argument for the view that logic-based AI should become a self-contained field, entirely divorced from paradigms that are currently still included under the AI ""umbrella""-paradigms such as connectionism and the continuous systems approach. The paper includes a self-contained summary of logic-based AI, as well as rebuttals to a number of objections that will inevitably be brought against the declaration of independence herein expressed. © 2008 Elsevier B.V. All rights reserved.",The logicist manifesto: At long last let logic-based artificial intelligence become a field unto itself,Journal of Applied Logic,Article,12/1/2008,Selmer,Bringsjord,2008
10.1016/j.jana.2017.02.004,"© 2017 Association of Nurses in AIDS CareSocietal prejudice against people living with HIV infection is a formidable public health challenge that can negatively impact health and well-being. We recruited a multiethnic sample of 129 gay and bisexual men living with HIV who completed a brief survey; a subset of participants completed semi-structured qualitative interviews to contextualize the data. In bivariate analyses, stigma was positively and significantly correlated with depression (r =.402, p <.001) and negatively correlated with social support (r = -.482, p <.001). Qualitative interview results captured the mental suffering caused by stigma and coping strategies the men had developed. Although some of the coping strategies reduced the likelihood of experiencing acts of stigmatization, they also exacerbated the psychological stress of living with a stigmatized disease and limited the potential for social support. Our results highlight the need to scale up stigma-reduction programs, particularly those that can bolster social support networks.",A Mixed-Method Study on Correlates of HIV-Related Stigma Among Gay and Bisexual Men in the Southern United States,Journal of the Association of Nurses in AIDS Care,Article,7/1/2017,W,Ross,2017
10.1016/j.knosys.2019.07.017,"© 2019 Elsevier B.V.Spoken language understanding (SLU) plays an integral part in every dialogue system. To understand the intention of the user and extract the necessary information to help the user achieve desired goals is a challenging task. In this work, we propose an end-to-end hierarchical multi-task model that can jointly perform both intent detection and slot filling tasks for the datasets of varying domains. The primary aim is to capture context information in a dialogue to help the SLU module in a dialogue system to correctly understand the user and assist the user in achieving the desired goals. It is vital for the SLU module to capture the past information along with the present utterance said by the user to retrieve correct information. The dependency and correlation between the two tasks, i.e. intent detection and slot filling makes the multi-task learning framework effective in capturing the desired information provided by the user. We use Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) to capture contextual information for the utterances. We employ Conditional Random Field (CRF) to model label dependency. Both character and word level embeddings are provided as input to the models. We create a benchmark corpus for the SLU tasks, on TRAINS and FRAMES dataset for capturing more realistic and natural utterances spoken by the speakers in a human/machine dialogue system. Experimental results on multiple datasets of various domains (ATIS, SNIP, TRAINS and FRAMES) show that our proposed approach is effective compared to the individual models and the state-of-the-art methods.",A Multi-Task Hierarchical Approach for Intent Detection and Slot Filling,Knowledge-Based Systems,Article,11/1/2019,Pushpak,Bhattacharyya,2019
10.1016/j.knosys.2021.107715,"© 2021 Elsevier B.V.The behavior, mental-health, emotion, life choices, social nature, and thought patterns of an individual are revealed by personality. Cyber forensics, personalized services, recommender systems are some of the examples of automatic personality prediction. A deep learning based personality prediction system has been developed in this work. Facial and ambient features are extracted from the visual modality using Multi-task Cascaded Convolutional Networks (MTCNN) and ResNet, respectively; the audio features are extracted using the VGGish Convolutional Neural Networks (VGGish CNN), and the text features are extracted using n-gram Convolutional Neural Networks (CNN). The extracted features are then passed to a fully connected layer followed by sigmoid for the final output prediction. Finally, the text, visual and audio modalities are combined in different ways: (i) concatenation of features in multi-modal setting, and (ii) application of different attention mechanisms for fusing features. The dataset released in Chalearn-17 is used for evaluating the performance of the system. From the obtained results, it can be concluded that, the concatenation of features extracted from different modalities attains comparable results with the averaging method (late fusion). It is also shown that a hand full of images are enough for attaining comparable performance.",A multi-modal personality prediction system,Knowledge-Based Systems,Article,1/5/2022,Pushpak,Bhattacharyya,2022
10.1016/j.knosys.2022.109924,"© 2022 Elsevier B.V.Sarcasm is a case of implicit emotion and needs additional information like context and multimodality for better detection. But sometimes, this additional information also fails to help in sarcasm detection. For example, the utterance “Oh yes, you've been so helpful. Thank you so much for all your help”, said in a polite tone with a smiling face, can be understood easily as non-sarcastic because of its positive sentiment. But, if the above message is accompanied by a frustrated emoji [Formula presented], the negative sentiment of the emoji becomes evident, and the intended sarcasm can be easily understood. Thus, in this paper, we propose the SEEmoji MUStARD, an extension of the multimodal MUStARD dataset. We annotate each utterance with relevant emoji, emoji's sentiment, and emoji's emotion. We propose an emoji-aware-multimodal multitask deep learning framework for sarcasm detection (i.e., primary task) and sentiment and emotion detection (i.e., secondary task) in a multimodal conversational scenario. Experimental results on the SEEmoji MUStARD show the efficacy of our proposed emoji-aware-multimodal approach for sarcasm detection over the existing models.",An emoji-aware multitask framework for multimodal sarcasm detection,Knowledge-Based Systems,Article,12/5/2022,Pushpak,Bhattacharyya,2022
10.1016/j.matt.2021.06.036,"© 2021Solutions to many of the world's problems depend upon materials research and development. However, advanced materials can take decades to discover and decades more to fully deploy. Humans and robots have begun to partner to advance science and technology orders of magnitude faster than humans do today through the development and exploitation of closed-loop, autonomous experimentation systems. This review discusses the specific challenges and opportunities related to materials discovery and development that will emerge from this new paradigm. Our perspective incorporates input from stakeholders in academia, industry, government laboratories, and funding agencies. We outline the current status, barriers, and needed investments, culminating with a vision for the path forward. We intend the article to spark interest in this emerging research area and to motivate potential practitioners by illustrating early successes. We also aspire to encourage a creative reimagining of the next generation of materials science infrastructure. To this end, we frame future investments in materials science and technology, hardware and software infrastructure, artificial intelligence and autonomy methods, and critical workforce development for autonomous research.",Autonomous experimentation systems for materials development: A community perspective,Matter,Review,9/1/2021,Carla,Gomes,2021
10.1016/j.neucom.2009.10.033,"The concept of glocal memory (i.e. memory involving systematic coordination between localized memory traces and globalized dynamical-attractor-based memory traces) is reviewed, and is argued to be a critical principle for the design of artificial brains and artificial general intelligence systems. Some exploratory experiments are reviewed, involving introduction of glocal memory into Hopfield neural networks, and also into economic attention networks as utilized in the OpenCog and Novamente integrated AI architectures. © 2010 Elsevier B.V.",Glocal memory: A critical design principle for artificial brains and minds,Neurocomputing,Article,12/1/2010,Ben,Goertzel,2010
10.1016/j.neucom.2021.11.096,"© 2021 Elsevier B.V.Hostile content on Social Media platforms is becoming a problem for governments and organizations. There is a need for AI based intervention which can filter hostile content at scale. The challenge lies in ambiguity of language, absence of training data and local context. In this paper, we investigate Hostile Post Detection for the Hindi Language, which is the topmost language in the Indian Subcontinent in terms of speaker population and third in the world. We extend our prior work in this area along the dimensions of (i) Representations (ii) Data and (iii) Architecture, exploring approaches like Transformers and Multi Task Learning among others, along the way. In this highly experimental study, comparisons are drawn, trends are discovered and insights are presented. We manage to improve on the baseline by 16.5% and 29.77% on the two evaluation metrics viz. Coarse Grained F1 Score and Fine Grained F1 Score. We are also able to beat our prior work results by 0.93% and 9.18% on these two evaluation metrics respectively. Experiments performed by us number 60 which is larger than the number reported in any other work for Hostility Detection in Hindi, to the best of our knowledge.",Investigating Hostile Post Detection in Hindi,Neurocomputing,Article,2/14/2022,Pushpak,Bhattacharyya,2022
10.1016/j.neucom.2022.08.021,"© 2022 Elsevier B.V.Retinal optical coherence tomography (OCT) images provide fundamental information regarding the health of the posterior eye (e.g., the retina and choroid). Thus, the development of automatic image analysis methods (e.g., segmentation) is fundamental to provide clinicians and researchers with quantitative data that facilitates decision making. In recent years, various machine learning (ML) methods have been developed to perform these automated image analyses, improving performance over traditional methods, increasing repeatability, and reducing the use of time-consuming manual analysis. Deep learning (DL), a sub-field of ML, represents a new approach that can improve image processing outcomes. Results published to date demonstrate that DL architectures generally achieve superior performance to previously proposed methods based on traditional image analysis or early ML techniques. Thus, DL methods for OCT image analysis have provided an important advance in the area of retinal layer segmentation in images from healthy eyes as well as those with pathologies. This paper provides a comprehensive narrative literature review of current DL layer segmentation methods applied to OCT images of the posterior segment of the eye. The manuscript provides an overview on the state-of-the-art in deep learning as well as highlighting some important areas for future developments to extend the analysis methods in this field.",Deep learning in retinal optical coherence tomography (OCT): A comprehensive survey,Neurocomputing,Short Survey,10/1/2022,Michael,Collins,2022
10.1016/j.neunet.2007.09.008,A model of conscious mechanisms called Axiomatic Consciousness Theory (ACT) is used to develop a neuroarchitectural model of visual phenomenology. The result is an extension of concepts in neural systems towards phenomenal intentionality: the lack of which is a common starting point for critiques of AI. Here the argument is developed at four interacting grain levels of description and the associated analysis. The visual domain is highlighted due to its dominance in discussions involving inner mental states. © 2007 Elsevier Ltd. All rights reserved.,Phenomenology and digital neural architectures,Neural Networks,Article,11/1/2007,Igor,Aleksander,2007
10.1016/j.neunet.2019.06.009,"© 2019 The Author(s)In this paper, we prove that depth with nonlinearity creates no bad local minima in a type of arbitrarily deep ResNets with arbitrary nonlinear activation functions, in the sense that the values of all local minima are no worse than the global minimum value of corresponding classical machine-learning models, and are guaranteed to further improve via residual representations. As a result, this paper provides an affirmative answer to an open question stated in a paper in the conference on Neural Information Processing Systems 2018. This paper advances the optimization theory of deep learning only for ResNets and not for other network architectures.",Depth with nonlinearity creates no bad local minima in ResNets,Neural Networks,Article,10/1/2019,Yoshua,Bengio,2019
10.1016/j.neunet.2022.07.012,"© 2022 The AuthorsAdversarial robustness has become a central goal in deep learning, both in the theory and the practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how the adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error ( when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain the adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%. Moreover, we provide mathematical analysis of Interpolated Adversarial Training to confirm its efficiencies and demonstrate its advantages in terms of robustness and generalization.",Interpolated Adversarial Training: Achieving robust neural networks without sacrificing too much accuracy,Neural Networks,Article,10/1/2022,Yoshua,Bengio,2022
10.1016/j.neurol.2010.10.004,"Introduction: Air embolism is a rare complication of various invasive medical procedures. Venous cerebral air embolism is usually the consequence of paradoxical embolism. We report a case of isolated cerebral air embolism resulting from a non-paradoxical mechanism. Case report: A few minutes after his central venous catheter had been accidentally disconnected, a 63-year-old man developed left-sided rhythmic jerking movements followed by left hemiplegia. There were no associated cardiologic or pulmonary signs. Brain CT showed air bubbles in the right frontal cortical sulci. The brain MRI DWI and FLAIR sequences showed a high intensity right frontal cortical lesion without reduction in ADC. Transesophageal echocardiogram did not find a patent foramen ovale. Conclusions: In this case of venous cerebral air embolism, the lack of any cardiopulmonary manifestation, the lack of a patent foramen ovale and the neuroradiological findings are not in favor of the hypothesis of paradoxical embolism. The hypothesis of retrograde venous cerebral air embolism is discussed. © 2011 Elsevier Masson SAS. Tous droits réservés.",Iatrogenic venous cerebral air embolism without pulmonary manifestation: A retrograde mechanism?,Revue Neurologique,Article,8/1/2011,François,Chollet,2011
10.1016/j.neuron.2021.04.024,"© 2021 Elsevier Inc.Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman's System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.",How does hemispheric specialization contribute to human-defining cognition?,Neuron,Review,7/7/2021,Yoshua,Bengio,2021
10.1016/j.optom.2022.09.004,"© 2022 Spanish General Council of OptometryOptical coherence tomography (OCT) has revolutionized ophthalmic clinical practice and research, as a result of the high-resolution images that the method is able to capture in a fast, non-invasive manner. Although clinicians can interpret OCT images qualitatively, the ability to quantitatively and automatically analyse these images represents a key goal for eye care by providing clinicians with immediate and relevant metrics to inform best clinical practice. The range of applications and methods to analyse OCT images is rich and rapidly expanding. With the advent of deep learning methods, the field has experienced significant progress with state-of-the-art-performance for several OCT image analysis tasks. Generative adversarial networks (GANs) represent a subfield of deep learning that allows for a range of novel applications not possible in most other deep learning methods, with the potential to provide more accurate and robust analyses. In this review, the progress in this field and clinical impact are reviewed and the potential future development of applications of GANs to OCT image processing are discussed.",A review of generative adversarial network applications in optical coherence tomography image analysis,Journal of Optometry,Article,1/1/2022,Michael,Collins,2022
10.1016/j.ortho.2023.100759,"© 2023 The Author(s)Introduction: The purpose of the present study was to create a machine learning (ML) algorithm with the ability to predict the extraction/non-extraction decision in a racially and ethnically diverse sample. Methods: Data was gathered from the records of 393 patients (200 non-extraction and 193 extraction) from a racially and ethnically diverse population. Four ML models (logistic regression [LR], random forest [RF], support vector machine [SVM], and neural network [NN]) were trained on a training set (70% of samples) and then tested on the remaining samples (30%). The accuracy and precision of the ML model predictions were calculated using the area under the curve (AUC) of the receiver operating characteristics (ROC) curve. The proportion of correct extraction/non-extraction decisions was also calculated. Results: The LR, SVM, and NN models performed best, with an AUC of the ROC of 91.0%, 92.5%, and 92.3%, respectively. The overall proportion of correct decisions was 82%, 76%, 83%, and 81% for the LR, RF, SVM, and NN models, respectively. The features found to be most helpful to the ML algorithms in making their decisions were maxillary crowding/spacing, L1-NB (mm), U1-NA (mm), PFH:AFH, and SN-MP(°), although many other features contributed significantly. Conclusions: ML models can predict the extraction decision in a racially and ethnically diverse patient population with a high degree of accuracy and precision. Crowding, sagittal, and vertical characteristics all featured prominently in the hierarchy of components most influential to the ML decision-making process.",A machine learning model for orthodontic extraction/non-extraction decision in a racially and ethnically diverse patient population,International Orthodontics,Article,9/1/2023,Jeff,Dean,2023
10.1016/j.patcog.2012.05.009,"Incremental learning of neural networks has attracted much interest in recent years due to its wide applicability to large scale data sets and to distributed learning scenarios. Moreover, nonstationary learning paradigms have also emerged as a subarea of study in Machine Learning literature due to the problems of classical methods when dealing with data set shifts. In this paper we present an algorithm to train single layer neural networks with nonlinear output functions that take into account incremental, nonstationary and distributed learning scenarios. Moreover, it is demonstrated that introducing a regularization term into the proposed model is equivalent to choosing a particular initialization for the devised training algorithm, which may be suitable for real time systems that have to work under noisy conditions. In addition, the algorithm includes some previous models as special cases and can be used as a block component to build more complex models such as multilayer perceptrons, extending the capacity of these models to incremental, nonstationary and distributed learning paradigms. In this paper, the proposed algorithm is tested with standard data sets and compared with previous approaches, demonstrating its higher accuracy. © 2012 Elsevier Ltd.","Nonlinear single layer neural network training algorithm for incremental, nonstationary and distributed learning scenarios",Pattern Recognition,Article,12/1/2012,Amparo Alonso,Betanzos,2012
10.1016/j.patcog.2016.08.005,"© 2016 Elsevier LtdRecent deep learning based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition (HCCR) by learning discriminative representations directly from raw data. Nevertheless, we believe that the long-and-well investigated domain-specific knowledge should still help to boost the performance of HCCR. By integrating the traditional normalization-cooperated direction-decomposed feature map (directMap) with the deep convolutional neural network (convNet), we are able to obtain new highest accuracies for both online and offline HCCR on the ICDAR-2013 competition database. With this new framework, we can eliminate the needs for data augmentation and model ensemble, which are widely used in other systems to achieve their best results. This makes our framework to be efficient and effective for both training and testing. Furthermore, although directMap+convNet can achieve the best results and surpass human-level performance, we show that writer adaptation in this case is still effective. A new adaptation layer is proposed to reduce the mismatch between training and test data on a particular source layer. The adaptation process can be efficiently and effectively implemented in an unsupervised manner. By adding the adaptation layer into the pre-trained convNet, it can adapt to the new handwriting styles of particular writers, and the recognition accuracy can be further improved consistently and significantly. This paper gives an overview and comparison of recent deep learning based approaches for HCCR, and also sets new benchmarks for both online and offline HCCR.",Online and offline handwritten Chinese character recognition: A comprehensive study and new benchmark,Pattern Recognition,Article,1/1/2017,Yoshua,Bengio,2017
10.1016/j.patrec.2021.04.003,"© 2021 Elsevier B.V.Electroencephalography (EEG) provides appealing biometrics by encompassing unique attributes including robustness against forgery, privacy compliance, and aliveness detection. Among the main challenges in deploying EEG biometric systems in real-world applications, stability and usability are two important ones. They respectively reflect the capacity of the system to provide stable performance within and across different states, and the ease of use of the system. Previous studies indicate that the usability of an EEG biometric system is largely affected by the number of electrodes and reducing channel density is an effective way to enhance usability. However, it is still unclear what is the impact of channel density on recognition performance and stability. This study examines this issue for systems using different feature extraction and classification methods. Our results reveal a trade-off between channel density and stability. With low-density EEG, the recognition accuracy and stability are compromised to varying degrees. Based on the analysis, we propose a framework that integrates channel density augmentation, functional connectivity estimation and deep learning models for practical and stable EEG biometric systems. The framework helps to improve the stability of EEG biometric systems that use consumer-grade low channel density devices, while retaining the advantages of high usability.",On the channel density of EEG signals for reliable biometric recognition,Pattern Recognition Letters,Article,7/1/2021,Hussein,Abbass,2021
10.1016/j.patter.2021.100205,"© 2021 The AuthorIt has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.” Machine learning (ML) increasingly permeates every sphere of life. Complex, contextual, continually moving social and political challenges are automated and packaged as mathematical and engineering problems. Simultaneously, research on algorithmic injustice shows how ML automates and perpetuates historical, often unjust and discriminatory, patterns. The negative consequences of algorithmic systems, especially on marginalized communities, have spurred work on algorithmic fairness. Still, most of this work is narrow in scope, focusing on fine-tuning specific models, making datasets more inclusive/representative, and “debiasing” datasets. Although such work can constitute part of the remedy, a fundamentally equitable path must examine the wider picture, such as unquestioned or intuitive assumptions in datasets, current and historical injustices, and power asymmetries. As such, this work does not offer a list of implementable solutions towards a “fair” system, but rather is a call for scholars and practitioners to critically examine the field. It is taken for granted that ML and data science are fields that solve problems using data and algorithms. Thus, challenges are often formulated as problem/solution. One of the consequences of such discourse is that challenges that refuse such a problem/solution formulation, or those with no clear “solutions”, or approaches that primarily offer critical analysis are systematically discarded and perceived as out of the scope of these fields. This work hopes for a system-wide acceptance of critical work as an essential component of AI ethics, fairness, and justice. It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data sciences, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”",Algorithmic injustice: a relational ethics approach,Patterns,Review,2/12/2021,Abeba,Birhane,2021
10.1016/j.procs.2014.11.098,"© The Authors. Published by Elsevier B.V.Synthesizing concepts and findings from a number of recent models of human consciousness, a unified model of the key properties characterizing human consciousness is outlined. Six key properties are emphasized: Dynamical representation of the focus of consciousness, Focusing of energetic resources and focusing of informational resources on a subset of system knowledge, Global Workspace dynamics as outlined by Bernard Baars in his cognitive theory of consciousness, Integrated Information as emphasized by Tononi, and correlation of attentional focus with self-modeling. It is proposed that the extent, and relative importance, of these properties may vary in different states of consciousness; and that any AI system displaying closely human-like intelligence will need to manifest these properties in its consciousness as well. The ""hard problem"" of consciousness is sidestepped throughout, via focusing on structures and dynamics posited to serve as neural or cognitive correlates of subjective conscious experience.",Characterizing human-like consciousness: An integrative approach,Procedia Computer Science,Conference Paper,1/1/2014,Ben,Goertzel,2014
10.1016/j.procs.2014.11.103,"© The Authors. Published by Elsevier B.V.The representation paradigm used by a cognitive architecture helps to determine the kind of processes that it can perform more efficiently. Vector LIDA is a variation of the LIDA cognitive architecture that employs high-dimensional Modular Composite Representation (MCR) vectors as its main representation model and Integer Sparse Distributed Memory as its main memory implementation technology. The advantages of this new model include a more realistic and biologically plausible model, better integration with its episodic memory, better integration with other low level perceptual processing (such as deep learning systems), better scalability, and easier learning mechanisms. Here, after briefly recapping the LIDA model and MCR, we describe Vector LIDA and argue for its several advantages.",Vector LIDA,Procedia Computer Science,Conference Paper,1/1/2014,Stan,Franklin,2014
10.1016/j.procs.2016.08.243,"© 2016 The Authors. Published by Elsevier B.V.Functional behavior is considered to be the most basic, yet a critical notion in order to determine the characteristics of a system. However, how to reason about the functional behavior of a system in a systematic manner, is mostly limited by our cognitive processing abilities. While the UML-based behavior models can support a visual conceptualization of the functional behavior, they lack the rigorous, machine-processable reasoning capabilities. In this paper, we present a practical, knowledge-based approach to model the functional behavior that incorporates the notions of Commonsense Reasoning and Functional Reasoning over its core defining aspects. We demonstrate our approach with a detailed example, along with a set of use case scenarios. The main motivation behind this work was to develop a rigorous, logic-based approach to verify the levels of functional consistencies between cross-platform event-based systems. The focus of this paper, however, is to present the representational facility that can be utilized for the consistency validation system. While we provide a brief overview of the consistency validation system in this paper, a separate article will be dedicated for the comprehensive overview of the validation system itself.",Modelling Functional Behavior of Event-based Systems: A Practical Knowledge-based Approach,Procedia Computer Science,Conference Paper,1/1/2016,Thomas,Dean,2016
10.1016/j.procs.2020.10.082,"© 2020 The Authors. Published by Elsevier B.V.The Science of Science is an emerging field that enables tracking the dynamics of science in the form of birth and death of scientific fields and research trends in them. This study proposes a framework to analyze the content of academic publications to extract research trends and explore their temporal evolution. In this proposed framework, state-of-the-art embedding techniques are reviewed and utilized to consider semantic vectors of publications' keywords, which have been used to produce semantic-based clusters reflecting research trends or sub-fields. We compare our proposed method with LDA, as a baseline method, to demonstrate the explainability of the clusters, applying them to the field of “Artificial Intelligence” (AI) as a case study.",A framework for understanding the dynamics of science: A case study on AI,Procedia Computer Science,Conference Paper,1/1/2020,Hussein,Abbass,2020
10.1016/j.simpa.2022.100406,"© 2022 The Author(s)Today, artificial intelligence systems driven by machine learning algorithms can be in a position to take important, and sometimes legally binding, decisions about our everyday lives. In many cases, however, these systems and their actions are neither regulated nor certified. To help counter the potential harm that such algorithms can cause we developed an open source toolbox that can analyse selected fairness, accountability and transparency aspects of the machine learning process: data (and their features), models and predictions, allowing to automatically and objectively report them to relevant stakeholders. In this paper we describe the design, scope, usage and impact of this Python package, which is published under the 3-Clause BSD open source licence.","FAT Forensics: A Python toolbox for algorithmic fairness, accountability and transparency[Formula presented]",Software Impacts,Article,11/1/2022,Peter,Flach,2022
10.1016/j.simpat.2014.07.009,"Agent-based modelling and simulation (ABMS) is highly instrumental for studying socio-technical systems. MAIA - Modelling Agents using Institutional Analysis - is an ABMS modelling framework that formalises social sciences knowledge. It enables handling the complexity of large complex systems, allows collaborative model development and the reuse of model components when building simulations. We detail the procedural semantics for transforming a MAIA model into an executable simulation. Its evaluation through various case studies of model development and simulation is described. The MAIA meta-model is a declarative language to conceptualise an ABM. A model description in MAIA thus provides sufficient information to translate it into a simulation model - it defines the agents, their decision-making process and their actions, all within an institutional and physical context, to affect system states that are defined in the MAIA model. A modeller can use MAIA to specify and document her model and to build her simulation by using MAIA's semi-automatic code generation option. © 2014 Elsevier B.V. All rights reserved.",Model-driven agent-based simulation: Procedural semantics of a MAIA model,Simulation Modelling Practice and Theory,Article,1/1/2014,Virginia,Dignum,2014
10.1016/j.techfore.2010.09.006,"The development of human-level AI has been a core goal of the AI field since its inception, though at present it occupies only a fraction of the field's efforts. To help understand the viability of this goal, this article presents an assessment of expert opinions regarding human-level AI research conducted at AGI-09, a conference for this AI specialty. We found that various experts strongly disagree with each other on certain matters, such as timing and ordering of key milestones. However, we did find that most experts expect human-level AI to be reached within upcoming decades, and all experts give at least some chance that some milestones will be reached within this time. Furthermore, a majority of experts surveyed favor an integrative approach to human-level AI rather than an approach centered on a particular technique. Finally, experts are skeptical about the impact of massive research funding, especially if it is concentrated in relatively few approaches. These results suggest that the possibility of achieving human-level AI in the near term should be given serious consideration. © 2010 Elsevier Inc.",How long until human-level AI? Results from an expert assessment,Technological Forecasting and Social Change,Note,1/1/2011,Ben,Goertzel,2011
10.1016/j.techfore.2018.11.010,"© 2018The accelerating investment in artificial intelligence has vast implications for economic and cognitive development globally. However, AI is currently dominated by an oligopoly of centralized mega-corporations, who focus on the interests of their stakeholders. There is a now universal need for AI services by businesses who lack access to capital to develop their own AI services, and independent AI developers lack visibility and a source of revenue. This uneven playing field has a high potential to lead to inequitable circumstances with negative implications for humanity. Furthermore, the potential of AI is hindered by the lack of interoperability standards. The authors herein propose an alternative path for the development of AI: a distributed, decentralized, and democratized market for AIs run on distributed ledger technology. We describe the features and ethical advantages of such a system using SingularityNET, a watershed project being developed by Ben Goertzel and colleagues, as a case study. We argue that decentralizing AI opens the doors for a more equitable development of AI and AGI. It will also create the infrastructure for coordinated action between AIs that will significantly facilitate the evolution of AI into true AGI that is both highly capable and beneficial for humanity and beyond.","Distributed, decentralized, and democratized artificial intelligence",Technological Forecasting and Social Change,Short Survey,4/1/2019,Ben,Goertzel,2019
10.1016/j.whi.2010.09.003,"Background: The purpose of this study was to explore and analyze social determinants that influence adherence among Colombian women living with HIV/AIDS in poverty conditions. Methods: A qualitative, descriptive-interpretative study was developed. Forty-seven women participated in five focus group discussions. Also, in-depth interviews with six women were conducted. Findings: Results showed that women with lower adherence sell their antiretroviral medication to satisfy economic needs, and prioritize the care of their HIV-positive children over their own adherence needs. In contrast, women with higher adherence were found to participate in social support groups offered by nongovernmental organizations. Conclusion: These findings underscore the need to understand the social determinants that facilitate and/or hinder adherence among women in poverty-associated conditions. Results indicate the need to facilitate access to treatment on a timely and continual basis; provide economic resources, including support to meet basic needs as well as subsidies for transportation to health care centers; and explore mechanism for supporting the care of their offspring. © 2011 Jacobs Institute of Women's Health.",Applying an Expanded Social Determinant Approach to the Concept of Adherence to Treatment: The Case of Colombian Women Living With HIV/AIDS,Women's Health Issues,Article,3/1/2011,W,Ross,2011
10.1016/S0004-3702(01)00163-1,"In obstetrics, cardiotocograph (CTG) and non-stress test readings are indispensable to antenatal monitoring and assessment. Difficulties in the interpretation of CTG records require methods for computer-assisted analysis. This article describes CAFE (Computer Aided Foetal Evaluator), an intelligent tightly coupled hybrid system developed to overcome the difficulties inherent in CTG analysis. It integrates algorithms (implemented via conventional programming techniques) with Artificial Intelligence (AI) paradigms (rule-based systems and artificial neural networks), in order to automate and perform all the phases involved in real time antenatal monitoring, from the analysis and interpretation of CTG signals to diagnosis. Its architecture, components and functional character will be described in detail. The validation of CAFE over 3450 minutes of signal time corresponding to 53 different patients in a real environment is discussed, and its performance with respect to a group of experts is evaluated. Most of the results obtained reflect acceptable levels of performance - equivalent to expert performance - and thus confirm the suitability of AI techniques to applications in this field. © 2001 Elsevier Science B.V. All rights reserved.",Intelligent analysis and pattern recognition in cardiotocographic signals using a tightly coupled hybrid system,Artificial Intelligence,Article,3/1/2002,Amparo Alonso,Betanzos,2002
10.1016/s0004-3702(98)00055-1,"Creativity is a fundamental feature of human intelligence, and a challenge for AI. AI techniques can be used to create new ideas in three ways: by producing novel combinations of familiar ideas; by exploring the potential of conceptual spaces; and by making transformations that enable the generation of previously impossible ideas. AI will have less difficulty in modelling the generation of new ideas than in automating their evaluation. © 1998 Elsevier Science B.V. All rights reserved.",Creativity and artificial intelligence,Artificial Intelligence,Article,1/1/1998,Margaret,Boden,1998
10.1016/s0004-3702(98)00071-x,"A survey is given on two decades of developments in the field, encompassing an increase in computing power by four orders of magnitude. The '4-D approach' integrating expectation-based methods from systems dynamics and control engineering with methods from AI has allowed to create vehicles with unprecedented capabilities in the technical realm: autonomous road vehicle guidance in public traffic on freeways at speeds beyond 130 km/h, on-board-autonomous landing approaches of aircraft, and landmark navigation for AGV's, for road vehicles including turn-offs onto cross-roads, and for helicopters in low-level flight (real-time, hardware-in-the-loop simulations in the latter case). © 1998 Elsevier Science B.V. All rights reserved.",Vehicles capable of dynamic vision: A new breed of technical beings?,Artificial Intelligence,Article,1/1/1998,Ernst,Dickmanns,1998
10.1016/s0004-3702(99)00080-6,"One of the original motivations for research in qualitative physics was the development of intelligent tutoring systems and learning environments for physical domains and complex systems. This article demonstrates how a synergistic combination of qualitative reasoning and other AI techniques can be used to create an intelligent learning environment for students learning to analyze and design thermodynamic cycles. Pedagogically this problem is important because thermodynamic cycles express the key properties of systems which interconvert work and heat, such as power plants, propulsion systems, refrigerators, and heat pumps, and the study of thermodynamic cycles occupies a major portion of an engineering student's training in thermodynamics. This article describes CyclePad, a fully implemented articulate virtual laboratory that captures a substantial fraction of the knowledge in an introductory thermodynamics textbook and provides explanations of calculations and coaching support for students who are learning the principles of such cycles. CyclePad employs a distributed coaching model, where a combination of on-board facilities and a server-based coach accessed via email provide help for students, using a combination of teleological and case-based reasoning. CyclePad is a fielded system, in routine use in classrooms scattered all over the world. We analyze the combination of ideas that made CyclePad possible and comment on some lessons learned about the utility of various AI techniques based on our experience in fielding CyclePad. © 1999 Elsevier Science B.V. All rights reserved.",CyclePad: An articulate virtual laboratory for engineering thermodynamics,Artificial Intelligence,Article,1/1/1999,Ken,Forbus,1999
10.1016/S0004-3702(99)00098-3,"Today's Web sites are intricate but not intelligent; while Web navigation is dynamic and idiosyncratic, all too often Web sites are fossils cast in HTML. In response, this paper investigates adaptive Web sites; sites that automatically improve their organization and presentation by learning from visitor access patterns. Adaptive Web sites mine the data buried in Web server logs to produce more easily navigable Web sites. To demonstrate the feasibility of adaptive Web sites, the paper considers the problem of index page synthesis and sketches a solution that relies on novel clustering and conceptual clustering techniques. Our preliminary experiments show that high-quality candidate index pages can be generated automatically, and that our techniques outperform existing methods (including the Apriori algorithm, K-means clustering, hierarchical agglomerative clustering, and COBWEB) in this domain.",Towards adaptive Web sites: conceptual framework and case study,Artificial Intelligence,Article,1/1/2000,Oren,Etzioni,2000
10.1016/S0079-6123(06)65033-4,"A common goal of computational neuroscience and of artificial intelligence research based on statistical learning algorithms is the discovery and understanding of computational principles that could explain what we consider adaptive intelligence, in animals as well as in machines. This chapter focuses on what is required for the learning of complex behaviors. We believe it involves the learning of highly varying functions, in a mathematical sense. We bring forward two types of arguments which convey the message that many currently popular machine learning approaches to learning flexible functions have fundamental limitations that render them inappropriate for learning highly varying functions. The first issue concerns the representation of such functions with what we call shallow model architectures. We discuss limitations of shallow architectures, such as so-called kernel machines, boosting algorithms, and one-hidden-layer artificial neural networks. The second issue is more focused and concerns kernel machines with a local kernel (the type used most often in practice) that act like a collection of template-matching units. We present mathematical results on such computational architectures showing that they have a limitation similar to those already proved for older non-parametric methods, and connected to the so-called curse of dimensionality. Though it has long been believed that efficient learning in deep architectures is difficult, recently proposed computational principles for learning in deep architectures may offer a breakthrough. © 2007 Elsevier B.V. All rights reserved.",On the challenge of learning complex functions,Progress in Brain Research,Review,10/8/2007,Yoshua,Bengio,2007
10.1016/S0140-6736(00)02302-3,"AIDS has invigorated and distorted the study of sexual behaviour. Because that study began so recently, there remain many unanswered questions about why we have sex at all, why we do sex one way rather than another, or even how we define sex. Yet in every instance in which well-designed and adequately resourced behavioural interventions have been implemented, these have netted success in the form of failing HIV incidences or prevalences. But, despite these successes, such interventions remain patchy and poorly supported. Perhaps humankind's traditional aversion for the public discussion of sexual matters underlies this reticence. Or maybe a new era of 'creeping absolutism' - in which biomedical advances are given premature credit for what they can achieve in HIV control - has arrived.This article focuses on the significance of determining sexual behavior in the prevention of HIV infection. Studies have been conducted to investigate the different sexual behaviors and to develop behavioral interventions to decrease the spread of HIV/AIDS infection. This article contains the malleable definition of sex, psychosocial reasons for having sex, reasons for taking money for sex, and the potential determinants of sexual behavior. The authors also discussed the positive shift of sexual behavior in relation to HIV infection. However, several reasons were noted for the lack of behavioral change despite the increasing spread of HIV infection. In response, various models for behavioral change were developed, and several barriers to safer sex were noted. The fledging field of sexual-health promotion still has a long way to go, however, much has been achieved. The authors emphasized the need for further sexual behavior interventions that would fully integrate HIV control and sexual-health promotion to produce substantial gains for the entire population.",Preventing HIV: Determinants of sexual behaviour,Lancet,Review,5/27/2000,W,Ross,2000
10.1016/s0147-1767(99)00026-7,"Fears of HIV infection and AIDS are important predictors of the responses of both health care workers and others to people with HIV disease. We looked at the structure of the Fear of AIDS Schedule (FAIDSS) in health workers in Nigeria, Australia and the United States and used factor analysis and factor comparison techniques to determine the similarities of the factor structures. In all three cultures, similar five-factor structures were derived (Fear of loss of control; Fear of sex; Fear of HIV infection via blood or illness; Fear of death and medical interventions; and Fear of contact with out-groups), and Cattell's s index indicated that these factors were composed of substantially the same items in each culture. These data suggest that the structure of HIV/AIDS fears are similar in Anglophone health workers in three different cultures. © 1999 Elsevier Science Ltd. All rights reserved.",Measuring AIDS fears in health workers: Structure of the FAIDSS across countries,International Journal of Intercultural Relations,Article,1/1/2000,W,Ross,2000
10.1016/S0166-4115(97)80105-7,"This chapter provides an overview of an approach to the study of learning that, in broad terms, has developed as a part of the field of Artificial Intelligence (AI), where it is called reinforcement learning due to its roots in reinforcement theories of animal learning. We introduce the field from the perspective of AI and engineering, describing some of its key features, providing a formal model of the reinforcement-learning problem, and defining basic concepts that are exploited by solution methods. Detailed discussion of solution methods themselves and their history are very broad topics that we do not attempt to cover here. © 1997 Elsevier B.V. All rights reserved.",Chapter 19 Reinforcement learning in artificial intelligence,Advances in Psychology,Book Chapter,1/1/1997,Andrew,Barto,1997
10.1016/S0277-9536(97)00153-6,"This study examines the social anxieties associated with HIV prevention in adolescents in three African countries (Nigeria, Kenya, and Zimbabwe). The subjects used in this study were black were black Africans in form 2 or grade 10 in public high schools (Nigeria, n = 387; Kenya, n = 274; Zimbabwe n = 313). Subjects responded to the 33 item AIDS Social Assertiveness Scale (ASAS). Data indicated similar factor structures for each of the three countries and included five factors. The combined sample factor intercorrelations were modestly but significantly correlated. The mean scores for each factor were compared, and ANOVA of the factors by country, by gender, and by interaction between country and gender were performed. The factor structures were very similar between countries, each including five factors that had similar themes: condom interactions, refusal of risk, confiding in significant others, contact with people with HIV/AIDS, and general assertiveness. These factor structures were also very similar to one found in previous studies of Australian adolescents on the ASAS. The Kenyan means for four of the five factors were significantly lower than those for Nigeria, and were also significantly lower than the Zimbabwean means for two of the five factors, suggesting that Kenyan students are less anxious about social situations related to HIV/AIDS than others. Significant variance was found for several factors due to gender, country, and the interaction between gender and country. These results have important implications for designing education programs. The similarities of anxieties regarding HIV/AIDS social situations suggest that these clusters of social barriers to reduction of HIV infection risk might form the basis of educational interventions, and that dimensions of HIV social anxieties are similar across countries.",HIV/AIDS-related social anxieties in adolescents in three african countries,Social Science and Medicine,Article,2/1/1998,W,Ross,1998
10.1016/S0738-3991(98)00096-2,"This study reviewed Internet technological capabilities for counselling and several appropriate counselling models and assessed the application of HIV/AIDS related counselling on the Internet. Sixteen health professionals in HIV/AIDS related face-to-face counselling positions were interviewed: their HIV/AIDS service background was limited to Texas-certified HIV pre/post test counsellors, Texas- licensed counsellors in HIV/AIDS service field and HIV/AIDS case managers and social workers. Duration of interviews ranged from 30 min to 60 min and were recorded on audio cassette for review and analysis. Responses were generated using an editing style of the long-interview process. Edited responses were then analyzed for content and themes. Four major themes evolved from interview responses: counsellor-client relationship, target population, ethics and operation. Major concerns included the lack of visual and verbal cues during interaction, problems of accessibility by the neediest, confidentiality, impersonal experience and increased client separation/isolation. Greater benefits may be attained by targeting the younger segment of the population and other health professionals. A majority of respondents indicated support for additional development of Internet-based HIV/AIDS related counselling. Copyright (C) 1999 Elsevier Science Ireland Ltd.",Assessing the application of HIV and AIDS related education and counselling on the Internet,Patient Education and Counseling,Review,3/1/1999,W,Ross,1999
10.1016/S0921-8890(99)00125-6,"Recently, a large emphasis has been devoted to Automatic Vehicle Guidance since the automation of driving tasks carries a large number of benefits, such as the optimization of the use of transport infrastructures, the improvement of mobility, the minimization of risks, travel time, and energy consumption. This paper surveys the most common approaches to the challenging task of Autonomous Road Following reviewing the most promising experimental solutions and prototypes developed worldwide using AI techniques to perceive the environmental situation by means of artificial vision. The most interesting results and trends in this field as well as the perspectives on the evolution of intelligent vehicles in the next decades are also sketched out.",Vision-based intelligent vehicles: State of the art and perspectives,Robotics and Autonomous Systems,Article,7/31/2000,Alberto,Broggi,2000
10.1017/9781316779651.009,"© Cambridge University Press 2019.CPMs differ, for instance, over just what concepts these are. Initially inspired by computer science, their development has reflected changes in artificial intelligence (AI), theoretical psychology, and neuroscience. At first, the focus was on symbolic AI - dubbed GOFAI, or “Good Old-Fashioned AI, "" by John Haugeland (1985: 112). Later, some CPMs replaced, or complemented, GOFAI concepts with ideas drawn from connectionism and dynamical systems.",COMPUTATIONAL PHILOSOPHIES OF MIND,"The Cambridge History of Philosophy, 1945-2015",Book Chapter,1/1/2019,Margaret,Boden,2019
10.1017/CBO9780511730191.023,"© M. A. Bedau and C. E. Cleland 2010.INTRODUCTION: To recognize alien life, we would need to know what we mean by alien, and what we mean by life. Let us take the first thing first. We would see life as alien, I suggest, if it were discovered on a different planet, if it involved a fundamentally different biochemistry on planet Earth or if it was artificially generated by artificial intelligence/artificial life (AI/A-life) research (robots, perhaps?), In particular, we would see it as alien if it consisted of purely virtual organisms: creatures existing only in computer memory, and manifested on the VDU screen. Most talk about life, alien or otherwise, assumes some physical object (some “body”)—perhaps microscopically small—existing as a material thing. But this is not applicable to purely virtual organisms. The claim that such cyber-creatures could properly be regarded as alive is the claim that “strong A-life” is possible. (Strong A-life is so-called by analogy with strong AI (Searle 1980).) And that claim cannot be assessed without considering what, in general, we would count as life, That is a notoriously difficult question. In this paper, I will concentrate on one of the commonly listed criteria of life: metabolism. As we shall see, metabolism (in the sense used by biologists) is a form of biochemical fine-tuning. It characterizes all known life, as a matter of fact—and, I will argue, as a matter of necessity too. If that is right, then something that does not metabolize cannot properly be regarded as being alive.",Alien life: How would we know?,The Nature of Life: Classical and Contemporary Perspectives from Philosophy and Science,Book Chapter,1/1/2010,Margaret,Boden,2010
10.1017/dap.2023.2,"© The Author(s), 2023. Published by Cambridge University Press.Explainability is highly desired in machine learning (ML) systems supporting high-stakes policy decisions in areas such as health, criminal justice, education, and employment. While the field of explainable ML has expanded in recent years, much of this work has not taken real-world needs into account. A majority of proposed methods are designed with generic explainability goals without well-defined use cases or intended end users and evaluated on simplified tasks, benchmark problems/datasets, or with proxy users (e.g., Amazon Mechanical Turk). We argue that these simplified evaluation settings do not capture the nuances and complexities of real-world applications. As a result, the applicability and effectiveness of this large body of theoretical and methodological work in real-world applications are unclear. In this work, we take steps toward addressing this gap for the domain of public policy. First, we identify the primary use cases of explainable ML within public policy problems. For each use case, we define the end users of explanations and the specific goals the explanations have to fulfill. Finally, we map existing work in explainable ML to these use cases, identify gaps in established capabilities, and propose research directions to fill those gaps to have a practical societal impact through ML. The contribution is (a) a methodology for explainable ML researchers to identify use cases and develop methods targeted at them and (b) using that methodology for the domain of public policy and giving an example for the researchers on developing explainable ML methods that result in real-world impact.","Explainable machine learning for public policy: Use cases, gaps, and research directions",Data and Policy,Article,2/20/2023,Rayid,Ghani,2023
10.1017/S0140525X17000139,"We agree with Lake et al.'s trenchant analysis of deep learning systems, including that they are highly brittle and that they need vastly more examples than do people. We also agree that human cognition relies heavily on structured relational representations. However, we differ in our analysis of human cognitive processing. We argue that (1) analogical comparison processes are central to human cognition; and (2) intuitive physical knowledge is captured by qualitative representations, rather than quantitative simulations.",Evidence from machines that learn and think like people,The Behavioral and brain sciences,Note,1/1/2017,Ken,Forbus,2017
10.1017/S0140525X97270019,"This commentary connects some of Glenberg's ideas to similar ideas from artificial intelligence. Second, it briefly discussed hidden assumptions relating to meaning, representations, and projectable properties. Finally, questions about mechanisms, mental imagery, and conceptualization in animals are posed.","Action patterns, conceptualization, and artificial intelligence",Behavioral and Brain Sciences,Short Survey,1/1/1997,Stan,Franklin,1997
10.1017/S026357470001506X,"This paper describes the principles of the advanced programming techniques often dubbed Artificial Intelligence involved in decision making as may be of some value in matters related to production engineering. Automated decision making in the context of production can adopt many aspects. At the most obvious level, a robot may have to plan a sequence of actions on the basis of signals obtained from changing conditions in its environment. These signals may, indeed, be quite complex, for example the input of visual information from a television camera. At another level, automated planning may be required to schedule the entire work cycle of a plant that includes many robots as well as other types of automated machinery. The often-quoted dark factory is an example of this, where not only some of the operations (such as welding) are done by robots, but also the transport of part-completed assemblies is automatically scheduled as a set of actions for autonomic transporters and cranes. It is common practice for this activity to be preprogrammed to the greatest detail. Automated decision making is aimed at adding flexibility to the process so that it can absolve the system designer from having to forsee every eventuality at the design stage. Frequent reference is made in this context to artificial intelligence (AI), knowledge-based and expert systems. Although these topics are more readily associated with computer science, it is the automated factory, in general, and the robot, in particular, that will benefit from success in these fields. In this part of the paper we try to sharpen up this perspective, while in part II we aim to discuss the history of artificial intelligence in this context. In part III we discuss the industrial prospects for the field. © 1987, Cambridge University Press. All rights reserved.",Artificial intelligence for production engineering: A historical approach,Robotica,Article,1/1/1987,Igor,Aleksander,1987
10.1017/S0269888900001090,"Both the artificial intelligence (AI) and the operations research (OR) communities are interested in developing techniques for solving hard combinatorial problems, in particular in the domain of planning and scheduling. AI approaches encompass a rich collection of knowledge representation formalisms for dealing with a wide variety of real-world problems. Some of the different techniques from AI and OR for planning and scheduling are discussed and compared, highlighting potential synergistic benefits from combining the techniques.",Artificial intelligence and operations research: Challenges and opportunities in planning and scheduling,Knowledge Engineering Review,Article,3/1/2000,Carla,Gomes,2000
10.1017/S0269888904000025,"This document describes COBRA-ONT, an ontology for supporting pervasive context-aware systems. COBRA-ONT, expressed in the Web Ontology Language OWL, is a collection of ontologies for describing places, agents and events and their associated properties in an intelligent meeting-room domain. This ontology is developed as a part of the Context Broker Architecture (CoBrA), a broker-centric agent architecture that provides knowledge sharing, context reasoning and privacy protection supports for pervasive context-aware systems. We also describe an inference engine for reasoning with information expressed using the COBRA-ONT ontology and the ongoing research in using the DAML-Time ontology for context reasoning.",An ontology for context-aware pervasive computing environments,Knowledge Engineering Review,Article,9/1/2003,Tim,Finin,2003
10.1017/S0269888904000049,"We have produced an ontology specifying a model of computer attack. Our ontology is based upon an analysis of over 4000 classes of computer intrusions and their corresponding attack strategies and is categorised according to system component targeted, means of attack, consequence of attack and location of attacker. We argue that any taxonomic characteristics used to define a computer attack be limited in scope to those features that are observable and measurable at the target of the attack. We present our model as a target-centric ontology that is to be refined and expanded over time. We state the benefits of forgoing dependence upon taxonomies in favour of ontologies for the classification of computer attacks and intrusions. We have specified our ontology using the DARPA Agent Markup Language+Ontology Inference Layer and have prototyped it using DAMLJessKB. We present our model as a target-centric ontology and illustrate the benefits of utilising an ontology in lieu of a taxonomy, by presenting a use-case scenario of a distributed intrusion detection system.",Using DAML + OIL to classify intrusive behaviours,Knowledge Engineering Review,Article,9/1/2003,Tim,Finin,2003
10.1017/S0305000906007719,"P. Bloom's (1990) data on subject omission are often taken as strong support for the view that child language can be explained in terms of full competence coupled with processing limitations in production. This paper examines whether processing limitations in learning may provide a more parsimonious explanation of the data without the need to assume full competence. We extended P. Bloom's study by using a larger sample (12 children) and measuring subject omission phenomena in three developmental phases. The results revealed a Verb Phrase-length effect consistent with that reported by P. Bloom. However, contrary to the predictions of the processing limitations account, the proportion of overt subjects that were pronominal increased with developmental phase. The data were simulated with MOSAIC, a computational model that learns to produce progressively longer utterances as a function of training. MOSAIC was able to capture all of the effects reported by P. Bloom through a resource-limited distributional analysis of child-directed speech. Since MOSAIC does not have any built-in linguistic knowledge, these results show that the phenomena identified by P. Bloom do not constitute evidence for underlying competence on the part of the child. They also underline the need to develop more empirically grounded models of the way that processing limitations in learning might influence the language acquisition process. © 2007 Cambridge University Press.",Understanding the developmental dynamics of subject omission: The role of processing limitations in learning,Journal of Child Language,Article,2/1/2007,Fernand,Gobet,2007
10.1017/S0305000909990523,"In this study, we use corpus analysis and computational modelling techniques to compare two recent accounts of the OI stage: Legate & Yang's (2007) Variational Learning Model and Freudenthal, Pine & Gobet's (2006) Model of Syntax Acquisition in Children. We first assess the extent to which each of these accounts can explain the level of OI errors across five different languages (English, Dutch, German, French and Spanish). We then differentiate between the two accounts by testing their predictions about the relation between children's OI errors and the distribution of infinitival verb forms in the input language. We conclude that, although both accounts fit the cross-linguistic patterning of OI errors reasonably well, only MOSAIC is able to explain why verbs that occur more frequently as infinitives than as finite verb forms in the input also occur more frequently as OI errors than as correct finite verb forms in the children's output. Copyright © 2010 Cambridge University Press.",Explaining quantitative variation in the rate of Optional Infinitive errors across languages: A comparison of MOSAIC and the Variational Learning Model,Journal of Child Language,Article,6/1/2010,Fernand,Gobet,2010
10.1017/S0956796899003408,Proof by mathematical induction plays a crucial role in reasoning about functional programs. A generalization step often holds the key to discovering an inductive proof. We present a generalization technique which is particularly applicable when reasoning about functional programs involving accumulating parameters. We provide empirical evidence for the success of our technique and show how it is contributing to the ongoing development of a parallelizing compiler for Standard ML.,Automatic verification of functions with accumulating parameters,Journal of Functional Programming,Article,1/1/1999,Alan,Bundy,1999
10.1021/acs.accounts.0c00699,"© 2020 American Chemical Society.ConspectusRecent advances in computer hardware and software have led to a revolution in deep neural networks that has impacted fields ranging from language translation to computer vision. Deep learning has also impacted a number of areas in drug discovery, including the analysis of cellular images and the design of novel routes for the synthesis of organic molecules. While work in these areas has been impactful, a complete review of the applications of deep learning in drug discovery would be beyond the scope of a single Account. In this Account, we will focus on two key areas where deep learning has impacted molecular design: the prediction of molecular properties and the de novo generation of suggestions for new molecules.One of the most significant advances in the development of quantitative structure-activity relationships (QSARs) has come from the application of deep learning methods to the prediction of the biological activity and physical properties of molecules in drug discovery programs. Rather than employing the expert-derived chemical features typically used to build predictive models, researchers are now using deep learning to develop novel molecular representations. These representations, coupled with the ability of deep neural networks to uncover complex, nonlinear relationships, have led to state-of-the-art performance. While deep learning has changed the way that many researchers approach QSARs, it is not a panacea. As with any other machine learning task, the design of predictive models is dependent on the quality, quantity, and relevance of available data. Seemingly fundamental issues, such as optimal methods for creating a training set, are still open questions for the field. Another critical area that is still the subject of multiple research efforts is the development of methods for assessing the confidence in a model.Deep learning has also contributed to a renaissance in the application of de novo molecule generation. Rather than relying on manually defined heuristics, deep learning methods learn to generate new molecules based on sets of existing molecules. Techniques that were originally developed for areas such as image generation and language translation have been adapted to the generation of molecules. These deep learning methods have been coupled with the predictive models described above and are being used to generate new molecules with specific predicted biological activity profiles. While these generative algorithms appear promising, there have been only a few reports on the synthesis and testing of molecules based on designs proposed by generative models. The evaluation of the diversity, quality, and ultimate value of molecules produced by generative models is still an open question. While the field has produced a number of benchmarks, it has yet to agree on how one should ultimately assess molecules ""invented""by an algorithm.",Applications of Deep Learning in Molecule Generation and Molecular Property Prediction,Accounts of Chemical Research,Article,1/19/2021,Regina,Barzilay,2021
10.1021/acs.jcim.1c00284,"© Access to structured chemical reaction data is of key importance for chemists in performing bench experiments and in modern applications like computer-aided drug design. Existing reaction databases are generally populated by human curators through manual abstraction from published literature (e.g., patents and journals), which is time consuming and labor intensive, especially with the exponential growth of chemical literature in recent years. In this study, we focus on developing automated methods for extracting reactions from chemical literature. We consider journal publications as the target source of information, which are more comprehensive and better represent the latest developments in chemistry compared to patents; however, they are less formulaic in their descriptions of reactions. To implement the reaction extraction system, we first devised a chemical reaction schema, primarily including a central product, and a set of associated reaction roles such as reactants, catalyst, solvent, and so on. We formulate the task as a structure prediction problem and solve it with a two-stage deep learning framework consisting of product extraction and reaction role labeling. Both models are built upon Transformer-based encoders, which are adaptively pretrained using domain and task-relevant unlabeled data. Our models are shown to be both effective and data efficient, achieving an F1 score of 76.2% in product extraction and 78.7% in role extraction, with only hundreds of annotated reactions.",Automated Chemical Reaction Extraction from Scientific Literature,Journal of Chemical Information and Modeling,Article,1/1/2021,Regina,Barzilay,2021
10.1021/acs.jmedchem.9b02120,"© 2020 American Chemical Society.Artificial intelligence and machine learning have demonstrated their potential role in predictive chemistry and synthetic planning of small molecules; there are at least a few reports of companies employing in silico synthetic planning into their overall approach to accessing target molecules. A data-driven synthesis planning program is one component being developed and evaluated by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, comprising MIT and 13 chemical and pharmaceutical company members. Together, we wrote this perspective to share how we think predictive models can be integrated into medicinal chemistry synthesis workflows, how they are currently used within MLPDS member companies, and the outlook for this field.",Current and Future Roles of Artificial Intelligence in Medicinal Chemistry Synthesis,Journal of Medicinal Chemistry,Review,8/27/2020,Regina,Barzilay,2020
10.1021/acs.jpclett.0c02535,© 2020 American Chemical Society.Amorphous molecular assemblies appear in a vast array of systems: from living cells to chemical plants and from everyday items to new devices. The absence of long-range order in amorphous materials implies that precise knowledge of their underlying structures throughout is needed to rationalize and control their properties at the mesoscale. Standard computational simulations suffer from exponentially unfavorable scaling of the required compute with system size. We present a method based on deep learning that leverages the finite range of structural correlations for an autoregressive generation of disordered molecular aggregates up to arbitrary size from small-scale computational or experimental samples. We benchmark performance on self-assembled nanoparticle aggregates and proceed to simulate monolayer amorphous carbon with atomistic resolution. This method bridges the gap between the nanoscale and mesoscale simulations of amorphous molecular systems.,Generating Multiscale Amorphous Molecular Structures Using Deep Learning: A Study in 2D,Journal of Physical Chemistry Letters,Article,10/15/2020,Yoshua,Bengio,2020
10.1021/acscombsci.6b00153,"© 2016 American Chemical Society.Rapid construction of phase diagrams is a central tenet of combinatorial materials science with accelerated materials discovery efforts often hampered by challenges in interpreting combinatorial X-ray diffraction data sets, which we address by developing AgileFD, an artificial intelligence algorithm that enables rapid phase mapping from a combinatorial library of X-ray diffraction patterns. AgileFD models alloying-based peak shifting through a novel expansion of convolutional nonnegative matrix factorization, which not only improves the identification of constituent phases but also maps their concentration and lattice parameter as a function of composition. By incorporating Gibbs' phase rule into the algorithm, physically meaningful phase maps are obtained with unsupervised operation, and more refined solutions are attained by injecting expert knowledge of the system. The algorithm is demonstrated through investigation of the V-Mn-Nb oxide system where decomposition of eight oxide phases, including two with substantial alloying, provides the first phase map for this pseudoternary system. This phase map enables interpretation of high-throughput band gap data, leading to the discovery of new solar light absorbers and the alloying-based tuning of the directallowed band gap energy of MnV2O6. The open-source family of AgileFD algorithms can be implemented into a broad range of high throughput workflows to accelerate materials discovery.",Automated phase mapping with AgileFD and its application to light absorber discovery in the V-Mn-Nb oxide system,ACS Combinatorial Science,Article,1/9/2017,Carla,Gomes,2017
10.1021/acsestengg.1c00269,"© 2021 American Chemical Society.Inefficiencies and imprecise input control in agriculture have caused devastating consequences to ecosystems. Urban controlled environment agriculture (CEA) is a proposed approach to mitigate the impacts of cultivation, but precise control of inputs (i.e., nutrient, water, etc.) is limited by the ability to monitor dynamic conditions. Current mechanistic and physiological plant growth models (MPMs) have not yet been unified and have uncovered knowledge gaps of the complex interplay among control variables. Moreover, because of their specificity, MPMs are of limited utility when extended to additional plant species or environmental conditions. Simultaneously, although machine learning (ML) can uncover latent interactions across conditions, phenotyping bottlenecks have hindered successful application. To bridge these gaps, we propose an integrative approach whereby MPMs are used to construct the foundations of ML algorithms, reducing data requirements and costs, and ML is used to elucidate parameters and causal inference in MPM. This review highlights research about control and automation in CEA, synthesizing literature into a framework whereby ML, MPM, and biofeedback inform what we call dynamically controlled environment agriculture (DCEA). We highlight synergistic characteristics of MPM and ML to illustrate that a DCEA framework could contribute to urban resilience, human health, and optimized productivity and nutritional content.",Dynamically Controlled Environment Agriculture: Integrating Machine Learning and Mechanistic and Physiological Models for Sustainable Food Cultivation,ACS ES and T Engineering,Review,1/14/2022,Frank,Dellaert,2022
10.1021/ci00068a023,"The Stony Brook SYNCHEM system is a large knowledge-based domain-specific heuristic problem-solving program that is able to find valid synthesis routes for organic molecules of substantial interest and complexity without online guidance on the part of its user. In common with many such AI performance programs, SYNCHEM requires a substantial knowledge base to make it routinely useful, but as the designers of most of these programs have discovered, it is very difficult to engage domain experts to the long-term dedication and intensity of commitment necessary to create a production-quality knowledge base. ISOLDE and TRISTAN are machine learning programs that use large computer-readable databases of specific reaction instances as a source of training examples for algorithms designed to extract the underlying reaction schemata via inductive and deductive generalization. ISOLDE learns principally by inductive generalization, while TRISTAN makes use of a methodology that is primarily deductive, and which is usually described as explanation-based learning. Since the individual reaction entries in most com¬puter-readable databases are often haphazardly sorted and classified, a taxonomy program called BRANGÄNE has been written to partition the input databases into coherent reaction classes using the methodology of conceptual clustering. © 1990, American Chemical Society. All rights reserved.",Building and Refining a Knowledge Base for Synthetic Organic Chemistry via the Methodology of Inductive and Deductive Machine Learning,Journal of Chemical Information and Computer Sciences,Article,11/1/1990,Herbert,Gelernter,1990
10.1023/A:1008311207953,"Alan Turing devised his famous test (TT) through a slight modification of the parlor game in which a judge tries to ascertain the gender of two people who are only linguistically accessible. Stevan Harnad has introduced the Total TT, in which the judge can look at the contestants in an attempt to determine which is a robot and which a person. But what if we confront the judge with an animal, and a robot striving to pass for one, and then challenge him to peg which is which? Now we can index TTT to a particular animal and its synthetic correlate. We might therefore have TTTrat, TTTcat, TTTdog, and so on. These tests, as we explain herein, are a better barometer of artificial intelligence (AI) than Turing's original TT, because AI seems to have ammunition sufficient only to reach the level of artificial animal, not artificial person. © 2000 Kluwer Academic Publishers.","Animals, zombanimals, and the total turing test: The essence of artificial intelligence","Journal of Logic, Language and Information",Article,1/1/2000,Selmer,Bringsjord,2000
10.1023/A:1011257022242,"Andrew Boucher (1997) argues that ""parallel computation is fundamentally different from sequential computation"" (p. 543), and that this fact provides reason to be skeptical about whether AI can produce a genuinely intelligent machine. But parallelism, as I prove herein, is irrelevant. What Boucher has inadvertently glimpsed is one small part of a mathematical tapestry portraying the simple but undeniable fact that physical computation can be fundamentally different from ordinary, ""textbook"" computation (whether parallel or sequential). This tapestry does indeed immediately imply that human cognition may be uncomputable.","In computation, parallel is nothing, physical everything",Minds and Machines,Article,2/1/2001,Selmer,Bringsjord,2001
10.1023/A:1013685612819,"Hypertext poses new research challenges for text classification. Hyperlinks, HTML tags, category labels distributed over linked documents, and meta data extracted from related Web sites all provide rich information for classifying hypertext documents. How to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question. This paper seeks a principled approach to providing the answers. Specifically, we define five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier. Using three hypertext datasets and three well-known learning algorithms (Naive Bayes, Nearest Neighbor, and First Order Inductive Learner), we examine these regularities in different domains, and compare alternative ways to exploit them. Our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems. We find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets. We also observed that extracting meta data from related Web sites was extremely useful for improving classification accuracy in some of those domains. Finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy Web pages.",A study of approaches to hypertext categorization,Journal of Intelligent Information Systems,Article,3/1/2002,Rayid,Ghani,2002
10.1023/A:1020385804671,"In the near future, we will see dramatic changes in computing and networking hardware. A large number of devices (e.g., phones, PDAs, even small household appliances) will become computationally enabled. Micro/nano sensors will be widely embedded in most engineered artifacts, from the clothes we wear to the roads we drive on. All of these devices will be (wirelessly) networked using Bluetooth, IEEE 802.15 or IEEE 802.11 for short range connectivity creating pervasive environment. In this age where a large number of wirelessly networked appliances and devices are becoming commonplace, there is a necessity for providing a standard interface to them that is easily accessible by any user. This paper outlines the design of Centaurus, an infrastructure for presenting services to heterogeneous mobile clients in a physical space via some short range wireless links. The infrastructure is communication medium independent; we have implemented the system over Bluetooth, CDPD and Infrared, three well-known wireless technologies. All the components in our model use a language based on Extensible Markup Language (XML) for communication, giving the system a uniform and easily adaptable interface. Centaurus defines a uniform infrastructure for heterogeneous services, both hardware and software, to be made available to diverse mobile users within a confined space.",Centaurus: An infrastructure for service management in ubiquitous computing environments,Wireless Networks,Article,1/1/2002,Tim,Finin,2002
10.1023/A:1021765902788,"The problem of integrating data from multiple data sources - either on the Internet or within enterprises - has received much attention in the database and AI communities. The focus has been on building data integration systems that provide a uniform query interface to the sources. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the query interface and the source schemas. Examples of mapping are ""element location maps to address"" and ""price maps to listed-price"". We propose a multistrategy learning approach to automatically find such mappings. The approach applies multiple learner modules, where each module exploits a different type of information either in the schemas of the sources or in their data, then combines the predictions of the modules using a meta-learner. Learner modules employ a variety of techniques, ranging from Naive Bayes and nearest-neighbor classification to entity recognition and information retrieval. We describe the LSD system, which employs this approach to find semantic mappings. To further improve matching accuracy. LSD exploits domain integrity constraints, user feedback, and nested structures in XML data. We test LSD experimentally on several real-world domains. The experiments validate the utility of multistrategy learning for data integration and show that LSD proposes semantic mappings with a high degree of accuracy.",Learning to match the schemas of data sources: A multistrategy approach,Machine Learning,Article,3/1/2003,Pedro,Domingos,2003
10.1023/A:1026241332041,"The features of analytical approaches to the Philosophy of information were discussed. Philosophical information (PI), as a philosophy of artificial intelligence (AI), attempted to develop a continuum conjecture concerning data, information and knowledge. The influence of PI on research in epistemoloy and the philosophy of mind, in computer science and logic, in the philosophy of science was also described.",Two Approaches to the Philosophy of Information,Minds and Machines,Review,11/1/2003,Luciano,Floridi,2003
10.1023/A:1027362900053,"This introductory paper describes how the technology behind BT's iBusiness programme, namely AI, has been employed in industry, and goes on to illustrate not only some of its current benefits, but also the future direction of this research, which will provide enormous potential for delivering value both to BT and to industry as a whole.",What has AI done for us?,BT Technology Journal,Article,10/1/2003,Michael,Georgeff,2003
10.1024/1421-0185/a000241,"© 2020 Hogrefe Verlag GmbH & Co. KG. All rights reserved.An important way to develop models in psychology and cognitive science is to express them as computer programs. However, computational modeling is not an easy task. To address this issue, some have proposed using artificial-intelligence (AI) techniques, such as genetic programming (GP) to semiautomatically generate models. In this paper, we establish whether models used to generate data can be recovered when GP evolves models accounting for such data. As an example, we use an experiment from decision-making which addresses a central question in decision-making research, namely, to understand what strategy, or policy, agents adopt in order to make a choice. In decision-making, this often means understanding the policy that best explains the distribution of choices and/or reaction times of twoalternative forced-choice decisions. We generated data from three models using different psychologically plausible policies and then evaluated the ability and extent of GP to correctly identify the true generating model among the class of virtually infinite candidate models. Our results show that, regardless of the complexity of the policy, GP can correctly identify the true generating process. Given these results, we discuss implications for cognitive science research and computational scientific discovery as well as possible future applications.",Modeling Value-Based Decision-Making Policies Using Genetic Programming: A Proof-of-Concept Study,Swiss Journal of Psychology,Article,12/1/2020,Fernand,Gobet,2020
10.1038/nature14541,"© 2015 Macmillan Publishers Limited. All rights reserved.How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.",Probabilistic machine learning and artificial intelligence,Nature,Review,5/27/2015,Zoubin,Ghahramani,2015
10.1038/s41467-019-14108-y,"© 2020, The Author(s).The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.",The role of artificial intelligence in achieving the Sustainable Development Goals,Nature Communications,Review,12/1/2020,Virginia,Dignum,2020
10.1038/s41467-021-27714-6,"© 2022, The Author(s).Fine-grained records of people’s interactions, both offline and online, are collected at large scale. These data contain sensitive information about whom we meet, talk to, and when. We demonstrate here how people’s interaction behavior is stable over long periods of time and can be used to identify individuals in anonymous datasets. Our attack learns the profile of an individual using geometric deep learning and triplet loss optimization. In a mobile phone metadata dataset of more than 40k people, it correctly identifies 52% of individuals based on their 2-hop interaction graph. We further show that the profiles learned by our method are stable over time and that 24% of people are still identifiable after 20 weeks. Our results suggest that people with well-balanced interaction graphs are more identifiable. Applying our attack to Bluetooth close-proximity networks, we show that even 1-hop interaction graphs are enough to identify people more than 26% of the time. Our results provide strong evidence that disconnected and even re-pseudonymized interaction data can be linked together making them personal data under the European Union’s General Data Protection Regulation.",Interaction data are identifiable even across long periods of time,Nature Communications,Article,12/1/2022,Michael,Bronstein,2022
10.1038/s41586-021-03544-w,"© 2021, The Author(s), under exclusive licence to Springer Nature Limited.Chip floorplanning is the engineering task of designing the physical layout of a computer chip. Despite five decades of research1, chip floorplanning has defied automation, requiring months of intense effort by physical design engineers to produce manufacturable layouts. Here we present a deep reinforcement learning approach to chip floorplanning. In under six hours, our method automatically generates chip floorplans that are superior or comparable to those produced by humans in all key metrics, including power consumption, performance and chip area. To achieve this, we pose chip floorplanning as a reinforcement learning problem, and develop an edge-based graph convolutional neural network architecture capable of learning rich and transferable representations of the chip. As a result, our method utilizes past experience to become better and faster at solving new instances of the problem, allowing chip design to be performed by artificial agents with more experience than any human designer. Our method was used to design the next generation of Google’s artificial intelligence (AI) accelerators, and has the potential to save thousands of hours of human effort for each new generation. Finally, we believe that more powerful AI-designed hardware will fuel advances in AI, creating a symbiotic relationship between the two fields.",A graph placement methodology for fast chip design,Nature,Article,6/10/2021,Jeff,Dean,2021
10.1038/s41591-019-0539-7,"© 2019, The Author(s), under exclusive licence to Springer Nature America, Inc.The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. Here we propose a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows. We demonstrate the utility of ARM in the detection of metastatic breast cancer and the identification of prostate cancer, with latency compatible with real-time use. We anticipate that the ARM will remove barriers towards the use of AI designed to improve the accuracy and efficiency of cancer diagnosis.",An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis,Nature Medicine,Article,9/1/2019,Jeff,Dean,2019
10.1038/s41591-021-01599-w,"© 2022, The Author(s), under exclusive licence to Springer Nature America, Inc.Screening programs must balance the benefit of early detection with the cost of overscreening. Here, we introduce a novel reinforcement learning-based framework for personalized screening, Tempo, and demonstrate its efficacy in the context of breast cancer. We trained our risk-based screening policies on a large screening mammography dataset from Massachusetts General Hospital (MGH; USA) and validated this dataset in held-out patients from MGH and external datasets from Emory University (Emory; USA), Karolinska Institute (Karolinska; Sweden) and Chang Gung Memorial Hospital (CGMH; Taiwan). Across all test sets, we find that the Tempo policy combined with an image-based artificial intelligence (AI) risk model is significantly more efficient than current regimens used in clinical practice in terms of simulated early detection per screen frequency. Moreover, we show that the same Tempo policy can be easily adapted to a wide range of possible screening preferences, allowing clinicians to select their desired trade-off between early detection and screening costs without training new policies. Finally, we demonstrate that Tempo policies based on AI-based risk models outperform Tempo policies based on less accurate clinical risk models. Altogether, our results show that pairing AI-based risk models with agile AI-designed screening policies has the potential to improve screening programs by advancing early detection while reducing overscreening.",Optimizing risk-based breast cancer screening policies with reinforcement learning,Nature Medicine,Article,1/1/2022,Regina,Barzilay,2022
10.1038/s41592-019-0666-6,"© 2019, The Author(s), under exclusive licence to Springer Nature America, Inc.Predicting interactions between proteins and other biomolecules solely based on structure remains a challenge in biology. A high-level representation of protein structure, the molecular surface, displays patterns of chemical and geometric features that fingerprint a protein’s modes of interactions with other biomolecules. We hypothesize that proteins participating in similar interactions may share common fingerprints, independent of their evolutionary history. Fingerprints may be difficult to grasp by visual analysis but could be learned from large-scale datasets. We present MaSIF (molecular surface interaction fingerprinting), a conceptual framework based on a geometric deep learning method to capture fingerprints that are important for specific biomolecular interactions. We showcase MaSIF with three prediction challenges: protein pocket-ligand prediction, protein–protein interaction site prediction and ultrafast scanning of protein surfaces for prediction of protein–protein complexes. We anticipate that our conceptual framework will lead to improvements in our understanding of protein function and design.",Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning,Nature Methods,Article,2/1/2020,Michael,Bronstein,2020
10.1038/s41597-023-02017-1,"© 2023, The Author(s).SPHERE is a large multidisciplinary project to research and develop a sensor network to facilitate home healthcare by activity monitoring, specifically towards activities of daily living. It aims to use the latest technologies in low powered sensors, internet of things, machine learning and automated decision making to provide benefits to patients and clinicians. This dataset comprises data collected from a SPHERE sensor network deployment during a set of experiments conducted in the ‘SPHERE House’ in Bristol, UK, during 2016, including video tracking, accelerometer and environmental sensor data obtained by volunteers undertaking both scripted and non-scripted activities of daily living in a domestic residence. Trained annotators provided ground-truth labels annotating posture, ambulation, activity and location. This dataset is a valuable resource both within and outside the machine learning community, particularly in developing and evaluating algorithms for identifying activities of daily living from multi-modal sensor data in real-world environments. A subset of this dataset was released as a machine learning competition in association with the European Conference on Machine Learning (ECML-PKDD 2016).",A multi-sensor dataset with annotated activities of daily living recorded in a residential setting,Scientific Data,Data Paper,12/1/2023,Peter,Flach,2023
10.1038/s41598-018-33706-2,"© 2018, The Author(s).A correction to this article has been published and is linked from the HTML and PDF versions of this paper. The error has been fixed in the paper.","Erratum to: How morphological development can guide evolution (Scientific Reports, (2018), 8, 1, (13934), 10.1038/s41598-018-31868-7)",Scientific Reports,Erratum,12/1/2018,Josh,Bongard,2018
10.1038/s41598-019-48909-4,"© 2019, The Author(s).We implemented Machine Learning (ML) techniques to advance the study of sperm whale (Physeter macrocephalus) bioacoustics. This entailed employing Convolutional Neural Networks (CNNs) to construct an echolocation click detector designed to classify spectrograms generated from sperm whale acoustic data according to the presence or absence of a click. The click detector achieved 99.5% accuracy in classifying 650 spectrograms. The successful application of CNNs to clicks reveals the potential of future studies to train CNN-based architectures to extract finer-scale details from cetacean spectrograms. Long short-term memory and gated recurrent unit recurrent neural networks were trained to perform classification tasks, including (1) “coda type classification” where we obtained 97.5% accuracy in categorizing 23 coda types from a Dominica dataset containing 8,719 codas and 93.6% accuracy in categorizing 43 coda types from an Eastern Tropical Pacific (ETP) dataset with 16,995 codas; (2) “vocal clan classification” where we obtained 95.3% accuracy for two clan classes from Dominica and 93.1% for four ETP clan types; and (3) “individual whale identification” where we obtained 99.4% accuracy using two Dominica sperm whales. These results demonstrate the feasibility of applying ML to sperm whale bioacoustics and establish the validity of constructing neural networks to learn meaningful representations of whale vocalizations.",Deep Machine Learning Techniques for the Detection and Classification of Sperm Whale Bioacoustics,Scientific Reports,Article,12/1/2019,Michael,Bronstein,2019
10.1038/s41598-019-49816-4,"© 2019, The Author(s).The analysis of the choroid in the eye is crucial for our understanding of a range of ocular diseases and physiological processes. Optical coherence tomography (OCT) imaging provides the ability to capture highly detailed cross-sectional images of the choroid yet only a very limited number of commercial OCT instruments provide methods for automatic segmentation of choroidal tissue. Manual annotation of the choroidal boundaries is often performed but this is impractical due to the lengthy time taken to analyse large volumes of images. Therefore, there is a pressing need for reliable and accurate methods to automatically segment choroidal tissue boundaries in OCT images. In this work, a variety of patch-based and fully-convolutional deep learning methods are proposed to accurately determine the location of the choroidal boundaries of interest. The effect of network architecture, patch-size and contrast enhancement methods was tested to better understand the optimal architecture and approach to maximize performance. The results are compared with manual boundary segmentation used as a ground-truth, as well as with a standard image analysis technique. Results of total retinal layer segmentation are also presented for comparison purposes. The findings presented here demonstrate the benefit of deep learning methods for segmentation of the chorio-retinal boundary analysis in OCT images.",Automatic choroidal segmentation in OCT images using supervised deep learning methods,Scientific Reports,Article,12/1/2019,Michael,Collins,2019
10.1038/s41598-021-04331-3,"© 2022, The Author(s).Temporal orientation is an important aspect of human cognition which shows how an individual emphasizes past, present, and future. Theoretical research in psychology shows that one’s emotional state can influence his/her temporal orientation. We hypothesize that measuring human temporal orientation can benefit from concurrent learning of emotion. To test this hypothesis, we propose a deep learning-based multi-task framework where we concurrently learn a unified model for temporal orientation (our primary task) and emotion analysis (secondary task) using tweets. Our multi-task framework takes users’ tweets as input and produces three temporal orientation labels (past, present or future) and four emotion labels (joy, sadness, anger, or fear) with intensity values as outputs. The classified tweets are then grouped for each user to obtain the user-level temporal orientation and emotion. Finally, we investigate the associations between the users’ temporal orientation and their emotional state. Our analysis reveals that joy and anger are correlated to future orientation while sadness and fear are correlated to the past orientation.",Investigating the impact of emotion on temporal orientation in a deep multitask setting,Scientific Reports,Article,12/1/2022,Pushpak,Bhattacharyya,2022
10.1038/s41598-021-87453-y,"© 2021, The Author(s).Consumer groups are pressuring modern farmers to be more efficient with a focus on better animal welfare. Herding risks farmer lives, involves stress from farm dogs, and if not performed often and intelligently, risks neglect. We examined the behavioural and physiological response of twelve Dorper sheep (Ovies aries) to a drone to adapt mathematical models of shepherding to the new dimension. The model aims to make it feasible for artificial intelligence to improve the autonomy of farmers and pilots in shepherding from the sky. Sheep acclimatised quickly and positively to the drone initiating drive of a flock, regardless of drone speed. Our results demonstrate that stimulating sheep auditory awareness during herding from the sky leads to varying sheep responses. When controlled, these auditory cues can maintain safer distances between the drone and the sheep, offering great potential for the agriculture industry. We outline our ongoing research plans to achieve more autonomous sky shepherding that is compassionate to animal welfare and trusted by farmers and the consuming public.",Drone approach parameters leading to lower stress sheep flocking and movement: sky shepherding,Scientific Reports,Article,12/1/2021,Hussein,Abbass,2021
10.1038/s41598-021-93227-3,"© 2021, The Author(s).This paper proposes a fully automatic method to segment the inner boundary of the bony orbit in two different image modalities: magnetic resonance imaging (MRI) and computed tomography (CT). The method, based on a deep learning architecture, uses two fully convolutional neural networks in series followed by a graph-search method to generate a boundary for the orbit. When compared to human performance for segmentation of both CT and MRI data, the proposed method achieves high Dice coefficients on both orbit and background, with scores of 0.813 and 0.975 in CT images and 0.930 and 0.995 in MRI images, showing a high degree of agreement with a manual segmentation by a human expert. Given the volumetric characteristics of these imaging modalities and the complexity and time-consuming nature of the segmentation of the orbital region in the human skull, it is often impractical to manually segment these images. Thus, the proposed method provides a valid clinical and research tool that performs similarly to the human observer.",A deep learning method for automatic segmentation of the bony orbit in MRI and CT images,Scientific Reports,Article,12/1/2021,Michael,Collins,2021
10.1038/s41746-018-0029-1,"© 2018, The Author(s).Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient’s record. We propose a representation of patients’ entire raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two US academic medical centers with 216,221 adult patients hospitalized for at least 24 h. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting: in-hospital mortality (area under the receiver operator curve [AUROC] across sites 0.93–0.94), 30-day unplanned readmission (AUROC 0.75–0.76), prolonged length of stay (AUROC 0.85–0.86), and all of a patient’s final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed traditional, clinically-used predictive models in all cases. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios. In a case study of a particular prediction, we demonstrate that neural networks can be used to identify relevant information from the patient’s chart.",Scalable and accurate deep learning with electronic health records,npj Digital Medicine,Article,12/1/2018,Jeff,Dean,2018
10.1038/s41746-020-00376-2,"© 2021, The Author(s).A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.",Deep learning-enabled medical computer vision,npj Digital Medicine,Review,12/1/2021,Jeff,Dean,2021
10.1038/s42256-021-00396-x,"© 2021, The Author(s), under exclusive licence to Springer Nature Limited.The growing use of machine learning in policy and social impact settings has raised concerns over fairness implications, especially for racial minorities. These concerns have generated considerable interest among machine learning and artificial intelligence researchers, who have developed new methods and established theoretical bounds for improving fairness, focusing on the source data, regularization and model training, or post-hoc adjustments to model scores. However, few studies have examined the practical trade-offs between fairness and accuracy in real-world settings to understand how these bounds and methods translate into policy choices and impact on society. Our empirical study fills this gap by investigating the impact of mitigating disparities on accuracy, focusing on the common context of using machine learning to inform benefit allocation in resource-constrained programmes across education, mental health, criminal justice and housing safety. Here we describe applied work in which we find fairness–accuracy trade-offs to be negligible in practice. In each setting studied, explicitly focusing on achieving equity and using our proposed post-hoc disparity mitigation methods, fairness was substantially improved without sacrificing accuracy. This observation was robust across policy contexts studied, scale of resources available for intervention, time and the relative size of the protected groups. These empirical results challenge a commonly held assumption that reducing disparities requires either accepting an appreciable drop in accuracy or the development of novel, complex methods, making reducing disparities in these applications more practical.",Empirical observation of negligible fairness–accuracy trade-offs in machine learning for public policy,Nature Machine Intelligence,Article,10/1/2021,Rayid,Ghani,2021
10.1038/s42256-022-00452-0,"© 2022, Springer Nature Limited.Biological organisms learn from interactions with their environment throughout their lifetime. For artificial systems to successfully act and adapt in the real world, it is desirable to similarly be able to learn on a continual basis. This challenge is known as lifelong learning, and remains to a large extent unsolved. In this Perspective article, we identify a set of key capabilities that artificial systems will need to achieve lifelong learning. We describe a number of biological mechanisms, both neuronal and non-neuronal, that help explain how organisms solve these challenges, and present examples of biologically inspired models and biologically plausible mechanisms that have been applied to artificial systems in the quest towards development of lifelong learning machines. We discuss opportunities to further our understanding and advance the state of the art in lifelong learning, aiming to bridge the gap between natural and artificial intelligence.",Biological underpinnings for lifelong learning machines,Nature Machine Intelligence,Review,3/1/2022,Josh,Bongard,2022
10.1038/sj.ph.1900834,"The aim of the study was to improve health workers' skills and confidence in dealing with patients with HIV disease and increase attention to patients' human rights. A longitudinal controlled trial was carried out in which one Nigerian state served as the intervention site and the adjacent state served as the control site for an intervention and dissemination of training in clinical management, health education, and attitudinal change toward patients with HIV disease. The intervention group n = 1072, control group n = 480. Following initial questionnaire-defining focus groups, nurses, laboratory technologists and physicians in all base hospitals in the intervention state were trained by influential role models who attended the initial training. Data were collected in all sites pre-training and 1 y later. Hierarchical multiple regression analysis controlling for baseline data, and orthogonal factor analysis to define scales were used. Data showed significant positive changes after 1 y in the intervention group on perception of population risk assessment, attitudes and beliefs about people with HIV disease, less fear and more sympathy for and responsibility toward HIV patients, and an increase in self-perceived clinical skills. There was increased willingness to treat and teach colleagues about people with HIV. Clinician fear and discrimination were significantly reduced, and the climate of fear that was associated with HIV was replaced with a professional concern. There was increased understanding of appropriate psychosocial, clinical and human rights issues associated with HIV treatment and prevention. This intervention, targeting health workers in an entire state and using HIV/AIDS information, role modeling, diffusion of training and discussions of discrimination and human rights, significantly affected the perception of risk groups and behaviors, perceived skills in treatment and counseling, reduced fears and increased concern for people with HIV disease, and improved the climate of treatment and prevention of HIV disease compared with a control state.",The impact of an intervention to change health workers' HIV/AIDS attitudes and knowledge in Nigeria: A controlled trial,Public Health,Article,1/1/2002,W,Ross,2002
10.1049/ep.1976.0108,"This article examines the contribution made to artificial intelligence by the exploitation of new developments in electronics, namely large-scale integrated circuits. The dividing line between pattern recognition and artificial intelligence is very slim. Consequently, the pattern-recognition ability of the random-access memory can be utilised in implementing networks of high 'intelligence'.",ARTIFICIAL INTELLIGENCE.,Electronics and Power,Article,1/1/1976,Igor,Aleksander,1976
10.1049/ree.1977.0058,"The much-discussed issues of privacy, unemployment, leisure, centralization of political power, and military misuse of technology are raised by work in artificial intelligence no less than by applications exploiting the ‘brute force’ of computers. But this paper focuses specifically on matters associated with the social use of intelligent machines in particular. Some current and predicted developments in machine intelligence are described and possible ill and good effects these may have on society outlined. Precautionary measures that might be taken in the writing and presentation of programs to forestall the social dangers implicit in this area of research are examined. © 1977, The Institution of Electronic and Radio Engineers. All rights reserved.",Social implications of intelligent machines,Radio and Electronic Engineer,Article,9/8/1977,Margaret,Boden,1977
10.1057/palgrave.jit.2000001,"Is artificial intelligence (AI) just something that is done in laboratories disconnected from the development of the pragmatic computing, which constitutes current information technology or does it contribute to progress in computing and information technology? It has even been suggested that advances in Al are merely a re-branding exercise for promises that are rarely kept, This paper is a personal view of the forces that have driven the development of Al in the past and what might be a serious paradigm shift in the future. The latter points to what appears to be the most abstruse corner of the subject: the modelling of the human brain and the possibility of designing systems with the brain's ability to create conscious thought. There have been accusations that Al is always ahead on promise and behind on delivery. This is an inaccurate view. In broad terms, the argument presented here suggests that as Al developed, progress was achieved by overcoming unforeseen difficulties in the pursuit of very ambitious targets, not just a re-branding of promises. This process not only advanced Al but also fed into the mainstream of computing that underpins the information technology of the present time. While the outcome of the paradigm shift towards conscious machines, which is examined at the end of this paper is still unclear, it is possible to speculate how information technology might be affected in the future.",Advances in intelligent information technology: Re-branding or progress towards conscious machines?,Journal of Information Technology,Review,3/1/2004,Igor,Aleksander,2004
10.1057/s41265-016-0032-4,"© 2016 JIT Palgrave Macmillan.As robots are generally thought to perform human-like tasks, they depend on the successes of information technology in the area of artificial intelligence to succeed in such pursuits. But robots, through their anthropomorphic character and their weighty presence in science fiction, attract the attention of the press and the media in a way that, at times, blurs the distinction between the actual state of the art and exaggerated claims. This makes it hard to assess the true functional positioning of robots, how this is likely to move forward and whether the outcome of progress could be detrimental to human society. The aim of this paper is to review the actual level of competence that is being achieved in robotics research laboratories and a plausible impact that this is likely to have on human control over life and jobs. The key thesis here is that cognition in machines and even an artificial form of consciousness lead to operations in a set of tasks (the 'algorithmic' category) which is different from that available to truly cognitive and conscious human beings (the 'life-need' category): that is, in the paper it is argued that a major category error (Ryle in The concept of mind, University of Chicago Press, Chicago, 1949) looms in predictions of serious threats to humanity. As far as a threat to jobs goes, it is argued that early attention to education and re-skilling of humans in the workplace can lead to an effective symbiosis between people and robots.",Partners of humans: A realistic assessment of the role of robots in the foreseeable future,Journal of Information Technology,Review,3/1/2017,Igor,Aleksander,2017
10.1063/1.4813535,"AlGaN/GaN high electron mobility transistors were electrically stressed using off-state high reverse gate biases. In devices demonstrating the largest, most rapid decrease in normalized maximum drain current, defects were found at the gate/AlGaN epilayer interface and characterized using high-angle annular dark-field scanning transmission electron microscopy. These defects appear to be a reaction between the Ni layer of the Ni/Au gate metal stack and the AlGaN epilayer. Additionally, simulations of the electric field lines from the defective devices match the defect morphology. These results provide important insight toward understanding failure mechanisms and improving reliability of Ni-gate AlGaN/GaN high electron mobility transistors. © 2013 AIP Publishing LLC.",Field-induced defect morphology in Ni-gate AlGaN/GaN high electron mobility transistors,Applied Physics Letters,Article,7/8/2013,I,J.,2013
10.1063/1.4818671,"AlGaN/GaN high electron mobility transistors (HEMTs) with polar and nonpolar ZnO nanowires modified gate exhibit significant changes in channel conductance upon exposure to different concentration of carbon monoxide (CO) at room temperature. The ZnO nanowires, grown by chemical vapor deposition, with perfect crystal quality will attach CO molecules and release electrons, which will lead to a change of surface charge in the gate region of the HEMTs, inducing a higher positive charge on the AlGaN surface, and increasing the piezo-induced charge density in the HEMTs channel. These electrons create an image positive charge on the gate region for the required neutrality, thus increasing the drain current of the HEMTs. The HEMTs source-drain current was highly dependent on the CO concentration. The limit of detection achieved was 400 ppm and 3200 ppm in the open cavity with continuous gas flow using a 50 × 50 µm2 gate sensing area for polar and nonpolar ZnO nanowire gated HEMTs sensor, respectively. © 2013 AIP Publishing LLC.",Characteristics of carbon monoxide sensors made by polar and nonpolar zinc oxide nanowires gated AlGaN/GaN high electron mobility transistor,Applied Physics Letters,Article,8/19/2013,I,J.,2013
10.1063/1.4866010,"The hydrogen detection characteristics of semipolar (112¯2) plane GaN Schottky diodes were investigated and compared to c-plane Ga- and N-polar and nonpolar a-plane (112¯0) GaN diodes. The semipolar GaN diodes showed large current response to 4% hydrogen in nitrogen gas with an accompanying Schottky barrier reduction of 0.53 eV at 25 °C, and the devices exhibited full recovery to the initial current level upon switching to a nitrogen ambient. The current-voltage characteristics of the semipolar devices remained rectifying after hydrogen exposure, in sharp contrast to the case of c-plane N-polar GaN. These results show that the surface atom configuration and polarity play a strong role in hydrogen sensing with GaN. © 2014 AIP Publishing LLC.",Hydrogen sensing characteristics of semipolar (11 2 ¯ 2) GaN Schottky diodes,Applied Physics Letters,Article,2/17/2014,I,J.,2014
10.1063/1.4866858,"The effect of proton irradiation on the off-state drain breakdown voltage of AlGaN/GaN high electron mobility transistors (HEMTs) grown on Si substrates was studied by irradiating protons from the backside of the samples through via holes fabricated directly under the active area of the HEMTs. There was no degradation of drain current nor enhancement of off-state drain voltage breakdown voltage observed for HEMTs irradiated with 275 keV protons, for which the defects created by the proton irradiation were intentionally placed in the GaN buffer. HEMTs with defects positioned in the 2 dimensional electron gas channel region and AlGaN barrier using 330 keV protons not only showed degradation of both drain current and extrinsic transconductance but also exhibited an improvement of the off-state drain breakdown voltage. Finite-element simulations showed the enhancement of the latter were due to a reduction in electric field strength at the gate edges by introduction of charged defects. © 2014 AIP Publishing LLC.",Effect of proton irradiation on AlGaN/GaN high electron mobility transistor off-state drain breakdown voltage,Applied Physics Letters,Article,2/24/2014,I,J.,2014
10.1063/1.4882715,"Deep hole traps were studied in bulk free-standing GaN crystals and in thinner (10-20µm) GaN films prepared by hydride vapor phase epitaxy (HVPE) on sapphire. Six hole traps in different combinations were detected in these crystals, H1 (activation energy 0.92-0.94eV), H2 (0.55eV), H3 (0.65-0.7eV), H4 (0.85-0.9eV), H5 (1.1-1.2eV), and H6 (0.95-1.05eV). The dominant traps in all samples were the H5 and H6 traps that were attributed, respectively, to gallium vacancy complexes with oxygen (VGa-O) and substitutional carbon related centers. We associate the H5 hole traps with the red luminescence bands, the H4 hole traps with the green luminescence bands, and the H6 hole traps with the yellow luminescence bands often observed in HVPE GaN. These attributions are based on the low energy thresholds of the deep traps optical excitation spectra and the depth of the respective trap levels. © 2014 AIP Publishing LLC.",Deep hole traps in undoped n-GaN films grown by hydride vapor phase epitaxy,Journal of Applied Physics,Article,6/14/2014,I,J.,2014
10.1063/1.4916632,© 2015 AIP Publishing LLC.The movement of basal plane segments of dislocations in low-dislocation-density GaN films grown by epitaxial lateral overgrowth as a result of irradiation with the probing beam of a scanning electron microscope was detected by means of electron beam induced current. Only a small fraction of the basal plane dislocations was susceptible to such changes and the movement was limited to relatively short distances. The effect is explained by the radiation enhanced dislocation glide for dislocations pinned by two different types of pinning sites: a low-activation-energy site and a high-activation-energy site. Only dislocation segments pinned by the former sites can be moved by irradiation and only until they meet the latter pinning sites.,Movement of basal plane dislocations in GaN during electron beam irradiation,Applied Physics Letters,Article,3/30/2015,I,J.,2015
10.1063/1.4918530,"© 2015 AIP Publishing LLC.The recovery effects of thermal annealing on dc and rf performance of off-state step-stressed AlGaN/GaN high electron mobility transistors were investigated. After stress, reverse gate leakage current and sub-threshold swing increased and drain current on-off ratio decreased. However, these degradations were completely recovered after thermal annealing at 450 °C for 10 mins for devices stressed either once or twice. The trap densities, which were estimated by temperature-dependent drain-current sub-threshold swing measurements, increased after off-state step-stress and were reduced after subsequent thermal annealing. In addition, the small signal rf characteristics of stressed devices were completely recovered after thermal annealing.",Recovery in dc and rf performance of off-state step-stressed AlGaN/GaN high electron mobility transistors with thermal annealing,Applied Physics Letters,Article,4/13/2015,I,J.,2015
10.1063/1.4939649,"© 2016 AIP Publishing LLC.Electrical and luminescent properties and deep trap spectra of Si doped GaN films grown by maskless epitaxial lateral overgrowth (MELO) are reported. The dislocation density in the wing region of the structure was 106cm-2, while in the seed region it was 108cm-2. The major electron traps present had activation energy of 0.56 eV and concentrations in the high 1015cm-3 range. A comparison of diffusion length values and 0.56 eV trap concentration in MELO GaN and epitaxial lateral overgrowth (ELOG) GaN showed a good correlation, suggesting these traps could be effective in carrier recombination. The doped MELO films were more uniform in their electrical properties than either ELOG films or undoped MELO films. We also discuss the differences in deep trap spectra and luminescence spectra of low-dislocation-density MELO, ELOG, and bulk n-GaN samples grown by hydride vapor phase epitaxy. It is suggested that the observed differences could be caused by the differences in oxygen and carbon contamination levels.","Electrical, luminescent, and deep trap properties of Si doped n-GaN grown by pendeo epitaxy",Journal of Applied Physics,Article,1/7/2016,I,J.,2016
10.1073/pnas.1700035114,"The United States spends more than $250 million each year on the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed several years. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may become an increasingly practical supplement to the ACS. Here, we present a method that estimates socioeconomic characteristics of regions spanning 200 US cities by using 50 million images of street scenes gathered with Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22 million automobiles in total (8% of all automobiles in the United States), were used to accurately estimate income, race, education, and voting patterns at the zip code and precinct level. (The average US precinct contains ~1,000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time.",Using deep learning and google street view to estimate the demographic makeup of neighborhoods across the United States,Proceedings of the National Academy of Sciences of the United States of America,Article,12/12/2017,Timnit,Gebru,2017
10.1073/pnas.1910837117,"© 2020 National Academy of Sciences. All rights reserved.Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.",A scalable pipeline for designing reconfigurable organisms,Proceedings of the National Academy of Sciences of the United States of America,Article,1/28/2020,Josh,Bongard,2020
10.1073/pnas.2105070118,"© 2021 National Academy of Sciences. All rights reserved.Effective treatments for COVID-19 are urgently needed. However, discovering single-agent therapies with activity against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been challenging. Combination therapies play an important role in antiviral therapies, due to their improved efficacy and reduced toxicity. Recent approaches have applied deep learning to identify synergistic drug combinations for diseases with vast preexisting datasets, but these are not applicable to new diseases with limited combination data, such as COVID-19. Given that drug synergy often occurs through inhibition of discrete biological targets, here we propose a neural network architecture that jointly learns drug-target interaction and drug-drug synergy. The model consists of two parts: a drug-target interaction module and a target-disease association module. This design enables the model to utilize drug-target interaction data and single-agent antiviral activity data, in addition to available drug-drug combination datasets, which may be small in nature. By incorporating additional biological information, our model performs significantly better in synergy prediction accuracy than previous methods with limited drug combination training data. We empirically validated our model predictions and discovered two drug combinations, remdesivir and reserpine as well as remdesivir and IQ-1S, which display strong antiviral SARS-CoV-2 synergy in vitro. Our approach, which was applied here to address the urgent threat of COVID-19, can be readily extended to other diseases for which a dearth of chemical-chemical combination data exists.",Deep learning identifies synergistic drug combinations for treating COVID-19,Proceedings of the National Academy of Sciences of the United States of America,Article,9/28/2021,Regina,Barzilay,2021
10.1073/pnas.2112672118,"© 2021 National Academy of Sciences. All rights reserved.All living systems perpetuate themselves via growth in or on the body, followed by splitting, budding, or birth. We find that synthetic multicellular assemblies can also replicate kinematically by moving and compressing dissociated cells in their environment into functional self-copies. This form of perpetuation, previously unseen in any organism, arises spontaneously over days rather than evolving over millennia. We also show how artificial intelligence methods can design assemblies that postpone loss of replicative ability and perform useful work as a side effect of replication. This suggests other unique and useful phenotypes can be rapidly reached from wild-type organisms without selection or genetic engineering, thereby broadening our understanding of the conditions under which replication arises, phenotypic plasticity, and how useful replicative machines may be realized.",Kinematic self-replication in reconfigurable organisms,Proceedings of the National Academy of Sciences of the United States of America,Article,12/7/2021,Josh,Bongard,2021
10.1073/pnas.2204781120,"Copyright © 2023 the Author(s). Published by PNAS.Machine learning (ML) techniques are increasingly prevalent in education, from their use in predicting student dropout to assisting in university admissions and facilitating the rise of massive open online courses (MOOCs). Given the rapid growth of these novel uses, there is a pressing need to investigate how ML techniques support longstanding education principles and goals. In this work, we shed light on this complex landscape drawing on qualitative insights from interviews with education experts. These interviews comprise in-depth evaluations of ML for education (ML4Ed) papers published in preeminent applied ML conferences over the past decade. Our central research goal is to critically examine how the stated or implied education and societal objectives of these papers are aligned with the ML problems they tackle. That is, to what extent does the technical problem formulation, objectives, approach, and interpretation of results align with the education problem at hand? We find that a cross-disciplinary gap exists and is particularly salient in two parts of the ML life cycle: the formulation of an ML problem from education goals and the translation of predictions to interventions. We use these insights to propose an extended ML life cycle, which may also apply to the use of ML in other domains. Our work joins a growing number of meta-analytical studies across education and ML research as well as critical analyses of the societal impact of ML. Specifically, it fills a gap between the prevailing technical understanding of machine learning and the perspective of education researchers working with students and in policy.",Reimagining the machine learning life cycle to improve educational outcomes of students,Proceedings of the National Academy of Sciences of the United States of America,Article,2/28/2023,Rediet,Abebe,2023
10.1075/is.18.2.02etz,"As Artificial Intelligence technology seems poised for a major take-off and changing societal dynamics are creating a high demand for caregivers for elders, children, and those infirmed, robotic caregivers may well be used much more often. This article examines the ethical concerns raised by the use of AI caregivers and concludes that many of these concerns are avoided when AI caregivers operate as partners rather than substitutes. Furthermore, most of the remaining concerns are minor and are faced by human caregivers as well. Nonetheless, because AI caregivers' systems are learning systems, an AI caregiver could stray from its initial guidelines. Therefore, subjecting AI caregivers to an AI-based oversight system is proposed to ensure that their actions remain both legal and ethical.",The ethics of robotic caregivers,Interaction Studies,Article,1/1/2017,Oren,Etzioni,2017
10.1075/is.9.2.03bar,"This paper presents two studies that investigate how people praise and punish robots in a collaborative game scenario. In a first study, subjects played a game together with humans, computers, and anthropomorphic and zoomorphic robots. The different partners and the game itself were presented on a computer screen. Results showed that praise and punishment were used the same way for computer and human partners. Yet robots, which are essentially computers with a different embodiment, were treated differently. Very machine-like robots were treated just like the computer and the human; robots very high on anthropomorphism / zoomorphism were praised more and punished less. However, barely any of the participants believed that they actually played together with a robot. After this first study, we refined the method and also tested if the presence of a real robot, in comparison to a screen representation, would influence the measurements. The robot, in the form of an AIBO, would either be present in the room or only be represented on the participants' computer screen (presence). Furthermore, the robot would either make 20% errors or 40% errors (error rate) in the collaborative game. We automatically measured the praising and punishing behavior of the participants towards the robot and also asked the participant to estimate their own behavior. Results show that even the presence of the robot in the room did not convince all participants that they played together with the robot. To gain full insight into this human-robot relationship it might be necessary to directly interact with the robot. The participants unconsciously praised AIBO more than the human partner, but punished it just as much. Robots that adapt to the users' behavior should therefore pay extra attention to the users' praises, compared to their punishments. © John Benjamins Publishing Company.",The carrot and the stick: The role of praise and punishment in human-robot interaction,Interaction Studies,Article,8/26/2008,Julie,Carpenter,2008
10.1080/0022250X.1984.9989954,"Artificial intelligence (AI) is the study of how to write programs enabling computers to do things that would require intelligence if done by people, and it could engage with social forecasting in two ways. First, it is part of the overall social-technological context within which forecasters work. Commercial AI-programs will affect markets and life-styles; and advice-giving “expert” systems will raise novel legal, social, and psychological problems. Second, AI-programs might be used for making the social forecasts. Unlike the (essentially quantitative) computer models used for this purpose today, they could reason (and explain themselves) in verbal form. Writing an expert system requires clarification of the theories, assumptions, and “rule-of-thumb” inferences concerned. It would be easier to identify the inherent moral-political bias than it is in models comprising sets of differential equations. © 1984, Taylor & Francis Group, LLC. All rights reserved.",Artificial intelligence and social forecasting,The Journal of Mathematical Sociology,Article,1/1/1984,Margaret,Boden,1984
10.1080/00224490701808142,"Using survey results from the 1998 Twin Cities Lesbian, Gay, Bisexual, and Transgender (LGBT) Pride Festival (N = 535), we explored associations between body image and unsafe anal intercourse (UAI) among men who have sex with men (MSM), and evaluated whether body satisfaction mediated this association. MSM who reported underweight body image had lower odds than those who reported average weight of UAI (AOR = 0.33; 95% CI = 0.13, 0.85); body satisfaction was not found to mediate this association. 13.3% of men who reported overweight/obese body image had engaged in UAI compared with 21.6% of those who reported average weight and 8.2% of those who reported underweight (p<.05). Compared with MSM in exclusive relationships, MSM in non exclusive relationships had increased odds of UAI (AOR = 5.78; 95% CI = 2.96, 11.29) as did men who were not partnered (AOR = 3.20; 3.20; 95% CI =1.72, 5.93). These findings highlight the importance of including body image in sexual behavior models of MSM to better understand body image's role in influencing sexual risk and sexually transmitted infections (STI)/human immunodeficiency virus (HIV) transmission. Copyright © Taylor & Francis Group.","Body image, body satisfaction, and unsafe anal intercourse among men who have sex with men",Journal of Sex Research,Article,12/1/2008,W,Ross,2008
10.1080/00224499.2021.1939846,"© 2021 The Society for the Scientific Study of Sexuality.Prostate cancer treatments disrupt receptive anal intercourse (RAI) for gay and bisexual men (GBM). Sexual dysfunction following prostate cancer treatment may include severe pain in the anorectum during RAI (i.e., anodyspareunia). The purpose of this study was to explore the impact of prostate cancer and its treatments on RAI among GBM. Data were from a cross-sectional online survey of 100 GBM prostate cancer survivors who reported pleasurable RAI prior to treatment. Approximately 47% of the sample reported recent RAI, which was more common among GBM in long-term relationships. RAI was also associated with engagement in other sexual behaviors (e.g., oral and insertive anal sex). Anodyspareunia was reported by 23% of the men who had attempted recent RAI. Anodyspareunia was negatively associated with mental health, performing oral sex on a partner, and bowel function. The overwhelming majority received no information from their healthcare providers about loss of RAI function prior to prostate cancer treatment. Culturally responsive cancer survivorship care may need to address the loss of RAI function for GBM prostate cancer survivors.",Pain and Loss of Pleasure in Receptive Anal Sex for Gay and Bisexual Men following Prostate Cancer Treatment: Results from the Restore-1 Study,Journal of Sex Research,Article,1/1/2022,W,Ross,2022
10.1080/00224499009551556,"In order to assess the reliability and validity of two data collection instruments for measuring sexual practices in homosexual men, we administered a recall data collection instrument to 30 sexually active men, and a diary instrument to a subset of 19 participants. Each instrument covered a period of one month. For the recall instrument, the correlation coefficients between the test-retest showed a good level of reliability for a number of infrequent sexual practices but was poor for frequent sexual practices. Correlation coefficients were calculated for sexual practices recorded in the first two and the second two weeks for both the recall and the diary. Generally, the level of agreement between the first and second two weeks for sexual practice data recorded in the diary was lower than the recall questionnaire. While the reliability of the recall method was found to be limited to certain sexual practices, it was concluded to be the most reliable and efficient method presently available for the collection of important high-risk AIDS-related sexual activities for a one-month period. © 1990, Taylor & Francis Group, LLC. All rights reserved.",Sexual Behaviour in AIDS-Related Research: Reliability and Validity of Recall and Diary Measures,The Journal of Sex Research,Article,5/1/1990,W,Ross,1990
10.1080/019697297126029,"This paper is primarily concerned with answering two questions: What are necessary elements of embodied architectures? How are we to proceed in a science of embodied systems? Autonomous agents, more specifically cognitive agents, are offered as the appropriate objects of study for embodied AI. The necessary elements of the architectures of these agents are then those of embodied AI as well. A concrete proposal is presented as to how to proceed with such a study. This proposal includes a synergistic parallel employment of an engineering approach and a scientific approach. It also supports the exploration of design space and of niche space. A general architecture for a cognitive agent is outlined and discussed. © 1997 Taylor & Francis Group, LLC.",Autonomous agents as embodied ai,Cybernetics and Systems,Article,1/1/1997,Stan,Franklin,1997
10.1080/08870448808400355,"Increases in condom use among homosexually active men are crucial to containing the spread of AIDS. The present study examined the components of attitudes and beliefs toward condom use in homosexual and bisexual men using a modified version of Brown's Attitude toward condoms scale. Factor analysis revealed five clear dimensions: viewing condoms as unreliable and unerotic; as protection from infection; as unavailable when needed; as interrupting sex; and viewing condoms as a responsibility and being comfortable with condom use. Five subscales constructed from these dimensions differentiated significantly between homosexual men who used condoms frequently and infrequently or never. Four of the subscales (excepting the Protection from Infection subscale) differentiated frequency of oral condom use; only the Responsibility and Comfort with Condom Use subscale differentiated frequency of anal condom use. The Homosexual Attitudes toward Condom Use scale demonstrates that (1) dimensions of beliefs and attitudes toward condom use in homosexually active men differ substantially from those in heterosexual individuals; (2) a reliable and valid scale for measuring such attitudes now exists; (3) factors influencing condom use in this population differ for oral and anal intercourse; (4) this scale enables further research on determinants of condom use, and effects of modifying attitudes toward condom use, in homosexually active men to be carried out. © 1988, Taylor & Francis Group, LLC. All rights reserved.",Attitudes Toward Condoms as AIDS Prophylaxis in Homosexual Men: Dimensions and Measurement,Psychology &amp; Health,Article,10/1/1988,W,Ross,1988
10.1080/08870449208402019,"The stability of the Fear of AIDS Scale (FAIDSS) was studied using three samples (two samples of health workers and one of social work students) on Cattel's s index. Using hyperplane cutoffs between 0.35 and 0.45, on the five factor solution of the FAIDSS, there were significant correlations between factors across samples and with few exceptions each factor correlated significantly with only one factor in its comparison samples. Concurrent validity was demonstrated by comparing scores on the Attitudes toward AIDS scale with the FAIDSS. Data indicated that the FAIDSS structure was stable across samples and is an appropriate instrument for measurement of fear of AIDS in the helping professions. © 1992, Taylor & Francis Group, LLC. All rights reserved.",Replication of The Factor Structure of The Fear of Aids Schedule (Faidss) Across Samples,Psychology &amp; Health,Article,1/1/1992,W,Ross,1992
10.1080/08870449408407482,"The 24 item AIDS Impact Scale was designed to measure dimensions relevant to the work of staff in HIV/AIDS units. Both positive and negative aspects of this work are measured. Five factors were isolated and corresponded to different dimensions of AIDS impact. These were gay affiliation. stigma/ discrimination, identification/responsibility, grief/powerlessness and recognition/reward. All scales had acceptable reliability (a =.72-.89). The gay affiliation factor was negatively related to homophobia, lack of personal accomplishment (burnout sub-scale) and depersonalisation (burnout sub-scale). The stigma/discrimination factor was negatively related to scores on social withdrawal and internal coping. The identification with and responsibility for people living with AIDS factor was positively related to scores on external coping and negatively related to social withdrawal and choice of work area. Grief and powerlessness was positively associated with social withdrawal and external coping strategies and was negatively related to internal coping strategies. The reward/recognition factor was negatively related to homophobia and social withdrawal and positively related to relationship stability and satisfaction. These significant associations demonstrate a degree of validity for the sub-factors of the AIDS Impact scale. This scale demonstrates support for the notion that the impact of HIV/AIDS is both measurable and scalable. © 1994, Taylor & Francis Group, LLC. All rights reserved.",The impact of working with hiv/aids on health care professionals: Development of the aids impact scale,Psychology &amp; Health,Article,4/1/1994,W,Ross,1994
10.1080/09528139108915298,"A careful adjudication of the connectionist-logicist clash in AI and cognitive science seems to disclose that it is a mirage. © 1992 Taylor & Francis Group, LLC.",Is the connectionist-logicist clash one of AI’s wonderful red herrings?,Journal of Experimental and Theoretical Artificial Intelligence,Article,1/1/1991,Selmer,Bringsjord,1991
10.1080/0952813X.2010.502312,"One of the central problems of artificial intelligence is capturing the breadth and flexibility of human common sense reasoning. One way to evaluate common sense is to use versions of human tests that rely on everyday reasoning. The Bennett Mechanical Comprehension Test consists of everyday reasoning problems posed via pictures and is used to evaluate technicians. This test is challenging because it requires conceptual knowledge spanning a broad range of domains, experience with a wide variety of everyday situations, and spatial reasoning. This article describes how we have extended our Companion Cognitive Architecture, which treats analogical processing as central, to perform well over a subset of the Bennett test. We introduce analogical model formulation as a robust method for reasoning about everyday scenarios, by analogy with cases that represent prior experiences. This enables a companion to perform qualitative reasoning (QR) without a complete domain theory, as typically required for QR. We introduce sketch annotations to communicate linkages between visual and conceptual properties in sketches. We introduce analogical reference frames to enable comparative analysis to operate over a broader range of problems than prior techniques. We show that these techniques enable a companion to score reasonably well on a difficult subset of the Bennett test. © 2011 Taylor & Francis.",Using analogical model formulation with sketches to solve Bennett Mechanical Comprehension Test problems,Journal of Experimental and Theoretical Artificial Intelligence,Article,9/1/2011,Ken,Forbus,2011
10.1080/0952813X.2010.502313,"Psychometric AI is a type of AI distinguished by the pursuit of intelligent systems able to excel on psychometrically validated human-level tests of cognitive abilities. We seek to build a system that solves a specific sub-test within Psychometric AI: the story arrangment test. Items in this test confront the test-taker with a set of jumbed snapshots (whether diagrammatic or otherwise) which must be ordered to tell a coherent story. We propose a dual-process system that combines bottom-up non- or sub-symbolic processing (e.g. neural network-based modelling) with top-down symbolic processing (e.g. deductive reasoning over declarative information represented as formulae in a logical system) for solving these tests of cognitive ability. The top-down process provides the benefits of a traceable proof, but requires a large amount of pre-existing knowledge. The bottom-up technique sacrifices provability and certainty on some problems for speed, but always yields some level of an answer to a given problem. This demonstrates a natural marriage between the two: the bottom-up approach seems especially powerful when used as a form of pre-processing in conjunction with a logic-based approach, because the latter approach would only need to consider a small number of possible orderings of snapshots. © 2011 Taylor & Francis.",A bottom-up complement to the logic-based top-down approach to the story arrangement test,Journal of Experimental and Theoretical Artificial Intelligence,Article,9/1/2011,Selmer,Bringsjord,2011
10.1080/0952813X.2010.502314,"The concept of psychometric artificial intelligence (PAI) is presented. Psychometric AI is the field devoted to building information-processing entities capable of at least solid performance on all established, validated tests of intelligence and mental ability. One of the experts, Newell, cites his work based on production systems, but makes it clear that the production-system approach is not the only way. Newell, focus on 'Complete Processing Models', 'Analyze a Complex Task', and 'One Program for Many Tasks'. Other experts, Cassimatis and Bignoli, explains a test-based approach to AI aimed at reaching human-level intelligence, and specifically recommend a version of this approach based on the concept of a microcosm, a suitably configured simulated environment. Another expert, Chapin, report on a hybrid approach to story arrangement, a subtest on the Wechsler Adutt Intelligent Scale (WAIS) test.",Psychometric artificial intelligence,Journal of Experimental and Theoretical Artificial Intelligence,Review,9/1/2011,Selmer,Bringsjord,2011
10.1080/0952813X.2014.895107,"A high-level artificial general intelligence (AGI) architecture called goal-oriented learning meta-architecture (GOLEM) is presented, along with an informal but careful argument that GOLEM may be capable of preserving its initial goals while radically improving its general intelligence. As a meta-architecture, GOLEM can be wrapped around a variety of different base-level AGI systems, and also has a role for a powerful narrow-AI subcomponent as a probability estimator. The motivation underlying these ideas is the desire to create AGI systems fulfilling the multiple criteria of being: massively and self-improvingly intelligent, probably beneficial and almost surely not destructive. © 2014 Taylor & Francis.",GOLEM: Towards an AGI meta-architecture enabling both goal preservation and radical self-improvement,Journal of Experimental and Theoretical Artificial Intelligence,Conference Paper,7/3/2014,Ben,Goertzel,2014
10.1080/09540091.2017.1345855,"© 2017 Informa UK Limited, trading as Taylor & Francis Group.Deep dreaming (DD) can combine and transform images in surprising ways. But, being based in deep learning (DL), it is not analytically understood. Collage is an art form that is constrained along various dimensions. DD will not be able to generate collages until DL can be guided in a disciplined fashion.",Is deep dreaming the new collage?,Connection Science,Article,10/2/2017,Margaret,Boden,2017
10.1080/09540120120076968,"The objective of this paper is to understand the intended sexual and condom behaviour patterns among teenage higher secondary school students in India. To achieve this, variables including perceived norms, perceived peer group norms, risk behaviour patterns, perceived chances of getting AIDS and relevant sociodemographic variables were regressed on intended sexual behaviour. Regression of actual sexual behaviour was carried out with perceived norms, perceived peer group norms and intended sexual behaviour as the independent variables. In this paper a conceptual model has been framed based on the theory of reasoned action, health belief model and self-efficacy theory. Cumulative scores are computed for perceived norms, perceived peer group norms, risk behaviour patterns, opinion on handling condoms and perceived chances of getting AIDS. Along with these variables, possible confounding variables such as age, gender, type of family, mother's education and father's education were considered for their effect on intended sexual and condom behaviour. The results revealed that perceived norms and perceived peer group norms showed significant association with intended sexual behaviour and actual sexual behaviour and that children of more highly educated parents are less likely to engage in sexual activities in their adolescent years.","Study of perceived norms, beliefs and intended sexual behaviour among higher secondary school students in India",AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,12/1/2001,W,Ross,2001
10.1080/09540120500100593,"At the end of 2001, AIDS-related deaths had left an estimated 900,000 living orphans in Kenya (UNAIDS/WHO Epidemiology fact sheet, Kenya report, 2004). Many of those orphans are also HIV+. In Eastern Kenya, the Lea Toto Kangemi Outreach Program provides support to families caring for HIV+ children, many of whom are orphaned or soon to be orphaned. A major challenge for these families is the stigma attached to the family. In 2003, the Kangemi Program conducted a household survey of client families. We examined markers of expressed stigma and the association between expressed stigma and other demographic and belief/knowledge domains. The focus of the present study was the specific belief/knowledge domain surrounding care/support of HIV+ persons. Our goal was to explore this domain in the Kangemi families and to examine its relationship to expressed stigma. We created an AIDS-related stigma scale from selected items in the household survey and cross-tabulated stigma scores with care/support knowledge items. We found significant associations between less expressed stigma and greater care/support knowledge. Our results have implications for interventions that reduce expressed stigma and/or improve quality of care. © 2005 Taylor & Francis.",The relationship between expressed HIV/AIDS-related stigma and beliefs and knowledge about care and support of people living with AIDS in families caring for HIV-infected children in Kenya,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,10/1/2005,W,Ross,2005
10.1080/09540120701367124,"The Joint United Nations Programme on HIV/AIDS (UNAIDS) estimates that in 2004, there were 39.4 million people living with HIV/AIDS worldwide (UNAIDS/WHO Report on the global HIV/AIDS epidemic, 2004). Children less than 15 years of age comprise 2.2 million of these individuals. As more children globally gain access to highly active antiretroviral therapy (HAART), more children are growing to the age when disclosure of their HIV status is inevitable. This information may affect a child's disease trajectory, and in the context of HAART, may have wide-ranging impact in the management of paediatric HIV infection. This study is an investigation of the effect of disclosure of a child's own HIV infection status on death and CD4 decline in a cohort of 325 HIV-infected Romanian children receiving highly active antiretroviral therapy (HAART). A retrospective database analysis was conducted. Data from a nearly three-year period were examined. Children who were aware of their HIV diagnosis were compared with those who were not aware. We found significant associations between not knowing the HIV diagnosis and death, and not knowing the HIV diagnosis and disease progression defined as either death or CD4 decline. Our results imply that in the context of HAART, knowledge of one's own HIV infection status is associated with delayed HIV disease progression. © 2007 Taylor & Francis.",The influence of disclosure of HIV diagnosis on time to disease progression in a cohort of Romanian children and teens,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,10/1/2007,W,Ross,2007
10.1080/09540120902883093,"Implementation of HIV care and treatment programs in sub-Saharan Africa is a complex undertaking that requires training of health care providers (HCPs). Many sub-Saharan African countries have introduced training programs to build human resources for health. Evaluation of the ongoing trainings is warranted so that programs can be improved. The purpose of this study was to evaluate Baylor International Pediatric AIDS Initiative's (BIPAI) HCP training program in Swaziland. The specific aims were: (1) to assess coverage and delivery of the training program; and (2) to determine the impact of the training program on HCPs' knowledge about HIV and pediatric practices, attitudes toward HIV/AIDS patients, and self-efficacy to provide antiretroviral therapy (ART). The evaluation was a multimethod design with two types of data collection and analysis: (1) one-group pretest-posttest survey with 101 HCPs; and (2) semi-structured in-depth interviews with seven trainers from Baylor College of Medicine and 16 local HCPs in Swaziland. Quantitative data were analyzed using Stata Statistical Software version 8.2 for descriptive and multivariate analysis while factor analysis was done using Statistical Program for Social Sciences version 14. The transcribed interviews were analyzed using a didactic approach. Process evaluation showed that the training had good coverage, was delivered as intended, and improved as the work progressed. The training program led to a significant increase (p=0.0000) in HCPs' knowledge about HIV/AIDS, ART, and relevant clinical pediatrics practices between pretest (mean 68.7% SD 13.7) and post training (mean 84.0% SD 12.0). The training program also increased trainees' self-efficacy to provide ART and their attitudes toward AIDS patients (p=0.0000 and 0.02, respectively). In conclusion, BIPAI training program in Swaziland had good coverage of all health care facilities and HCPs in Swaziland. The training was effective in imparting knowledge and skills to HCPs and in their attitudes toward HIV/AIDS patients. © 2009 Taylor & Francis.",Effectiveness of a training program to increase the capacity of health care providers to provide HIV/AIDS care and treatment in Swaziland,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,11/1/2009,W,Ross,2009
10.1080/09540121.2011.565035,"This study aimed to understand how person, health and sexual behavior, HIV-risk cognitions, and gay community involvement relate to barebacking among 3634 men who have sexwith men (MSM) recruited by way of the Swedish website QX.se. In this sample, 10% reported that they had engaged in barebacking in the past year. Variables found to be statistically significant in the bivariate analyses were incorporated into a logistic regression domain-specific model. Variables that remained significantly associated with barebacking in domain 1 were being HIV-positive and having had a sexually transmitted infection (STI) in the past year. The variable talked with someone in HIV services remained significant in domain 2, and for domain 3, the variable used the Internet to look for a bareback partner remained significant. Two variables, believing taking semen in one's mouth involves no or low risk of HIV transmission and believing that engaging in insertive unprotected anal intercourse (IUAI) involves no or low risk of HIV transmission, remained significant in domain 4. The final multivariate regression analysis included sixvariables (from domains 1 to 4) and had a significant fit (x 2(6)=2.571, p=0.958). The likelihood of engaging in barebacking was higher for those men who reported being HIV-positive (odds ratio [OR]=2.77), having had an STI in the past year (OR=1.67), and having used the Internet to look for a bareback partner (OR=12.59). This first study to explore the predictors of bareback sexamong a Nordic MSM sample suggests that bareback sex among northern European MSM is less common than among other samples. The findings reconfirm that MSM who engage in bareback sexmay represent a unique subset of MSM with distinct HIV prevention needs. © 2011 Taylor & Francis.",Predictors of reporting bareback sex among a diverse sample of MSM recruited through a Swedish website,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,12/1/2011,W,Ross,2011
10.1080/09540121.2011.647678,"Studies on HIV/AIDS treatment adherence have been carried out in a limited number of geographic settings, but few studies have explored it in people of higher socioeconomic status in Latin America. This qualitative study explored and compared determinants of adherence behaviors among 52 HIV-positive Colombian women in medium and high socioeconomic positions (SPs). Findings indicated that the two SP groups reported high adherence behaviors related to taking medication, following a diet, and executing lifestyle changes in line with healthcare providers recommendations. Nevertheless, differences were observed between the two groups. While women with a medium SP disclosed their diagnosis, were empowered, and had acceptable access to economic resources that resulted in favorable adherence, their better off counterparts tended to hide their status and made a conscious effort to keep their adherence behaviors in secret due to HIV-related stigma. More studies on adherence of people living with HIV/AIDS from high SPs should be conducted to better understand how psychosocial support can be provided and to advance the knowledge of how and why adherence practices in these groups are undertaken. © 2012 Taylor & Francis.",HIV/AIDS treatment adherence in economically better off women in Colombia,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,7/1/2012,W,Ross,2012
10.1080/09540121.2012.689809,"Depression in people with HIV has wide-spread implications related to faster progression to AIDS, poor drug compliance, and lower quality of life (QOL). Although there have been studies that have examined the role of sociodemographic variables in people with HIV, there have only been a few on the assessment of QOL and its association with depression among people with HIV in South India. The objectives of this study were to diagnose major depressive disorder (MDD) and examine the association of depression with health-related quality of life (HRQOL) among people with HIV in coastal South India. Structured questionnaires detailing sociodemographic and HIV related variables were filled out by 103 patients with HIV attending a tertiary care center. Interviews were carried out by a psychiatrist to diagnose ICD-10 MDD and a clinical psychologist to rate the severity of depression using the Hamilton Depression Scale (HAMD). Subjective HRQOL was assessed using HIV/AIDS targeted quality of life questionnaire in these patients. Fifty patients were diagnosed with MDD. Among them, 23 (46%) were mildly depressed, 19 (38%) were moderately depressed, 7 (14%) were severely depressed, and 1 (2%) was very severely depressed. Mean QOL scores for all dimensions except sexual function were significantly and inversely correlated (p<0.05) with HAMD implying that patients with greater severity of depressive symptoms had poorer HRQOL. Individuals with ICD-10 diagnosis of MDD presented significantly lower scores of QOL compared to individuals without MDD. The implication is that early diagnosis and referral of depressed patients needs to be incorporated into intervention programs to improve patient outcomes and QOL. More research is needed to investigate the impact of antidepressant therapy on QOL using this study as a comparison group in a similar population. © 2013 Copyright Taylor and Francis Group, LLC.",Association of quality of life with major depressive disorder among people with HIV in South India,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,2/1/2013,W,Ross,2013
10.1080/09540121.2012.699671,"Although studies have been conducted in neighboring countries, there are no published data on men who have sex with men (MSM) in mainland Tanzania. We report on a respondent-driven sampling study of 271 MSM in Dar es Salaam, Tanzania. The sample covered a wide range of educational attainment and employment, median age was 24, and all respondents had heard of HIV/AIDS, mostly through public media. Those satisfied with media information on HIV were younger, had lower education, and had obtained their information from health facilities. Over two-thirds believed that having one faithful partner and using condoms would protect against HIV: nevertheless, more than two-thirds were worried about HIV infection. Two-thirds had had a relationship with a woman, one-third in the past year. Predictors of non-use of condoms for anal sex with last casual partner were younger age, not being worried about HIV infection, and agreeing to have sex even if a condom was refused. There was no significance in proportion using a condom with last casual (43%) and last regular (49%) partner. Most partners (MSM knew a median of 10 other MSM) were met in bars, music halls, and in the home/local environment, and 70% of MSM described their sexual position as ""bottom."" Sixty percent reported having an HIV test and the great majority was comfortable discussing condoms with partners and friends: half would refuse to have sex if condoms were not agreed to. These data suggest a significant ""gay"" community in Dar es Salaam with relatively accurate HIV information but moderate condom use, HIV testing and ability to refuse unsafe sex. There is clearly scope for targeted HIV prevention programs in the MSM community in Tanzania. © 2013 Copyright Taylor and Francis Group, LLC.","Condom use and HIV-related behaviors in urban Tanzanian men who have sex with men: A study of beliefs, HIV knowledge sources, partner interactions and risk behaviors",AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,2/1/2013,W,Ross,2013
10.1080/09540121.2013.764965,"The purpose of this study was to examine the effects of smoking (past and current) on multiple domains of cognitive functioning in a sample of people living with HIV/AIDS (PLWHA). We hypothesized that among PLWHA, current smokers would demonstrate poorer cognitive functioning when compared to non-smokers, specifically in the cognitive domains of auditory-verbal learning and memory, visuospatial memory, overall cognitive efficiency, executive skills, processing speed, and working memory. Results suggest that in patients being treated for HIV infection, current smoking is negatively associated with learning, memory, and global cognitive functioning. There was also some evidence that cognitive deficits in learning associated with smoking were more pronounced among men compared to women. However, the cause of these effects is not at all clear. In multivariate models, the differences associated with smoking were non-significant when adjusting for education and hepatitis C virus infection. Therefore, smoking may simply reflect a general tendency to more widespread deficits and comorbidities rather than directly impacting cognitive function. Future studies should attempt to examine a priori cognitive factors which contribute to smoking debut and other associated risk factors in order to understand why smoking may be a marker for other risk factors and may ultimately influence neurocognitive functioning critical to daily activities and adherence. © Taylor and Francis.",The effects of cigarette smoking on learning and memory performance among people living with HIV/AIDS,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,10/1/2013,Kate,Devlin,2013
10.1080/09540121.2014.951597,"© 2014 Taylor & Francis.In sub-Saharan Africa, the prevalence of stigma-related abuse and violence among men who have sex with men (MSM) and its potential impact on the HIV/AIDS epidemic is unknown. This study estimated the prevalence and source of violence and abuse among a sample of MSM in Tanzania and characterized the association between levels of violence and sexual and mental health variables. Data were taken from a larger study of 200 MSM in Tanzania. Frequency tabulations, bivariate analysis, and logistic regression were performed to describe the prevalence and source of abuse and to determine the association between levels of violence and sexual demographics and mental health variables. The MSM sample for this study was young (median age 23), somewhat educated with the majority having attained secondary school (80%) and mostly employed (60%). Verbal (48.5%) and moral (32.5%) abuses were the most predominant types of abuse among the sample and were mostly from people in the street and neighbors. Sexual abuse (30%) was mostly from partners, and physical violence (29.5%) was largely from people in the street. Participants in the high-violence level group had a significantly greater number of sexual partners, depression scores, and internalized homonegativity (IH) scores. IH predicted HIV infection and verbal abuse predicted IH.There is a need for an increased awareness of violence and abuse faced by MSM in Tanzania, as well as effective programs to specifically target the issue of violence among MSM, and its implication for mental health and for risky sexual behaviors and HIV transmission.",High prevalence of stigma-related abuse among a sample of men who have sex with men in Tanzania: Implications for HIV prevention,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,1/15/2015,W,Ross,2015
10.1080/09540129008257749,"Following a national campaign in Australia which had shown no change in level of knowledge about AIDS (using random samples of the population over 16 years, before and 5 months after the campaign), we assessed the change of attitudes towards, and beliefs about AIDS in the same samples. Results indicated that there were changes in beliefs about how much is known about the transmission of HIV, and that people were less concerned about casual transmission. Those respondents reportedly influenced most by the campaign were those with greater fear of diseases and death. We conclude that media campaigns may have a significant effect on attitudes and beliefs toward AIDS even where there is no effect on level of knowledge, and that the attitudinal changes which may be promoted by such campaigns should also be considered as objectives in campaign design. © 1990, Taylor & Francis Group, LLC. All rights reserved.",The effect of a national campaign on attitudes toward aids,AIDS Care,Article,12/1/1990,W,Ross,1990
10.1080/09540129108253060,"We examined the structure of the Fear of AIDS Schedule (FAIDSS) in a sample of 134 health care workers. Factor analysis indicated that there were five discrete dimension of fear of AIDS: fears of loss of control, of sex of HIV infection through blood and illness, of death and medical interventions, and of contact with outsiders. These dimensions had low to moderate intercorrelations. The dimensions of fear of HIV infection through blood or illness, was significantly correlated with desired personal social distance from people with AIDS, and this dimension along with fear of death and medical interventions were correlated with desired public social distance. Fear of infection through blood and illness were predictors of both desired personal social distance and public social distance from people with HIV infection. The data suggest greater discrimination of AIDS fears with greater closeness of interaction with people with HIV disease, and that these dimensions of the FAIDSS are both reliable and valid measures of AIDS fears. © 1991, Taylor & Francis Group, LLC. All rights reserved.","Dimensions, content and validation of the fear of AIDS schedule in health professionals",AIDS Care,Article,4/1/1991,W,Ross,1991
10.1080/09540129308258581,"Strong social support networks have been associated with positive outcomes for health and well-being throughout the life-cycle. This paper investigates the structural and functional nature of social support networks of 100 injecting drug users (IDUs) in Sydney and the implications for HIV/AIDS services. Using a modified ISEL respondents saw support in terms of its tangibility of people and support, in terms of an appraisal of having friends, and as a self-esteem measure. We found the majority of respondents ‘hung around’ with other IDUs, lived with other IDUs, and were satisfied with the support they received from their friends. Friends appeared to be a more important source of social support than biological families and if respondents were to become HIV infected they would be more open about their status with friends than family. Where family was involved in support it was likely to be provided by mothers and siblings who were also the family members who knew about the respondent's drug use. There was no relationship between numbers of supports and satisfaction of support, suggesting quality and quantity of support were independent. Non-social supports were conceptualized primarily in terms of medical services. © 1993, Taylor & Francis Group, LLC. All rights reserved.",Significant relationships and social supports of injecting drug users and their implications for HIV/AIDS services,AIDS Care,Article,1/1/1993,W,Ross,1993
10.1080/09540129408258027,"Thirty-two men who participated in a trial of Zidovudine (> 500 CD4 cells/106 L) were surveyed regarding their reasons for participation in clinical trials. The major source of influence to enter the trial was the clinic doctor, and importance for self in participation was rated as the least important reason. Medical science and medical researchers were seen as the major beneficiaries. Being seen to do something about one’s illness was also seen as being important and probability of delaying AIDS was rated lowest. Chance of occurrence of risks of treatment were rated as slightly below 50%. Results of Flesch and Fry tests of informed consent documents suggested that they were written in the style of a scientific article, required the readability level of a university graduate and that recall was imperfect. Those who saw more benefits in their trial participation appeared to be most knowledgeable. These data suggest that participation in this clinical trial was based on altruistic, rather than personal reasons and that participants had realistic perceptions of outcomes. © 1994 Taylor … Francis Group, LLC. All rights reserved.",Reasons for entry into and understanding of HIV/AIDS clinical trials: A preliminary study,AIDS Care,Article,1/1/1994,W,Ross,1994
10.1080/09540129408258655,"Attitudes of people with HIV disease towards HIV have seldom been measured. However, a well-established scale to measure attitudes toward cancer in those with the disease, the 38-item Mental Adjustment to Cancer (MAC) scale was modified to assess adjustment to HIV disease. We administered the scale to 107 Australian men with HIV infection, of whom 36 had an AIDS-defining condition, who were patients at an ambulatory care facility and in a research study. The data were factor analyzed using a method identical to that used in the development of the MAC scale to determine the latent dimensions of attitudes toward HIV/AIDS. The Mental Adjustment to HIV scale (MAH) factor analysis revealed five factors: Helplessness-Hopelessness, Fighting Spirit, and Denial-Avoidance as in the original MAC scale, plus a Fatalism subscale which also measured Preoccupation, and a new subscale, which measured Belief in Influencing the Course of the Disease. Together, these five factors accounted for half of the variance. These data suggest that while there are similarities between mental attitude to cancer and mental attitude to HIV in the latent dimensions of the questionnaire items, there are also some differences. Most significant is the belief in people with HIV disease in being able to personally influence the course of the illness, and the combination of Preoccupation with Fatalism. The five subscales of the MAH scale had Cronbach’s alpha reliabilities between 0.80 and 0.55. The MAH appears to be a useful way to measure total attitudes and subscale scores of people with HIV infection, including AIDS, to their disease. © 1994, Taylor … Francis Group, LLC. All rights reserved.",The Mental Adjustment to HIV Scale: Measurement and dimensions of response to AIDS/HIV disease,AIDS Care,Article,1/1/1994,W,Ross,1994
10.1080/09540129550126876,"There is some evidence, although conflicting, to suggest that socialization or acculturation into the gay community is associated with lower levels of unsafe sex. We attempted to determine the relationship between acculturation and sexual safety. We examined data from 282 men (from the Dallas AIDS Community Demonstration Project who had sex with men and were not in a monogamous relationship to determine the associations between safer sex and indices of acculturation. The indices of acculturation included regular reading of local and national gay newspapers and magazines, and belonging to an organization for gay men. The data indicated that there were significant relationships between acculturation, talking to sexual partners about HIV risk reduction and sexual identity, and the dependent variable of frequency of condom use for anal sex. A regression equation indicated that 21% of the variance of anal condom use was predicted by these variables. These data suggest that acculturation into the gay community is associated with safer sexual behaviour, and we discuss the implications of these data for using role models and normative beliefs in HIV prevention programmes. © 1995, Taylor & Francis Group, LLC. All rights reserved.",Relationship between safe sex and acculturation into the gay subculture,AIDS Care,Article,2/1/1995,W,Ross,1995
10.1080/09540129650125821,"Burnout in volunteer workers in the HIV/AIDS area results in the loss of dedicated personnel, consequently straining the HIV/AIDS care delivery system. By assessing the predictors of burnout and grief, this study describes the role of grief in HIV/AIDS volunteer burnout. Voluntary and anonymous questionnaires were sent to members of the Foundation for Interfaith Research and Ministry (FIRM), a multi-religious organization formed to provide volunteer work in HIV/AIDS care facilities around Houston, Texas. In 174 valid responses, grief was measured against work characteristics, burnout, rewards, stressors, and the general health questionnaire (GHQ). No significant relationship was found between grief and burnout; however, burnout in volunteers may be different from that in health care professionals for the following reasons: (1) volunteers choose to work in the HIV/AIDS area; (2) they have control over the time they spend volunteering; (3) volunteers are internally motivated to work in the HIV/AIDS area; and (4) if the volunteer does not enjoy the work they can terminate their involvement with minimal cost. The best univariate predictors of grief are time spent as a volunteer and volunteer hours per week, where those who spend the most hours volunteering experience less grief. The Reward/Stress measures most significantly associated with grief include empathy/self-knowing reward, emotional support reward, and emotional overload stress. The strongest predictors of grief in the regression analysis, which account for 21% of the variance, were time as a volunteer, emotional support, emotional overload, GHQ-somatic symptoms, and GHQ-social dysfunction. The data suggest that in order to reduce grief, special attention should be paid to allowing volunteers freely to express problems with emotional overload and workload adjustments, and providing clear emotional support as a reward.",Prediction of grief and HIV/AIDS-related burnout in volunteers,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,1/1/1996,W,Ross,1996
10.1080/09540129650125830,"The positive psychological and sociological dimensions of AIDS care provision may produce important information to assist burnout prevention. While most studies on stress and burnout in AIDS health care have focused on the negative and difficult aspects of this work, few have considered the notion that the rewards of care-giving may buffer against stress or counterbalance experiences that may otherwise lead to burnout. A study of HIV/AIDS volunteers examined the relationship between stressors, rewards and burnout, using the HIV Volunteer Inventory and the Maslach Burnout Inventory. Correlation data indicates that a lack of a sense of personal accomplishment is a contributor to frequency of burnout. Over a fifth of the variance of burnout frequency can be accounted for by both stress and reward factors. Qualitative interview data also support the importance of rewards. Rewards in the form of gratitude from clients and recognition and support front management positively influenced the organizational climate. This study points to the need to find ways to increase the recognition and rewards experienced by carers. The potential benefits include reduced attrition and burnout and enhanced quality of life in the work setting.","The relationship between recognition, rewards and burnout in AIDS caring",AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,1/1/1996,W,Ross,1996
10.1080/09540129947631,"Burnout among HIV/AIDS volunteers contributes to the loss of dedicated personnel resulting in strain on the HIV/AIDS care system. Past research has suggested that there were significant stresses and burnout associated with AIDS caregiving. We investigated the predictors of dropout in AIDS volunteers over time, and specifically which of the variables of the stressors and rewards of being a volunteer (collected at baseline) predicted who would drop out two years later. The volunteers were the subjects of Nesbitt et al., who were members of an interfaith religious-based organization in Houston, Texas. The subjects were re-contacted by mail after two years, and 76 of the 174 respondents completed a brief questionnaire which gave details of current volunteering activity, reasons for dropout (if they had dropped out) and completed the Texas Revised Inventory of Grief (TRIG). Forty dropped-out from volunteering while 36 continued. Data show the independent variables of total stressor score, the Maslach Burnout Inventory score of Depersonalization intensity and the three subscale scores involving stress: client problems and role ambiguity, emotional overload and organizational factors as being significant in predicting dropout in HIV/AIDS volunteers over time. The best predictors of the dropping-out of HIV/AIDS volunteers can be divided into the stresses (client problems and role ambiguity, emotional overload and organizational factors) and depersonalization intensity. The results showed that volunteers who experienced more client problems and role ambiguity, more emotional overload and more problems with organizational factors are more likely to drop out from the volunteer programme. They also show that the dropout volunteers have a significantly higher level of depersonalization intensity than the continuing volunteers, with the risk of dropout increasing by almost a third in the highest tertile of depersonalization intensity scorers compared with those with lower scores. These data indicate that it is the stressors of AIDS volunteering, including the intensity, of depersonalization, which lead to dropout, and that rewards do not appear to have a protective effect.",Predictors of dropout and burnout in AIDS volunteers: A longitudinal study,AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV,Article,12/1/1999,W,Ross,1999
10.1080/13548500500477667,"As the incidence of HIV increases, one of the major steps in preventing a widespread epidemic is to make certain that medical students are prepared to recognize and treat HIV infections and their related conditions, and to counsel patients about avoiding risks that might lead to infections. This cross-sectional study assessed the knowledge level of 357 medical students and their attitudes about AIDS and HIV enrolled in a Medical College in Karachi, Pakistan. Only 6% of the students had complete knowledge on symptoms of HIV/AIDS and 7% of the students had complete knowledge on the modes of transmission of HIV. Statistical analysis of demographic factors affecting knowledge was done. Linear regression and Maentel - Haenszel tests showed that older and clinical students were more knowledgeable of symptoms and modes of transmission of HIV/AIDS. Ten attitudes were correlated with knowledge and none of these showed an association. These results on knowledge indicate that education about HIV/AIDS should be incorporated in the curriculum and interventions must be taken by public health professionals to avoid poor treatment outcomes. © 2007 Taylor & Francis.",Knowledge and attitudes of Pakistani medical students towards HIV-positive and/or AIDS patients,"Psychology, Health and Medicine",Article,1/1/2007,W,Ross,2007
10.1080/17460441.2021.1915982,"© 2021 Informa UK Limited, trading as Taylor & Francis Group.Introduction: Artificial Intelligence (AI) has become a component of our everyday lives, with applications ranging from recommendations on what to buy to the analysis of radiology images. Many of the techniques originally developed for other fields such as language translation and computer vision are now being applied in drug discovery. AI has enabled multiple aspects of drug discovery including the analysis of high content screening data, and the design and synthesis of new molecules. Areas covered: This perspective provides an overview of the application of AI in several areas relevant to drug discovery including property prediction, molecule generation, image analysis, and organic synthesis planning. Expert opinion: While a variety of machine learning methods are now being routinely used to predict biological activity and ADME properties, methods of representing molecules continue to evolve. Molecule generation methods are relatively new and unproven but hold the potential to access new, unexplored areas of chemical space. The application of AI in drug discovery will continue to benefit from dedicated research, as well as AI developments in other fields. With this pairing algorithmic advancements and high-quality data, the impact of AI in drug discovery will continue to grow in the coming years.",Critical assessment of AI in drug discovery,Expert Opinion on Drug Discovery,Article,1/1/2021,Regina,Barzilay,2021
10.1080/19317610903393142,"We conducted a qualitative study to understand and describe experiences of people living with HIV and AIDS with regard to HIV-associated stigma in Nepal. The study has revealed four key themes associated with HIV stigma: a hierarchy of stigma (sexual transmission and women stigmatized more than injecting-drug transmission and men); exclusion and rejection (denial of care services, rejection from family); death as a form of punishment (untimely death is seen as a punishment for something done wrong in the past); and Mumbaiya disease (caught from working in ""other places""). Cultural contexts are the best ways to understand HIV stigma in Nepal along with socially and culturally established gender roles. This study has confirmed that stigma manifests at different levels: individual, social, and structural, with denial and rejection being a key mechanism of stigma. © Taylor & Francis Group, LLC.",Issues related to HIV stigma in Nepal,International Journal of Sexual Health,Article,1/1/2010,W,Ross,2010
10.1080/19317611.2013.853720,"Objectives: Stigma connected with HIV/AIDS has decreased considerably since the early epidemic yet affects those living with HIV in many ways. Little research, particularly qualitative research, concerning HIV stigma from the perspective of gay men has emerged. The present qualitative study aimed to fill this evidence gap by examining how HIV stigma is perceived and experienced by gay men who have become HIV-infected and how they respond to this stigma. Methods: Thematic analysis of 19 gay men's narratives identified six main themes. Results: Encountering HIV stigmatization was common and was linked to the physical stigmata identifying respondents as HIV-positive. Overwhelmingly, they found stigmatization to be most intensely felt within gay communities. One profound theme was internalized HIV stigma, referring to respondents' internalized negative feelings about their HIV status. A related theme was the closeted nature of HIV. Lastly, regarding how the men dealt with the HIV diagnosis and experiences of HIV stigma, a theme of adaptation became clear. Conclusions: Although exploratory, the results can serve as a beginning framework for understanding and assisting seropositive gay men who experience HIV stigma. The findings are important because it is realistic to expect that in a climate in which HIV has become increasingly invisible and closeted and in which infections are on the rise, gay and bisexual men will be increasingly affected and infected by HIV. © 2014 The Author(s). Published with license by Taylor & Francis.",The Second Closet: A Qualitative Study of HIV Stigma Among Seropositive Gay Men in a Southern U.S. City,International Journal of Sexual Health,Article,1/1/2014,W,Ross,2014
10.1088/0268-1242/12/10/020,"We have compared the use of GaN, InN and AIN powders for providing a nitrogen partial pressure within a graphite susceptor during high-temperature rapid thermal annealing of GaN, AIN, InN and InAIN. At temperatures above ~750 °C vapour transport of In from InN powder produces In droplet condensation on the surface of all nitride samples being annealed. GaN powder provides better surface protection than AIN powders for temperatures up to ~1050 °C when annealing GaN and AIN samples. Dissociation of nitrides from the surface is found to occur with approximate activation energies of 3.8 eV, 4.4 eV and 3.4 eV, respectively, for GaN, AIN and InN.","Comparison of GaN, InN and AIN powders for susceptor-based rapid annealing of group III nitride materials",Semiconductor Science and Technology,Review,10/1/1997,I,J.,1997
10.1088/0963-0252/6/4/007,"We report results of a study on the inductively coupled plasma (ICP) etching of GaAs, AIGaAs, GaSb, and GaP in pure Ar, CH4/H2/Ar and CH4/H2/N2 plasma chemistries. Etch rates of the semiconductors initially increase with ICP source power, reaching maxima around 1500-2000 Å min-1 with ~500-700 W ICP power, and then decrease with further increase of ICP power because of the decrease in cathode dc self-bias. Etch rates increase at fixed ICP power with rf chuck power in the range of 100 W to 450 W, while they showed little dependence on the chamber pressure (2 mTorr-20 mTorr) in CH4/H2/Ar discharges. We found that the dc self-bias on the rf chuck decreased exponentially as ICP power increased, while it increased with rf chuck power and pressure in these plasma chemistries. A simple calculation of ion flux and etch yield based on dc self-bias of the plasmas on the sample chuck was used to measure typical values of etch yield. In both ICP CH4/H2/Ar and CH4/H2/N2 discharges these were close to the pure Ar sputter yield.",Etching of Ga-based III-V semiconductors in inductively coupled Ar and CH<inf>4</inf>/H<inf>2</inf>-based plasma chemistries,Plasma Sources Science and Technology,Article,12/1/1997,I,J.,1997
10.1089/rej.2007.0627,"Novel artificial intelligence methodologies were applied to analyze gene expression microarray data gathered from mice under a calorie restriction (CR) regimen. The data were gathered from three previously published mouse studies; these datasets were merged together into a single composite dataset for the purpose of conducting a broader-based analysis. The result was a list of genes that are important for the impact of CR on lifespan, not necessarily in terms of their individual actions but in terms of their interactions with other genes. Furthermore, a map of gene interrelationships was provided, suggesting which intergene interactions are most important for the effect of CR on life extension. In particular our analysis showed that the genes Mrpl12, Uqcrh, and Snip1 play central roles regarding the effects of CR on life extension, interacting with many other genes (which the analysis enumerates) in carrying out their roles. This is the first time that the genes Snip1 and Mrpl12 have been identified in the context of aging. In a follow-up analysis aimed at validating these results, the analytic process was rerun with a fourth dataset included, yielding largely comparable results. Broadly, the biological interpretation of these analytical results suggests that the effects of CR on life extension are due to multiple factors, including factors identified in prior theories of aging, such as the hormesis, development, cellular, and free radical theories. © 2008 Mary Ann Liebert, Inc.",Identifying the genes and genetic interrelationships underlying the impact of calorie restriction on maximum lifespan: An artificial intelligence-based approach,Rejuvenation Research,Article,8/1/2008,Ben,Goertzel,2008
10.1093/acprof:oso/9780195370676.001.0001,"© 2009 by Joscha Bach. All rights reserved.Although computational models of cognition have become very popular, these models are relatively limited in their coverage of cognition - they usually only emphasize problem solving and reasoning, or treat perception and motivation as isolated modules. The first architecture to cover cognition more broadly is the Psi theory, developed by Dietrich Dörner. By integrating motivation and emotion with perception and reasoning, and including grounded neuro-symbolic representations, the Psi contributes significantly to an integrated understanding of the mind. It provides a conceptual framework that highlights the relationships between perception and memory, language and mental representation, reasoning and motivation, emotion and cognition, autonomy and social behavior. So far, the Psi theory's origin in psychology, its methodology, and its lack of documentation have limited its impact. This book adapts the Psi theory to cognitive science and artificial intelligence, by elucidating both its theoretical and technical frameworks, and clarifying its contribution to how we have come to understand cognition.",Principles of Synthetic Intelligence PSI An Architecture of Motivated Cognition,Principles of Synthetic Intelligence PSI An Architecture of Motivated Cognition,Book,5/1/2009,Joscha,Bach,2009
10.1093/bib/bbab159,"© 2021 The Author(s). Published by Oxford University Press.Graph machine learning (GML) is receiving growing interest within the pharmaceutical and biotechnology industries for its ability to model biomolecular structures, the functional relationships between them, and integrate multi-omic datasets- A mongst other data types. Herein, we present a multidisciplinary academic-industrial review of the topic within the context of drug discovery and development. After introducing key terms and modelling approaches, we move chronologically through the drug development pipeline to identify and summarize work incorporating: Target identification, design of small molecules and biologics, and drug repurposing. Whilst the field is still emerging, key milestones including repurposed drugs entering in vivo studies, suggest GML will become a modelling framework of choice within biomedical machine learning.",Utilizing graph machine learning within drug discovery and development,Briefings in Bioinformatics,Article,11/1/2021,Michael,Bronstein,2021
10.1093/BIOINFORMATICS/BTAA488,"© 2020 Oxford University Press. All rights reserved.Motivation: The recent development of sequencing technologies revolutionized our understanding of the inner workings of the cell as well as the way disease is treated. A single RNA sequencing (RNA-Seq) experiment, however, measures tens of thousands of parameters simultaneously. While the results are information rich, data analysis provides a challenge. Dimensionality reduction methods help with this task by extracting patterns from the data by compressing it into compact vector representations. Results: We present the factorized embeddings (FE) model, a self-supervised deep learning algorithm that learns simultaneously, by tensor factorization, gene and sample representation spaces. We ran the model on RNA-Seq data from two large-scale cohorts and observed that the sample representation captures information on single gene and global gene expression patterns. Moreover, we found that the gene representation space was organized such that tissue-specific genes, highly correlated genes as well as genes participating in the same GO terms were grouped. Finally, we compared the vector representation of samples learned by the FE model to other similar models on 49 regression tasks. We report that the representations trained with FE rank first or second in all of the tasks, surpassing, sometimes by a considerable margin, other representations.",Factorized embeddings learns rich and biologically meaningful embedding spaces using factorized tensor decomposition,Bioinformatics,Article,1/1/2020,Yoshua,Bengio,2020
10.1093/comjnl/33.4.356,"Techniques for solving constraint satisfiability problems have received much attention in artificial intelligence, operation research and symbolic logic. Applications which may be viewed as CSPs are found in scene identification in computer vision, space and motion planning, database consistency, combinatorial optimization, and cryptarithm puzzle solving. Our research effort attempts to determine which algorithms perform best, and under what conditions, in solving a special CSP known as the student scheduling problem (SSP). Since constraint satisfiability problems are, in general, NP-complete, it is of interest to develop and compare the effectiveness and efficiency of heuristic algorithms as applied, in particular, to our application. In this paper, we assign priorities to the constraints and investigate optimization algorithms for finding schedules which rank high with respect to the priorities. Experimental results have been collected and are reported here. Our system was developed for and used at Bar-Ilan University during the registration period, being available for students to construct their timetables.",Optimization algorithms for student scheduling via constraint satisfiability,Computer Journal,Article,1/1/1990,Martin Charles,Golumbic,1990
10.1093/comjnl/42.4.318,"We propose a new inductive principle, which we call the complexity approximation principle (CAP). This principle is a natural generalization of Rissanen's minimum description length (MDL) principle and Wallace's minimum message length (MML) principle and is based on the notion of predictive complexity, a recent generalization of Kolmogorov complexity. Like the MDL principle, CAP can be regarded as an implementation of Occam's razor.",Complexity approximation principle,Computer Journal,Article,1/1/1999,Alexander,Gammerman,1999
10.1093/ehjci/jeaa135,"© 2020 The Author(s).Aims: Both left ventricular (LV) diastolic dysfunction (LVDD) and hypertrophy (LVH) as assessed by echocardiography are independent prognostic markers of future cardiovascular events in the community. However, selective screening strategies to identify individuals at risk who would benefit most from cardiac phenotyping are lacking. We, therefore, assessed the utility of several machine learning (ML) classifiers built on routinely measured clinical, biochemical, and electrocardiographic features for detecting subclinical LV abnormalities. Methods and results: We included 1407 participants (mean age, 51 years, 51% women) randomly recruited from the general population. We used echocardiographic parameters reflecting LV diastolic function and structure to define LV abnormalities (LVDD, n = 252; LVH, n = 272). Next, four supervised ML algorithms (XGBoost, AdaBoost, Random Forest (RF), Support Vector Machines, and Logistic regression) were used to build classifiers based on clinical data (67 features) to categorize LVDD and LVH. We applied a nested 10-fold cross-validation set-up. XGBoost and RF classifiers exhibited a high area under the receiver operating characteristic curve with values between 86.2% and 88.1% for predicting LVDD and between 77.7% and 78.5% for predicting LVH. Age, body mass index, different components of blood pressure, history of hypertension, antihypertensive treatment, and various electrocardiographic variables were the top selected features for predicting LVDD and LVH. Conclusion: XGBoost and RF classifiers combining routinely measured clinical, laboratory, and electrocardiographic data predicted LVDD and LVH with high accuracy. These ML classifiers might be useful to pre-select individuals in whom further echocardiographic examination, monitoring, and preventive measures are warranted.",Applying machine learning to detect early stages of cardiac remodelling and dysfunction,European Heart Journal Cardiovascular Imaging,Article,10/1/2021,Amparo Alonso,Betanzos,2021
10.1093/eurpub/ckx200,"© The Author 2017.Background: Using data from a large internet-based survey of European men having sex with men (MSM), we assessed factors associated with HIV testing and reasons for dissatisfaction with HIV testing and counselling among Hungarian MSM. Methods: A total of 2052 Hungarian MSM provided evaluable data for the European MSM Internet Survey (EMIS) in 2010. <U+03C7>2 tests and Poisson regression with a robust variance estimator were used to assess factors associated with HIV testing and dissatisfaction with HIV testing and counselling. Results: A total of 42.1% of MSM reported never being testing for HIV. Over one-half of men (54.1%) who reported condomless anal intercourse (CAI) in the prior 12 months with a person of unknown or sero-discordant HIV status reported no lifetime HIV testing. The factor most strongly associated with dissatisfaction with HIV testing and counselling was test site with increased dissatisfaction with inpatient hospital settings vs. community-based organizations. Both lack of HIV testing and dissatisfaction with testing were independently associated with MSM who reported that no one, or only a few people, knew they were attracted to men. Conclusions: Lack of HIV testing was strongly associated with CAI. MSM reported that community-based organizations better supported confidentiality and were more respectful during HIV testing.",Lack of HIV testing and dissatisfaction with HIV testing and counselling among men having sex with men in Hungary,European Journal of Public Health,Article,8/1/2018,W,Ross,2018
10.1093/eurpub/cky023,"© The Author(s) 2018.Background: Measuring homophobia at country level is important to guide public health policy as reductions in stigma are associated with improved health outcomes among gay men and other men who have sex with men. Methods: We developed a Homophobic Climate Index incorporating institutional and social components of homophobia. Institutional homophobia was based on the level of enforcement of laws that criminalise, protect or recognise same-sex relations. Social homophobia was based on the level of acceptance and justifiability of homosexuality. We estimated the Index for 158 countries and assessed its robustness and validity. Results: Western Europe is the most inclusive region, followed by Latin America. Africa and the Middle East are home to the most homophobic countries with two exceptions: South Africa and Cabo Verde. We found that a 1% decrease in the level of homophobia is associated with a 10% increase in the gross domestic product per capita. Countries whose citizens face gender inequality, human rights abuses, low health expenditures and low life satisfaction are the ones with a higher homophobic climate. Moreover, a 10% increase in the level of homophobia at country level is associated with a 1.7-year loss in life expectancy for males. A higher level of homophobia is associated with increased AIDS-related death among HIV-positive men. Conclusion: The socioecological approach of this index demonstrates the negative social, economic and health consequences of homophobia in low- and middle-income countries. It provides sound evidence for public health policy in favour of the inclusion of sexual minorities.",A socioecological measurement of homophobia for all countries and its public health impact,European Journal of Public Health,Article,10/1/2018,W,Ross,2018
10.1093/her/3.4.367,"The level of knowledge of AIDS was assessed by six questions in a geographically stratified random sample of 2601 adults in all states and territories in Australia. Results indicated that knowledge of AIDS was incomplete and that marital status, age, personal knowledge of somebody in a risk category for infection and occupational rank were all consistently associated with degree of knowledge about AIDS. Concern about AIDS as a social issue was related to better knowledge about AIDS, and personal concern was related to lower levels of knowledge. Sources of knowledge also discriminated degree of accurate information. These data suggest the need for more specific targeting of particular groups and an emphasis on personaly-sourced AIDS information where possible. © 1988 IRL Press Ltd.",Knowledge of AIDS in Australia: A national study,Health Education Research,Article,1/1/1988,W,Ross,1988
10.1093/her/4.3.273,"The studies reviewed in this paper suggest that each of the major health education models (taxonomy of educational objectives, health belief model, theory of reasoned action and PRECEDE model) makes a contribution to explaining the effectiveness of AIDS education. The major finding is that information on its own, without modification of attitudes or perception of AIDS as a personal concern that one can do something about, will have no effect on knowledge or behavior. Knowledge about AIDS is generally unrelated to behavior without the modification of attitudes and beliefs. The potential importance of perceived norms and social supports for behavior change are also apparent in these data and are acknowledged as important predisposing and reinforcing factors. The importance of such factors (including drug and alcohol use) as situational determinants of behavior, despite the level of motivation and knowledge, is apparent. The critical importance of providing motivation (through personalization of information) to activate information and behavior change is also apparent. These aspects of the health education models would appear to be of considerable importance in designing health education campaigns at this stage of the AIDS epidemic. © 1989 IRL Press.",Education and AIDS risks: A review,Health Education Research,Article,9/1/1989,W,Ross,1989
10.1093/jsxmed/qdad009,"© The Author(s) 2023. Published by Oxford University Press on behalf of The International Society of Sexual Medicine. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.BACKGROUND: Anodyspareunia may be an adverse outcome of prostate cancer (PCa) treatment for gay, bisexual, and other men who have sex with men (GBM). AIM: The aims of this study were to (1) describe the clinical symptoms of painful receptive anal intercourse (RAI) in GBM following PCa treatment, (2) estimate the prevalence of anodyspareunia, and (3) identify clinical and psychosocial correlates. METHODS: This was a secondary analysis of baseline and 24-month follow-up data from the Restore-2 randomized clinical trial of 401 GBM treated for PCa. The analytic sample included only those participants who attempted RAI during or since their PCa treatment (N = 195). OUTCOMES: Anodyspareunia was operationalized as moderate to severe pain during RAI for =6 months that resulted in mild to severe distress. Additional quality-of-life outcomes included the Expanded Prostate Cancer Index Composite (bowel function and bother subscales), the Brief Symptom Inventory-18, and the Functional Assessment of Cancer Therapy-Prostate. RESULTS: Overall 82 (42.1%) participants reported pain during RAI since completing PCa treatment. Of these, 45.1% experienced painful RAI sometimes or frequently, and 63.0% indicated that the pain was persistent. The pain at its worst was moderate to very severe for 79.0%. The experience of pain was at least mildly distressing for 63.5%. Painful RAI worsened for a third (33.4%) of participants after completing PCa treatment. Of the 82 GBM, 15.4% were classified as meeting criteria for anodyspareunia. Antecedents of anodyspareunia included a lifelong history of painful RAI and bowel dysfunction following PCa treatment. Those reporting symptoms of anodyspareunia were more likely to avoid RAI due to pain (adjusted odds ratio, 4.37), which was negatively associated with sexual satisfaction (mean difference, -2.77) and self-esteem (mean difference, -3.33). The model explained 37.2% of the variance in overall quality of life. CLINICAL IMPLICATIONS: Culturally responsive PCa care should include the assessment of anodyspareunia among GBM and explore treatment options. STRENGTHS AND LIMITATIONS: This is the largest study to date focused on anodyspareunia among GBM treated for PCa. Anodyspareunia was assessed with multiple items characterizing the intensity, duration, and distress related to painful RAI. The external validity of the findings is limited by the nonprobability sample. Furthermore, the cause-and-effect relationships between the reported associations cannot be established by the research design. CONCLUSIONS: Anodyspareunia should be considered a sexual dysfunction in GBM and investigated as an adverse outcome of PCa treatment.",Unrecognized sexual dysfunction in gay and bisexual men after prostate cancer treatment: the antecedents and impact of anodyspareunia,The journal of sexual medicine,Article,3/31/2023,W,Ross,2023
10.1093/oso/9780199674923.003.0011,"© Oxford University Press, 2018 and University of Tartu Press, 2012.Embodied cognition is the view that intelligence arises out of the interaction between an agent’s body and its environment. Taking such a view generates novel scientific hypotheses about biological intelligence and opportunities for advancing artificial intelligence. In this chapter we review one such set of hypotheses regarding how a robot may generate models of self, and others, and then exploit those models to recover from damage or exhibit the rudiments of social cognition. This modeling of self and others draws mainly on three concepts from neuroscience and AI: forward and inverse models in the brain, the neuronal replicator hypothesis, and the brain as a hierarchical prediction machine. The chapter concludes with future directions, including the integration of deep learning methods with embodied cognition.",Modeling self and others,Living Machines: A Handbook of Research in Biomimetic and Biohybrid Systems,Book Chapter,1/1/2018,Josh,Bongard,2018
10.1097/00126334-199305000-00013,"We compared three groups of injecting drug users in a large cross–sectional study on HIV/AIDS and risk behaviors in Sydney, Australia, to determine what differences existed between those who had never been in treatment (n = 458), had previously been in treatment (n = 387), and were currently in treatment (n = 367). Drug use for those currently in treatment was assessed during the last typical using month before treatment. Those previously and currently in treatment were similar in terms of drug use patterns and HIV risk–related injecting behaviors. Those never in treatment were younger, had a lower level of HIV risk–related injecting behaviors, and reported lower drug use and less involvement in the drug subculture. There was little difference between the three groups on HIV risk–related sexual behaviors. These data suggest that those never in treatment are less dysfunctional and possibly less involved in drug–using careers, whereas there appears to be a close relationship between being previously and currently in treatment. © 1993 Raven Press, Ltd., New York.","A comparison of drug use and HIV infection risk behavior between injecting drug users currently in treatment, previously in treatment, and never in treatment",Journal of Acquired Immune Deficiency Syndromes,Article,1/1/1993,W,Ross,1993
10.1097/OLQ.0b013e31829186e5,"Background: This study examined venue-based networks constituted by affiliation with gay bars and street intersections where male sex workers (MSWs) congregate to find their sexual/drug-sharing partners and network influence on risky sexual behavior (e.g., unprotected anal intercourse [UAI]) and HIV infection. Methods: Data collected in 2003 to 2004 in Houston, Texas, consists of 208 MSWs affiliated with 15 gay bars and 51 street intersections. Two-mode network analysis was conducted to examine structural characteristics in affiliation networks, as well as venue-based network influence on UAI and HIV infection. Results: Centralized affiliation patterns were found where only a few venues were popular among MSWs, and these were highly interdependent. Distinctive structural patterns of venue-based clustering were associated with UAI and infection. Individuals who shared venue affiliation with MSWs who engage in UAI were less likely to have UAI themselves. This suggests a downhill effect; that is, individuals compensate for their risk of infection by adjusting their own risk-taking behavior, based on their perceptions of their venue affiliates. Conclusions: Venue-based HIV/AIDs interventions could be tailored to specific venues so as to target specific clusters that are more likely to engage in risky sexual behavior. © 2013 Lippincott Williams & Wilkins.",Venue-based affiliation networks and HIV risk-taking behavior among male sex workers,Sexually Transmitted Diseases,Article,6/1/2013,W,Ross,2013
10.1098/rsif.2017.0937,"© 2018 The Author(s).Evolution sculpts both the body plans and nervous systems of agents together over time. By contrast, in artificial intelligence and robotics, a robot's body plan is usually designed by hand, and control policies are then optimized for that fixed design. The task of simultaneously co-optimizing the morphology and controller of an embodied robot has remained a challenge. In psychology, the theory of embodied cognition posits that behaviour arises from a close coupling between body plan and sensorimotor control, which suggests why co-optimizing these two subsystems is so difficult: most evolutionary changes to morphology tend to adversely impact sensorimotor control, leading to an overall decrease in behavioural performance. Here, we further examine this hypothesis and demonstrate a technique for 'morphological innovation protection', which temporarily reduces selection pressure on recently morphologically changed individuals, thus enabling evolution some time to 'readapt' to the new morphology with subsequent control policy mutations. We show the potential for this method to avoid local optima and converge to similar highly fit morphologies across widely varying initial conditions, while sustaining fitness improvements further into optimization. While this technique is admittedly only the first of many steps that must be taken to achieve scalable optimization of embodied machines, we hope that theoretical insight into the cause of evolutionary stagnation in current methods will help to enable the automation of robot design and behavioural training'while simultaneously providing a test bed to investigate the theory of embodied cognition.",Scalable co-optimization of morphology and control in embodied machines,Journal of the Royal Society Interface,Article,1/1/2018,Josh,Bongard,2018
10.1098/rspa.2021.0068,"© 2022 The Authors.A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.",Inductive biases for deep learning of higher-level cognition,"Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",Article,10/26/2022,Yoshua,Bengio,2022
10.1098/rsta.2018.0081,"© 2018 The Author(s) Published by the Royal Society. All rights reserved.The article discusses the governance of the digital as the new challenge posed by technological innovation. It then introduces a new distinction between soft ethics, which applies after legal compliance with legislation, such as the General Data Protection Regulation in the European Union, and hard ethics, which precedes and contributes to shape legislation. It concludes by developing an analysis of the role of digital ethics with respect to digital regulation and digital governance. This article is part of the theme issue 'Governing artificial intelligence: ethical, legal, and technical opportunities and challenges'.","Soft ethics, the governance of the digital and the General Data Protection Regulation","Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",Article,11/28/2018,Luciano,Floridi,2018
10.1098/rsta.2020.0364,"© 2021 The Author(s).Symbiosis is a physiological phenomenon where organisms of different species develop social interdependencies through partnerships. Artificial agents need mechanisms to build their capacity to develop symbiotic relationships. In this paper, we discuss two pillars for these mechanisms: machine education (ME) and bi-directional communication. ME is a new revolution in artificial intelligence (AI) which aims at structuring the learning journey of AI-enabled autonomous systems. In addition to the design of a systematic curriculum, ME embeds the body of knowledge necessary for the social integration of AI, such as ethics, moral values and trust, into the evolutionary design and learning of the AI. ME promises to equip AI with skills to be ready to develop logic-based symbiosis with humans and in a manner that leads to a trustworthy and effective steady-state through the mental interaction between humans and autonomy; a state we name symbiomemesis to differentiate it from ecological symbiosis. The second pillar, bi-directional communication as a discourse enables information to flow between the AI systems and humans. We combine machine education and communication theory as the two prerequisites for symbiosis of AI agents and present a formal computational model of symbiomemesis to enable symbiotic human-autonomy teaming. This article is part of the theme issue 'Towards symbiotic autonomous systems'.",A model of symbiomemesis: Machine education and communication as pillars for human-autonomy symbiosis,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",Article,10/4/2021,Hussein,Abbass,2021
10.1109/10.76387,"Assessment of the fetus in a high-risk pregnancy uses a variety of tests for screening and continued detection of in utero compromise. This paper describes Foetos, an expert system designed to help clinical personnel to interpret several fetal assessment tests: fetal biophysical profile, contraction stress test, and nonstress tests. Foetos has been built using the knowledge engineering tool Genie, which adopts a mixed frame-and rule-based approach to represent the clinical knowledge in the field. Foetos includes diagnostic, prognostic, and therapeutic structures, based on heuristic interpretation of such tests and contextual structures which relate interpretation to the overall clinical picture. The results of initial retrospective and prospective program validation are included in the report. These results show a substantial level of agreement between Foetos’ recommendations and clinical management. Interpretation of the results indicates that the obstetrical field could be an area of interest for the application of AI techniques. © 1991 IEEE",Foetos: An Expert System for Fetal Assessment,IEEE Transactions on Biomedical Engineering,Article,1/1/1991,Amparo Alonso,Betanzos,1991
10.1109/2.970591,A security architecture based on trust for pervasive computing systems is proposed. A distributed trust model which extends simple public key infrastructure (SPKI) and role-based access control (RBAC) is presented. The approach uses ontologies including properties and constraints expressed in an XML-based language.,Trust-based security in pervasive computing environments,Computer,Article,12/1/2001,Tim,Finin,2001
10.1109/3DV.2017.00015,"© 2017 IEEE.The last several years have seen significant progress in using depth cameras for tracking articulated objects such as human bodies, hands, and robotic manipulators. Most approaches focus on tracking skeletal parameters of a fixed shape model, which makes them insufficient for applications that require accurate estimates of deformable object surfaces. To overcome this limitation, we present a 3D model-based tracking system for articulated deformable objects. Our system is able to track human body pose and high resolution surface contours in real time using a commodity depth sensor and GPU hardware. We implement this as a joint optimization over a skeleton to account for changes in pose, and over the vertices of a high resolution mesh to track the subject's shape. Through experimental results we show that we are able to capture dynamic sub-centimeter surface detail such as folds and wrinkles in clothing. We also show that this shape estimation AIDS kinematic pose estimation by providing a more accurate target to match against the point cloud. The end result is highly accurate spatiotemporal and semantic information which is well suited for physical human robot interaction as well as virtual and augmented reality systems.",Dynamic High Resolution Deformable Articulated Tracking,"Proceedings - 2017 International Conference on 3D Vision, 3DV 2017",Conference Paper,5/25/2018,Dieter,Fox,2018
10.1109/5254.846286,"Osirix exploits the advantages of the world wide web and of ontologies for knowledge management, to enable enterprise-ontology-guided search in XML documents. © 2000 IEEE.",Building and searching an XML-based corporate memory,IEEE Intelligent Systems and Their Applications,Article,5/1/2000,Rose,Dieng-Kuntz,2000
10.1109/72.950146,"The recognition of accelerative and decelerative patterns in the fetal heart rate (FHR) is one of the tasks carried out manually by obstetricians when they analyze cardiotocograms for information respecting the fetal state. However, any delay in the detection of an anomaly and in subsequent clinical intervention obviously increases the risk of problems arising during birth or in the early months of life. For this reason it is essential for any computerized detection system to function in real time. The classical approaches to this problem have focused on the development of algorithmic models which, however, pose problems in terms of their adaptation of the baseline of the signal. In order to overcome this problem, an approach based on artificial neural networks (ANNs) formed by a multilayer perceptron (MLP) has been developed. However, since the system utilizes the FHR signal as direct input, an anterior stage has had to be incorporated that applies a principal component analysis (PCA) so as to make the system independent of the signal baseline. Furthermore, the introduction of multiresolution into the PCA has resolved other problems that were detected in the application of the system. Presented in this paper are the results of a validation of these systems - henceforth designated the PCA-MLP and multiresolution principal component analysis (MR-PCA) systems - against three clinical experts.",Adaptive pattern recognition in the analysis of cardiotocographic records,IEEE Transactions on Neural Networks,Article,9/1/2001,Amparo Alonso,Betanzos,2001
10.1109/ACCESS.2016.2571058,"© 2013 IEEE.Despite the advances made in artificial intelligence, software agents, and robotics, there is little we see today that we can truly call a fully autonomous system. We conjecture that the main inhibitor for advancing autonomy is lack of trust. Trusted autonomy is the scientific and engineering field to establish the foundations and ground work for developing trusted autonomous systems (robotics and software agents) that can be used in our daily life, and can be integrated with humans seamlessly, naturally, and efficiently. In this paper, we review this literature to reveal opportunities for researchers and practitioners to work on topics that can create a leap forward in advancing the field of trusted autonomy. We focus this paper on the trust component as the uniting technology between humans and machines. Our inquiry into this topic revolves around three subtopics: 1) reviewing and positioning the trust modeling literature for the purpose of trusted autonomy; 2) reviewing a critical subset of sensor technologies that allow a machine to sense human states; and 3) distilling some critical questions for advancing the field of trusted autonomy. The inquiry is augmented with conceptual models that we propose along the way by recompiling and reshaping the literature into forms that enable trusted autonomous systems to become a reality. This paper offers a vision for a Trusted Cyborg Swarm, an extension of our previous Cognitive Cyber Symbiosis concept, whereby humans and machines meld together in a harmonious, seamless, and coordinated manner.",A Review of Theoretical and Practical Challenges of Trusted Autonomy in Big Data,IEEE Access,Review,1/1/2016,Hussein,Abbass,2016
10.1109/ACCESS.2019.2926040,"© 2013 IEEE.Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Advanced Vehicle Technology (MIT-AVT) study are to 1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning-based internal and external perception systems; 2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology; and 3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium-term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for the analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, and CAN messages, and high-definition video streams of the driver's face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15610 days of participation, 511638 mi, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.",MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction with Automation,IEEE Access,Article,1/1/2019,Lex,Fridman,2019
10.1109/ACCESS.2020.2977355,"© 2013 IEEE.Deep learning methods provide a platform to segment boundaries within the retina and choroid in OCT images of the posterior eye, with the ultimate goal of having a robust model that works well across a wide range of different datasets. However, since most studies of deep learning methods use datasets exhibiting similar image quality for both training and evaluation, the effect of varied image quality on such methods is not normally explored in the context of OCT image segmentation. An understanding of the effects of image quality factors is vital to determine the robustness of the methods and their ability to be applied in clinical practice where images exhibiting a range of different qualities are encountered. This study examined a range of factors that can affect standard OCT image quality and determined how and why the performance of an existing neural network based segmentation method can subsequently degrade as a result. Three image quality factors (noise, contrast reduction, and gamma correction) all had a negative impact upon performance, while more robust performance was maintained in the presence of both JPEG and JPEG2000 image compression. Improving the method's robustness to each of these degradations is also demonstrated with marked performance improvements identified by applying a fine-tuning approach to the network. This study improves our understanding of the effect of OCT image degradation on neural network performance, the effect that fine-tuning with poor-image quality data has on the network and highlights the benefit and importance of training resilient models using data augmentation.",Effect of Altered OCT Image Quality on Deep Learning Boundary Segmentation,IEEE Access,Article,1/1/2020,Michael,Collins,2020
10.1109/ACCESS.2022.3153357,"© 2013 IEEE.Identification and delineation of craniofacial characteristics support the clinical and molecular diagnosis of genetic syndromes. Deep learning (DL) frameworks for syndrome identification from 2D facial images are trained on large clinical datasets using standard convolutional neural networks for classification. In contrast, despite the increased availability of 3D scanners in clinical setups, similar frameworks remain absent for 3D facial photographs. The main challenges involve working with smaller datasets and the need for DL operations applicable to 3D geometric data. Therefore, to date, most 3D methods refrain from working across multiple syndromic groups and/or are solely based on traditional machine learning. The first contribution of this work is the use of geometric deep learning with spiral convolutions in a triplet-loss architecture. This geometric encoding (GE) learns a lower dimensional metric space from 3D facial data that is used as input to linear discriminant analysis (LDA) performing multiclass classification. Benchmarking is done against principal component analysis (PCA), a common technique in 3D facial shape analysis, and related work based on 65 distinct 3D facial landmarks as input to LDA. The second contribution of this work involves a part-based implementation to 3D facial shape analysis and multi-class syndrome classification, and this is applied to both GE and PCA. Based on 1,786 3D facial photographs of controls and individuals from 13 different syndrome classes, a five-fold cross-validation was used to investigate both contributions. Results indicate that GE performs better than PCA as input to LDA, and this especially so for more compact (lower dimensional) spaces. In addition, a part-based approach increases performance significantly for both GE and PCA, with a more significant improvement for the latter. I.e., this contribution enhances the power of the dataset. Finally, and interestingly, according to ablation studies within the part-based approach, the upper lip is the most distinguishing facial segment for classifying genetic syndromes in our dataset, which follows clinical expectation. This work stimulates an enhanced use of advanced part-based geometric deep learning methods for 3D facial imaging in clinical genetics.",Multi-Scale Part-Based Syndrome Classification of 3D Facial Images,IEEE Access,Article,1/1/2022,Michael,Bronstein,2022
10.1109/ACCESS.2022.3180032,"© 2013 IEEE.Research in multi-agent teaming has increased substantially over recent years. Underneath these attempts sits a suite of communication functions to enable effective teaming. The Artificial Intelligence (AI) systems supporting the teaming arrangement have primarily relied on knowledge-based systems, with rules triggered based on the mode of interaction. Enabling humans to join the team of AI agents effectively calls for both the humans and AI agents to share their understanding and representation of their shared worlds. Such shared understanding requires formal representations of concepts to support transparency during bi-directional communications between team members. Little research has been done in this space, especially when humans need to team with a swarm of agents. We present an ontology designed specifically for human-agent teaming to address this research gap. The ontology is general, but we then contextualise it into a particular swarm-shepherding scenario to illustrate its use in a particular context. The proposed Ontology for Generalised Multi-Agent Teaming Onto4MAT offers the underlying building blocks for effective communication and shared understanding between humans and multi-agent teams.",Onto4MAT: A Swarm Shepherding Ontology for Generalized Multiagent Teaming,IEEE Access,Article,1/1/2022,Hussein,Abbass,2022
10.1109/ACCESS.2022.3197648,"© 2013 IEEE.Linking event triggers with their respective arguments is an essential component for building an event extraction system. It is challenging to link event triggers with their corresponding argument triggers when the sentence contains multiple event and argument triggers. The task becomes even more challenging in a low-resource setup due to the unavailability of natural language processing resources and tools. In this paper, we study the event-argument linking task based on disaster event ontology in a low resource setup. We use BERT and non-BERT-based deep learning models in both monolingual and cross-lingual event-argument linking tasks. We also perform an ablation study of various features like position embeddings (PE), position indicator (PI), and segment ID (SI) to understand their contribution to performance improvement in non-BERT-based models. Using three different languages viz. Hindi, Bengali, and Marathi, we compare the results with multilingual BERT-based deep neural models in both monolingual and cross-lingual scenarios. We observe that the multilingual BERT-based model outperforms the best performing non-BERT-based model in cross-lingual settings. But in monolingual settings, the performance is similar in Hindi and Bengali datasets and slightly better in Marathi dataset. We choose the disaster domain due to its social implications. Our current experiments can be helpful in mining important information related to disaster events from news articles and building event knowledge graphs in low-resource languages.",Event-Argument Linking in Disaster Domain,IEEE Access,Article,1/1/2022,Pushpak,Bhattacharyya,2022
10.1109/ACCESS.2022.3217522,"© 2013 IEEE.In today's world, news outlets have changed dramatically; newspapers are obsolete, and radio is no longer in the picture. People look for news online and on social media, such as Twitter and Facebook. Social media contributors share information and trending stories before verifying their truthfulness, thus, spreading rumors. Early identification of rumors from social media has attracted many researchers. However, a relatively smaller number of studies focused on other languages, such as Arabic. In this study, an Arabic rumor detection model is proposed. The model was built using transformer-based deep learning architecture. According to the literature, transformers are neural networks with outstanding performance in natural language processing tasks. Two transformers-based models, AraBERT and MARBERT, were employed, tested, and evaluated using three recently developed Arabic datasets. These models are extensions to the BERT, Bidirectional Encoder Representations from Transformers, a deep learning model that uses transformer architecture to learn the text representations and leverages the attention mechanism. We have also mitigated the challenges introduced by the imbalanced training datasets by employing two sampling techniques. The experimental results of our proposed approaches achieved a maximum accuracy of 0.97. This result demonstrated the effectiveness of the proposed method and outperformed other existing Arabic rumor detection methods.",Arabic Rumor Detection Using Contextual Deep Bidirectional Language Modeling,IEEE Access,Article,1/1/2022,Fatmah,Baothman,2022
10.1109/ACII52823.2021.9597419,"© 2021 IEEE.Recognizing a speaker's emotion from their speech can be a key element in emergency call centers. End-to-end deep learning systems for speech emotion recognition now achieve equivalent or even better results than conventional machine learning approaches. In this paper, in order to validate the performance of our neural network architecture for emotion recognition from speech, we first trained and tested it on the widely used corpus accessible by the community, IEMOCAP. We then used the same architecture with the real life corpus, CEMO, comprised of 440 dialogs (2h16m) from 485 speakers. The most frequent emotions expressed by callers in these real-life emergency dialogues are fear, anger and positive emotions such as relief. In the IEMOCAP general topic conversations, the most frequent emotions are sadness, anger and happiness. Using the same end-to-end deep learning architecture, an Unweighted Accuracy Recall (UA) of 63% is obtained on IEMOCAP and a UA of 45.6% on CEMO, each with 4 classes. Using only 2 classes (Anger, Neutral), the results for CEMO are 76.9% UA compared to 81.1% UA for IEMOCAP. We expect that these encouraging results with CEMO can be improved by combining the audio channel with the linguistic channel. Real-life emotions are clearly more complex than acted ones, mainly due to the large diversity of emotional expressions of speakers.",End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings,"2021 9th International Conference on Affective Computing and Intelligent Interaction, ACII 2021",Conference Paper,1/1/2021,Laurence,Devillers,2021
10.1109/ANZIIS.1995.705748,"© 1995 IEEE. All rights reserved.A novel, modular connectionist architecture for grammar induction is proposed, the ""Markovian Language Network"" (MLN). Based on Zellig Harris's theory of syntax, MLN would seem to be a plausible biological and psychological model. Modules for extraction of Markov information and for categorization are interlinked and are used for multiple purposes. MLN has not yet been fully implemented, but simple experiments have been conducted with an hybrid system. MLN is seen as a first step toward a complete connectionist implementation of Harrisian linguistics.",The Markovian Language network: Steps toward a connectionist architecture for grammar induction,ANZIIS 1995 - Proceedings of the 3rd Australian and New Zealand Conference on Intelligent Information Systems,Conference Paper,1/1/1995,Ben,Goertzel,1995
10.1109/ASAMA.1999.805406,"Jackal is a Java-based tool for communication using the KQML agent communication language. Some features that make it extremely valuable to agent development are its conversation management facilities, flexible, blackboard style interface and ease of integration. Jackal has been developed in support of an investigation of the use of agents in enterprise-wide integration of planning and execution for manufacturing. This paper describes Jackal at a surface and design level, and demonstrates its use in a multiagent system that supports intelligent integration of enterprise planning and execution.",An agent-based infrastructure for enterprise integration,"Proceedings - 1st International Symposium on Agent Systems and Applications and 3rd International Symposium on Mobile Agents, ASA/MA 1999",Conference Paper,1/1/1999,Tim,Finin,1999
10.1109/CDCS.2001.918705,"© 2001 IEEE.In an age where wirelessly networked appliances and devices are becoming commonplace, there is a necessity for connecting them to work together for a mobile user. The design outlined in the paper provides an infrastructure and communication protocol for providing 'smart' services to these mobile devices. This flexible framework allows any medium to be used for communication between the system and the portable device, including infra-red, and BlueTooth. Using Extensible Markup Language (XML) for information passing, gives the system a uniform and easily adaptable interface. We explain our trade-offs in implementation and through experiments we show that the design is feasible and that it indeed provides a flexible structure for providing services. Centaurus provides a uniform infrastructure for heterogeneous services, both hardware and software services, to be made available to the users everywhere where they are needed.",Centaurus: A framework for intelligent services in a mobile environment,Proceedings - 21st International Conference on Distributed Computing Systems Workshops,Conference Paper,1/1/2001,Tim,Finin,2001
10.1109/CEC.2010.5586168,"An efficient design of a Multi-Objective Learning Classifier System for multi-flight navigation is presented. A classifier is represented by a set of rules, which are used to simultaneously navigate all the flights in the airspace. Navigation of a flight is based on the relation of the flight with factors of the air traffic environment such as wind, storm as well as other flights. This system continually learns and refines the rules of classifiers by a multi-objective optimization algorithm - NSGAII - to discover the trade-off set of classifiers which navigate flights without any conflict, minimal distance of flying, minimal discomfort defined by storm level and the time duration of flights passing through storm areas, and minimizing total delay time of flights. We propose to detect conflicts between flights by grouping trajectory segments in 3-D (abscissa-x, ordinate-y, and time-t) boxes. The conflict detection is only implemented in a box, thus the number of conflict detection times approximates to the number of conflicts. Further, conflicts between flights are resolved using a hill climber by propagating delays in the takeoff time of conflicting flights. The advantage of the proposed system is that the classifier outputs its rules in a symbolic representation, making the overall process transparent to the user and reusable. Moreover, the system successfully discovered rules in all runs to optimize its performance. © 2010 IEEE.",A Pittsburgh Multi-Objective Classifier for user preferred trajectories and flight navigation,"2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010",Conference Paper,12/1/2010,Hussein,Abbass,2010
10.1109/CVPR.2013.91,"Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel. While progress in deep learning shows the importance of learning features through multiple layers, it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP), a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building blocks are MI-KSVD, a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP), we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200. © 2013 IEEE.",Multipath sparse coding using hierarchical matching pursuit,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,Conference Paper,11/15/2013,Dieter,Fox,2013
10.1109/CVPR.2017.576,"© 2017 IEEE.Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches.",Geometric deep learning on graphs and manifolds using mixture model CNNs,"Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017",Conference Paper,11/6/2017,Michael,Bronstein,2017
10.1109/CVPR.2018.00430,"© 2018 IEEE.We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: 'Are there any apples in the fridge?' The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98.",IQA: Visual Question Answering in Interactive Environments,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,Conference Paper,12/14/2018,Dieter,Fox,2018
10.1109/CVPR42600.2020.01143,"© 2020 IEEE.Graph convolution operators bring the advantages of deep learning to a variety of graph and mesh processing tasks previously deemed out of reach. With their continued success comes the desire to design more powerful architectures, often by adapting existing deep learning techniques to non-Euclidean data. In this paper, we argue geometry should remain the primary driving force behind innovation in the emerging field of geometric deep learning. We relate graph neural networks to widely successful computer graphics and data approximation models: radial basis functions (RBFs). We conjecture that, like RBFs, graph convolution layers would benefit from the addition of simple functions to the powerful convolution kernels. We introduce affine skip connections, a novel building block formed by combining a fully connected layer with any graph convolution operator. We experimentally demonstrate the effectiveness of our technique, and show the improved performance is the consequence of more than the increased number of parameters. Operators equipped with the affine skip connection markedly outperform their base performance on every task we evaluated, i.e., shape reconstruction, dense shape correspondence, and graph classification. We hope our simple and effective approach will serve as a solid baseline and help ease future research in graph neural networks.",Geometrically principled connections in graph neural networks,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,Conference Paper,1/1/2020,Michael,Bronstein,2020
10.1109/CVPR46437.2021.01502,"© 2021 IEEEProteins' biological functions are defined by the geometric and chemical structure of their 3D molecular surfaces. Recent works have shown that geometric deep learning can be used on mesh-based representations of proteins to identify potential functional sites, such as binding targets for potential drugs. Unfortunately though, the use of meshes as the underlying representation for protein structure has multiple drawbacks including the need to pre-compute the input features and mesh connectivities. This becomes a bottleneck for many important tasks in protein science. In this paper, we present a new framework for deep learning on protein structures that addresses these limitations. Among the key advantages of our method are the computation and sampling of the molecular surface on-the-fly from the underlying atomic point cloud and a novel efficient geometric convolutional layer. As a result, we are able to process large collections of proteins in an end-to-end fashion, taking as the sole input the raw 3D coordinates and chemical types of their atoms, eliminating the need for any hand-crafted pre-computed features. To showcase the performance of our approach, we test it on two tasks in the field of protein structural bioinformatics: the identification of interaction sites and the prediction of protein-protein interactions. On both tasks, we achieve state-of-the-art performance with much faster run times and fewer parameters than previous models. These results will considerably ease the deployment of deep learning methods in protein science and open the door for end-to-end differentiable approaches in protein modeling tasks such as function prediction and design.",Fast end-to-end learning on protein surfaces,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,Conference Paper,1/1/2021,Michael,Bronstein,2021
10.1109/CVPR52688.2022.01253,"© 2022 IEEE.We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff). Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direction, and time and outputs density and radiance. The background stuff is represented by a similar MLP that additionally outputs semantic labels. Each object MLPs are instance-specific and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-specific priors incorporated via meta-learned initialization. Our model builds a panoptic radiance field representation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly optimize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color images and pseudo-supervision from predicted semantic segmentations. During experiments with real-world dynamic scenes, we find that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmentation, 3D scene editing, and multiview depth prediction.",Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,Conference Paper,1/1/2022,Frank,Dellaert,2022
10.1109/CVPRW.2019.00173,"© 2019 IEEE.We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an 'arguing machines' framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is sufficient to improve the accuracy of the overall system given human supervision over disagreements. We demonstrate this system in two applications: (1) image classification and (2) large-scale real-world semi-autonomous driving. For the first application, we apply this framework to image classification achieving a reduction from 8.0% to 2.8% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human as challenging.",Arguing machines: Human supervision of black box ai systems that make life-critical decisions,IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,Conference Paper,6/1/2019,Lex,Fridman,2019
10.1109/DICTA51227.2020.9363402,"© 2020 IEEE.In recent years, deep learning-based OCT segmentation methods have addressed many of the limitations of traditional segmentation approaches and are capable of performing rapid, consistent and accurate segmentation of the chorio-retinal layers. However, robust deep learning methods require a sufficiently large and diverse dataset for training, which is not always feasible in many biomedical applications. Generative adversarial networks (GANs) have demonstrated the capability of producing realistic and diverse high-resolution images for a range of modalities and datasets, including for data augmentation, a powerful application of GAN methods. In this study we propose the use of a StyleGAN inspired approach to generate chorio-retinal optical coherence tomography (OCT) images with a high degree of realism and diversity. We utilize the method to synthesize image and segmentation mask pairs that can be used to train a deep learning semantic segmentation method for subsequent boundary delineation of three chorioretinal layer boundaries. By pursuing a dual output solution rather than a mask-to-image translation solution, we remove an unnecessary constraint on the generated images and enable the synthesis of new unseen area mask labels. The results are encouraging with near comparable performance observed when training using purely synthetic data, compared to the real data. Moreover, training using a combination of real and synthetic data results in zero measurable performance loss, further demonstrating the reliability of this technique and feasibility for data augmentation in future work.",Dual image and mask synthesis with GANs for semantic segmentation in optical coherence tomography,"2020 Digital Image Computing: Techniques and Applications, DICTA 2020",Conference Paper,11/29/2020,Michael,Collins,2020
10.1109/DICTA52665.2021.9647099,"© 2021 IEEE.Deep learning methods provide state-of-the-art performance for the semantic segmentation of the retina and choroid in optical coherence tomography (OCT) images, enabling rapid, accurate and automatic analyses. However, high difficulty scans can still pose a problem even for the current state-of-the-art methods. Generative adversarial networks (GANs), are a family of deep learning methods that provide significant benefits for several applications due to their ability to learn complex data distributions, such as those of large image datasets. Segmentation is one of these applications that has been investigated in several modalities including retinal fundus image analysis, resulting in performance improvements when incorporating an adversarial loss for segmentation. However, the application of GAN-based segmentation to OCT images has not been investigated in detail and has not been studied at all in the context of choroidal segmentation. In this study, we investigate the use of a GAN to perform semantic segmentation of the retina and choroid in OCT images, by replacing the traditional segmentation loss with an adversarial loss. A detailed analysis of important training parameters and network architecture choices is provided to 1) better understand their behavior and 2) to optimize performance for chorio-retinal segmentation in OCT images. A key difference of this study is that, by considering the loss in isolation and comparing to traditional segmentation losses using an identical segmentation network, an unbiased and transparent comparison is performed. Using an optimized adversarial loss, strong performance is observed, providing near comparable performance to traditional segmentation losses. The results from this experiment provide a strong foundation for future work with GAN-based OCT retinal and choroidal segmentation.",OCT chorio-retinal segmentation with adversarial loss,DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications,Conference Paper,1/1/2021,Michael,Collins,2021
10.1109/DICTA52665.2021.9647154,"© 2021 IEEE.Semantic segmentation methods based on deep learning techniques have transformed the analysis of many medical imaging modalities, including the extraction of retinal layers from ocular optical coherence tomography images. Despite the high accuracy of these methods, the automatic techniques are not free of labelling errors, which means that a clinician may need to engage in the time-consuming process of reviewing the outcome of the segmentation method. Given this shortcoming, having access to segmentation techniques that can provide a confidence metric associated with the output (probability class map) are desirable. In this study, the use of Monte-Carlo dropout combined with a residual U-net architecture is explored as a way to provide segmentation pixel-wise prediction maps as well as corresponding uncertainty maps. While assessing the proposed network on a dataset of subjects with a retinal pathology (Stargardt disease), the uncertainty map exhibited a high correlation with the boundary error metric. Thus, confirming the potential of the technique to extract metrics that are a surrogate of the segmentation error. While the Monte-Carlo dropout seems to have no detrimental effect on performance, the uncertainty metric derived from this technique has potential for a range of important clinical (i.e. ranking of scans to be reviewed by a human expert) and research (i.e. network fine-tuning with a focus on high uncertainty/high error regions) applications.",Use of uncertainty quantification as a surrogate for layer segmentation error in Stargardt disease retinal OCT images,DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications,Conference Paper,1/1/2021,Michael,Collins,2021
10.1109/DICTA52665.2021.9647266,"© 2021 IEEE.Optical coherence tomography (OCT) images of the posterior eye provide valuable clinical information. To quantify these images and extract appropriate biomarkers, methods to segment the different retinal boundaries are needed. In recent years, deep learning methods have been applied to perform this image analysis task, providing state of the art performance. However, these methods can be affected by image variability, particularly if the network is trained with images that do not match in terms of features to those of the testing dataset. One of the common sources of variability in OCT is speckle noise. In this work, the effect of noise on the semantic segmentation process is investigated and the use of a CycleGAN method as an image-to-image translation to reduce noise and its impact on segmentation are assessed. The results show promising performance and a proof of the potential of this generative adversarial network (GAN) method to positively impact medical image segmentation, obtaining good performance results when compared in terms of Dice coefficient overlap and boundary error metrics. The findings of this work may be translated to other applications such as 'OCT instrument translation' to create instrument-agnostic segmentation solutions.",OCT retinal image-to-image translation: Analysing the use of CycleGAN to improve retinal boundary semantic segmentation,DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications,Conference Paper,1/1/2021,Michael,Collins,2021
10.1109/GRID.2007.4354120,"This paper presents WS-DAIOnt-RDF(S), a system that provides an homogeneous access mechanism for using heterogeneous and distributed RDF(S) ontologies in Grid applications. These ontologies may be stored in different RDF(S) storage systems, geographically distributed across the Grid infrastructure. The paper describes in detail the design of the system, its rationale, the current implementation, and the ongoing standardisation effort. © 2007 IEEE.",WS-DAIOnt-RDF(S): Ontology access provision in grids,Proceedings - IEEE/ACM International Workshop on Grid Computing,Conference Paper,1/1/2007,Asunción Gómez,Pérez,2007
10.1109/HICSS.1997.663229,"We report on a successful project for transference of advanced planning and scheduling technology for outage management, a collaboration between Rome Laboratory, the Electrical Power Research Institute, Kaman Science and Kestrel Institute, as part of DOD's dual-use program. The main goal of the project was to evaluate the use of transformational approaches and AI technology to solve real-world planning and scheduling problems involving complex constraints. ROMAN (Rome Lab Outage Manager) is the prototype system that was developed as a result of this project. ROMAN's main innovation compared to the current state of the art of outage management tools is its integrated approach to outage management automatically enforcing safety constraints during the planning and scheduling phase. Another innovative aspect of ROMAN is its generation of more robust schedules that are feasible over time windows. In other words, ROMAN generates a family of schedules by assigning time intervals as start times to activities rather than single point start times, without affecting the overall duration of the project.",Transformational approach applied to outage management of nuclear power plants,Proceedings of the Hawaii International Conference on System Sciences,Conference Paper,12/1/1997,Carla,Gomes,1997
10.1109/HPCSIM.2009.5195313,"This paper reports our work on application of concurrent/ distributed techniques to SAT. These were investigated using each of the following methods: the DPLL [5] algorithm, the dilemma rule of the St°almarck's algorithm [12] and a concurrent/distributed hybrid SAT solver: DPLL-St°almarck, using a combination of the DPLL and the dilemma rule based algorithm. The prototypes have been implemented using Alice [10], an SML based language with support for distribution and concurrency. Our prototype framework allows for rapid-prototyping of and experimentation with application of various concurrent/distributed programming techniques to SAT. The emphasis is not on building an industry-standard SAT solver, but rather an investigation of the efficacy of use of these techniques for SAT at various levels of granularity. We discuss in detail the workings of DPLL-St°almarck. DPLL and St°almarck are complementary methods given their depth-first and breadth-first approaches. A single threaded execution limits the possibilities of combining the two methods. We have used our prototype framework to explore the use of concurrent/distributed programming techniques for achieving cooperation between the two methods. Empirical evaluation is in progress using the metrics of time, size of search space, and number of machines. The work reported is part of an ongoing project investigating the use of concurrent-distributed programming techniques for theorem proving © 2009 IEEE.",Concurrent-distributed programming techniques for SAT using DPLL-sta°lmarck,"Proceedings of the 2009 International Conference on High Performance Computing and Simulation, HPCS 2009",Conference Paper,11/19/2009,Alan,Bundy,2009
10.1109/IAT.2005.23,"Mozart/Oz is an advanced development platform for intelligent, distributed, powerful and highly functional applications, developed under the European ACCLAIM project. The platform at present lacks a tool for agent communications based on a standard such as ACL-FIPA or KQML. In this work, a tool for agent communication in Mozart/Oz based on KQML has been developed. The communication platform was designed, moreover, as a modular architecture that permits flexibility, high efficiency and inter-operability, which considerably enhances the extensibility of the system. In this paper, this architecture is presented, together with a comparative analysis against two of the most popular agent development platforms in Java, one of which uses KQML (JATLite) and the other, FIPA/ACL (JADE). © 2005 IEEE.",A tool for agent communication in Mozart/Oz,"Proceedings - 2005 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT'05",Conference Paper,12/1/2005,Amparo Alonso,Betanzos,2005
10.1109/ICASSP.2013.6639349,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error. © 2013 IEEE.",Advances in optimizing recurrent networks,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,10/18/2013,Yoshua,Bengio,2013
10.1109/ICASSP.2018.8461822,"© 2018 IEEE.Singing voice separation based on deep learning relies on the usage of time-frequency masking. In many cases the masking process is not a learnable function or is not encapsulated into the deep learning optimization. Consequently, most of the existing methods rely on a post processing step using the generalized Wiener filtering. This work proposes a method that learns and optimizes (during training) a source-dependent mask and does not need the aforementioned post processing step. We introduce a recurrent inference algorithm, a sparse transformation step to improve the mask generation process, and a learned denoising filter. Obtained results show an increase of 0.49 dB for the signal to distortion ratio and 0.30 dB for the signal to interference ratio, compared to previous state-of-the-art approaches for monaural singing voice separation.",Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,9/10/2018,Yoshua,Bengio,2018
10.1109/ICASSP.2018.8462545,"© 2018 IEEE.In the last years, Graph Convolutional Neural Networks gained popularity in the Machine Learning community for their capability of extracting local compositional features on signals defined on non-Euclidean domains. Shape correspondence, document classification, molecular properties predictions are just few of the many different problems where these techniques have been successfully applied. In this paper we will present Deep Geometric Matrix Completion, a recent application of Graph Convolutional Neural Networks to the matrix completion problem. We will illustrate MGCNN (a multi-graph CNN able to deal with signals defined over multiple domains) and we will show how coupling such technique with a RNN, a learnable diffusion process can be realized for reconstructing the desired information. Extensive experimental evaluation shows how Geometric Deep Learning techniques allow to outperform previous state of the art solutions on the matrix completion problem.",Deep geometric matrix completion: A new way for recommender systems,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,9/10/2018,Michael,Bronstein,2018
10.1109/ICASSP.2019.8682880,"© 2019 IEEE.Recent character and phoneme-based parametric TTS systems using deep learning have shown strong performance in natural speech generation. However, the choice between character or phoneme input can create serious limitations for practical deployment, as direct control of pronunciation is crucial in certain cases. We demonstrate a simple method for combining multiple types of linguistic information in a single encoder, named representation mixing, enabling flexible choice between character, phoneme, or mixed representations during inference. Experiments and user studies on a public audiobook corpus show the efficacy of our approach.",Representation Mixing for TTS Synthesis,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,5/1/2019,Yoshua,Bengio,2019
10.1109/ICASSP.2019.8683705,"© 2019 IEEE.Despite the success of deep learning in speech recognition, multi-dialect speech recognition remains a difficult problem. Although dialect-specific acoustic models are known to perform well in general, they are not easy to maintain when dialect-specific data is scarce and the number of dialects for each language is large. Therefore, a single unified acoustic model (AM) that generalizes well for many dialects has been in demand. In this paper, we propose a novel acoustic modeling technique for accurate multi-dialect speech recognition with a single AM. Our proposed AM is dynamically adapted based on both dialect information and its internal representation, which results in a highly adaptive AM for handling multiple dialects simultaneously. We also propose a simple but effective training method to deal with unseen dialects. The experimental results on large scale speech datasets show that the proposed AM outperforms all the previous ones, reducing word error rates (WERs) by 8.11% relative compared to a single all-dialects AM and by 7.31% relative compared to dialect-specific AMs.",A Highly Adaptive Acoustic Model for Accurate Multi-dialect Speech Recognition,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,5/1/2019,Yoshua,Bengio,2019
10.1109/ICASSP.2019.8683713,"© 2019 IEEE.The availability of open-source software is playing a remarkable role in the popularization of speech recognition and deep learning. Kaldi, for instance, is nowadays an established framework used to develop state-of-the-art speech recognizers. PyTorch is used to build neural networks with the Python language and has recently spawn tremendous interest within the machine learning community thanks to its simplicity and flexibility.The PyTorch-Kaldi project aims to bridge the gap between these popular toolkits, trying to inherit the efficiency of Kaldi and the flexibility of PyTorch. PyTorch-Kaldi is not only a simple interface between these software, but it embeds several useful features for developing modern speech recognizers. For instance, the code is specifically designed to naturally plug-in user-defined acoustic models. As an alternative, users can exploit several pre-implemented neural networks that can be customized using intuitive configuration files. PyTorch-Kaldi supports multiple feature and label streams as well as combinations of neural networks, enabling the use of complex neural architectures. The toolkit is publicly-released along with a rich documentation and is designed to properly work locally or on HPC clusters.Experiments, that are conducted on several datasets and tasks, show that PyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech recognizers.",The Pytorch-kaldi Speech Recognition Toolkit,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",Conference Paper,5/1/2019,Yoshua,Bengio,2019
10.1109/ICKG52313.2021.00010,"© 2021 IEEE.Both symbolic and sub-symbolic AI have their limi-tations, but their combination can be more than the sum of their parts. For instance, statistical machine learning has been hugely successful at classification and decision-making tasks, but not so good at deliberative systematic reasoning nor at explanation. We argue that by combining symbolic and sub-symbolic reasoning into hybrid systems, the whole will be more than the sum of its parts. To illustrate the potential of hybrid AI system, we describe the FRANK query answering system. FRANK infers new knowledge from the diverse and immense knowledge sources on the Web, using a combination of both deductive and statistical reasoning. This enables it to make predictions. For instance, to answer the question 'Which country in Europe will have the highest GDP growth rate by 2032?', it (i) decomposes Europe into its constituent countries, (ii) then for each country uses regression over their previous GDP growth rates to extrapolate each to 2032 and (iii) then returns the country which is predicted to then have the maximum value. The decompositions are explained deductively and the regressions by a prediction model that can be rendered graphically. This explanation of FRANK's reasoning merges deduction and statistics. In this paper, we highlight recent work on FRANK that focus on leveraging hybrid AI to tackle question answering with emphasis on explainability of the inference process and its inferred answers. We aim for whole system reasoning; that is, we are automating the choices of knowledge sources and the planning that constructs the inference process from the facts found in these knowledge sources. We intend that these 'engineering' choices are also explained to the user.",Combining Deductive and Statistical Explanations in the FRANK Query Answering System,"Proceedings - 12th IEEE International Conference on Big Knowledge, ICBK 2021",Conference Paper,1/1/2021,Alan,Bundy,2021
10.1109/ICKG52313.2021.00044,"© 2021 IEEE.Identifying discriminative attributes between prod-uct variations, e.g., the same wristwatch models but in different finishes, is crucial for improving e-commerce search engines and recommender systems. Despite the importance of these discrimi-native attributes, values for such attributes are often not available explicitly and instead are mentioned only in unstructured fields such as product title or product description. In this work, we introduce the novel task of discriminative attribute extraction which involves identifying the attributes that distinguish product variations, such as finish, and also, at the same time, extracting the values for these attributes from unstructured text. This task differs from the standard attribute value extraction task that has been well-studied in literature, as in our task we also need to identify the attribute, in addition to finding the value. We propose DiffXtract, a novel end-to-end, deep learning based approach that jointly identifies both the discriminative attribute and extracts its values from the product variations. The proposed approach is trained using a multitask objective and explicitly models the semantic representation of the discriminative attribute and uses it to extract the attribute values. We show that existing product attribute extraction approaches have several drawbacks, both theoretically and empirically. We also introduce a novel dataset based on a corpus of data previously crawled from a large number of e-commerce websites. In our empirical evaluation, we show that DiffXtract outperforms state-of-the-art deep learning-based and dictionary-based attribute extraction approaches by up to 8% F1 score when identifying attributes, and up to 10% F1 score when extracting attribute values.",DiffXtract: Joint Discriminative Product Attribute-Value Extraction,"Proceedings - 12th IEEE International Conference on Big Knowledge, ICBK 2021",Conference Paper,1/1/2021,Lise,Getoor,2021
10.1109/ICME.2009.5202846,"Recent advances in wireless sensor technology facilitate the development of remote healthcare systems, which can significantly reduce the healthcare cost. Despite the initial promising results, there remain many obstacles to apply this technology to the practical medical care context. One of the critical issues is to assure the timely and robust delivery of the life-critical medical data in the resource-constrained wireless sensor networking environment. This paper addresses this issue and presents a Quality of Service (QoS) support mechanism for wireless remote healthcare system, which integrates XML-based QoS specification, patient admission policy, and differentiated scheduling and queue management. The proposed QoS support mechanism is implemented in CareNet, our wireless sensor system for remote healthcare. Extensive performance study is presented to validate and evaluate our solution. ©2009 IEEE.",Providing QoS support for wireless remote healthcare system,"Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009",Conference Paper,11/20/2009,Ruzena,Bajcsy,2009
10.1109/ICMLA.2018.00064,"© 2018 IEEE.Tasks involving the analysis of geometric (graph-and manifold-structured) data have recently gained prominence in the machine learning community, giving birth to a rapidly developing field of geometric deep learning. In this work, we leverage graph neural networks to improve signal detection in the IceCube neutrino observatory. The IceCube detector array is modeled as a graph, where vertices are sensors and edges are a learned function of the sensors spatial coordinates. As only a subset of IceCubes sensors is active during a given observation, we note the adaptive nature of our GNN, wherein computation is restricted to the input signal support. We demonstrate the effectiveness of our GNN architecture on a task classifying IceCube events, where it outperforms both a traditional physics-based method as well as classical 3D convolution neural networks.",Graph Neural Networks for IceCube Signal Classification,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",Conference Paper,1/15/2019,Michael,Bronstein,2019
10.1109/ICPR48806.2021.9412166,"© 2020 IEEEFace recognition is a widely accepted biometric verification tool, as the face contains a lot of information about the identity of a person. In this study, a 2-step neural-based pipeline is presented for matching 3D facial shape to multiple DNA-related properties (sex, age, BMI and genomic background). The first step consists of a triplet loss-based metric learner that compresses facial shape into a lower dimensional embedding while preserving information about the property of interest. Most studies in the field of metric learning have only focused on 2D Euclidean data. In this work, geometric deep learning is employed to learn directly from 3D facial meshes. To this end, spiral convolutions are used along with a novel mesh-sampling scheme that retains uniformly sampled 3D points at different levels of resolution. The second step is a multi-biometric fusion by a fully connected neural network. The network takes an ensemble of embeddings and property labels as input and returns genuine and imposter scores. Since embeddings are accepted as an input, there is no need to train classifiers for the different properties and available data can be used more efficiently. Results obtained by a 10-fold cross-validation for biometric verification show that combining multiple properties leads to stronger biometric systems. Furthermore, the proposed neural-based pipeline outperforms a linear baseline, which consists of principal component analysis, followed by classification with linear support vector machines and a Naïve Bayes-based score-fuser.",3D facial matching by spiral convolutional metric learning and a biometric fusion-net of demographic properties,Proceedings - International Conference on Pattern Recognition,Conference Paper,1/1/2020,Michael,Bronstein,2020
10.1109/ICPR48806.2021.9412859,"© 2020 IEEEIn many application domains such as computer vision, Convolutional Layers (CLs) are key to the accuracy of deep learning methods. However, it is often required to assemble a large number of CLs, each containing thousands of parameters, in order to reach state-of-the-art accuracy, thus resulting in complex and demanding systems that are poorly fitted to resource-limited devices. Recently, methods have been proposed to replace the generic convolution operator by the combination of a shift operation and a simpler 1x1 convolution. The resulting block, called Shift Layer (SL), is an efficient alternative to CLs in the sense it allows to reach similar accuracies on various tasks with faster computations and fewer parameters. In this contribution, we introduce Shift Attention Layers (SALs), which extend SLs by using an attention mechanism that learns which shifts are the best at the same time the network function is trained. We demonstrate SALs are able to outperform vanilla SLs (and CLs) on various object recognition benchmarks while significantly reducing the number of float operations and parameters for the inference.",Attention based pruning for shift networks,Proceedings - International Conference on Pattern Recognition,Conference Paper,1/1/2020,Yoshua,Bengio,2020
10.1109/ICRA.2017.7989307,"© 2017 IEEE.Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics.",Visual closed-loop control for pouring liquids,Proceedings - IEEE International Conference on Robotics and Automation,Conference Paper,7/21/2017,Dieter,Fox,2017
10.1109/ICRA.2018.8460968,"© 2018 IEEE.We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.",Socially compliant navigation through raw depth inputs with generative adversarial imitation learning,Proceedings - IEEE International Conference on Robotics and Automation,Conference Paper,9/10/2018,Wolfram,Burgard,2018
10.1109/ICRA.2018.8462979,"© 2018 IEEE.Localization is an indispensable component of a robot's autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-the-art SIFT-based approaches.",Deep Auxiliary Learning for Visual Localization and Odometry,Proceedings - IEEE International Conference on Robotics and Automation,Conference Paper,9/10/2018,Wolfram,Burgard,2018
10.1109/ICRA.2019.8794223,"© 2019 IEEE.End-to-end learning for autonomous navigation has received substantial attention recently as a promising method for reducing modeling error. However, its data complexity, especially around generalization to unseen environments, is high. We introduce a novel image-based autonomous navigation technique that leverages in policy structure using the Riemannian Motion Policy (RMP) framework for deep learning of vehicular control. We design a deep neural network to predict control point RMPs of the vehicle from visual images, from which the optimal control commands can be computed analytically. We show that our network trained in the Gibson environment can be used for indoor obstacle avoidance and navigation on a real RC car, and our RMP representation generalizes better to unseen environments than predicting local geometry or predicting control commands directly.",Neural autonomous navigation with riemannian motion policy,Proceedings - IEEE International Conference on Robotics and Automation,Conference Paper,5/1/2019,Dieter,Fox,2019
10.1109/ICRA40945.2020.9196644,"© 2020 IEEE.Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.",Scaling Local Control to Large-Scale Topological Navigation,Proceedings - IEEE International Conference on Robotics and Automation,Conference Paper,5/1/2020,Dieter,Fox,2020
10.1109/ICSMC.2004.1401130,"This paper presents a Modified Sparse Distributed Memory architecture for use in software agents with natural language processing capabilities. We have modified Kanerva's Sparse Distributed Memory (SDM) into an architecture with a ternary memory space. This enables the memory to be used in IDA, the Intelligent Distribution Agent built for the U.S. Navy. IDA implements Boars' global workspace theory, a psychological theory of consciousness. As a result, it can react to novel and problematic situations in a more flexible, more human-like way than traditional AI systems. IDA performs a Junction, namely billet assignment, heretofore reserved for humans. We argue that such flexibility requires advanced memory systems such as transient episodic memory and auto-biographical memory. Here, we present the architecture, tests and results of this modified SDM system which can be used as a transient episodic memory in suitable software agents. © 2004 IEEE.",Modified sparse distributed memory as transient episodic memory for cognitive software agents,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",Conference Paper,12/1/2004,Stan,Franklin,2004
10.1109/ICSTW.2009.8,"This paper presents an approach and tool to automatically instrument dynamic web applications using source transformation technology, and to reverse engineer a UML 2.1 sequence diagram from the execution traces generated by the resulting instrumentation. The result can be directly imported and visualized in a UML toolset such as Rational Software Architect. Our approach dynamically filters traces to reduce redundant information that may complicate program understanding. While our current implementation works on PHP-based applications, the framework is easily extended to other scripting languages in plug-and-play fashion. In addition to supporting web application understanding, our tool is being used to recover traces from dynamic web applications in support of web application security analysis and testing. We demonstrate our method on the analysis of the popular internet bulletin board system PhpBB 2.0. © 2009 IEEE.",Automated reverse engineering of UML sequence diagrams for dynamic web applications,"IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2009",Conference Paper,9/11/2009,Thomas,Dean,2009
10.1109/IJCB48548.2020.9304894,"© 2020 IEEE.The research in biometric recognition using hand shape has been somewhat stagnating in the last decade. Meanwhile, computer vision and machine learning have experienced a paradigm shift with the renaissance of deep learning, which has set the new state-of-the-art in many related fields. Inspired by successful applications of deep learning for other biometric modalities, we propose a novel approach to 3D hand shape recognition from RGB-D data based on geometric deep learning techniques. We show how to train our model on synthetic data and retain the performance on real samples during test time. To evaluate our method, we provide a new dataset NNHand RGB- D of short video sequences and show encouraging performance compared to diverse baselines on the new data, as well as current benchmark dataset HKPolyU. Moreover, the new dataset opens door to many new research directions in hand shape recognition.",Clustered Dynamic Graph CNN for Biometric 3D Hand Shape Recognition,IJCB 2020 - IEEE/IAPR International Joint Conference on Biometrics,Conference Paper,9/28/2020,Michael,Bronstein,2020
10.1109/ijcnn.2006.246716,"It is proposed that the creation of Artificial General Intelligence (AGI) at the human level and ultimately beyond is a problem addressable via integrating computer science algorithms and data structures within a cognitive architecture oriented toward experiential learning. A general conceptual framework for AGI is presented, beginning with a philosophy of mind based on the concept of pattern, then moving to a general mathematical and conceptual framework for modeling intelligent systems, Self-Modifying Evolving Probabilistic Hypergraphs (SMEPH), and finally to an overview of a specific design for AGI, the Novamente AI Engine. The problem of teaching an AGI system is discussed, in the context of Novamente's embodiment in the AGI-SIM simulation world. An educational program based loosely on Piaget's developmental stages is outlined, followed by more detailed consideration of the learning by Novamente in AGI-SIM of the Piagetan infant-level capability of ""object permanence."" © 2006 IEEE.","Patterns, hypergraphs and embodied general intelligence",IEEE International Conference on Neural Networks - Conference Proceedings,Conference Paper,1/1/2006,Ben,Goertzel,2006
10.1109/IJCNN.2012.6252377,"Although computer Go players are now better than humans on small board sizes, they are still a fair way from the top human players on standard board sizes. Thus the nature of human expertise is of great interest to artificial intelligence. Human play relies much more on pattern memory and has been extensively explored in chess. The big challenge in Go is local-global interaction - local search is good but global integration is weak. We used techniques based on the cognitive neuroscience of chess to predict optimal areas to move using perceptual chunks, which we cross-validated against game records comprising upwards of five million positions. Prediction to within a small window was about 50%, a remarkable result. © 2012 IEEE.",Neuro-cognitive model of move location in the game of Go,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,8/22/2012,Fernand,Gobet,2012
10.1109/IJCNN.2014.6889723,"© 2014 IEEE.The Deep Spatio-Temporal Inference Network (DeSTIN) is a deep learning architecture which combines un-supervised learning and Bayesian inference. The original version of DeSTIN incorporates k-means clustering inside each processing node. Here we propose to replace k-means with a more sophisticated algorithm, online EM (Expectation Maximization), and show that this improves DeSTIN's performance on image classification and restoration tasks.",Improving machine vision via incorporating expectation-maximization into Deep Spatio-Temporal learning,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,9/3/2014,Ben,Goertzel,2014
10.1109/IJCNN.2017.7965845,"© 2017 IEEE.Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.1",A robust adaptive stochastic gradient method for deep learning,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,6/30/2017,Yoshua,Bengio,2017
10.1109/IJCNN.2018.8489565,"© 2018 IEEE.Monaural singing voice separation task focuses on the prediction of the singing voice from a single channel music mixture signal. Current state of the art (SOTA) results in monaural singing voice separation are obtained with deep learning based methods. In this work we present a novel recurrent neural approach that learns long-term temporal patterns and structures of a musical piece. We build upon the recently proposed Masker-Denoiser (MaD) architecture and we enhance it with the Twin Networks, a technique to regularize a recurrent generative network using a backward running copy of the network. We evaluate our method using the Demixing Secret Dataset and we obtain an increment to signal-to-distortion ratio (SDR) of 0.37 dB and to signal-to-interference ratio (SIR) of 0.23 dB, compared to previous SOTA results.",MaD TwinNet: Masker-Denoiser Architecture with Twin Networks for Monaural Sound Source Separation,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,10/10/2018,Yoshua,Bengio,2018
10.1109/IJCNN.2019.8851805,"© 2019 IEEE.Speech act provides valuable insight for understanding the communicative intention and behaviour of a user utterance. This applies for conversation on any platform which includes social media network such as Twitter. This paper presents a deep learning based tweet act classifier (speech acts for Twitter) for understanding the content and intention of tweets and for discovering the valuable communications amongst the tweeters. A generic taxonomy of seven speech acts has been created for the task. A Convolution Neural Network (CNN) based Deep Learning (DL) architecture is integrated with different classifiers at the final layer such as softmax or linear Support Vector Machine (SVM) to develop a tweet act classifier. To make the classifier even more robust, various hand-crafted features are added to the system to boost its effectiveness towards improving the performance of the system. Experimental results indicate that the proposed model achieved good results with an average F1-score of 0.71 and outperformed state of the art approaches.",Tweet Act Classification : A Deep Learning based Classifier for Recognizing Speech Acts in Twitter,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/1/2019,Pushpak,Bhattacharyya,2019
10.1109/IJCNN.2019.8851943,"© 2019 IEEE.Dialogue Act (DA) Classification plays a signifi-cant role in the understanding of an utterance in a dialogue. Components of Spoken Dialogue System (SDS) such as Natural Language Understanding (NLU) and Dialogue Management (DM) modules can significantly exploit the output of the DA classification. In this paper, we propose a task-oriented DA classifier based on both traditional supervised Machine Learning (ML) as well as Deep Learning (DL) techniques. The type and nature of dialogues basically depend on the domain and the DA itself. So, in order to make the model task-oriented, a new tag-set has been designed by studying the properties of the target domain. On the benchmark SwitchBoard (SWBD) and TRAINS corpus, our proposed models have performed exceptionally well with the new tag-set. Experimental results indicate that our proposed models have achieved good accuracy on both the datasets and outperformed several state of the art approaches and the new tag-set is well suited for task-oriented applications.",Exploring Machine Learning and Deep Learning Frameworks for Task-Oriented Dialogue Act Classification,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/1/2019,Pushpak,Bhattacharyya,2019
10.1109/IJCNN.2019.8852209,"© 2019 IEEE.Swarm control is a difficult problem due to the need to guide a large number of agents simultaneously. We cast the problem as a shepherding problem, similar to biological dogs guiding a group of sheep towards a goal. The shepherd needs to deal with complex and dynamic environments and make decisions in order to direct the swarm from one location to another. In this paper, we design a novel curriculum to teach an artificial intelligence empowered agent to shepherd in the presence of the large state space associated with the shepherding problem and in a transparent manner. The results show that a properly designed curriculum could indeed enhance the speed of learning and the complexity of learnt behaviours.",Transparent Machine Education of Neural Networks for Swarm Shepherding Using Curriculum Design,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/1/2019,Hussein,Abbass,2019
10.1109/IJCNN48605.2020.9206620,"© 2020 IEEE.Text generator systems have become extremely popular with the advent of recent deep learning models such as encoder-decoder. Controlling the information and style of the generated output without supervision is an important and challenging Natural Language Processing (NLP) task. In this paper we define the task of constructing a coherent paragraph from a set of disaster domain tweets, without any parallel data. We tackle the problem by building two systems in pipeline. The first system focuses on unsupervised style transfer to convert the individual tweets into news sentences. The second system stitches together the outputs from the first system to form a coherent news paragraph. We also propose a novel training mechanism, by splitting the sentences into propositions and training the second system to merge the sentences. We create a validation and test set consisting of tweet-sets and their equivalent news paragraphs to perform empirical evaluation. We also perform human evaluations on our model. In a completely unsupervised setting our model was able to achieve a BLEU score of 19.32, while successfully transferring styles and joining tweets to form a meaningful news paragraph.",Tweet to News Conversion: An Investigation into Unsupervised Controllable Text Generation,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/1/2020,Pushpak,Bhattacharyya,2020
10.1109/IJCNN52387.2021.9533878,"© 2021 IEEE.Developing an adequate and human-like virtual agent has been one of the primary applications of artificial intelligence. In the last few years, task-oriented dialogue systems have gained huge popularity because of their upsurging relevance and positive outcomes. In real-world, users may not always have a predefined and rigid task goal beforehand; they upgrade/downgrade/change their goal component dynamically depending upon their utility value and agent's serving capability. However, existing virtual agents fail to incorporate this dynamic behavior, leading to either unsuccessful task completion or an ungratified user experience. The paper presents an end to end multimodal dialogue system for dynamic and co-operative goal setting, which incorporates i) a multi-modal semantic state representation in policy learning to deal with multi-modal inputs, ii) a goal manager module in a traditional dialogue manager for handling dynamic and goal unavailability scenarios effectively, iii) an accumulative reward (task/persona/sentiment) for task success, personalized persuasion and user-adaptive behavior, respectively. The obtained experimental results and the comparisons with baselines firmly establish the need and efficacy of the proposed system.",Multi-Modal Dialogue Policy Learning for Dynamic and Co-operative Goal Setting,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/18/2021,Pushpak,Bhattacharyya,2021
10.1109/IJCNN52387.2021.9533931,"© 2021 IEEE.Emotion analysis from texts has emerged as an important area of research in the field of Natural Language Processing (NLP) in the past few years. Several benchmark datasets have been released for this task, however most of the datasets are open domain in nature and for resource-rich language like English. These datasets may not always be sufficient for capturing domain specific emotions, and may tend to skew towards an emotion based on the domain. In this paper, we provide a framework for multilingual emotion detection to deal with the crisis situations. We collect and annotate disaster domain social media tweets and news data in two languages, namely English and Hindi. We derive 6 emotions from Plutchik's wheel of emotions suitable for disaster domain, and annotate the data using these disaster specific emotions. In total we create four emotionally enriched datasets i.e. 2 tweets datasets (English and Hindi) and 2 news dataset (English and Hindi). We also establish strong baselines on the dataset using two popular deep learning algorithms, stacked Bi-LSTM+CNN and BERT. Evaluation shows that the best model achieves the averaged accuracy of 68% across the four different datasets.",Emotion driven Crisis Response: A benchmark Setup for Multi-lingual Emotion Analysis in Disaster Situations,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/18/2021,Pushpak,Bhattacharyya,2021
10.1109/IJCNN52387.2021.9534032,"© 2021 IEEE.In adversarial imitation learning, a discriminator is trained to differentiate agent episodes from expert demonstrations representing the desired behavior. However, as the trained policy learns to be more successful, the negative examples (the ones produced by the agent) become increasingly similar to expert ones. Despite the fact that the task is successfully accomplished in some of the agent's trajectories, the discriminator is trained to output low values for them. We hypothesize that this inconsistent training signal for the discriminator can impede its learning, and consequently leads to worse overall performance of the agent. We show experimental evidence for this hypothesis and that the 'False Negatives' (i.e. successful agent episodes) significantly hinder adversarial imitation learning, which is the first contribution of this paper. Then, we propose a method to alleviate the impact of false negatives and test it on the BabyAI environment. This method consistently improves sample efficiency over the baselines by at least an order of magnitude.",Combating False Negatives in Adversarial Imitation Learning,Proceedings of the International Joint Conference on Neural Networks,Conference Paper,7/18/2021,Yoshua,Bengio,2021
10.1109/IROS.2006.282632,"To operate outdoors or on non-flat surfaces, mobile robots need appropriate data structures that provide a compact representation of the environment and at the same time support important tasks such as path planning and localization. One such representation that has been frequently used in the past are elevation maps which store in each cell of a discrete grid the height of the surface in the corresponding area. Whereas elevation maps provide a compact representation, they lack the ability to represent vertical structures or even multiple levels. In this paper, we propose a new representation denoted as multi-level surface maps (MLS maps). Our approach allows to store multiple surfaces in each cell of the grid. This enables a mobile robot to model environments with structures like bridges, underpasses, buildings or mines. Additionally, they allow to represent vertical structures. Throughout this paper we present algorithms for updating these maps based on sensory input, to match maps calculated from two different scans, and to solve the loop-closing problem given such maps. Experiments carried out with a real robot in an outdoor environment demonstrate that our approach is well-suited for representing large-scale outdoor environments. © 2006 IEEE.",Multi-level surface maps for outdoor terrain mapping and loop closing,IEEE International Conference on Intelligent Robots and Systems,Conference Paper,12/1/2006,Wolfram,Burgard,2006
10.1109/IROS45743.2020.9340931,"© 2020 IEEE.In autonomous driving, accurately estimating the state of surrounding obstacles is critical for safe and robust path planning. However, this perception task is difficult, particularly for generic obstacles/objects, due to appearance and occlusion changes. To tackle this problem, we propose an end-to-end deep learning framework for LIDAR-based flow estimation in bird's eye view (BeV). Our method takes consecutive point cloud pairs as input and produces a 2-D BeV flow grid describing the dynamic state of each cell. The experimental results show that the proposed method not only estimates 2-D BeV flow accurately but also improves tracking performance of both dynamic and static objects.",PillarFlow: End-to-end birds-eye-view flow estimation for autonomous driving,IEEE International Conference on Intelligent Robots and Systems,Conference Paper,10/24/2020,Wolfram,Burgard,2020
10.1109/IROS47612.2022.9981703,"© 2022 IEEE.Existing Deep Learning (DL) frameworks typically do not provide ready-to-use solutions for robotics, where very specific learning, reasoning, and embodiment problems exist. Their relatively steep learning curve and the different methodologies employed by DL compared to traditional approaches, along with the high complexity of DL models, which often leads to the need of employing specialized hardware accelerators, further increase the effort and cost needed to employ DL models in robotics. Also, most of the existing DL methods follow a static inference paradigm, as inherited by the traditional computer vision pipelines, ignoring active perception, which can be employed to actively interact with the environment in order to increase perception accuracy. In this paper, we present the Open Deep Learning Toolkit for Robotics (OpenDR). OpenDR aims at developing an open, non-proprietary, efficient, and modular toolkit that can be easily used by robotics companies and research institutions to efficiently develop and deploy AI and cognition technologies to robotics applications, providing a solid step towards addressing the aforementioned challenges. We also detail the design choices, along with an abstract interface that was created to overcome these challenges. This interface can describe various robotic tasks, spanning beyond traditional DL cognition and inference, as known by existing frameworks, incorporating openness, homogeneity and robotics-oriented perception e.g., through active perception, as its core design principles.","OpenDR: An Open Toolkit for Enabling High Performance, Low Footprint Deep Learning for Robotics",IEEE International Conference on Intelligent Robots and Systems,Conference Paper,1/1/2022,Wolfram,Burgard,2022
10.1109/IROS47612.2022.9982071,"© 2022 IEEE.The automated cleaning of surfaces such as furniture, bathroom sinks, and even human bodies is challenging due to the three-dimensional nature of their geometries. Yet, enabling robots to effectively and safely perform these tasks would not only reduce user efforts spent on household cleaning chores, but would also alleviate the strenuous workload of caretakers as the elderly population continues to grow at an unprecedented rate. In this work, we unify the applications of wiping objects and bathing humans as a general contour-following problem. To this end, we utilize a depth camera-based soft tactile sensor to extract the contact geometries and force-correlated measures during interaction between the robot and the target object or body part, and design a general contour-following controller that not only maintains contact with the target throughout the cleaning process, but also regulates the amount of force applied. Our system enables successful cleaning of pipes, shelving, and even human limbs and torsos without the need for data-driven methods such as deep learning, upon which the majority of existing works have relied.",Soft Tactile Contour Following for Robot-Assisted Wiping and Bathing,IEEE International Conference on Intelligent Robots and Systems,Conference Paper,1/1/2022,Ruzena,Bajcsy,2022
10.1109/ISBI.2010.5490406,"In this paper, we present an unsupervised, automated technique for brain tissue segmentation based on multivariate magnetic resonance (MR) and spectroscopy images, for patients with gliomas. The algorithm uses spectroscopy data for coarse detection of the tumor region. Once the tumor area is identified, further processing is done on the FLAIR image in the neighborhood of the tumor to determine the hyper-intense abnormality in this region. Areas of contrast enhancement and necrosis are then identified by analyzing the FLAIR abnormality in gadolinium-enhanced T1-weighted images. The healthy brain tissue is then segmented into white matter, gray matter, and cerebrospinal fluid (CSF) using a hierarchical graphical model whose parameters are estimated using the EM algorithm. ©2010 IEEE.",Unsupervised segmentation of brain tissue in multivariate MRI,"2010 7th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, ISBI 2010 - Proceedings",Conference Paper,8/9/2010,Ruzena,Bajcsy,2010
10.1109/ISI.2009.5137331,"This paper describes our approach to assured information sharing. The research is being carried out under a MURI 9Multiuniversiyt Research Initiative) project funded by the Air Force Office of Scientific Research (AFOSR). The main objective of our project is: define, design and develop an Assured Information Sharing Lifecycle (AISL) that realizes the DoD's information sharing value chain. In this paper we describe the problem faced by the Department of Defense and our solution to developing an AISL System. ©2009 IEEE.",Assured information sharing life cycle,"2009 IEEE International Conference on Intelligence and Security Informatics, ISI 2009",Conference Paper,10/22/2009,Tim,Finin,2009
10.1109/ITAB.2009.5394447,"Conformal Prediction provides a framework for extending traditional Machine Learning algorithms, in order to complement predictions with reliable measures of confidence. The provision of such measures is significant for medical diagnostic systems, as more informed diagnoses can be made by medical experts. In this paper, we introduce a Conformal Predictor based on Genetic Algorithms, and we apply our method on the Wisconsin Breast Cancer Diagnosis (WBCD) problem. We give results in which we show that our method is efficient, in terms of accuracy, and can provide useful confidence measures. ©2009 IEEE.",Evolutionary conformal prediction for breast cancer diagnosis,"Final Program and Abstract Book - 9th International Conference on Information Technology and Applications in Biomedicine, ITAB 2009",Conference Paper,12/1/2009,Alexander,Gammerman,2009
10.1109/IV47402.2020.9304677,"© 2020 IEEE.Solving the driving scene perception problem for driver-assistance systems and autonomous vehicles requires accurate and robust performance in both regularly-occurring driving scenarios (termed 'common cases') and rare outlier scenarios (termed 'edge cases'). We propose an automated method for clustering common cases and detecting edge cases based on the visual characteristics of the external scene using deep learning. We apply this approach to develop a large-scale real-world video driving scene dataset of edge cases and common cases. This dataset consists of 1,156,592 10-second video clips, including 450 clusters of common cases, and 5,601 edge cases. We assign human-interpretable metadata labels (e.g., weather, lighting conditions) to the clusters through manual annotation. We further propose two automated methods for large-scale evaluation of scene segmentation models on naturalistic driving datasets that can capture potential system failures without human inspection. Video illustrations of select clusters will be made available to help with future research.",MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios,"IEEE Intelligent Vehicles Symposium, Proceedings",Conference Paper,1/1/2020,Lex,Fridman,2020
10.1109/IWW-BCI.2018.8311491,"© 2018 IEEE.We recorded high-density EEG in a flanker task experiment (31 subjects) and an online BCI control paradigm (4 subjects). On these datasets, we evaluated the use of transfer learning for error decoding with deep convolutional neural networks (deep ConvNets). In comparison with a regularized linear discriminant analysis (rLDA) classifier, ConvNets were significantly better in both intra- A nd inter-subject decoding, achieving an average accuracy of 84.1 % within subject and 81.7 % on unknown subjects (flanker task). Neither method was, however, able to generalize reliably between paradigms. Visualization of features the ConvNets learned from the data showed plausible patterns of brain activity, revealing both similarities and differences between the different kinds of errors. Our findings indicate that deep learning techniques are useful to infer information about the correctness of action in BCI applications, particularly for the transfer of pre-trained classifiers to new recording sessions or subjects.",Deep transfer learning for error decoding from non-invasive EEG,"2018 6th International Conference on Brain-Computer Interface, BCI 2018",Conference Paper,3/9/2018,Wolfram,Burgard,2018
10.1109/JAS.2020.1003545,"© 2014 Chinese Association of Automation.Transparency is a widely used but poorly defined term within the explainable artificial intelligence literature. This is due, in part, to the lack of an agreed definition and the overlap between the connected - sometimes used synonymously - concepts of interpretability and explain ability. We assert that transparency is the overarching concept, with the tenets of interpretability, explainability, and predictability subordinate. We draw on a portfolio of definitions for each of these distinct concepts to propose a human-swarm-teaming transparency and trust architecture (HST3-Architecture). The architecture reinforces transparency as a key contributor towards situation awareness, and consequently as an enabler for effective trustworthy human-swarm teaming (HST).",Human-Swarm-Teaming Transparency and Trust Architecture,IEEE/CAA Journal of Automatica Sinica,Article,7/1/2021,Hussein,Abbass,2021
10.1109/JCDL.2017.7991559,"© 2017 IEEE.Citations implicitly encode a community's judgment of a paper's importance and thus provide a unique signal by which to study scientific impact. Efforts in understanding and refining this signal are reflected in the probabilistic modeling of citation networks and the proliferation of citation-based impact measures such as Hirsch's h-index. While these efforts focus on understanding the past and present, they leave open the question of whether scientific impact can be predicted into the future. Recent work addressing this deficiency has employed linear and simple probabilistic models; we show that these results can be handily outperformed by leveraging non-linear techniques. In particular, we find that these AI methods can predict measures of scientific impact for papers and authors, namely citation rates and h-indices, with surprising accuracy, even 10 years into the future. Moreover, we demonstrate how existing probabilistic models for paper citations can be extended to better incorporate refined prior knowledge. While predictions of scientific impact should be approached with healthy skepticism, our results improve upon prior efforts and form a baseline against which future progress can be easily judged.",Learning to Predict Citation-Based Impact Measures,Proceedings of the ACM/IEEE Joint Conference on Digital Libraries,Conference Paper,7/25/2017,Oren,Etzioni,2017
10.1109/JCDL.2019.00039,"© 2019 IEEE.Present day peer review is a time-consuming process and is still the only gatekeeper of scientific knowledge and wisdom. However, the rapid increase in research article submissions these days across different fields is posing significant challenges to the current system. Hence the incorporation of Artificial Intelligence (AI) techniques to better streamline the existing peer review system is an immediate need in this age of rapid scientific progress. Among many, one particular challenge these days is that the journal editors and conference program chairs are overwhelmed with the ever-increasing rise in article submissions. Studies show that a lot many submissions are not well-informed and do not fit within the scope of the intended journal or conference. Here in this work, we embark on to investigate how an AI could assist the editors and program chairs in identifying potential out-of-scope submissions based on the past accepted papers of the particular journal or conference. We design a multimodal deep neural architecture and investigate the role of every possible channel of information in a research article (full-text, bibliography, images) to determine its appropriateness to the concerned venue. Our approach does not involve any handcrafted features, solely depends on the past accepting activity of the venue, and thereby achieves significant performance on two real-life datasets. Our findings suggest that a system of this kind is possible and with reasonable accuracy could assist the editors/chairs in flagging out inappropriate submissions.",A deep multimodal investigation to determine the appropriateness of scholarly submissions,Proceedings of the ACM/IEEE Joint Conference on Digital Libraries,Conference Paper,6/1/2019,Pushpak,Bhattacharyya,2019
10.1109/JPROC.2006.876969,"Our understanding of social insect behavior has significantly influenced artificial intelligence (AD and multirobot systems' research (e.g., ant algorithms and swarm, robotics). In this work, however, we focus on the opposite question: ""How can multirobot systems research contribute to the understanding of social animal behavior?"" As we show, we are able to contribute at several levels. First, using algorithms that originated in the robotics community, we can track animals under observation to provide essential quantitative data for animal behavior research. Second, by developing and applying algorithms originating in speech recognition and computer vision, we can automatically label the behavior of animals under observation. In some cases the automatic labeling is more accurate and consistent than manual behavior identification. Our ultimate goal, however, is to automatically create, from observation, executable models of behavior. An executable model is a control program for an agent that can run in simulation (or on a robot). The representation for these executable models is drawn from research in multirobot systems programming. In this paper we present the algorithms we have developed for tracking, recognizing, and learning models of social animal behavior, details of their implementation, and quantitative experimental results using them to study social insects. © 2006 IEEE.",How multirobot systems research will accelerate our understanding of social animal behavior,Proceedings of the IEEE,Article,1/1/2006,Frank,Dellaert,2006
10.1109/JSEN.2021.3091471,"© 2001-2012 IEEE.The liver is a vital human body organ and its functionality can be degraded by several diseases such as hepatitis, fatty liver disease, and liver cancer and so forth. Hence, the early diagnosis of liver diseases is extremely crucial for saving human lives. With the rapid development of multimedia technology, it is now possible to design and implement a non-invasive system that can chronic liver diseases. For this purpose, machine learning and Artificial Intelligence (AI) have been used within the past few years. In this regard, digital image processing supported by AI methods has been implemented in the diagnosis of diseases that also showed high reliability. Therefore, in this paper, an iris feature-based non-invasive technique is proposed by incorporating a novel machine-learning algorithm. The experimental setup involved data set for the models' training included 879 subjects from Pakistan, of which 453 subjects have chronic liver disease and 426 are healthy. The iris images were collected using an infrared camera that consists of a lens, a thermal sensor and digital electronics processing. The lens focuses on the infrared energy on the sensor, using distinctive forms of features twenty-two physiological and thirty-three iris features. The designed classification model for a non-invasive system combined eleven different classifiers and used cross-validation techniques for comparing the results. The overall performance of the model was analyzed using five parameters: accuracy, precision, F-score, specificity, and sensitivity. The results confirmed that the proposed non-invasive model is capable of predicting chronic liver diseases with 98% of accuracy.",Infrared Sensing Based Non-Invasive Initial Diagnosis of Chronic Liver Disease Using Ensemble Learning,IEEE Sensors Journal,Article,9/1/2021,Fatmah,Baothman,2021
10.1109/JSYST.2017.2681918,"© 2017 IEEE.The papers in this special section presents state-of-the-art research in the area of human-level artificial intelligence and robotic systems from a complex systems perspective. These papers aim to explore questions such as: What is a reasonable cognitive framework? How to build an embodied robotics system? How complex is human-level intelligence? What kinds of subsystems must be integrated to achieve human-level intelligence? To what degree does human-level intelligence rely on emergent properties? To what extent is complexity science (attractors, self-organizing criticality, network theory, and so on) useful for understanding human-level intelligence? To what extent can human-level intelligence be achieved by integrating components that have been engineered, tested, and perhaps commercially deployed for other, more specialized purposes?.",Guest Editorial Special Issue on Human-Like Intelligence and Robotics,IEEE Systems Journal,Article,9/1/2017,Ben,Goertzel,2017
10.1109/LRA.2016.2634089,"© 2016 IEEE.Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labeled, typically) training data are required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong three-dimensional generative model to automatically label correspondences in RGB-D video data. A fully convolutional network is trained using a contrastive loss to produce viewpoint-and lighting-invariant descriptors. As a proof of concept, we collected two datasets: The first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects rearranged within. Our datasets focus on revisitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes toward identifying nonlabeled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-The-Art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.",Self-Supervised Visual Descriptor Learning for Dense Correspondence,IEEE Robotics and Automation Letters,Article,4/1/2017,Dieter,Fox,2017
10.1109/LRA.2018.2869640,"© 2018 IEEE.Semantic understanding and localization are fundamental enablers of robot autonomy that have been tackled as disjoint problems for the most part. While deep learning has enabled recent breakthroughs across a wide spectrum of scene understanding tasks, its applicability to state estimation tasks has been limited due to the direct formulation that renders it incapable of encoding scene-specific constrains. In this letter, we propose the VLocNet++ architecture that employs a multitask learning approach to exploit the inter-task relationship between learning semantics, regressing 6-DoF global pose and odometry, for the mutual benefit of each of these tasks. Our network overcomes the aforementioned limitation by simultaneously embedding geometric and semantic knowledge of the world into the pose regression network. We propose a novel adaptive weighted fusion layer to aggregate motion-specific temporal information and to fuse semantic features into the localization stream based on region activations. Furthermore, we propose a self-supervised warping technique that uses the relative motion to warp intermediate network representations in the segmentation stream for learning consistent semantics. Finally, we introduce a first-of-a-kind urban outdoor localization dataset with pixel-level semantic labels and multiple loops for training deep networks. Extensive experiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc dataset demonstrate that our approach exceeds the state-of-the-art outperforming local feature-based methods while simultaneously performing multiple tasks and exhibiting substantial robustness in challenging scenarios.",VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry,IEEE Robotics and Automation Letters,Article,10/1/2018,Wolfram,Burgard,2018
10.1109/LRA.2022.3222956,"© 2016 IEEE.Visual Inertial Odometry (VIO) is one of the most established state estimation methods for mobile platforms. However, when visual tracking fails, VIO algorithms quickly diverge due to rapid error accumulation during inertial data integration. This error is typically modeled as a combination of additive Gaussian noise and a slowly changing bias which evolves as a random walk. In this work, we propose to train a neural network to learn the true bias evolution. We implement and compare two common sequential deep learning architectures: LSTMs and Transformers. Our approach follows from recent learning-based inertial estimators, but, instead of learning a motion model, we target IMU bias explicitly, which allows us to generalize to locomotion patterns unseen in training. We show that our proposed method improves state estimation in visually challenging situations across a wide range of motions by quadrupedal robots, walking humans, and drones. Our experiments show an average 15% reduction in drift rate, with much larger reductions when there is total vision failure. Importantly, we also demonstrate that models trained with one locomotion pattern (human walking) can be applied to another (quadruped robot trotting) without retraining.",Deep IMU Bias Inference for Robust Visual-Inertial Odometry With Factor Graphs,IEEE Robotics and Automation Letters,Article,1/1/2023,Frank,Dellaert,2023
10.1109/LRA.2022.3229236,"© 2022 IEEE.Accurate value estimates are important for off-policy reinforcement learning. Algorithms based on temporal difference learning typically are prone to an over- or underestimation bias building up over time. In this letter, we propose a general method called Adaptively Calibrated Critics (ACC) that uses the most recent high variance but unbiased on-policy rollouts to alleviate the bias of the low variance temporal difference targets. We apply ACC to Truncated Quantile Critics [1], which is an algorithm for continuous control that allows regulation of the bias with a hyperparameter tuned per environment. The resulting algorithm adaptively adjusts the parameter during training rendering hyperparameter search unnecessary and sets a new state of the art on the OpenAI gym continuous control benchmark among all algorithms that do not tune hyperparameters for each environment. ACC further achieves improved results on different tasks from the Meta-World robot benchmark. Additionally, we demonstrate the generality of ACC by applying it to TD3 [2] and showing an improved performance also in this setting.",Adaptively Calibrated Critic Estimates for Deep Reinforcement Learning,IEEE Robotics and Automation Letters,Article,2/1/2023,Wolfram,Burgard,2023
10.1109/MC.2005.350,"Most knowledge on the Web is presented as natural-language text with occassional pictures and graphics. Although it is convenient for human users to read and view, this format limits the indexing capabilities of state-of-the-art search engines because they cannot infer meaning. Thus, users share a significant burden in terms of constructing search queries intelligently. Even with increased use of XML-encoded information, computers still must use application-dependent semantics to process the tags and literal symbols. Developers must experiment with how much and where a Semantic Web search engine should reason over the contents of documents and queries. © 2005 IEEE.",Search on the Semantic Web,Computer,Article,10/1/2005,Tim,Finin,2005
10.1109/MC.2022.3148714,"© 1970-2012 IEEE.Machine learning (ML) workloads have rapidly grown, raising concerns about their carbon footprint. We show four best practices to reduce ML training energy and carbon dioxide emissions. If the whole ML field adopts best practices, we predict that by 2030, total carbon emissions from training will decline.","The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",Computer,Article,7/1/2022,Jeff,Dean,2022
10.1109/MCG.2020.3025425,"© 1981-2012 IEEE.Public awareness and concern about climate change often do not match the magnitude of its threat to humans and our environment. One reason for this disagreement is that it is difficult to mentally simulate the effects of a process as complex as climate change and to have a concrete representation of the impact that our individual actions will have on our own future, especially if the consequences are long term and abstract. To overcome these challenges, we propose to use cutting-edge artificial intelligence (AI) approaches to develop an interactive personalized visualization tool, the AI climate impact visualizer. It will allow a user to enter an address-be it their house, their school, or their workplace - and it will provide them with an AI-imagined possible visualization of the future of this location in 2050 following the detrimental effects of climate change such as floods, storms, and wildfires. This image will be accompanied by accessible information regarding the science behind climate change, i.e., why extreme weather events are becoming more frequent and what kinds of changes are happening on a local and global scale.",Using Artificial Intelligence to Visualize the Impacts of Climate Change,IEEE Computer Graphics and Applications,Article,1/1/2021,Yoshua,Bengio,2021
10.1109/MCI.2010.939578,"One of the main challenges when introducing the concept of an Agent to newcomers in the field of Multi-agent Systems (MAS) is to demarcate what an agent is and what an agent is not. The word Agent has been used by different groups of researchers in areas ranging from Software Engineering and Artificial Intelligence (AI) to Political Sciences, Economics and Social Sciences. Software engineers relate the concept of an agent to the concept of objects in Object Oriented Programming (OOP). © 2011 IEEE.","Computational red teaming: Past, present and future",IEEE Computational Intelligence Magazine,Conference Paper,2/1/2011,Hussein,Abbass,2011
10.1109/MCI.2013.2247823,"More than 15 years after the early studies in Affective Computing (AC), [1] the problem of detecting and modeling emotions in the context of human-computer interaction (HCI) remains complex and largely unexplored. The detection and modeling of emotion is, primarily, the study and use of artificial intelligence (AI) techniques for the construction of computational models of emotion. The key challenges one faces when attempting to model emotion [2] are inherent in the vague definitions and fuzzy boundaries of emotion, and in the modeling methodology followed. In this context, open research questions are still present in all key components of the modeling process. These include, first, the appropriateness of the modeling tool employed to map emotional manifestations and responses to annotated affective states; second, the processing of signals that express these manifestations (i.e., model input); and third, the way affective annotation (i.e., model output) is handled. This paper touches upon all three key components of an affective model (i.e., input, model, output) and introduces the use of deep learning (DL) [3], [4], [5] methodologies for affective modeling from multiple physiological signals. © 2005-2012 IEEE.",Learning deep physiological models of affect,IEEE Computational Intelligence Magazine,Article,4/22/2013,Yoshua,Bengio,2013
10.1109/MIS.2006.56,"The characteristics of autonomous agents that can solve multiple and evolving goals in a complex, messy environments in collaboration with some agents and in conflicts with others, are discussed. To achieve the ambitious applications, the representations must be a fluent, and must evolve under machine control. Automatic representation development, evolution, and repair must be a major goal of AI research for the years to come. A reasoning system must able to develop, evolve, and repair their underlying representations as well as reasons with them. If a reasoning system is restricted to a fixed ontology, it will rapidly get out of date. McNeill's approach assumes that there has been an attempt to standardize ontologies, and with any sufficient large agent community, small differences between the ontologies will be inevitable. McNeill's ontology refinement system (ORS) identifies and repairs these ontological mismatch at runtime.",Representation as a fluent: An AI challenge fer the next half century,IEEE Intelligent Systems,Review,5/1/2006,Alan,Bundy,2006
10.1109/MIS.2009.71,"The Companion cognitive architecture supports experiments in achieving human-level intelligence. The seven key features of cognitive architecture include analogical processing, extensive conceptual knowledge, flexible reasoning, and coarse-grained distributed implementation, broad learning at multiple levels, continuous operation, and natural interaction. The model for analogical matching is the Structure-Mapping Engine (SME) that computes mappings using algorithm, operating in polynomial time. The model for similarity-based retrieval is many are called/few are chosen (MAC/FAC) that takes as input a probe and a case library. Another design goal for Companions is to emulate the parallelism that's evident in human behavior. Companions are implemented as distributed systems that allocate individual nodes of a cluster computer to semi-independent, asynchronous processes (agents). Agents communicate internally using the Knowledge Query and Manipulation Language (KQML) with callbacks to support asynchronous queries and subscriptions to events.",Companion cognitive systems: Design goals and lessons learned so far,IEEE Intelligent Systems,Article,1/1/2009,Ken,Forbus,2009
10.1109/MITP.2004.92,"The changes that are expected to take place in the field of information technology by 2010 are evaluated. As regards peer set nodes and paths, by 2010, each member of a peer set would be having its own international model of what its environment is supposed to look like. 2010 would also experience the evolution of an unusual devious spyware, the HIV-like attackers (shivas), named in reference to their AIDS like ability to take over a computer's immune system. Another interesting feature that is likely to take place in 2010 is a partial collapse of the international patent system, centering around claims of 'patent feudalism' in software and biotechnology industries.",Software in the year 2010,IT Professional,Review,11/1/2004,Terry,Bollinger,2004
10.1109/MM.2018.112130030,"© 1981-2012 IEEE.The end of Moores law and Dennard scaling has led to the end of rapid improvement in general-purpose program performance. Machine learning (ML), and in particular deep learning, is an attractive alternative for architects to explore. It has recently revolutionized vision, speech, language understanding, and many other fields, and it promises to help with the grand challenges facing our society. The computation at its core is low-precision linear algebra. Thus, ML is both broad enough to apply to many domains and narrow enough to benefit from domain-specific architectures, such as Googles Tensor Processing Unit (TPU). Moreover, the growth in demand for ML computing exceeds Moores law at its peak, just as it is fading. Hence, ML experts and computer architects must work together to design the computing systems required to deliver on the potential of ML. This article offers motivation, suggestions, and warnings to computer architects on how to best contribute to the ML revolution.",A New Golden Age in Computer Architecture: Empowering the Machine-Learning Revolution,IEEE Micro,Article,3/1/2018,Jeff,Dean,2018
10.1109/MRA.2005.1458329,"TOURBOT and WebFAIR are two projects funded by the European Union (EU) to address the development of mobile robots capable of operating in populated environments in order to provide alternative means of interaction with local and remote visitors. The former pursued the development of an interactive tour-guide robot able to provide individual access to museums' exhibits over the Internet. The latter introduced teleconferencing between the remote user and on-site attendants and employed a multirobot platform, thus facilitating simultaneous robot control by multiple users. Technical developments in the framework of both projects resulted in robust and reliable systems that have been demonstrated and validated in real-world conditions.",Tourbot and WebFAIR: Web-operated mobile robots for tele-presence in populated exhibitions,IEEE Robotics and Automation Magazine,Article,6/1/2005,Wolfram,Burgard,2005
10.1109/MRA.2018.2873029,"© 1994-2011 IEEE.Discusses ways in which scientists and technologies should address the current and future use of artificial intelligence and robotics in the education system as well as its implications for both society and ethical considerations. We should very clearly spell out under what assumptions our technologies will or will not work. We must clarify the range of parameters for which the systems we build will be safe and reliable. Error analysis, robustness, and privacy guarantees should be the standards of such a system's descriptions. Recently, I have observed that we've adopted quite a bit of marketing methodology instead of adhering to scientific clarity and sobriety. In the past, 50 years ago, academic presentations were strictly technical, and marketing was the preserve of business people. Today, perhaps we have gone to another extreme.","Robotics in Service and the Responsibility to Society [Ethical, Legal, and Societal Issues]",IEEE Robotics and Automation Magazine,Review,12/1/2018,Ruzena,Bajcsy,2018
10.1109/MTS.2020.2967486,© 1982-2012 IEEE.Examines ethical principles and guidelines that surround machine learning and artificial intelligence.,On the Morality of Artificial Intelligence [Commentary],IEEE Technology and Society Magazine,Review,3/1/2020,Yoshua,Bengio,2020
10.1109/MWCN.2002.1045711,© 2002 IEEE.This paper proposes a novel distributed service discovery protocol for mobile ad hoc networks. The protocol is based on the concept of peer-to-peer caching of service advertisements and group-based intelligent forwarding of service requests. It does not require a service to register to a registry or lookup server. Services are described using an ontology based on the DARPA agent markup language (DAML+OIL). We exploit the semantic class/subclass hierarchy of DAML to describe service groups and use this semantic information to selectively forward service requests to respective nodes. DAML-based service description helps us in achieving increased flexibility in service matching. We also present simulation results of our protocol and show that our protocol achieves increased efficiency in discovering services by efficiently utilizing bandwidth by controlling forwarding of service requests.,GSD: A novel group-based service discovery protocol for MANETS,"2002 4th International Workshop on Mobile and Wireless Communications Network, MWCN 2002",Conference Paper,1/1/2002,Tim,Finin,2002
10.1109/PROC.1979.11436,"Computer-based models of medical decision making account for a large portion of clinical computing efforts. This article reviews representative examples from each of several major medical computing paradigms. These include 1) clinical algorithms, 2) clinical databanks that include analytic functions, 3) mathematical models of physical processes, 4) pattern recognition, 5) Bayesian statistics, 6) decision analysis, and 7) symbolic reasoning or artificial intelligence. Because the techniques used in the various systems cannot be examined exhaustively, the case studies in each category are used as a basis for studying general strengths and limitations. It is noted that no one method is best for all applications. However, emphasis is given to the limitations of early work that have made artificial intelligence techniques and knowledge engineering research particularly attractive. We stress that considerable basic research in medical computing remains to be done and that powerful new approaches may lie in the melding of two or more established techniques. © 1979 IEEE.",Knowledge Engineering for Medical Decision Making: A Review of Computer-Based Clinical Decision Aids,Proceedings of the IEEE,Article,1/1/1979,Edward,Feigenbaum,1979
10.1109/ROMAN.2015.7333654,"© 2015 IEEE.The protean word 'autonomous' has gained broad currency as a descriptive adjective for AI research projects, robotic and otherwise. Depending upon context, 'autonomous' at present connotes anything from a shallow, purely reactive system to a sophisticated cognitive architecture reflective of much of human cognition; hence the term fails to pick out any specific set of constitutive functionality. However, philosophers and ethicists have something relatively well-defined in mind when they talk about the idea of autonomy. For them, an autonomous agent is often by definition potentially morally responsible for its actions. Moreoever, as a prerequisite to correct ascription of 'autonomous,' a certain capacity to choose freely is assumed - even if this freedom is understood to be semi-constrained by societal conventions, moral norms, and the like.",Constraints on freely chosen action for moral robots: Consciousness and control,Proceedings - IEEE International Workshop on Robot and Human Interactive Communication,Conference Paper,11/20/2015,Selmer,Bringsjord,2015
10.1109/ROMAN.2015.7333698,"© 2015 IEEE.Self-consciousness would seem to be a sine qua non for moral competence in a social world. You and we are morally competent in no small part because you know what you ought to do, and we know what we ought to do. A mouse, in contrast, cannot say to itself: 'I ought to share this cheese, even if my brother refuses to do so.' But can robots be self-conscious? Approaching this question from the standpoint of so-called Psychometric AI, we note that prior work by Govindarajulu and Bringsjord led to the engineering of a robot (Cogito) able to provably pass the famous mirror test of self-consciousness. But a more challenging test for robot self-consciousness has been provided by Floridi; this test is an ingenious and much-harder variant of the well-known-in-AI wise-man puzzle: Each of three robots is given one pill from a group of five, three of which are innocuous, but two of which, when taken, immediately render the recipient dumb. In point of fact, two robots (R1 and R2) are given potent pills, but R3 receives one of the three placebos. The human tester says: 'Which pill did you receive? No answer is correct unless accompanied by a proof!' Given a formal regimentation of this test previously formulated by Bringsjord, it can be proved that, in theory, a future robot represented by R3 can answer provably correctly (which for plausible reasons, explained by Floridi, entails that R3 has satisfied some of the structural requirements for self-consciousness). In this paper we explain and demonstrate the engineering that now makes this theoretical possibility actual, both in the simulator known as 'PAGI World' (used for testing AIs), and in real (= physical) robots interacting with a human tester. These demonstrations involve scenarios that demand the passing of Floridi's test for self-consciousness, where for us, passing such a test is required for an agent to be regarded morally competent.",Real robots that pass human tests of self-consciousness,Proceedings - IEEE International Workshop on Robot and Human Interactive Communication,Conference Paper,11/20/2015,Selmer,Bringsjord,2015
10.1109/SCAM.2003.1238041,"© 2003 IEEE.We present a method for unique renaming declarations and references in Java programs using source transformation to XML markup. Each entity declaration and reference in the Java program is assigned a globally unique identifier (UID) based on its declaration scope and file. The UID serves as a key by which the entity's original declaration and all references can be found, and more importantly, by which information about the entity can be stored or retrieved from the design database. The resulting uniquely renamed source code makes it convenient and efficient to do further business logic and technical analysis that crosses the boundary between source code and the design database. UIDs are attached to entity references in the source code using XML markup, so that both the UID and the original source text of the declaration or reference are available in the renamed source program. While it is possible to generate unique names in an ad hoc manner, we show how to generate them using a combination of source transformations and design database inferences. This ensures that the notion of UID is consistent and well defined.",Unique renaming of Java using source transformation,"Proceedings - 3rd IEEE International Workshop on Source Code Analysis and Manipulation, SCAM 2003",Conference Paper,1/1/2003,Thomas,Dean,2003
10.1109/SMC.2018.00106,"© 2018 IEEE.Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks' learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.",Intracranial Error Detection via Deep Learning,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",Conference Paper,1/16/2019,Wolfram,Burgard,2019
10.1109/SMC.2018.00188,"© 2018 IEEE.EEG signals could reveal unique information of an individual's brain activities. They have been regarded as one of the most promising biometric signals for person identification and verification. Steady-State Visual Evoked Potentials (SSVEPs), as EEG responses to visual stimulations at specific frequencies, could provide biometric information. However, current methods on SSVEP biometrics with hand-crafted power spectrum features and canonical correlation analysis (CCA) present only a limited range of individual distinctions and suffer relatively low accuracy. In this paper, we propose convolution neural networks (CNNs) with raw SSVEPs for person identification and verification without the need for any hand-crafted features. We conduct a comprehensive comparison between the performance of CNN with raw signals and a number of classical methods on two SSVEP datasets consisting of four and ten subjects, respectively. The proposed method achieved an averaged identification accuracy of 96.8%±0.01, which outperformed the other methods by an average of 45.5% (p-value < 0.05). In addition, it achieved an averaged False Acceptance Rate (FAR) of 1.53%±0.01 and True Acceptance Rate (TAR) of 97.09%±0.02 for person verification. The averaged verification accuracy is 98.34% ± 0.01, which outperformed the other methods by an average of 11.8% (p-value < 0.05). The proposed method based on deep learning offers opportunities to design a general-purpose EEG-based biometric system without the need for complex pre-processing and feature extraction techniques, making it feasible for real-time embedded systems.",Convolution Neural Networks for Person Identification and Verification Using Steady State Visual Evoked Potential,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",Conference Paper,1/16/2019,Hussein,Abbass,2019
10.1109/SMC.2019.8914378,"© 2019 IEEE.Human and Autonomous control of a swarm of robots is a non-trivial task compounded by the multi-agent nature of a swarm and the tight-coupling in swarm dynamics. Our previous work has been successful in designing controllers to shepherd a swarm of sheep by learning from human demonstrations. However, humans could get disengaged easily if the platform to collect demonstrations is uninteresting. In this paper, we propose a smartphone based Shepherding game as a platform to collect data on the behaviour of Shepherding models. The game is designed to allow both humans and artificial intelligence (AI) empowered shepherds to control the swarm of sheep. The game offers a plug-and-play ability for different human and AI controllers of the shepherd. The game logs all movements made by the human during controlling the shepherd for use by supervised learning algorithms to develop autonomous controllers. We present and evaluate different components of the game.",Cyber-shepherd: A smartphone-based game for human and autonomous swarm control,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",Conference Paper,10/1/2019,Hussein,Abbass,2019
10.1109/SMC52423.2021.9658859,"© 2021 IEEE.Image captioning is a multi-modal problem linking computer vision and natural language processing, which combines image analysis and text generation challenges. In the literature, most of the image captioning works have been accomplished in the English language only. This paper proposes a new approach for image captioning in the Hindi language using deep learning-based encoder-decoder architecture. Hindi, widely spoken in India and South Asia, is the fourth most spoken language globally; it is India's official language. In recent years, significant advancement has been made in image captioning, utilizing encoder-decoder architectures based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Encoder CNN extracts features from input images, whereas decoder RNN performs language modeling. The proposed encoder-decoder architecture utilizes information multiplexing in the encoder CNN to achieve a performance gain in feature extraction. Extensive experimentation is carried out on the benchmark MSCOCO Hindi dataset, and significant improvements in BLEU score are reported compared to the baselines. Manual human evaluation in terms of adequacy and fluency of the generated captions further establishes the proposed method's efficacy in generating good quality captions.",An Information Multiplexed Encoder-Decoder Network for Image Captioning in Hindi,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",Conference Paper,1/1/2021,Pushpak,Bhattacharyya,2021
10.1109/SSCI.2016.7850043,"© 2016 IEEE.Artifacts such as voluntarily and involuntarily muscle movements are usually seen as a source of noise in EEG signals. In this paper, we see artifacts as a source of information in a signal. For example, eye movements can generate a traceable change in the EEG signals. We use eye movements as an effective marker for direction of movement. We propose two experiments for classification of four eye movement directions (left, right, up and down). In the first experiment, we utilize feature partitioning method based on J48 decision tree to tackle the effect of concept drift in the training dataset resulting from dynamic non-stationarity characteristics of EEG signals. Afterward, we feed the extracted partitions to three different classifiers: multilayer perceptron (MLP) (with 10 hidden layers), logistic regression (LR) and random forest decision tree (RFDT) respectively, while comparing their classification accuracy. In the second experiment, we explored an ensemble learning mechanism as an alternative criterion to deal with the dynamic nature EEG signals. We trained the last three classifiers simultaneously on each training example, followed by a voting method to determine the dominant class label. The ensemble approach increased classification accuracy from 86.2% in the first experiment to 90.1% in the second.",Eye movements as information markers in EEG data,"2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016",Conference Paper,2/9/2017,Hussein,Abbass,2017
10.1109/SSCI.2017.8285387,"© 2017 IEEE.Ground-Air coordination is a very complex environment for a machine learning algorithm. We focus on the case where an Unmanned Aerial Vehicle (UAV) needs to support a group of Unmanned Ground Vehicles (UGVs). The UAV is required to broadcast an image that contains all UGVs, thus, offering a bird-eye-view on the group as a whole. The source of complexity in this task is twofold. First, coordination needs to occur without communication between the UAV and UGVs. Second, the ability of the UAV to sense the UGVs is coupled with the ability of the UAV to learn how to track laterally the UGVs and adapt its vertical position so that the images of the UGVs are appropriately spaced within the camera field of view. In this paper, we propose using the Deep Actor Network component of an Actor-Critic Deep Reinforcement Learning architecture as a supervised learner. The advantage of this approach is that it offers a step towards autonomous learning whereby the full Actor-Critic model can be utilized in the future. Human demonstrations are collected for the deep Actor network to learn from. The system is built using the Gazebo Simulator, Robot Operating System, and the OpenAI Gym. We show that the proposed setup is able to train the UAV to follow the UGVs while maintaining all UGVs within camera range in situations where UGVs are performing complex maneuvers.",Supervised deep actor network for imitation learning in a ground-air UAV-UGVs coordination task,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings",Conference Paper,2/2/2018,Hussein,Abbass,2018
10.1109/SSCI44817.2019.9002756,"© 2019 IEEE.Apprenticeship learning (AL) is a learning scheme using demonstrations collected from human operators. Apprenticeship learning via inverse reinforcement learning (AL via IRL) has been used as one of the primary candidate approaches to obtain a near optimal policy that is as good as that of the human policy. The algorithm works by attempting to recover and approximate the human reward function from the demonstrations. This approach assists in overcoming limitations such as the sensitivity associated with the variance in the quality of human data and the short sighted decision time that does not consider future states. However, addressing the problem of continuous action and state spaces has still been challenging in the AL via IRL algorithm. In this paper, we propose a new AL via IRL approach that is able to work with continuous action and state spaces. Our approach is used to train an artificial intelligence (AI) agent acting as a shepherd of artificial sheep-inspired swarm agents in a complex and dynamic environment. The results show that the performance of our approach is as good as that of the human operator, and particularly, the agent's movements seem to be smoother and more effective.",Apprenticeship Learning for Continuous State Spaces and Actions in a Swarm-Guidance Shepherding Task,"2019 IEEE Symposium Series on Computational Intelligence, SSCI 2019",Conference Paper,12/1/2019,Hussein,Abbass,2019
10.1109/SSP.2009.5278515,"Modelling of interferometric signals related to tear film surface quality is considered. In the context of tear film surface quality estimation in normal healthy eyes, two clinical parameters are of interest: the build-up time, and the average interblink surface quality. The former is closely related to the signal derivative while the latter to the signal itself. Polynomial signal models, chosen for a particular set of noisy interferometric measurements, can be optimally selected, in some sense, with a range of information criteria such as AIC, MDL, CP, and CME. Those criteria, however, do not always guarantee that the true derivative of the signal is accurately represented and they often overestimate it. Here, a practical method for judicious selection of model order in a polynomial fitting to a signal is proposed so that the derivative of the signal is adequately represented. The paper highlights the importance of context-based signal modelling in model order selection. © 2009 IEEE.",Context-based modelling of interferometric signals for the assessment of tear-film surface quality,IEEE Workshop on Statistical Signal Processing Proceedings,Conference Paper,12/25/2009,Michael,Collins,2009
10.1109/TAFFC.2019.2926724,"© 2010-2012 IEEE.We propose a multi-task ensemble framework that jointly learns multiple related problems. The ensemble model aims to leverage the learned representations of three deep learning models (i.e., CNN, LSTM and GRU) and a hand-crafted feature representation for the predictions. Through multi-task framework, we address four problems of emotion and sentiment analysis, i.e., 'emotion classification intensity', 'valence, arousal dominance for emotion', 'valence arousal for sentiment', and '3-class categorical 5-class ordinal classification for sentiment'. The underlying problems cover two granularity (i.e., coarse-grained and fine-grained) and a diverse range of domains (i.e., tweets, Facebook posts, news headlines, blogs, letters etc.). Experimental results suggest that the proposed multi-task framework outperforms the single-task frameworks in all experiments.","All-in-One: Emotion, Sentiment and Intensity Prediction Using a Multi-Task Ensemble Framework",IEEE Transactions on Affective Computing,Article,1/1/2022,Pushpak,Bhattacharyya,2022
10.1109/TALE52509.2021.9678853,"© 2021 IEEE.Artificial Intelligence (AI) and machine learning (ML) are having a great impact on all aspects of society. However, due to the technical competencies and mathematical understanding required for implementing solutions leveraging these technologies, access to the communities working on these technologies is limited to those having these skills. This limits the ability of domain experts to directly transfer their knowledge and contribute to the development of AI and ML systems. To address this problem, we propose the Human Education AI Teaming (HEAT) framework, in which we draw on human education to design an innovative education system to enable collaboration between humans and AI cognitive agents. The main aim of HEAT is to promote the social integration of AI by allowing domain experts to focus more on communicating a body of knowledge to the machine, and less on the computational, data, and engineering concepts associated with how the machine learns. We follow an educational theory-driven approach to derive the content knowledge and competencies required by each agent. We conclude the paper with a demonstration case study explaining how the complex autonomous guidance of a flock of sheep could leverage HEAT to make the technology accessible by empowering non-AI specialists, livestock farmers in our example.",Towards a systematic educational framework for human-machine teaming,"TALE 2021 - IEEE International Conference on Engineering, Technology and Education, Proceedings",Conference Paper,1/1/2021,Hussein,Abbass,2021
10.1109/TASE.2021.3064065,"© 2004-2012 IEEE.To Perform reliably and consistently over sustained periods of time, large-scale automation critically relies on computer simulation. Simulation allows us and supervisory AI to effectively design, validate, and continuously improve complex processes, and helps practitioners to gain insight into the operation and justify future investments. While numerous successful applications of simulation in industry exist, such as circuit simulation, finite element methods, and computeraided design (CAD), state-of-The-Art simulators fall short of accurately modeling physical phenomena, such as friction, impact, and deformation.",Sim2Real in Robotics and Automation: Applications and Challenges,IEEE Transactions on Automation Science and Engineering,Review,4/1/2021,Dieter,Fox,2021
10.1109/TASLP.2015.2487051,"© 2014 IEEE.The search of a small acoustic feature set for emotion recognition faces three main challenges. Such a feature set must be robust to large diversity of contexts in real-life applications; model parameters must also be optimized for reduced subsets; finally, the result of feature selection must be evaluated in cross-corpus condition. The goal of the present study is to select a consensual set of acoustic features for valence recognition using classification and non-classification based feature ranking and cross-corpus experiments, and to optimize emotional models simultaneously. Five realistic corpora are used in this study: three of them were collected in the framework of the French project on robotics ROMEO, one is a game corpus (JEMO) and one is the well-known AIBO corpus. Combinations of features found with non-classification based methods (information gain and Gaussian mixture models with Bhattacharyya distance) through multi-corpora experiments are tested under cross-corpus conditions, simultaneously with SVM parameters optimization. Reducing the number of features goes in pair with optimizing model parameters. Experiments carried on randomly selected features from two acoustic feature sets show that a feature space reduction is needed to avoid over-fitting. Since a Grid search tends to find non-standard values with small feature sets, the authors propose a multi-corpus optimization method based on different corpora and acoustic feature subsets which ensures more stability. The results show that acoustic families selected with both feature ranking methods are not relevant in cross-corpus experiments. Promising results have been obtained with a small set of 24 voiced cepstral coefficients while this family was ranked in the 2nd and 5th positions with both ranking methods. The proposed optimization method is more robust than the usual Grid search for cross-corpus experiments with small feature sets.",Towards a Small Set of Robust Acoustic Features for Emotion Recognition: Challenges,IEEE/ACM Transactions on Audio Speech and Language Processing,Article,1/1/2016,Laurence,Devillers,2016
10.1109/TCDS.2017.2726083,"© 2016 IEEE.Understanding and profiling player motivation complements and extends research on gameflow, player profiling, and game artificial intelligence, which helps us design entertaining games. However, automated identification of a player's motive profile remains an open challenge. An emerging technology that shows promise as a novel technique for identifying cognitive phenomena is electroencephalography (EEG). This paper begins with a survey of literature applying EEG to measure cognitive characteristics relevant to player motivation types. Then we present conceptual models that link motivation theory to mental states that can be identified using EEG including emotion, risk-taking, and social attitudes. We conclude this paper by examining the research challenges associated with using EEG to validate these models.",Toward Electroencephalographic Profiling of Player Motivation: A Survey,IEEE Transactions on Cognitive and Developmental Systems,Review,9/1/2018,Hussein,Abbass,2018
10.1109/TCS.1979.1084672,"An approach based on artificial intelligence concepts is developed for automatic generation of test programs for analog circuits. The programs will, with the help of automatic test equipment (ATE), make appropriate measurements and deduce the location of potential faults in analog circuit boards. © 1979 IEEE",On the Use of Procedural Models for Generation of Test Programs,IEEE Transactions on Circuits and Systems,Article,1/1/1979,Robert Tienwen,Chien,1979
10.1109/TEC.1961.5219285,"This is the author's report of his visit to the Soviet Union in June and July, 1960. The purpose of the trip was to attend the First Congress of the International Federation of Automatic Control (IFAC) as an official American delegate. The author also arranged to meet with certain scientists in psychology, physiology, and the computer sciences, and to visit some Russian research institutions doing work in these areas. Soviet research in cybernetics, neuro-cybernetics, artificial intelligence, mechanical translation, and automatic programming are discussed, and new developments in Soviet computing machines are described. The author describes his discussions with several important Soviet personalities in the computer sciences, which dealt with their particular work and the work of their research institutions. He concludes that Soviet research in the computer sciences lags behind Western developments, but that the gap is neither large nor based on a lack of understanding of fundamental principles. He believes that the Soviets will move ahead rapidly if and when priority, in terms of accessibility to computing machines, is given to their research. COPYRIGHT © 1962—THE INSTITUTE OF RADIO ENGINEERS, INC.",Soviet Cybernetics and Computer Sciences—1960,IRE Transactions on Electronic Computers,Article,1/1/1961,Edward,Feigenbaum,1961
10.1109/TETCI.2017.2762739,"© 2017 IEEE.A field that has directly benefited from the recent advances in deep learning is automatic speech recognition (ASR). Despite the great achievements of the past decades, however, a natural and robust human-machine speech interaction still appears to be out of reach, especially in challenging environments characterized by significant noise and reverberation. To improve robustness, modern speech recognizers often employ acoustic models based on recurrent neural networks (RNNs) that are naturally able to exploit large time contexts and long-term speech modulations. It is thus of great interest to continue the study of proper techniques for improving the effectiveness of RNNs in processing speech signals. In this paper, we revise one of the most popular RNN models, namely, gated recurrent units (GRUs), and propose a simplified architecture that turned out to be very effective for ASR. The contribution of this work is twofold: First, we analyze the role played by the reset gate, showing that a significant redundancy with the update gate occurs. As a result, we propose to remove the former from the GRU design, leading to a more efficient and compact single-gate model. Second, we propose to replace hyperbolic tangent with rectified linear unit activations. This variation couples well with batch normalization and could help the model learn long-term dependencies without numerical issues. Results show that the proposed architecture, called light GRU, not only reduces the per-epoch training time by more than 30% over a standard GRU, but also consistently improves the recognition accuracy across different tasks, input features, noisy conditions, as well as across different ASR paradigms, ranging from standard DNN-HMM speech recognizers to end-to-end connectionist temporal classification models.",Light Gated Recurrent Units for Speech Recognition,IEEE Transactions on Emerging Topics in Computational Intelligence,Article,4/1/2018,Yoshua,Bengio,2018
10.1109/THMS.2016.2634921,"© 2013 IEEE.Writer identification is an important topic for pattern recognition and artificial intelligence. Traditional methods rely heavily on sophisticated hand-crafted features to represent the characteristics of different writers. In this paper, we propose an end-to-end framework for online text-independent writer identification by using a recurrent neural network (RNN). Specifically, the handwriting data of a particular writer are represented by a set of random hybrid strokes (RHSs). Each RHS is a randomly sampled short sequence representing pen tip movements (xy-coordinates) and pen-down or pen-up states. RHS is independent of the content and language involved in handwriting; therefore, writer identification at the RHS level is more general and convenient than the character level or the word level, which also requires character/word segmentation. The RNN model with bidirectional long short-term memory is used to encode each RHS into a fixed-length vector for final classification. All the RHSs of a writer are classified independently, and then, the posterior probabilities are averaged to make the final decision. The proposed framework is end-to-end and does not require any domain knowledge for handwriting data analysis. Experiments on both English (133 writers) and Chinese (186 writers) databases verify the advantages of our method compared with other state-of-the-art approaches.",End-to-End Online Writer Identification with Recurrent Neural Network,IEEE Transactions on Human-Machine Systems,Article,4/1/2017,Yoshua,Bengio,2017
10.1109/TIV.2021.3094836,"© 2016 IEEE.Semantic scene segmentation has primarily been addressed by forming high-level visual representations of single images. The problem of semantic segmentation in dynamic scenes has begun to receive attention with the video object segmentation and tracking problem. While there has been some recent work attempt to use deep learning models on the video level, what is not known is how the temporal dynamics information is contributing to the full scene segmentation. Moreover, most existing datasets only provide full scene annotation on non-consecutive images to ensure the variability of scenes, making it even harder to explore novel methods on video-level modeling. To address the above issues, our work takes steps to explore the behavior of modern spatiotemporal modeling approaches by: 1) constructing the MIT DriveSeg dataset, a large-scale video driving scene segmentation dataset, densely annotated for pixel-level semantic classes with 5000 consecutive video frames, and 2) proposing a joint-learning framework that reveals the contribution of temporal dynamics information in regard to different semantic classes in the driving scene. This work is intended to help assess current methods and support further exploration of the value of temporal dynamics information in video-level scene segmentation.",Value of Temporal Dynamics Information in Driving Scene Segmentation,IEEE Transactions on Intelligent Vehicles,Article,3/1/2022,Lex,Fridman,2022
10.1109/TNN.2004.841777,"Training multilayer neural networks is typically carried out using descent techniques such as the gradient-based backpropagation (BP) of error or the quasi-Newton approaches including the Levenberg-Marquardt algorithm. This is basically due to the fact that there are no analytical methods to find the optimal weights, so iterative local or global optimization techniques are necessary. The success of iterative optimization procedures is strictly dependent on the initial conditions, therefore, in this paper, we devise a principled novel method of backpropagating the desired response through the layers of a multilayer perceptron (MLP), which enables us to accurately initialize these neural networks in the minimum mean-square-error sense, using the analytic linear least squares solution. The generated solution can be used as an initial condition to standard iterative optimization algorithms. However, simulations demonstrate that in most cases, the performance achieved through the proposed initialization scheme leaves little room for further improvement in the mean-square-error (MSE) over the training set. In addition, the performance of the network optimized with the proposed approach also generalizes well to testing data. A rigorous derivation of the initialization algorithm is presented and its high performance is verified with a number of benchmark training problems including chaotic time-series prediction, classification, and nonlinear system identification with MLPs. © 2005 IEEE.",Linear-least-squares initialization of multilayer perceptrons through backpropagation of the desired response,IEEE Transactions on Neural Networks,Article,3/1/2005,Amparo Alonso,Betanzos,2005
10.1109/TPAMI.2013.50,"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. © 1979-2012 IEEE.",Representation learning: A review and new perspectives,IEEE Transactions on Pattern Analysis and Machine Intelligence,Article,7/12/2013,Yoshua,Bengio,2013
10.1109/TPAMI.2017.2695539,"© 1979-2012 IEEE.Recent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in the world. Previous research has mainly focused on recognizing handwritten Chinese characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task is to teach a machine to automatically write (pictographic) Chinese characters. In this paper, we propose a framework by using the recurrent neural network (RNN) as both a discriminative model for recognizing Chinese characters and a generative model for drawing (generating) Chinese characters. To recognize Chinese characters, previous methods usually adopt the convolutional neural network (CNN) models which require transforming the online handwriting trajectory into image-like representations. Instead, our RNN based approach is an end-to-end system which directly deals with the sequential structure and does not require any domain-specific knowledge. With the RNN system (combining an LSTM and GRU), state-of-the-art performance can be achieved on the ICDAR-2013 competition database. Furthermore, under the RNN framework, a conditional generative model with character embedding is proposed for automatically drawing recognizable Chinese characters. The generated characters (in vector format) are human-readable and also can be recognized by the discriminative RNN model with high accuracy. Experimental results verify the effectiveness of using RNNs as both generative and discriminative models for the tasks of drawing and recognizing Chinese characters.",Drawing and Recognizing Chinese Characters with Recurrent Neural Network,IEEE Transactions on Pattern Analysis and Machine Intelligence,Article,4/1/2018,Yoshua,Bengio,2018
10.1109/TPAMI.2022.3170249,"© 1979-2012 IEEE.Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. Often, this assumption is not true since the graph may be noisy, or partially and even completely unknown. In such cases, it would be helpful to infer the graph directly from the data, especially in inductive settings where some nodes were not present in the graph at training time. Furthermore, learning a graph may become an end in itself, as the inferred structure may provide complementary insights next to the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function that predicts edge probabilities in the graph which are optimal for the downstream task. DGM can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.",Differentiable Graph Module (DGM) for Graph Convolutional Networks,IEEE Transactions on Pattern Analysis and Machine Intelligence,Article,2/1/2023,Michael,Bronstein,2023
10.1109/TSP.2005.847822,"We propose a relative optimization framework for quasi-maximum likelihood (QML) blind deconvolution and the relative Newton method as its particular instance. Special Hessian structure allows fast Newton system construction and solution, resulting in a fast-convergent algorithm with iteration complexity comparable to that of gradient methods. We also propose the use of rational infinite impulse response (HR) restoration kernels, which constitute a richer family of filters than the traditionally used finte impulse response (FIR) kernels. We discuss different choices of nonlinear functions that are suitable for deconvolution of super- and sub-Gaussian sources and formulate the conditions under which the QML estimation is stable. Simulation results demonstrate the efficiency of the proposed methods. © 2005 IEEE.",Relative optimization for blind deconvolution,IEEE Transactions on Signal Processing,Article,6/1/2005,Michael,Bronstein,2005
10.1109/TSP.2005.849221,"In this correspondence, we consider the problem of multi-input multi output (MIMO) quasi maximum likelihood (QML) blind deconvolution. We examine two classes of estimators, which are commonly believed to be suitable for super- and sub-Gaussian sources. We state the consistency conditions and demonstrate a source distribution, for which the studied estimators are unsuitable, in the sense that they are inconsistent. © 2005 IEEE.",Quasi maximum likelihood MIMO blind deconvolution: Super- and sub-Gaussianity versus consistency,IEEE Transactions on Signal Processing,Article,7/1/2005,Michael,Bronstein,2005
10.1109/TSP.2018.2879624,"© 1991-2012 IEEE.The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach, in comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification, and matrix completion tasks.",CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters,IEEE Transactions on Signal Processing,Article,1/1/2019,Michael,Bronstein,2019
10.1109/WCRE.2008.30,"Data modeling is an essential part of the software development process, and together with application modeling forms the core of the model-driven approach to software engineering. While UML is considered the standard for application modeling, there is really no corresponding open standard for data modeling. In this paper, we propose an approach and a tool to help bridge the gap between application and data modeling based on source transformation technology. The tool, called SQL2XMI, automatically transforms an SQL schema into a UML-ER model expressed in XML Meta Interchange (XMI) 2.1. By bringing the data model to the UML world, both data and application models can be manipulated using the same UML-based tools. © 2008 IEEE.",SQL2XMI: Reverse engineering of UML-ER diagrams from relational database schemas,"Proceedings - Working Conference on Reverse Engineering, WCRE",Conference Paper,12/29/2008,Thomas,Dean,2008
10.1109/WI-IAT.2009.145,"A Multi-Agent System is often conceived as an organization of autonomous software agents that participate into social and evolving structures (e.g., organizational configurations) suitable to deal with highly dynamic environments. Nevertheless, systems based on agent technologies rarely capitalize on their potentials since their systemic properties - e.g., flexibility, robustness and efficiency - are typically only the byproduct of the (AI) techniques deployed at the implementation level, and are neither explicit object of study nor are taken into consideration at a requirements engineering phase. The paper presents a method, based on graph theory, to exactly compare and evaluate software design system configurations in the engineering of multi-agent systems. The theoretical results are presented and validated on a crisis management scenario. © 2009 IEEE.",Evaluating organizational configurations,"Proceedings - 2009 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT 2009",Conference Paper,12/1/2009,Virginia,Dignum,2009
10.1109/WI-IATW.2006.27,"Computer systems keep growing in complexity, processing power and web connectivity. To leverage this rich environment and to better assist users, a new type of intelligent assistant software is required. Building intelligent assistants is a difficult task that requires expertise in many AI and engineering related fields. We believe that providing a unified tool and a set of associated methodologies to create end-to-end intelligent software will bring many benefits to this area of research. Our solution, the Active frame-work, introduces the original concept of Active Ontologies to model and implement intelligent applications in a single and coherent software environment. As an example, this paper illustrates how Active has been used to create an intelligent assistant to help mobile users retrieve online information using a multimodal dialog approach. © 2006 IEEE.",Active: A unified platform for building intelligent web interaction assistants,Proceedings - 2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2006 Workshops Proceedings),Conference Paper,1/1/2006,Adam,Cheyer,2006
10.1109/WI.2003.1241176,"© 2003 IEEE.It has been widely recognised that matchmaking is an important component for environments populated with heterogeneous services. Several researchers have developed powerful techniques for the matchmaking problem in general. There are also specific representations of service capabilities such as DAML-S, which provide a more specific framework for matchmaking. Most approaches to matchmaking have assumed a sequential search for a service with matching capabilities. This may become intractable when the number of available services gets large. We consider how matchmaking can be developed into service directories that can be searched and maintained efficiently. Our main contribution is to show how matchmaking with DAML-S specifications can be integrated with efficient methods for searching and maintaining balanced directory trees. We also report on experimental results using an implementation based on generalised search trees.",Efficient matchmaking and directory services,"Proceedings - IEEE/WIC International Conference on Web Intelligence, WI 2003",Conference Paper,1/1/2003,Boi,Faltings,2003
10.1109/WI.2007.88,"Implicitly structured content on the Web such as HTML tables and lists can be extremely valuable for web search, question answering, and information retrieval, as the implicit structure in a page often reflects the underlying semantics of the data. Unfortunately, exploiting this information presents significant challenges due to the immense amount of implicitly structured content on the web, lack of schema information, and unknown source quality. We present TQA, a web-scale system for automatic question answering that is often able to find answers to real natural language questions from the implicitly structured content on the web. Our experiments over more than 200 million structures extracted from a partial web crawl demonstrate the promise of our approach. © 2007 IEEE.",Question answering over implicitly structured Web content,"Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence, WI 2007",Conference Paper,12/1/2007,Eric,Brill,2007
10.1109/WPC.2001.921726,"© 2001 IEEE.HSML, the Hot Spot Markup Language, is an ultra-high level executable specification language designed for concisely specifying source code hot spots of all kinds. Each HSML rule specifies the abstract syntactic class of the items to be marked as hot using a nonterminal of the target language grammar, and the semantic conditions under which such items are to be marked using an algebraic expression on the design properties of the item. Conditions can include restrictions on abstract syntactic structure (patterns), design recovered semantic properties (queries on the design database), and semantic properties induced by other markup rules. HSML has been used in industrial practice to specify source code hot spots for the Year 2000 and a wide range of other application maintenance tasks on systems implemented in Cobol, PL/I and RPG. We introduce the basic concepts of HSML and demonstrate its use in real software maintenance tasks.",HSML: Design directed source code hot spots,Proceedings - IEEE Workshop on Program Comprehension,Conference Paper,1/1/2001,Thomas,Dean,2001
10.1109/WSE.2012.6320525,"This paper presents an original Model-Driven-Engineering (MDE) approach to support the verification and testing of security properties in dynamic web applications. Based on a previously recovered UML-based fine-grained security model, the approach begins by transforming the model into a Prolog-based formal model. The Prolog model is then checked to verify whether the application conforms to specified access control security properties. We demonstrate the use of our method on the popular open source bulletin board system PhpBB 2.0, in the context of three test scenarios: testing for unauthorized access, web application security maintenance, and web application re-engineering. © 2012 IEEE.",Automated verification of role-based access control security models recovered from dynamic web applications,"Proceedings of IEEE International Symposium on Web Systems Evolution, WSE",Conference Paper,12/14/2012,Thomas,Dean,2012
10.1111/1475-3995.00009,"Operational research (OR) and artificial intelligence (AI) models are primary contributors to the area of intelligent decision support systems (IDSS). Constraint logic programming (CLP) has been used successfully to substantiate the integration of OR and AI. We present a meta-level modular representation for integrating OR and AI models using CLP in an IDSS framework. The use of this representation is illustrated using a CLP-like meta-language, and the potential usefulness of this language is demonstrated using an example from the dairy industry. International Federation of Operational Research Societies 2001.",A meta-representation for integrating operational research and artificial intelligence in an intelligent decision support paradigm,International Transactions in Operational Research,Article,1/1/2001,Hussein,Abbass,2001
10.1111/j.1360-0443.1994.tb00928.x,"We examined the explanations given by a sample of 1245 injecting drug users in Sydney, Australia for accepting used injection equipment. Factor analysis of these reasons revealed three dimensions of sharing: not caring when withdrawing or intoxicated, unavailability of equipment, and not seeing it as high risk or ease of injecting. The most common reasons given were difficulty in obtaining sterile equipment (73% of cases), the dangers not seeming so important when in withdrawal (40%) and sharing being something done with friends or lovers (31%). Most common reasons for not sharing were related to health issues (91% citing AIDS and 67% hepatitis). These data suggest that interventions target provision of sterile equipment, and education which highlights risk situations such as intoxication and withdrawal. Copyright © 1994, Wiley Blackwell. All rights reserved",Explanations for sharing injection equipment in injecting drug users and barriers to safer drug use,Addiction,Article,1/1/1994,W,Ross,1994
10.1111/j.1365-2141.2005.05545.x,"We have prospectively analysed and correlated the gene expression profiles of children presenting with acute leukaemia to the Royal London and Great Ormond Street Hospitals with morphological diagnosis, immunophenotype and karyotype. Total RNA extracted from freshly sorted blast cells was obtained from 84 lymphoblastic [acute lymphoblastic leukaemia (ALL)], 20 myeloid [acute myeloid leukaemia (AML)] and three unclassified acute leukaemias and hybridised to the high density Affymetrix U133A oligonucleotide array. Analysis of variance and significance analysis of microarrays was used to identify discriminatory genes. A novel 50-gene set accurately identified all patients with ALL and AML and predicted for a diagnosis of AML in three patients with unclassified acute leukaemia. A unique gene set was derived for each of eight subtypes of acute leukaemia within our data set. A common profile for children with ALL with an ETV6-RUNX1 fusion, amplification or deletion of ETV6, amplification of RUNX1 or hyperdiploidy with an additional chromosome 21 was identified. This suggests that these rearrangements share a commonality in biological pathways that maintains the leukaemic state. The gene TERF2 was most highly expressed in this group of patients. Our analyses demonstrate that not only is microarray analysis the single most effective tool for the diagnosis of acute leukaemias of childhood but it has the ability to identify unique biological pathways. To further evaluate its prognostic value it needs to be incorporated into the routine diagnostic analysis for large-scale clinical trials in childhood acute leukaemias. © 2005 Blackwell Publishing Ltd.",Prospective gene expression analysis accurately subtypes acute leukaemia in children and establishes a commonality between hyperdiploidy and t(12;21) in acute lymphoblastic leukaemia,British Journal of Haematology,Article,1/1/2005,Alexander,Gammerman,2005
10.1111/j.1365-3156.2006.01583.x,"OBJECTIVES: To quantify expressed stigma in clients of the Kangemi program for HIV+ children, and to characterize the association between stigma and other population characteristics. METHODS: By means of a household survey we created a stigma index and indices for other social and knowledge domains that influence HIV-related healthcare. We used <U+03C7>2, anova, and correlation to identify associations between domains. RESULTS: The mean (±SD) expressed stigma on a six points scale (6 = least stigma) was 3.65 ± 1.64. Composite scores on knowledge about AIDS were skewed toward more knowledge; and analysis of individual knowledge items indicates that most respondents reject erroneous traditional beliefs and myths about the causes and transmission routes of AIDS. Respondents who were younger, had never married, and had less education expressed greater stigma. Differences in stigma were associated with poor knowledge about AIDS and negative attitudes toward testing, but not with gender or tribal affiliation. Condom use at last intercourse, unrelated to stigma, was only 40% (n = 218). CONCLUSIONS: While this population has good knowledge about AIDS and appraises risks realistically, it fails to reduce these risks. Associations between stigma and other domains can inform interventions that improve HIV care and mitigate spread of HIV. © 2006 Blackwell Publishing Ltd.",Relationship between expressed HIV/AIDS-related stigma and HIV-beliefs/knowledge and behaviour in families of HIV infected children in Kenya,Tropical Medicine and International Health,Review,4/1/2006,W,Ross,2006
10.1111/j.1467-8640.2010.00366.x,"The family of decision tree learning algorithms is among the most widespread and studied. Motivated by the desire to develop learning algorithms that can generalize when learning highly varying functions such as those presumably needed to achieve artificial intelligence, we study some theoretical limitations of decision trees. We demonstrate formally that they can be seriously hurt by the curse of dimensionality in a sense that is a bit different from other nonparametric statistical methods, but most importantly, that they cannot generalize to variations not seen in the training set. This is because a decision tree creates a partition of the input space and needs at least one example in each of the regions associated with a leaf to make a sensible prediction in that region. A better understanding of the fundamental reasons for this limitation suggests that one should use forests or even deeper architectures instead of trees, which provide a form of distributed representation and can generalize to variations not encountered in the training data. © 2010 Wiley Periodicals, Inc.",Decision trees do not generalize to new variations,Computational Intelligence,Article,11/1/2010,Yoshua,Bengio,2010
10.1111/j.1467-9973.2010.01639.x,"In the course of seeking an answer to the question ""How do you know you are not a zombie?"" Floridi (2005) issues an ingenious, philosophically rich challenge to artificial intelligence (AI) in the form of an extremely demanding version of the so-called knowledge game (or ""wise-man puzzle,"" or ""muddy-children puzzle"") - one that purportedly ensures that those who pass it are self-conscious. In this article, on behalf of (at least the logic-based variety of) AI, I take up the challenge - which is to say, I try to show that this challenge can in fact be met by AI in the foreseeable future. © 2010 Metaphilosophy LLC and Blackwell Publishing Ltd.",Meeting Floridi's challenge to artificial intelligence from the knowledge-game test for self-consciousness,Metaphilosophy,Article,4/1/2010,Selmer,Bringsjord,2010
10.1111/j.1468-0017.1989.tb00256.x,"There is a school of thought which links connectionist models of cognition to eliminativism-the thesis that the constructs of commonsense psychology (principally, beliefs and desires) do not exist. This way of construing the impact of connectionist modelling is, I argue, deeply mistaken and depends crucially on a shallow analysis of the notion of explanation. I argue that good, higher level descriptions may group together physically heterogenous mechanisms, and that the constructs of folk psychology may fulfil such a grouping function even if they fail to pick out discrete states of individual processing. More speculatively, the paper goes on to suggest that the virtues which recommend such constructs from a theorist's third person perspective may nonetheless have close analogues in individual processing. Various difficulties for what I term a pure distributed connectionism may be solved by systems which utilise both distributed and classical symbolic representations-the latter exhibiting the discreteness and semantic interpretability which the eliminativist (wrongly) requires to vindicate common-sense psychology. If human beings turned out to be such mixed systems, then the eliminativist claim would be doubly misguided. It would be false as a conditional since even if pure distributed connectionism were a complete and accurate formal model of individual processing, it would not follow that the other, higher level constructs were not accurate and essential tools for a different kind of explanation. And it would involve a false antecedent since in a mixed system the symbolic descriptions may indeed be incarnate in the system's own individual processing. The paper is in four main sections. Section 1 attempts to systematise a certain received picture of the relative status and accuracy of various levels of description of a (pure distributed) connectionist system. Section 2 then introduces a general model of explanations which aim to group systems into equivalence classes defined for various purposes. Each grouping requires a special vocabulary and the constructs of any given vocabulary are legitimate just insofar as the grouping is interesting and useful. Section 3 goes on to show that, relative to such a model of explanation, the constructs of both symbolic AI and commonsense psychology may have a legitimate role to play in giving psychological explanations. This role is not just that of a useful approximation. The paper ends with a speculative section in which the argument for the theoretical usefulness of such symbolic constructs is extended, in a very natural way, to the domain of individual processing. Here the cognizer, in the process of regulating, debugging and understanding her own representations, creates symbols to stand for sets of distributed activity patterns. The section points out some difficulties for a pure distributed approach which may be eased by the addition of such symbolic constructs and relates the speculations to the current debate over the ‘correct’architecture of cognition. If the speculations are on target, this whole debate turns out to be fundamentally illposed. Copyright © 1989, Wiley Blackwell. All rights reserved",Beyond Eliminativism,Mind &amp; Language,Article,1/1/1989,Andy,Clark,1989
10.1111/j.1559-1816.1991.tb00452.x,"One hundred and thirty-four health professionals read one of 12 fictional case histories in which the patient was diagnosed as being either HIV- or Hepatitis B-positive. For each diagnosis infection was attributed to sexual contact. IV drug use, or a transfusion of contaminated blood. Within each diagnostic category, and for each source of infection, the patient was identified as either heterosexual or homosexual. Although homophobia has been suggested as a major contributor to negative attitudes toward people with AIDS, the present results remained significant even after homophobia, as measured by Hudson and Ricketts (1980), had been controlled for statistically. Regardless of disease, patients infected through IV drug use or sexual contact were seen as equally culpable and more responsible for their condition than those infected by transfusion. HIV, but not Hepatitis B, patients infected by sex or IV drug use were perceived as having less moral integrity than those infected by transfusion. Source of infection also influenced respondents' desire for close personal interaction. Negativity toward particular patient groups based merely on information about patient lifestyles was clearly demonstrated and it is suggested that negative attitudes toward people with AIDS may be a reflection of negative attitudes toward sexuality generally, rather than homosexuality. Copyright © 1991, Wiley Blackwell. All rights reserved",Determinants of Health-Care Workers' Attitudes Toward People with AIDS,Journal of Applied Social Psychology,Article,1/1/1991,W,Ross,1991
10.1111/j.1740-9713.2015.00796.x,"© 2015 The Royal Statistical Society.Creating artificial human intelligence has proved more difficult than first imagined. But thanks to statistical ideas and models, our machines are getting smarter. Brian Tarran reports Statistics, superintelligence and the fate of humanity A Q&A with Zoubin Ghahramani discussing his Google-backed project to create an artificial intelligence for data science.",How machines learned to think statistically,Significance,Article,2/1/2015,Zoubin,Ghahramani,2015
10.1111/j.1746-1561.1989.tb04732.x,"ABSTRACT: Educational efforts to prevent the spread of AIDS require a combination of accurate information and the application of that information to personal behavior. In this preliminary study, a scale was developed to evaluate the social and interpersonal skills of adolescents in AIDS-related and non-AIDS-related interactions. The instrument was administered to 101 Australian students in grades 10 and 11 of an urban high school. Findings indicate ratings of AIDS-related activities of the adolescents were not significantly different from their other social activities, suggesting levels of comfort and assertiveness among adolescents in AIDS-related activities can be modified. Students felt most anxious about problems with sexually transmitted diseases or drug use, suggesting difficulty in dealing with the possible public disclosure of these stigmatized conditions. Data suggest both individual and group scores among adolescents may improve on the five dimensions of the scale following interventions to promote social and interpersonal skills designed to apply knowledge obtained from AIDS education. 1989 American School Health Association",A Preliminary Study of Social Issues in AIDS Prevention Among Adolescents,Journal of School Health,Article,1/1/1989,W,Ross,1989
10.1111/j.1746-1561.1991.tb07415.x,"ABSTRACT: The AIDS Social Assertiveness Scale (ASAS) and a measure of AIDS knowledge were administered to 490 high school students from 10 South Australian schools selected across a range of public/private, rural/urban, and socioeconomic statuses. Age, grade, hours of AIDS education, and sexual experience (and condom use) also were ascertained. Data indicated the factor structure of the ASAS was similar to that obtained in a previous sample. A close association existed between both age and hours of AIDS education, and AIDS knowledge. Students from country and working class area schools were less knowledgeable about AIDS. These data suggest AIDS social assertiveness is measurable and that it is associated with AIDS knowledge, age, and previous sexual experience. AIDS knowledge in high school students differed between country and urban adolescents and between students in working and middle class areas, but AIDS education is associated with the same level of improvement in all schools sampled. 1991 American School Health Association",Relationship of AIDS Education and Knowledge to AIDS-Related Social Skills in Adolescents,Journal of School Health,Article,1/1/1991,W,Ross,1991
10.1111/j.1753-6405.1988.tb00593.x,"In a cross-sectional survey of 172 homosexual men in a city of one million inhabitants, questions were asked on testing for AIDS and counselling history, sexual practices, prophylactic behaviours, condom use and associations with the gay subculture along with other background variables. Respondents were divided into four groups: those who had had both HIV testing and safer sex counselling, those who had only had testing, those who had only had counselling, and those who had had neither. All but 4 per cent of respondents had heard of safer sex. Results showed that increase in comdom use for both oral and anal sex, and stopping prior to ejaculation were associated with the combined testing and counselling group, and occurred significantly more often than the counselling only and no intervention groups. The testing only group fell midway between the counselling plus testing and the counselling only, and no intervention groups. A scale constructed by summing the four measures of safer sex showed that testing only, and counselling and testing combined were significantly superior in terms of safer sex practices than the counselling and no intervention groups. Correlates of being given a condom at point of testing included increased insertive anal intercourse and stopping before ejaculation as well as increased condom use during anal intercourse. These findings suggest that provision of free condoms in a context of professional and peer support may enhance condom usage during behaviours known to transmit HIV. While these data may be interpreted with caution with regard to causality, they do imply (in the absence of evidence that such behaviours characterise individuals who present for HIV-related interventions) that some interventions or combinations of interventions are significantly more effective than others. 1988 Public Health Association of Australia",RELATIONSHIP OF COMBINATIONS OF AIDS COUNSELLING AND TESTING TO SAFER SEX AND CONDOM USE IN HOMOSEXUAL MEN,Community Health Studies,Article,1/1/1988,W,Ross,1988
10.1111/j.1753-6405.1992.tb00074.x,"Abstract: We report on media habits of 797 members of a sample of 1245 injecting drug users interviewed in Sydney, Australia. While preferred hours of television viewing and radio listening were similar to those of the general population, the preferred channels and stations were different. These findings could assist in targeting injecting drug users with information about HIV/AIDS prevention. However as the self-regulatory advertising process has constrained broadcast and publication of overt messages directed at homosexual and bisexual men, similar restrictions may prevent optimal mass media approaches to educating this other important group at risk of HIV infection. 1992 Public Health Association of Australia",Media sources of HIV/AIDS information in injecting drug users,Australian Journal of Public Health,Article,1/1/1992,W,Ross,1992
10.1111/j.1753-6405.1994.tb00226.x,Abstract: The number of injecting drug users in the Central and Eastern Sydney Area Health Services (CSAHS and ESAHS) in 1989–1990 was estimated by applying the Petersen mark-recapture method to data obtained from the Australian National AIDS and Injecting Drug Use Surveys conducted in 1989 and in 1990. The population estimates for injecting drug users were 8368 (95 per cent confidence interval (CI) 6099 to 11 829) for ESAHS and 1466 (CI 742 to 2841) for CSHS. An estimate was also obtained for the number of injecting opioid users residing within the ESAHS; our figure of 3597 (CI 2731 to 5737) was consistent with previously reported estimates obtained using mark-recapture but based on different data sets. 1994 Public Health Association of Australia,Mark-recapture estimates of injecting drug users in Sydney,Australian Journal of Public Health,Article,1/1/1994,W,Ross,1994
10.1111/j.1756-8765.2010.01083.x,"Artificial Intelligence (AI) is a core area of Cognitive Science, yet today few AI researchers attend the Cognitive Science Society meetings. This essay examines why, how AI has changed over the last 30years, and some emerging areas of potential interest where AI and the Society can go together in the next 30years, if they choose. © 2010 Cognitive Science Society, Inc.",AI and cognitive science: The past and next 30 years,Topics in Cognitive Science,Article,7/1/2010,Ken,Forbus,2010
10.1111/j.1756-8765.2010.01095.x,"Recently, there has been a resurgence of interest in general, comprehensive models of human cognition. Such models aim to explain higher-order cognitive faculties, such as deliberation and planning. Given a computational representation, the validity of these models can be tested in computer simulations such as software agents or embodied robots. The push to implement computational models of this kind has created the field of artificial general intelligence (AGI). Moral decision making is arguably one of the most challenging tasks for computational approaches to higher-order cognition. The need for increasingly autonomous artificial agents to factor moral considerations into their choices and actions has given rise to another new field of inquiry variously known as Machine Morality, Machine Ethics, Roboethics, or Friendly AI. In this study, we discuss how LIDA, an AGI model of human cognition, can be adapted to model both affective and rational features of moral decision making. Using the LIDA model, we will demonstrate how moral decisions can be made in many domains using the same mechanisms that enable general decision making. Comprehensive models of human cognition typically aim for compatibility with recent research in the cognitive and neural sciences. Global workspace theory, proposed by the neuropsychologist Bernard Baars (1988), is a highly regarded model of human cognition that is currently being computationally instantiated in several software implementations. LIDA (Franklin, Baars, Ramamurthy, & Ventura, 2005) is one such computational implementation. LIDA is both a set of computational tools and an underlying model of human cognition, which provides mechanisms that are capable of explaining how an agent's selection of its next action arises from bottom-up collection of sensory data and top-down processes for making sense of its current situation. We will describe how the LIDA model helps integrate emotions into the human decision-making process, and we will elucidate a process whereby an agent can work through an ethical problem to reach a solution that takes account of ethically relevant factors. © 2010 Cognitive Science Society, Inc.",A conceptual and computational model of moral decision making in human and artificial agents,Topics in Cognitive Science,Article,7/1/2010,Stan,Franklin,2010
10.1111/lang.12580,"© 2023 The Authors. Language Learning published by Wiley Periodicals LLC on behalf of Language Learning Research Club, University of Michigan.This study extended an existing crosslinguistic model of verb-marking errors in children's early multiword speech (MOSAIC) by adding a novel mechanism that defaults to the most frequent form of the verb where this accounts for a high proportion of forms in the input. Our simulations showed that the resulting model not only provides a better explanation of the data on typically developing children but also captures the crosslinguistic pattern of verb-marking error in children with developmental language disorder, including the tendency of English-speaking children to show higher rates of optional-infinitive errors and the tendency of Dutch-, German-, and Spanish-speaking children to show higher rates of agreement errors. The new version of MOSAIC thus provides a unified crosslinguistic model of the pattern of verb-marking errors in typically developing children and children with developmental language disorder.",MOSAIC+: A Crosslinguistic Model of Verb-Marking Errors in Typically Developing Children and Children With Developmental Language Disorder,Language Learning,Article,1/1/2023,Fernand,Gobet,2023
10.1111/ocr.12521,"© 2021 John Wiley & Sons A/S. Published by John Wiley & Sons LtdObjectives: Palatal shape contains a lot of information that is of clinical interest. Moreover, palatal shape analysis can be used to guide or evaluate orthodontic treatments. A statistical shape model (SSM) is a tool that, by means of dimensionality reduction, aims at compactly modeling the variance of complex shapes for efficient analysis. In this report, we evaluate several competing approaches to constructing SSMs for the human palate. Setting and Sample Population: This study used a sample comprising digitized 3D maxillary dental casts from 1,324 individuals. Materials and methods: Principal component analysis (PCA) and autoencoders (AE) are popular approaches to construct SSMs. PCA is a dimension reduction technique that provides a compact description of shapes by uncorrelated variables. AEs are situated in the field of deep learning and provide a non-linear framework for dimension reduction. This work introduces the singular autoencoder (SAE), a hybrid approach that combines the most important properties of PCA and AEs. We assess the performance of the SAE using standard evaluation tools for SSMs, including accuracy, generalization, and specificity. Results: We found that the SAE obtains equivalent results to PCA and AEs for all evaluation metrics. SAE scores were found to be uncorrelated and provided an optimally compact representation of the shapes. Conclusion: We conclude that the SAE is a promising tool for 3D palatal shape analysis, which effectively combines the power of PCA with the flexibility of deep learning. This opens future AI driven applications of shape analysis in orthodontics and other related clinical disciplines.",Exploring palatal and dental shape variation with 3D shape analysis and geometric deep learning,Orthodontics and Craniofacial Research,Article,12/1/2021,Michael,Bronstein,2021
10.1111/ocr.12641,"© 2023 The Authors. Orthodontics & Craniofacial Research published by John Wiley & Sons Ltd.Objective: To investigate the utility of machine learning (ML) in accurately predicting orthodontic extraction patterns in a heterogeneous population. Materials and Methods: The material of this retrospective study consisted of records of 366 patients treated with orthodontic extractions. The dataset was randomly split into training (70%) and test sets (30%) and was stratified according to race/ethnicity and gender. Fifty-five cephalometric and demographic input data were used to train and test multiple ML algorithms. The extraction patterns were labelled according to the previous treatment plan. Random Forest (RF), Logistic Regression (LR), and Support Vector Machine (SVM) algorithms were used to predict the patient's extraction patterns. Results: The highest class accuracy percentages were obtained for the upper and lower 1st premolars (U/L4s) (RF: 81.63%, LR: 63.27%, SVM: 63.27%) and upper 1st premolars only (U4s) extraction patterns (RF: 61.11%, LR: 72.22%, SVM: 72.22%). However, all methods revealed low class accuracy rates (<50%) for the upper 1st and lower 2nd premolars (U4/L5s), upper 2nd and lower 1st premolars (U5/L4s), and upper and lower 2nd premolars (U/L5s) extraction patterns. For the overall accuracy, RF yielded the highest percentage with 54.55%, followed by SVM with 52.73% and LR with 49.09%. Conclusion: All tested supervised ML techniques yielded good accuracy in predicting U/L4s and U4s extraction patterns. However, they predicted poorly for the U4/L5s, U5/L4s, and U/L5s extraction patterns. Molar relationship, mandibular crowding, and overjet were found to be the most predictive indicators for determining extraction patterns.",Can we predict orthodontic extraction patterns by using machine learning?,Orthodontics and Craniofacial Research,Article,1/1/2023,Jeff,Dean,2023
10.1111/tops.12262,"Copyright © 2017 Cognitive Science Society, Inc.Computational modeling of sketch understanding is interesting both scientifically and for creating systems that interact with people more naturally. Scientifically, understanding sketches requires modeling aspects of visual processing, spatial representations, and conceptual knowledge in an integrated way. Software that can understand sketches is starting to be used in classrooms, and it could have a potentially revolutionary impact as the models and technologies become more advanced. This paper looks at one such effort, Sketch Worksheets, which have been used in multiple classroom experiments already, with students ranging from elementary school to college. Sketch Worksheets are a software equivalent of pencil and paper worksheets commonly found in classrooms, but they provide on-the-spot feedback based on what students draw. They are built on the CogSketch platform, which provides qualitative visual and spatial representations and analogical processing based on computational models of human cognition. This paper explores three issues. First, we examine how research from cognitive science and artificial intelligence, combined with the constraints of creating new kinds of educational software, led to the representations and processing in CogSketch. Second, we examine how these capabilities have been used in Sketch Worksheets, drawing upon experiments with fifth-grade students in biology and college students in engineering design and in geoscience. Finally, we examine some open issues in sketch understanding that need to be addressed to better model high-level aspects of vision, and for sketch understanding systems to reach their full potential for supporting education.",The Cognitive Science of Sketch Worksheets,Topics in Cognitive Science,Article,10/1/2017,Ken,Forbus,2017
10.11144/Javeriana.rgyps15-31.qpdi,"The purpose of this study was to analyze the mortality due to HIV/AIDS in Cali, Colombia, in the 1986 - 2012 period, and, from there, understand from a conceptual point of view the related social determinants. We carried out a retrospective observational longitudinal study with a total of 14,192 cases. We found a growing tendency in the mortality rate, particularly starting in 1996. The higher rate for the whole period was seen in 2011. The inequality measure revealed a higher number of deaths in lower and middle social strata, when compared with the high strata. This study provides evidence on the persistent inequalities in the mortality due to HIV/AIDS, as an expression of the structural inequalities present in Cali. In Breilh's perspective, critical social and economic determination processes could have affected mortality, especially in the groups whose social rights have been violated the most.","What Came First, Inequalities or Inequities? Regarding the Mortality due to HIV/AIDS Case in Cali, Colombia (1986-2012)",Revista Gerencia y Politicas de Salud,Review,7/1/2016,W,Ross,2016
10.1116/1.1819927,"Doping of AIN with Cr at percent level concentrations produces ferromagnetism persisting to above 300 K. We have examined the electrical and optical properties of Cr-doped AlN grown by molecular beam epitaxy under conditions that produce single-phase or multiple phase material, as measured by x-ray diffraction. The band gap of single-phase AlN decreases from 6.2 to 6.1 eV for a Cr concentration of 2 at. %. This change originates from the Franz-Keldysh broadening of the band edge due to potential fluctuations caused by heavy Cr doping. The effect was more pronounced in multiple-phase samples (the secondary phases are Cr 2N and Al xCr y), producing an apparent band gap of 5.8 eV. Two strong defect absorption bands with thresholds of 3 and 5 eV are introduced by the Cr doping. The resistivity of single-phase AlCrN samples is higher than the resistivity of similarly grown undoped AlN films. Multiple-phase AlCrN samples show a high conductivity of the hopping type. The optical transmission spectra of multiphase AlCrN indicates appreciable absorption by free carriers and strong scattering, both most likely due to the presence of conducting inclusions. © 2004 American Vacuum Society.",Optical and electrical properties of AlCrN films grown by molecular beam epitaxy,Journal of Vacuum Science and Technology B: Microelectronics and Nanometer Structures,Article,11/1/2004,I,J.,2004
10.1116/1.582037,"A systematic study of the etch characteristics of GaN, AIN, and InN has been performed with boron halide- (BI3 and BBr3) and interhalogen- (ICl and IBr) based inductively coupled plasmas. Maximum etch selectivities of ~ 100:1 were achieved for InN over both GaN and AIN in the BI3 mixtures due to the relatively high volatility of the InIx etch products and the lower bond strength of InN. Maximum selectivities of ~ 14 for InN over GaN and <25 for InN over AIN were obtained with ICl and IBr chemistries. The etched surface morphologies of GaN in these four mixtures are similar or better that those of the control sample. © 1999 American Vacuum Society.",III-nitride dry etching: Comparison of inductively coupled plasma chemistries,"Journal of Vacuum Science and Technology A: Vacuum, Surfaces and Films",Conference Paper,12/1/1999,I,J.,1999
10.1117/12.263460,"Recent developments have made it possible to interoperate complex business applications at much lower costs. Application interoperation, along with business process re- engineering can result in significant savings by eliminating work created by disconnected business processes due to isolated business applications. However, we believe much greater productivity benefits can be achieved by facilitating timely decision-making, utilizing information from multiple enterprise perspectives. The CIIMPLEX enterprise integration architecture is designed to enable such productivity gains by helping people to carry out integrated enterprise scenarios. An enterprise scenario is triggered typically by some external event. The goal of an enterprise scenario is to make the right decisions considering the full context of the problem. Enterprise scenarios are difficult for people to carry out because of the interdependencies among various actions. One can easily be overwhelmed by the large amount of information. We propose the use of software agents to help gathering relevant information and present them in the appropriate context of an enterprise scenario. The CIIMPLEX enterprise integration architecture is based on the FAIME methodology for application interoperation and plug-and-play. It also explores the use of software agents in application plug-and- play. ©2005 Copyright SPIE - The International Society for Optical Engineering.",Integrating manufacturing softwares for intelligent planning execution: A CIIMPLEX perspective,Proceedings of SPIE - The International Society for Optical Engineering,Conference Paper,12/1/1997,Tim,Finin,1997
10.1117/12.542775,"Computational models that give us insight into the behavior of individuals and the organizations to which they belong will be invaluable assets in our nation's war against terrorists, and state sponsorship of terror organizations. Reasoning and decision-making are essential ingredients in the formula for human cognition, yet the two have almost exclusively been studied in isolation from one another. While we have witnessed the emergence of strong traditions in both symbolic logic, and decision theory, we have yet to describe an acceptable interface between the two. Mathematical formulations of decision-making and reasoning have been developed extensively, but both fields make assumptions concerning human rationality that are untenable at best. True to this tradition, artificial intelligence has developed architectures for intelligent agents under these same assumptions. While these digital models of ""cognition"" tend to perform superbly, given their tremendous capacity for calculation, it is hardly reasonable to develop simulacra of human performance using these techniques. We will discuss some the challenges associated with the problem of developing integrated cognitive systems for use in modelling, simulation, and analysis, along with some ideas for the future.","Cognitive architectures, rationality and next-generation AI: A prolegomenon",Proceedings of SPIE - The International Society for Optical Engineering,Conference Paper,12/24/2004,Selmer,Bringsjord,2004
10.1126/sciadv.abg4930,"Copyright © 2021 The Authors, some rights reserved.Autonomous experimentation enabled by artificial intelligence offers a new paradigm for accelerating scientific discovery. Nonequilibrium materials synthesis is emblematic of complex, resource-intensive experimentation whose acceleration would be a watershed for materials discovery. We demonstrate accelerated exploration of metastable materials through hierarchical autonomous experimentation governed by the Scientific Autonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis using lateral gradient laser spike annealing and optical characterization along with a hierarchy of AI methods to map out processing phase diagrams. Efficient exploration of the multidimensional parameter space is achieved with nested active learning cycles built upon advanced machine learning models that incorporate the underlying physics of the experiments and end-to-end uncertainty quantification. We demonstrate SARA's performance by autonomously mapping synthesis phase boundaries for the Bi2O3 system, leading to orders-of-magnitude acceleration in the establishment of a synthesis phase diagram that includes conditions for stabilizing d-Bi2O3 at room temperature, a critical development for electrochemical technologies.",Autonomous materials synthesis via hierarchical active learning of nonequilibrium phase diagrams,Science Advances,Article,12/1/2021,Carla,Gomes,2021
10.1126/science.197.4308.1041,The methods of artificial intelligence are applied to the problem of organic synthesis route discovery.,Empirical explorations of SYNCHEM,Science,Article,1/1/1977,Herbert,Gelernter,1977
10.1126/scitranslmed.aba4373,"Copyright © 2021 The Authors.Improved breast cancer risk models enable targeted screening strategies that achieve earlier detection and less screening harm than existing guidelines. To bring deep learning risk models to clinical practice, we need to further refine their accuracy, validate them across diverse populations, and demonstrate their potential to improve clinical workflows. We developed Mirai, a mammography-based deep learning model designed to predict risk at multiple timepoints, leverage potentially missing risk factor information, and produce predictions that are consistent across mammography machines. Mirai was trained on a large dataset from Massachusetts General Hospital (MGH) in the United States and tested on held-out test sets from MGH, Karolinska University Hospital in Sweden, and Chang Gung Memorial Hospital (CGMH) in Taiwan, obtaining C-indices of 0.76 (95% confidence interval, 0.74 to 0.80), 0.81 (0.79 to 0.82), and 0.79 (0.79 to 0.83), respectively. Mirai obtained significantly higher 5-year ROC AUCs than the Tyrer-Cuzick model (P < 0.001) and prior deep learning models Hybrid DL (P < 0.001) and Image-Only DL (P < 0.001), trained on the same dataset. Mirai more accurately identified high-risk patients than prior methods across all datasets. On the MGH test set, 41.5% (34.4 to 48.5) of patients who would develop cancer within 5 years were identified as high risk, compared with 36.1% (29.1 to 42.9) by Hybrid DL (P = 0.02) and 22.9% (15.9 to 29.6) by the Tyrer-Cuzick model (P < 0.001).",Toward robust mammography-based models for breast cancer risk,Science Translational Medicine,Article,1/27/2021,Regina,Barzilay,2021
10.1136/bmj.l6927,"© © Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",The BMJ,Article,3/20/2020,Rayid,Ghani,2020
10.1136/jme.15.2.74,"The psychosocial morbidity associated with HIV infection and responses to such infection may exceed morbidity associated with medical sequelae of such infection. This paper argues that negative judgements on those with HIV infection or in groups associated with such infection will cause avoidable psychological and social distress. Moral judgements made regarding HIV infection may also harm the common good by promoting conditions which may increase the spread of HIV infection. This paper examines these two lines of argument with regard to the ethical aspects of psychological bases of health care, clinical contact, public perceptions of AIDS and the comparative perspective. It is concluded that the psychosocial aspects of HIV infection impose ethical psychological, as well as medical, obligations to reduce harm and prevent the spread of infection.",Psychosocial ethical aspects of AIDS,Journal of Medical Ethics,Conference Paper,1/1/1989,W,Ross,1989
10.1137/1.9781611972801.78,"Many approaches to active learning involve periodically training one classifier and choosing data points with the lowest confidence. An alternative approach is to periodically choose data instances that maximize disagreement among the label predictions across an ensemble of classifiers. Many classifiers with different underlying structures could fit this framework, but some ensembles are more suitable for some data sets than others. The question then arises as to how to find the most suitable ensemble for a given data set. In this work we introduce a method that begins with a heterogeneous ensemble composed of multiple instances of different classifier types, which we call adaptive informative sampling (AIS). The algorithm periodically adds data points to the training set, adapts the ratio of classifier types in the heterogeneous ensemble in favor of the better classifier type, and optimizes the classifiers in the ensemble using stochastic methods. Experimental results show that the proposed method performs consistently better than homogeneous ensembles. Comparison with random sampling and uncertainty sampling shows that the algorithm effectively draws informative data points for training. Copyright © by SIAM.",Adaptive informative sampling for active learning,"Proceedings of the 10th SIAM International Conference on Data Mining, SDM 2010",Conference Paper,1/1/2010,Josh,Bongard,2010
10.1142/9789812794246_0004,© 2003 by World Scientific Publishing Co. Pte. Ltd.The following sections are included: • Introduction • Concepts of Statistical Learning Theory • Hypothesis Testing: an Example • Parameter Optimization: an Example • Mathematical Objectives • The Precision Criterion • The Fairness Criterion • Methodology • Models • Constant Model • Linear Model • Table-Based Methods • Greedy Multiplicative Model • Generalized Linear Model • CHAID Decision Trees • Combination of CHAID and Linear Model • Ordinary Neural Network • How Can Neural Networks Represent Nonlinear Interactions? • Softplus Neural Network • Regression Support Vector Machine • Mixture Models • Experimental Results • Mean-Squared Error Comparisons • Evaluating Model Fairness • Comparison with Current Premiums • Application to Risk Sharing Pool Facilities • Conclusion • Appendix • References.,Statistical learning algorithms applied to automobile insurance ratemaking,Intelligent And Other Computational Techniques In Insurance: Theory And Applications,Book Chapter,1/1/2003,Yoshua,Bengio,2003
10.1142/S0218213012400167,"Conformal Predictors (CPs) are Machine Learning algorithms that can provide reliable confidence measures to their predictions. In this work, we make use of the Conformal Prediction framework for the assessment of stroke risk based on ultrasound images of atherosclerotic carotid plaques. For this application, images were recorded from 137 asymptomatic and 137 symptomatic plaques (symptoms are Stroke, Transient Ischaemic Attack (TIA), and Amaurosis Fugax (AF)). Two feature sets were extracted from the plaques; the first based on morphological image analysis and the second based on image texture analysis. Both sets were used in order to evaluate the performance of CPs on this problem. Four CPs were constructed using four popular classification methods, namely Artificial Neural Networks (ANNs), Support Vector Machines (SVMs), Naive Bayes Classification (NBC), and k-Nearest Neighbours. The results given by all CPs demonstrate the reliability and importance of the obtained confidence measures on the problem of stroke risk assessment. © 2012 World Scientific Publishing Company.",Evaluation of the risk of stroke with confidence predictions based on ultrasound carotid image analysis,International Journal on Artificial Intelligence Tools,Article,8/1/2012,Alexander,Gammerman,2012
10.1142/S1793843011000601,"A novel theory of reflective consciousness, will and self is presented, based on modeling each of these entities using self-referential mathematical structures called hypersets. Pattern theory is used to argue that these exotic mathematical structures may meaningfully be considered as parts of the minds of physical systems, even finite computational systems. The hyperset models presented are hypothesized to occur as patterns within the ""moving bubble of attention"" of the human brain and any roughly human-mind-like AI system. These ideas appear to be compatible with both panpsychist and materialist views of consciousness, and probably other views as well. Their relationship with the CogPrime AI design and its implementation in the OpenCog software framework is elucidated in detail. © 2011 World Scientific Publishing Company.","Hyperset models of self, will and reflective consciousness",International Journal of Machine Consciousness,Article,6/1/2011,Ben,Goertzel,2011
10.1145/1007568.1007584,"© 2015 The Authors.Many Web sites, especially those that dynamically generate HTML pages to display the results of a user's query, present information in the form of list or tables. Current tools that allow applications to programmatically extract this information rely heavily on user input, often in the form of labeled extracted records. The sheer size and rate of growth of the Web make any solution that relies primarily on user input is infeasible in the long term. Fortunately, many Web sites contain much explicit and implicit structure, both in layout and content, that we can exploit for the purpose of information extraction. This paper describes an approach to automatic extraction and segmentation of records from Web tables. Automatic methods do not require any user input, but rely solely on the layout and content of the Web source. Our approach relies on the common structure of many Web sites, which present information as a list or a table, with a link in each entry leading to a detail page containing additional information about that item. We describe two algorithms that use redundancies in the content of table and detail pages to aid in information extraction. The first algorithm encodes additional information provided by detail pages as constraints and finds the segmentation by solving a constraint satisfaction problem. The second algorithm uses probabilistic inference to find the record segmentation. We show how each approach can exploit the web site structure in a general, domain-independent manner, and we demonstrate the effectiveness of each algorithm on a set of twelve Web sites.",Using the structure of web sites for automatic segmentation of tables,Proceedings of the ACM SIGMOD International Conference on Management of Data,Conference Paper,1/1/2004,Lise,Getoor,2004
10.1145/1102351.1102407,"Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortestfirst search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.",Learning the structure of markov logic networks,ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning,Conference Paper,12/1/2005,Pedro,Domingos,2005
10.1145/1145581.1145615,"A Semantic Web portal is a Web application that offers information and services related to a specific domain, and that has been developed with Semantic Web technology. For the time being, the main difference with respect to a traditional Web portal is based on technological aspects: traditional Web portals are based on standard Web technology (HTML, XML, servlets, JSPs, etc.); semantic portals are based on that technology plus the use of Semantic Web languages like RDF, RDF Schema and OWL. This paper describes a unifying architecture for both types of portals, based on the MVC paradigm, which is implemented in the ODESeW framework. ODESeW has been used successfully in the development of a set of portals for the management of European R&D projects and for the management of research groups.",A platform for the development of semantic web portals,ICWE'06: The Sixth International Conference on Web Engineering,Conference Paper,12/1/2006,Asunción Gómez,Pérez,2006
10.1145/1276920.1276926,"Interest in XML databases has been expanding rapidly over the last few years. In this paper, we study the problem of incorporating probabilistic information into XML databases. We propose the Probabilistic Interval XML (PIXML for short) data model in this paper. Using this data model, users can express probabilistic information within XML markups. In addition, we provide two alternative formal model-theoretic semantics for PIXML data. The first semantics is a global semantics which is relatively intuitive, but is not directly amenable to computation. The second semantics is a local semantics which supports efficient computation. We prove several correspondence results between the two semantics. To our knowledge, this is the first formal model theoretic semantics for probabilistic interval XML. We then provide an operational semantics that may be used to compute answers to queries and that is correct for a large class of probabilistic instances. © 2007 ACM.",Probabilistic interval XML,ACM Transactions on Computational Logic,Article,8/1/2007,Lise,Getoor,2007
10.1145/1321211.1321214,"Web standards such as XHTML and CSS are rapidly coming into practice and have many advantages, including compatibility, consistency across browsers, and increased ease of maintenance. Unfortunately large numbers of existing websites still use the deprecated tablebased layout style in which page style is unique to each page. Existing tools for automating the transition to stylesheets provide little help, converting page-by-page using a flattened structure and local inline styles rather than a common CSS stylesheet. This approach ignores hierarchical structure and defeats the main purpose of moving to the new standard, losing all of the advantages. In this work we present an automated method for converting table-based layout websites to standards-compliant modern CSS stylesheet-based websites using a two-step process. Pages of the site are first converted pageby- page using table recognition technology to preserve hierarchical structure and layout semantics in local styles. Software clone detection technology is then utilized to recognize common layout styles in the pages and extract and minimize them to a common CSS stylesheet for the site. The result is a maintainable, efficient modern standards-compliant website with the same look and feel as the original but with all the maintenance advantages of a custom programmed new site. Copyright © 2007 Andy Y. Mao, James R. Cordy and Thomas R. Dean.",Automated conversion of table-based websites to structured stylesheets using table recognition and clone detection,"Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research, CASCON '07",Conference Paper,12/1/2007,Thomas,Dean,2007
10.1145/1553374.1553440,"Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Learning MLN structure from a relational database involves learn-ing the clauses and weights. The state-of-the-art MLN structure learners all involve some element of greedily generating candidate clauses, and are susceptible to local optima. To address this prob- lem, we present an approach that directly utilizes the data in constructing candidates. A relational database can be viewed as a hypergraph with con-stants as nodes and relations as hyperedges. We find paths of true ground atoms in the hyper-graph that are connected via their arguments. To make this tractable (there are exponentially many paths in the hypergraph), we lift the hypergraph by jointly clustering the constants to form higher- level concepts, and nd paths in it. We variabilize the ground atoms in each path, and use them to form clauses, which are evaluated using a pseudo-likelihood measure. In our experiments on three real-world datasets, we find that our algorithm outperforms the state-of-the-art approaches. Copyright 2009.",Learning markov logic network structure via hypergraph lifting,ACM International Conference Proceeding Series,Conference Paper,9/15/2009,Pedro,Domingos,2009
10.1145/1569901.1569915,"A central tenet of embodied artificial intelligence is that intelligent behavior arises out of the coupled dynamics between an agent's body, brain and environment. It follows that the complexity of an agents's controller and morphology must match the complexity of a given task. However, more complex task environments require the agent to exhibit different behaviors, which raises the question as to how to distribute responsibility for these behaviors across the agents's controller and morphology. In this work a robot is trained to locomote and manipulate an object, but the assumption of functional specialization is relaxed: the robot has a segmented body plan in which the front segment may participate in locomotion and object manipulation, or it may specialize to only participate in object manipulation. In this way, selection pressure dictates the presence and degree of functional specialization rather than such specialization being enforced a priori. It is shown that for the given task, evolution tends to produce functionally specialized controllers, even though successful generalized controllers can also be evolved. Moreover, the robot's initial conditions and training order have little effect on the frequency of finding specialized controllers, while the inclusion of additional proprioceptive feedback increases this frequency. Copyright 2009 ACM.",Evolution of functional specialization in a morphologically homogeneous robot,"Proceedings of the 11th Annual Genetic and Evolutionary Computation Conference, GECCO-2009",Conference Paper,12/31/2009,Josh,Bongard,2009
10.1145/174147.169675,"Temporal events are regarded here as intervals on a time line, This paper deals with problems m reasoning about such intervals when the prccisc topological relationship between them is unknown or only partially specified, This work unifies notions of interval algebras in artificial intelligence with those of interval orders and mterwd gr~phs m combmatorlcs. The satqfahihty, znuumal Iabeltng, all solutLons. and all rcakatlons problems we considered for temporal (internal ) datti. Several versions are investigated by restricting the possible interval relationships yielding different complexity results, We show that even when the temporal data comprises of subsets of relatlons based on mtersectlon and precedence only, the satisfiabdlty question IS NP-complete. On the positive side, we give efficient algorithms for several restrictions of the problem. In the process, the irzterLa/,qrap/z satzdwzc/z problem is introduced, and is shown to be NP-complete This problem IS also important in molecular hlology, where it arlscs In physical mapping of DNA material. © 1993, ACM. All rights reserved.",Complexity and Algorithms for Reasoning About Time: A Graph-Theoretic Approach,Journal of the ACM (JACM),Article,1/11/1993,Martin Charles,Golumbic,1993
10.1145/1741906.1742049,"Traditionally reasoning systems have been implemented using symbolic methods of artificial intelligence. Connectionist methods of implementing reasoning systems form an alternative paradigm. Among the connectionist reasoning systems two types of representational methods can be used. They are i) localist and ii) distributed representational methods. In the literature, some localist methods for reasoning were used in connectionist systems. Since those systems used localist representations, advantages of distributed representations are not obtainable by them. In this paper, we describe the design and implementation of a connectionist knowledge based system which integrates a connectionist predicate logic reasoning system and a connectionist semantic network. The system uses distributed coarse-coded representations. The connectionist predicate logic system supports both simple rules as well as a complex rule having multiple conjunctions. Distributed representations have advantages of increased fault tolerance, graceful degradation of performance; neural plausibility, cognitive modeling and parallel distributed processing. The system besides showing above features allows the communication between these two connectionist systems and makes it possible to access the information of attributes and corresponding values from the connectionist semantic network for the entities used in the connectionist predicate logic system. Copyright 2010 ACM.",A connectionist knowledge based system using coarse-coded distributed representations,"ICWET 2010 - International Conference and Workshop on Emerging Trends in Technology 2010, Conference Proceedings",Conference Paper,5/21/2010,Pushpak,Bhattacharyya,2010
10.1145/2069618.2069676,"Copyright 2011 ACM.After providing some context via (i) earlier work on literary creativity carried out by Bringsjord et al., and (ii) an account of creativity espoused by Cope, which stands in rather direct opposition to Bringsjord's account, we summarize our nascent attempt to engineer an artificial conductor: Handle. Handle is a microcosmic version of part of a larger, much more ambitious system: CAIRA. Both are under development courtesy of a three-year CreativeIT grant from the National Science Foundation (PI Braasch, Co-PIs Oliveros & Bringsjord).",Creativity and conducting: Handle in the CAIRA Project,C and C 2011 - Proceedings of the 8th ACM Conference on Creativity and Cognition,Conference Paper,11/3/2011,Selmer,Bringsjord,2011
10.1145/2124295.2124351,"Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood. Copyright 2012 ACM.",Beyond ten blue links: Enabling user click modeling in federated Web search,WSDM 2012 - Proceedings of the 5th ACM International Conference on Web Search and Data Mining,Conference Paper,1/1/2012,Danqi,Chen,2012
10.1145/2157689.2157843,"Teamwork has become a widely accepted metaphor for describing the nature of multi-robot and multi-agent cooperation. By virtue of teamwork models, team members attempt to manage general responsibilities and commitments to each other in a coherent fashion that both enhances performance and facilitates recovery when unanticipated problems arise. Whereas early research on teamwork focused mainly on interaction within groups of autonomous agents or robots, there is a growing interest in leveraging human participation effectively. Unlike autonomous systems designed primarily to take humans out of the loop, many important applications require people, agents, and robots to work together in close and relatively continuous interaction. For software agents and robots to participate in teamwork alongside people in carrying out complex real-world tasks, they must have some of the capabilities that enable natural and effective teamwork among groups of people. Just as important, developers of such systems need tools and methodologies to assure that such systems will work together reliably and safely, even when they have been designed independently. The purpose of the HART workshop is to explore theories, methods, and tools in support of humans, agents and robots working together in teams. Position papers that combine findings from fields such as computer science, artificial intelligence, cognitive science, anthropology, social and organizational psychology, human-computer interaction to address the problem of HART are strongly encouraged. The workshop will formulate perspectives on the current state-of-the-art, identify key challenges and opportunities for future studies, and promote community-building among researchers and practitioners. The workshop will be structured around four two-hour sessions on themes relevant to HART. Each session will consist of presentations and questions on selected position papers, followed by a whole-group discussion of the current state-of-the-art and the key challenges and research opportunities relevant to the theme. During the final hour, the workshop organizers will facilitate a discussion to determine next steps. The workshop will be deemed a success when collaborative scientific projects for the coming year are defined, and publication venues are explored. For example, results from the most recent HART workshop (Lorentz Center, Leiden, The Netherlands, December 2010) will be reflected in a special issue of IEEE Intelligent Systems on HART that is slated to appear in January/February 2012. © 2012 Authors.",Human-agent-robot teamwork,HRI'12 - Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction,Conference Paper,4/26/2012,Virginia,Dignum,2012
10.1145/229459.229471,"Alan Turing has made a significant contribution to AI and computer science and on the future of these fields. In recognition of his works, ACM established the A.M. Turing Award given annually to researchers who excel on the same field considered by Alan Turing during his time. The age of electronic computers had barely dawned when Turing boldly and brilliantly envisioned their role as the medium for computational models of thought and their use as artifacts with intelligence. From Turing's works, it has been learned that envisioning is an important activity in science. It not only sets research directions, but also leads to important questions and motivations.",How the 'what' becomes the 'how',Communications of the ACM,Article,5/1/1996,Edward,Feigenbaum,1996
10.1145/2590748.2590766,"Textual use cases are commonly used to represent software requirements at initial stages. However in most of the cases, these documents are unstructured. In this paper, we present a linguistic engine for processing textual use cases and extract a structured model in terms of an annotation model out of these use cases. An annotation model of a use case can further be used to generate various UML requirements models, Business Process Models and ontology. The implementation details of Natural Language Processing (NLP) technique employed by us for the linguistic engine is described in this paper in detail. Also, we consider a corpus containing 123 use cases from real-life industrial projects within our company, and translate them into annotation models using our NLP technique. For evaluating the performance of conversion we use a few metrics and report some promising results for our linguistic engine. Copyright 2014 ACM.",Enforcing structure on textual use cases via annotation models,ACM International Conference Proceeding Series,Conference Paper,1/1/2014,Pushpak,Bhattacharyya,2014
10.1145/2739480.2754788,"© 2015 ACM.Despite recent demonstrations that deep learning methods can successfully recognize and categorize objects using high dimensional visual input, other recent work has shown that these methods can fail when presented with novel input. However, a robot that is free to interact with objects should be able to reduce spurious differences between objects belonging to the same class through motion and thus reduce the likelihood of overfitting. Here we demonstrate a robot that achieves more robust categorization when it evolves to use proprioceptive sensors and is then trained to rely increasingly on vision, compared to a similar robot that is trained to categorize only with visual sensors. This work thus suggests that embodied methods may help scaffold the eventual achievement of robust visual classification.",An embodied approach for evolving robust visual classifiers,GECCO 2015 - Proceedings of the 2015 Genetic and Evolutionary Computation Conference,Conference Paper,7/11/2015,Josh,Bongard,2015
10.1145/2818048.2820024,"© 2016 ACM.Successful online communities (e.g., Wikipedia, Yelp, and StackOverflow) can produce valuable content. However, many communities fail in their initial stages. Starting an online community is challenging because there is not enough content to attract a critical mass of active members. This paper examines methods for addressing this cold-start problem in datamining-bootstrappable communities by attracting non-members to contribute to the community. We make four contributions: 1) we characterize a set of communities that are ""datamining-bootstrappable"" and define the bootstrapping problem in terms of decision-theoretic optimization, 2) we estimate the model parameters in a case study involving the Open AI Resources website, 3) we demonstrate that non-members' predicted interest levels and request design are important features that can significantly affect the contribution rate, and 4) we ran a simulation experiment using data generated with the learned parameters and show that our decision-theoretic optimization algorithm can generate as much community utility when bootstrapping the community as our strongest baseline while issuing only 55% as many contribution requests.",Toward automatic bootstrapping of online communities using decision-theoretic optimization,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW",Conference Paper,2/27/2016,Oren,Etzioni,2016
10.1145/2933575.2935321,"© 2016 ACM.Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to a wide variety of problems in natural language understanding, vision, computational biology, social networks and others, and is the basis of the open-source Alchemy system.",Unifying Logical and Statistical AI,Proceedings - Symposium on Logic in Computer Science,Conference Paper,7/5/2016,Pedro,Domingos,2016
10.1145/2955091,"OPERATIONAL AI SYSTEMS (for example, self-driving cars) need to obey both the law of the land and our values. We propose AI oversight systems (""AI Guardians"") as an approach to addressing this challenge, and to respond to the potential risks associated with increasingly autonomous AI systems.a These AI oversight systems serve to verify that operational systems did not stray unduly from the guidelines of their programmers and to bring them back in compliance if they do stray. The introduction of such second-order, oversight systems is not meant to suggest strict, powerful, or rigid (from here on 'strong') controls. Operations systems need a great degree of latitude in order to follow the lessons of their learning from additional data mining and experience and to be able to render at least semiautonomous decisions (more about this later). However, all operational systems need some boundaries, both in order to not violate the law and to adhere to ethical norms. Developing such oversight systems, AI Guardians, is a major new mission for the AI community.",Viewpoint designing ai systems that obey our laws and values calling for research on automatic oversight for artificial intelligence systems.,Communications of the ACM,Review,9/1/2016,Oren,Etzioni,2016
10.1145/2979672,"State-of-the-art tools from machine learning and artificial intelligence are making inroads to automate parts of the peer-review process; however, many opportunities for further improvement remain. Profiling, matching, and open-world expert finding are key tasks that can be addressed using feature-based representations commonly used in machine learning. Such streamlining tools also offer perspectives on how the peer-review process might be improved, in particular, the idea of profiling naturally leads to a view of peer review being aimed at finding the best publication venue (if any) for a submitted paper. To aid assigning submitted papers to reviewers a short list of subject keywords is often required by mainstream CMS tools as part of the submission process, either from a controlled vocabulary, such as the ACM Computing Classification System (CCS),a or as a free-text folksonomy. When a pair of keywords does not literally match, despite having been chosen to refer to the same underlying concept, one technique often used to improve matching is to also match their synonyms or syntactic variants. Publishers and bibliographic databases like DBLP and Google Scholar have developed their own proprietary UID schemes for identifying contributors to published works. However, there is now considerable momentum behind the non-proprietary Open Researcher and Contributor ID (ORCID)e and publishers are increasingly mapping their own UIDs onto ORCID UIDs. Creating a more global embedding for the peer-review process that transcends individual conferences or conference series by means of persistent reviewer and author profiles is key to a more robust and less arbitrary peer-review process.",Computational support for academic peer review: A perspective from artificial intelligence,Communications of the ACM,Review,3/1/2017,Peter,Flach,2017
10.1145/301136.301233,"Jackal is a Java-based tool for communicating with the KQML agent communication language. Some features that make it extremely valuable to agent development axe its conversation management facilities, flexible, blackboard style interface and ease of integration. Jackal has been developed in support of an investigation of the use of agents in enterprise-wide integration of planning and execution for manufacturing.",Agent development with Jackal,Proceedings of the International Conference on Autonomous Agents,Conference Paper,1/1/1999,Tim,Finin,1999
10.1145/3079856.3080246,"© 2017 Association for Computing Machinery.Many architects believe that major improvements in cost-energyperformance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile responsetime requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an NVIDIA K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X-30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X-80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.",In-datacenter performance analysis of a tensor processing unit,Proceedings - International Symposium on Computer Architecture,Conference Paper,6/24/2017,Jeff,Dean,2017
10.1145/3109761.3158395,"© 2017 Association for Computing Machinery.Data mining and information extraction from data is a field that has gained relevance in recent years thanks to techniques based on artificial intelligence and use of machine and deep learning. The main aim of the present work is the development of a tool based on a previous behaviour study of security audit tools (oriented to SQL pentesting) with the purpose of creating testing sets capable of performing an accurate detection of a SQL attack. The study is based on the information collected through the generated web server logs in a pentesting laboratory environment. Then, making use of the common extracted patterns from the logs, each attack vector has been classified in risk levels (dangerous attack, normal attack, non-attack, etc.). Finally, a training with the generated data was performed in order to obtain a classifier system that has a variable performance between 97 and 99 percent in positive attack detection. The training data is shared to other servers in order to create a distributed network capable of deciding if a query is an attack or is a real petition and inform to connected clients in order to block the petitions from the attacker's IP.",Collaborative SQL-injections detection system with machine learning,ACM International Conference Proceeding Series,Conference Paper,10/17/2017,Pino Caballero,Gil,2017
10.1145/3122814,"Determining whether a system truly displays artificial intelligence is difficult and complex, and well-known assessments like the Turing Test are not suited to the task.The Allen Institute for Artificial Intelligence suggests that answering science exam questions successfully is a better measure of machine intelligence and designed a global competition to engage the research community in this approach. The outcome of the Allen AI Science Challenge highlights the current limitations of AI research in language understanding, reasoning, and commonsense knowledge.",Moving beyond the turing test with the allen AI science challenge,Communications of the ACM,Article,9/1/2017,Oren,Etzioni,2017
10.1145/3148011.3148020,"© 2017 Copyright held by the owner/author(s).Knowledge Graphs (KG) are becoming core components of most artificial intelligence applications. Linked Data, as a method of publishing KGs, allows applications to traverse within, and even out of, the graph thanks to global dereferenceable identifiers denoting entities, in the form of IRIs. However, as we show in this work, after analyzing several popular datasets (namely DBpedia, LOD Cache, and Web Data Commons JSON-LD data) many entities are being represented using literal strings where IRIs should be used, diminishing the advantages of using Linked Data. To remedy this, we propose an approach for identifying such strings and replacing them with their corresponding entity IRIs. The proposed approach is based on identifying relations between entities based on both ontological axioms as well as data profiling information and converting strings to entity IRIs based on the types of entities linked by each relation. Our approach showed 98% recall and 76% precision in identifying such strings and 97% precision in converting them to their corresponding IRI in the considered KG. Further, we analyzed how the connectivity of the KG is increased when new relevant links are added to the entities as a result of our method. Our experiments on a subset of the Spanish DBpedia data show that it could add 25% more links to the KG and improve the overall connectivity by 17%.",Repairing hidden links in linked data: Enhancing the quality of RDF knowledge graphs,"Proceedings of the Knowledge Capture Conference, K-CAP 2017",Conference Paper,12/4/2017,Asunción Gómez,Pérez,2017
10.1145/3170427.3170632,"© 2018 Copyright is held by the owner/author(s).With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community. This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions. The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR. There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community. This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals…) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.",General data protection regulation: An opportunity for the HCI community?,Conference on Human Factors in Computing Systems - Proceedings,Conference Paper,4/20/2018,Virginia,Dignum,2018
10.1145/3170427.3170661,"Copyright held by the owner/author(s).We will explore how deep learning approaches can be used for perceiving and interpreting the state and behavior of human beings in images, video, audio, and text data. The course will cover how convolutional, recurrent and generative neural networks can be used for applications of face recognition, eye tracking, cognitive load estimation, emotion recognition, natural language processing, voice-based interaction, and activity recognition. The course is open to beginners and is designed for those who are new to deep learning, but it can also benefit advanced researchers in the field looking for a practical overview of deep learning methods and their application.",Deep learning for understanding the human,Conference on Human Factors in Computing Systems - Proceedings,Conference Paper,4/20/2018,Lex,Fridman,2018
10.1145/3197026.3203910,© 2018 Authors.Editorial pre-screening is the first step in academic peer review. The deluge of research papers and the huge amount of submissions being made to journals these days makes editorial decision a very challenging task. The current work attempts to investigate certain impact factors that may have a role in the editorial decision making process. The proposed work exhibits potential for the development of an AI-assisted peer review system which could aid the editors as well as the authors in making appropriate decisions in reasonable time and thus accelerate the overall process of scholarly publishing.,Investigating Impact Features in Editorial Pre-Screening of Research Papers,Proceedings of the ACM/IEEE Joint Conference on Digital Libraries,Conference Paper,5/23/2018,Pushpak,Bhattacharyya,2018
10.1145/3205455.3205529,"© 2018 Association for Computing Machinery.Typically, AI researchers and roboticists try to realize intelligent behavior in machines by tuning parameters of a predefined structure (body plan and/or neural network architecture) using evolutionary or learning algorithms. Another but not unrelated longstanding property of these systems is their brittleness to slight aberrations, as highlighted by the growing deep learning literature on adversarial examples. Here we show robustness can be achieved by evolving the geometry of soft robots, their control systems, and how their material properties develop in response to one particular interoceptive stimulus (engineering stress) during their lifetimes. By doing so we realized robots that were equally fit but more robust to extreme material defects (such as might occur during fabrication or by damage thereafter) than robots that did not develop during their lifetimes, or developed in response to a different interoceptive stimulus (pressure). This suggests that the interplay between changes in the containing systems of agents (body plan and/or neural architecture) at different temporal scales (evolutionary and developmental) along different modalities (geometry, material properties, synaptic weights) and in response to different signals (interoceptive and external perception) all dictate those agents' abilities to evolve or learn capable and robust strategies.",Interoceptive robustness through environment-mediated morphological development,GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference,Conference Paper,7/2/2018,Josh,Bongard,2018
10.1145/3219819.3219835,"© 2018 Association for Computing Machinery.Water infrastructure in the United States is beginning to show its age, particularly through water main breaks. Main breaks cause major disruptions in everyday life for residents and businesses. Water main failures in Syracuse, N.Y. (as in most cities) are handled reactively rather than proactively. A barrier to proactive maintenance with limited resources is the city's inability to properly prioritize the allocation of its resources. We built a Machine Learning system to assess the risk of a water mains breaking. Using historical data on which mains have failed, descriptors of pipes, and other data sources, we evaluated several models' abilities to predict breaks three years into the future. Our results show that our system using gradient boosted decision trees performed best out of several algorithms and expert heuristics, achieving precision at 1% (P@1) of 0.62. Our model outperforms a random baseline (P@1 of 0.08) and expert heuristics such as water main age (P@1 of 0.10) and history of past main breaks (P@1 of 0.48). The model is currently deployed in the City of Syracuse. We are conducting a pilot by calculating the risk of failure for each city block over the period 2016-2018 using data up to the end of 2015 and, as of the end of 2017, there have been 42 breaks on our riskiest 52 mains. This has been a successful initiative for the city of Syracuse in improving its infrastructure and we believe this approach can be applied to other cities.",Using machine learning to assess the risk of and prevent water main breaks,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Conference Paper,7/19/2018,Rayid,Ghani,2018
10.1145/3219819.3219883,"© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.The SPHERE project is devoted to advancing eHealth in a smart-home context, and supports full-scale sensing and data analysis to enable a generic healthcare service. We describe, from a data-science perspective, our experience of taking the system out of the laboratory into more than thirty homes in Bristol, UK. We describe the infrastructure and processes that had to be developed along the way, describe how we train and deploy Machine Learning systems in this context, and give a realistic appraisal of the state of the deployed systems.",Releasing eHealth analytics into the wild: Lessons learnt from the SPHERE project,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Conference Paper,7/19/2018,Peter,Flach,2018
10.1145/3238797,"© 2018 Association for Computing Machinery.Existing supervised solutions for Named Entity Recognition (NER) typically rely on a large annotated corpus. Collecting large amounts of NER annotated corpus is time-consuming and requires considerable human effort. However, collecting small amounts of annotated corpus for any language is feasible, but the performance degrades due to data sparsity. We address the data sparsity by borrowing features from the data of a closely related language. We use hierarchical neural networks to train a supervised NER system. The feature borrowing from a closely related language happens via the shared layers of the network. The neural network is trained on the combined dataset of the low-resource language and a closely related language, also termed Multilingual Learning. Unlike existing systems, we share all layers of the network between the two languages. We apply multilingual learning for NER in Indian languages and empirically show the benefits over a monolingual deep learning system and a traditional machine-learning system with some feature engineering. Using multilingual learning, we show that the low-resource language NER performance increases mainly due to (1) increased named entity vocabulary, (2) cross-lingual subword features, and (3) multilingual learning playing the role of regularization.",Improving NER tagging performance in low-resource languages via multilingual learning,ACM Transactions on Asian and Low-Resource Language Information Processing,Article,12/1/2018,Pushpak,Bhattacharyya,2018
10.1145/3241978,© 2019 ACMMarkov logic can be used as a general framework for joining logical and statistical AI.,Unifying logical and statistical AI with Markov logic,Communications of the ACM,Review,7/1/2019,Pedro,Domingos,2019
10.1145/3273931,"© 2018 Association for Computing Machinery.In the era of deep learning-based systems, efficient input representation is one of the primary requisites in solving various problems related to Natural Language Processing (NLP), data mining, text mining, and the like. Absence of adequate representation for an input introduces the problem of data sparsity, and it poses a great challenge to solve the underlying problem. The problem is more intensified with resource-poor languages due to the absence of a sufficiently large corpus required to train a word embedding model. In this work, we propose an effective method to improve the word embedding coverage in less-resourced languages by leveraging bilingual word embeddings learned from different corpora. We train and evaluate deep Long Short Term Memory (LSTM)-based architecture and show the effectiveness of the proposed approach for two aspect-level sentiment analysis tasks (i.e., aspect term extraction and sentiment classification). The neural network architecture is further assisted by hand-crafted features for prediction. We apply the proposed model in two experimental setups: multi-lingual and cross-lingual. Experimental results show the effectiveness of the proposed approach against the state-of-the-art methods.",Improving word embedding coverage in less-resourced languages through multi-linguality and cross-linguality: A case study with aspect-based sentiment analysis,ACM Transactions on Asian and Low-Resource Language Information Processing,Conference Paper,12/1/2018,Pushpak,Bhattacharyya,2018
10.1145/3278721.3278733,"© 2018 ACM.AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.",Fairness in Relational Domains,"AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",Conference Paper,12/27/2018,Lise,Getoor,2018
10.1145/3297001.3297043,"© 2019 Association for Computing Machinery.Sarcasm is a form of speech in which the the implied sentiment is the opposite of literal meaning. In this paper, we present the task of sarcasm interpretation, defined as converting a sarcastic utterance into its non-sarcastic (literal) interpretation. We present three approaches for the task: (a) a rule-based approach that considers sarcasm as a form of dropped negation and associate negation words with verbs present in the sarcastic utterance, (b) statistical machine translation-based (SMT) approach that address the sarcasm interpretation task as monolingual machine translation and (c) three deep learning-based (DL) architectures, Encoder-Decoder Network, Attention Network and Pointer Generator Network. We also discuss the scope of future work to further enhance the proposed models for sarcasm interpretation.",Deep models for converting sarcastic utterances into their non sarcastic interpretation,ACM International Conference Proceeding Series,Conference Paper,1/3/2019,Pushpak,Bhattacharyya,2019
10.1145/3299869.3300079,"© 2019 Association for Computing Machinery.As data science and artificial intelligence become ubiquitous, they have an increasing impact on society. While many of these impacts are beneficial, others may not be. So understanding and managing these impacts is required of every responsible data scientist. Nevertheless, most human decision-makers use algorithms for efficiency purposes and not to make a better (i.e., fairer) decisions. Even the task of risk assessment in the criminal justice system enables efficiency instead of (and often at the expense of) fairness. So we need to frame the problem with fairness, and other societal impacts, as primary objectives.",The responsibility challenge for data,Proceedings of the ACM SIGMOD International Conference on Management of Data,Conference Paper,6/25/2019,Lise,Getoor,2019
10.1145/3314344.3332495,"© 2019 Copyright held by the owner/author(s).Citizen science programs have been instrumental in boosting sustainability projects, large-scale scientific discovery, and crowdsourced experimentation. Nevertheless, these programs witness challenges in submissions’ quality, such as sampling bias resulting from citizens’ preferences to complete some tasks over others. The sampling bias frequently manifests itself in the program’s dataset as spatially clustered submissions, which reduce the efficacy of the dataset for subsequent scientific studies. To address the spatial clustering problem, programs use reward schemes obtained from game-theoretical models to incentivize citizens to perform tasks that are more meaningful from a scientific point of view. Herein we propose a GPU-accelerated approach for the Avicaching game, which was recently introduced by the eBird citizen science program to incentivize birdwatchers to collect bird data from under-sampled locations. Avicaching is a Principal-Agent game, in which the principal corresponds to the citizen science program (eBird) and the agents to the birdwatchers or citizen scientists. Previous approaches for solving the Avicaching game used approximations based on mixed-integer programming and knapsack algorithms combined with learning algorithms, using standard CPU hardware. Following the recent advances in scalable deep learning and parallel computation on Graphical Processing Units (GPUs), we propose a novel approach to solve the Avicaching game, which takes advantage of neural networks and parallelism for large-scale games. We demonstrate that our approach better captures agents’ behavior, which allows better learning and more effective incentive distribution in a real-world bird observation dataset. Our approach also allows for massive speedups using GPUs. As Avicaching is representative of games that are aimed at reducing spatial clustering in citizen science programs, our scalable reformulation for Avicaching enables citizen science programs to tackle sampling bias and improve submission quality on a large scale.",GPU-accelerated principal-agent game for scalable citizen science,COMPASS 2019 - Proceedings of the 2019 Conference on Computing and Sustainable Societies,Conference Paper,7/3/2019,Carla,Gomes,2019
10.1145/3338501.3357369,"© 2019 Copyright held by the owner/author(s).Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%.",Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy,Proceedings of the ACM Conference on Computer and Communications Security,Conference Paper,11/11/2019,Yoshua,Bengio,2019
10.1145/3339399,"© 2019 ACMTHESE ARE EXCITING times for computational sciences with the digital revolution permeating a variety of areas and radically transforming business, science, and our daily lives. The Internet and the World Wide Web, GPS, satellite communications, remote sensing, and smartphones are dramatically accelerating the pace of discovery, engendering globally connected networks of people and devices. The rise of practically relevant artificial intelligence (AI) is also playing an increasing part in this revolution, fostering e-commerce, social networks, personalized medicine, IBM Watson and AlphaGo, self-driving cars, and other groundbreaking transformations. Unfortunately, humanity is also facing tremendous challenges. Nearly a billion people still live below the international poverty line and human activities and climate change are threatening our planet and the livelihood of current and future generations. Moreover, the impact of computing and information technology has been uneven, mainly benefiting profitable sectors, with fewer societal and environmental benefits, further exacerbating inequalities and the destruction of our planet.",Computational sustainability: <inf>Computing for</inf> a better world and a sustainable future,Communications of the ACM,Article,9/1/2019,Carla,Gomes,2019
10.1145/3351095.3372829,"© 2020 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.",Lessons from archives: Strategies for collecting sociocultural data in machine learning,"FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",Conference Paper,1/27/2020,Timnit,Gebru,2020
10.1145/3351095.3372870,"© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.",Explainability fact sheets: A framework for systematic assessment of explainable approaches,"FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",Conference Paper,1/27/2020,Peter,Flach,2020
10.1145/3358912,"AI is a communication medium between developers and users, not just users-to-users. As with other communication mediums, symbolic representation is important to people using the medium. Furthermore, symbolic annihilation of representation negatively affects everyone. Humanlike AI interfaces must be crafted in a way that offers fair representation of potential users.",Why project q is more than the world's first nonbinary voice for technology,Interactions,Article,11/1/2019,Julie,Carpenter,2019
10.1145/3372328,"© 2020 ACM.Event extraction is one of the crucial tasks in biomedical text mining that aims to extract specific information concerning incidents embedded in the texts. In this article, we propose a deep learning framework that aims to identify the attributes (severity, course, temporal expression, and document creation time) associated with the medical concepts extracted from electronic medical records. The bi-directional long short-term memory network assisted by the attention mechanism is utilized to uncover the important aspects of the patient's medical conditions. The attention mechanism specific to the medical disorder mention can focus on various parts of the sentence when different disorders are considered as input. The proposed methodology is evaluated on benchmark ShARe/CLEF eHealth Evaluation Lab 2014 shared task 2 datasets. In addition to the CLEF dataset, we also used the social media text, especially the medical blog posts. Experimental results of the proposed approach illustrate that our proposed approach achieves significant performance improvements over the state-of-the-art techniques and the highly competitive deep learning - based baseline methods.",Exploring disorder-aware attention for clinical event extraction,"ACM Transactions on Multimedia Computing, Communications and Applications",Article,4/1/2020,Pushpak,Bhattacharyya,2020
10.1145/3375627.3375832,"© 2020 Copyright held by the owner/author(s).The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.",Diversity and inclusion metrics in subset selection,"AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",Conference Paper,2/7/2020,Timnit,Gebru,2020
10.1145/3375627.3375855,"© 2020 Copyright held by the owner/author(s).The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.",Robot rights? Let's talk about humanwelfare instead,"AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",Conference Paper,2/7/2020,Abeba,Birhane,2020
10.1145/3381831,"Researchers suggest that creating efficiency in artificial intelligence (AI) research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets. The field of AI has reported remarkable progress on a broad range of capabilities including object recognition, game playing, speech recognition, and machine translation. Much of this progress has been achieved by increasingly large and computationally intensive deep learning models. An important study has estimated the carbon footprint of several NLP models and argued this trend is both environmentally unfriendly and prohibitively expensive, raising barriers to participation in NLP research, which is known as Red AI. An alternative is Green AI, which treats efficiency as a primary evaluation criterion along with accuracy. To measure efficiency, we suggest reporting the number of floating-point operations required to generate a result. Green AI research will decrease AI’s environmental footprint and increase its inclusivity.",Green AI,Communications of the ACM,Article,11/17/2020,Oren,Etzioni,2020
10.1145/3412841.3441926,"© 2021 ACM.The last decade has seen amazing performance improvements in deep learning. However, the black-box nature of this approach makes it difficult to provide explanations of the generated models. In some fields such as psychology and neuroscience, this limitation in explainability and interpretability is an important issue. Approaches such as genetic programming are well positioned to take the lead in these fields because of their inherent white box nature. Genetic programming, inspired by Darwinian theory of evolution, is a population-based search technique capable of exploring a high-dimensional search space intelligently and discovering multiple solutions. However, it is prone to generate very large solutions, a phenomenon often called ""bloat"". The bloated solutions are not easily understandable. In this paper, we propose two techniques for simplifying the generated models. Both techniques are tested by generating models for a well-known psychology experiment. The validity of these techniques is further tested by applying them to a symbolic regression problem. Several population dynamics are studied to make sure that these techniques are not compromising diversity - an important measure for finding better solutions. The results indicate that the two techniques can be both applied independently and simultaneously and that they are capable of finding solutions at par with those generated by the standard GP algorithm - but with significantly reduced program size. There was no loss in diversity nor reduction in overall fitness. In fact, in some experiments, the two techniques even improved fitness.",On-the-fly simplification of genetic programming models,Proceedings of the ACM Symposium on Applied Computing,Conference Paper,3/22/2021,Fernand,Gobet,2021
10.1145/3422622,"© 2020 ACM.Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.",Generative adversarial networks,Communications of the ACM,Article,10/22/2020,Yoshua,Bengio,2020
10.1145/3474381,"© 2021 Owner/Author.Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question-whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-The-Art models achieve considerably lower accuracy (59.4%-79.1%) on WINOGRANDE compared to humans (94%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset. Furthermore, we report new state-of-The-Art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.",WinoGrande,Communications of the ACM,Article,9/1/2021,Yejin,Choi,2021
10.1145/3479601,"© 2021 ACM.Co-design is a widely applied design process with well-documented values, including mutual learning and collective creativity. However, the real-world challenges of conducting multidisciplinary co-design research to inform the design of self-care technologies are not well established. We provide a qualitative account of a multidisciplinary project that aimed to co-design machine learning applications for Type 1 Diabetes (T1D) self-management. Through interviews, we identify not only perceived social, technological and strategic benefits of co-design but also organisational, translational and pragmatic design challenges: participants with T1D experienced difficulties in co-designing systems that met their individual self-care needs as part of group activities; HCI and AI researchers described challenges resulting from applying co-design outcomes to data-driven ML work; and industry collaborators highlighted academic data sharing regulations as cross-organisational challenges that can impede co-design efforts. Based on this understanding, we discuss opportunities for supporting multidisciplinary collaborations and aligning individual health needs with collaborative co-design activities.",Co-Designing Personal Health? Multidisciplinary Benefits and Challenges in Informing Diabetes Self-Care Technologies,Proceedings of the ACM on Human-Computer Interaction,Article,10/18/2021,Peter,Flach,2021
10.1145/3483597,"© 2021 Association for Computing Machinery.Image captioning refers to the process of generating a textual description that describes objects and activities present in a given image. It connects two fields of artificial intelligence, computer vision, and natural language processing. Computer vision and natural language processing deal with image understanding and language modeling, respectively. In the existing literature, most of the works have been carried out for image captioning in the English language. This article presents a novel method for image captioning in the Hindi language using encoder-decoder based deep learning architecture with efficient channel attention. The key contribution of this work is the deployment of an efficient channel attention mechanism with bahdanau attention and a gated recurrent unit for developing an image captioning model in the Hindi language. Color images usually consist of three channels, namely red, green, and blue. The channel attention mechanism focuses on an image's important channel while performing the convolution, which is basically to assign higher importance to specific channels over others. The channel attention mechanism has been shown to have great potential for improving the efficiency of deep convolution neural networks (CNNs). The proposed encoder-decoder architecture utilizes the recently introduced ECA-NET CNN to integrate the channel attention mechanism. Hindi is the fourth most spoken language globally, widely spoken in India and South Asia; it is India's official language. By translating the well-known MSCOCO dataset from English to Hindi, a dataset for image captioning in Hindi is manually created. The efficiency of the proposed method is compared with other baselines in terms of Bilingual Evaluation Understudy (BLEU) scores, and the results obtained illustrate that the method proposed outperforms other baselines. The proposed method has attained improvements of 0.59%, 2.51%, 4.38%, and 3.30% in terms of BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores, respectively, with respect to the state-of-The-Art. Qualities of the generated captions are further assessed manually in terms of adequacy and fluency to illustrate the proposed method's efficacy.",Efficient Channel Attention Based Encoder-Decoder Approach for Image Captioning in Hindi,ACM Transactions on Asian and Low-Resource Language Information Processing,Article,5/1/2022,Pushpak,Bhattacharyya,2022
10.1145/3486622.3493937,"© 2021 ACM.AI applications find widespread use in a variety of domains. For further acceptance, mostly when multiple agents interact with the system, we must aim to preserve the privacy of participants information in such applications. Towards this, the Yao's Millionaires' problem (YMP), i.e., to determine the richer among two millionaires' privately, finds relevance. This work presents a novel, practical, and verifiable solution to YMP, namely, Secure Comparison Protocol (SCP). We show that SCP achieves this comparison in a constant number of rounds, without using encryption and not requiring the participants' continuous involvement. SCP uses semi-trusted third parties - which we refer to as privacy accountants - for the comparison, who do not learn any information about the values. That is, the probability of information leak is negligible in the problem size. In SCP, we also leverage the Ethereum network for pseudo-anonymous communication, unlike computationally expensive secure channels such as Tor. We present a Secure Truthful cOmbinatorial aUction Protocol (STOUP) for single-minded bidders to demonstrate SCP's significance. We show that STOUP, unlike previous works, preserves the privacies relevant to an auction even from the auctioneer. We demonstrate the practicality of STOUP through simulations.",Blockchain-based Practical Multi-agent Secure Comparison and its Application in Auctions,ACM International Conference Proceeding Series,Conference Paper,12/14/2021,Boi,Faltings,2021
10.1145/3488560.3500242,"© 2022 Owner/Author.Scale appears to be the winning recipe in today's AI leaderboards. And yet, extreme-scale neural models are still brittle to make errors that are often nonsensical and even counterintuitive. In this talk, I will argue for the importance of knowledge, especially commonsense knowledge, and demonstrate how smaller models developed in academia can still have an edge over larger industry-scale models, if powered with knowledge. First, I will introduce ""symbolic knowledge distillation"", a new framework to distill larger neural language models into smaller commonsense models, which leads to a machine-authored KB that wins, for the first time, over a human-authored KB in all criteria: scale, accuracy, and diversity. Next, I will present an experimental conceptual framework toward computational social norms and commonsense morality, so that neural language models can learn to reason that ""helping a friend""is generally a good thing to do, but ""helping a friend spread fake news""is not. Finally, I will discuss an approach to multimodal script knowledge demonstrating the power of complex raw data, which leads to new SOTA performances on a dozen leaderboards that require grounded, temporal, and causal commonsense reasoning.","Knowledge is power: Symbolic knowledge distillation, commonsense morality, &amp; multimodal script knowledge",WSDM 2022 - Proceedings of the 15th ACM International Conference on Web Search and Data Mining,Conference Paper,2/11/2022,Yejin,Choi,2022
10.1145/3531146.3533083,"© 2022 Owner/Author.Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",The Values Encoded in Machine Learning Research,ACM International Conference Proceeding Series,Conference Paper,6/21/2022,Abeba,Birhane,2022
10.1145/3531146.3533157,"© 2022 Owner/Author.How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people's lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people's experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.",The Forgotten Margins of AI Ethics,ACM International Conference Proceeding Series,Conference Paper,6/21/2022,Abeba,Birhane,2022
10.1145/3536220.3558038,"© 2022 ACM.There has been growing interest in using deep learning techniques to recognize emotions from speech. However, real-life emotion datasets collected in call centers are relatively rare and small, making the use of deep learning techniques quite challenging. This research focuses on the study of Transformer-based models to improve the speech emotion recognition of patients' speech in French emergency call center dialogues. The experiments were conducted on a corpus called CEMO, which was collected in a French emergency call center. It includes telephone conversations with more than 800 callers and 6 agents. Four emotion classes were selected for these experiments: Anger, Fear, Positive and Neutral state. We compare different Transformer encoders based on the wav2vec2 and BERT models, and explore their fine-tuning as well as fusion of the encoders for emotion recognition from speech. Our objective is to explore how to use these pre-trained models to improve model robustness in the context of a real-life application. We show that the use of specific pre-trained Transformer encoders improves the model performance for emotion recognition in the CEMO corpus. The Unweighted Accuracy (UA) of the french pre-trained wav2vec2 adapted to our task is 73.1%, whereas the UA of our baseline model (Temporal CNN-LSTM without pre-training) is 55.8%. We also tested BERT encoders models: in particular FlauBERT obtained good performance for both manual 67.1% and automatic 67.9% transcripts. The late and model-level fusion of the speech and text models also improve performance (77.1% (late) - 76.9% (model-level)) compared to our best speech pre-trained model, 73.1% UA. In order to place our work in the scientific community, we also report results on the widely used IEMOCAP corpus with our best fusion strategy, 70.8% UA. Our results are promising for constructing more robust speech emotion recognition system for real-world applications.",Investigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations.,ACM International Conference Proceeding Series,Conference Paper,11/7/2022,Laurence,Devillers,2022
10.1145/3544793.3560383,"© 2022 ACM.The circulation of knowledge is an important function for enriching our social life. Knowledge circulation, which has traditionally been conducted manually, is undergoing a transformation in the age of artificial intelligence. In this paper, we describe our project called ""Learning Cyclotron (LeCycl),""which was initiated to accelerate the circulation of knowledge for building a knowledge ecosystem based on artificial intelligence technology. The three functions of sensing, mastering, and transferring knowledge are effectively operated through the power of AI-empowered digital nudging strategies. We outline what has been accomplished to date and summarize future directions for the ultimate goal of LeCycl.",Learning Cyclotron: An Ecosystem of Knowledge Circulation,UbiComp/ISWC 2022 Adjunct - Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2022 ACM International Symposium on Wearable Computers,Conference Paper,9/11/2022,Laurence,Devillers,2022
10.1145/3545947.3576267,"© 2022 Owner/Author.As artificial intelligence involves and shapes personal and professional lives, there is a critical need to nurture and prepare AI-enabled problem-solvers. FutureMakers is designed as a six-week program that introduces foundational knowledge and essential skills to develop innovative solutions with AI responsibly. Our study utilized a convergent mixed-method design to evaluate the impact of the FutureMakers program on students' learning outcomes and shifting perspectives on AI. Quantitative data showed a shift in students' AI literacy with a large effect size. Qualitative data, based on student interviews, showed an awareness of an ethical engineering design process in applying technical skills to solve real-world problems. The program showed the impact of the computational action approach to tackle authentic challenges.",Designing a Computational Action Program to Tackle Global Challenges,SIGCSE 2023 - Proceedings of the 54th ACM Technical Symposium on Computer Science Education,Conference Paper,3/6/2023,Hal,Abelson,2023
10.1145/3551624.3555290,"© 2022 Owner/Author.Participatory approaches to artificial intelligence (AI) and machine learning (ML) are gaining momentum: the increased attention comes partly with the view that participation opens the gateway to an inclusive, equitable, robust, responsible and trustworthy AI. Among other benefits, participatory approaches are essential to understanding and adequately representing the needs, desires and perspectives of historically marginalized communities. However, there currently exists lack of clarity on what meaningful participation entails and what it is expected to do. In this paper we first review participatory approaches as situated in historical contexts as well as participatory methods and practices within the AI and ML pipeline. We then introduce three case studies in participatory AI. Participation holds the potential for beneficial, emancipatory and empowering technology design, development and deployment while also being at risk for concerns such as cooptation and conflation with other activities. We lay out these limitations and concerns and argue that as participatory AI/ML becomes in vogue, a contextual and nuanced understanding of the term as well as consideration of who the primary beneficiaries of participatory activities ought to be constitute crucial factors to realizing the benefits and opportunities that participation brings.",Power to the People? Opportunities and Challenges for Participatory AI,ACM International Conference Proceeding Series,Conference Paper,10/6/2022,Abeba,Birhane,2022
10.1145/376284.375731,"A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.",Reconciling schemas of disparate data sources: A machine-learning approach,SIGMOD Record (ACM Special Interest Group on Management of Data),Article,1/1/2001,Pedro,Domingos,2001
10.1145/584792.584868,"We describe an approach to retrieval of documents that contain of both free text and semantically enriched markup. In particular, we present the design and implementation prototype of a framework in which both documents and queries can be marked up with statements in the DAML+OIL semantic web language. These statements provide both structured and semi-structured information about the documents and their content. We claim that indexing text and semantic markup together will significantly improve retrieval performance. Our approach allows inferencing to be done over this information at several points: when a document is indexed, when a query is processed and when query results are evaluated.",Information retrieval on the semantic web,"International Conference on Information and Knowledge Management, Proceedings",Conference Paper,1/1/2002,Tim,Finin,2002
10.1145/584993.585017,"Access to cultural exhibits is a central issue in museums and exhibition galleries that is recently approached under a new, technological perspective. Although the cultural industries' practices in the cases of museums and cultural exhibits have remained practically unchanged for long, in recent years we are witnessing a gradual adoption of media-technologies in various aspects, such as collections archiving and digital document preservation, media- and Web-presentation, graphical animations, etc. Lately, Internet and Web-based technologies have been employed for providing access, mostly to images of exhibited objects. In few cases, the incorporation of higher-end technology, such as virtual reality, artificial intelligence, or robotics, is explored. In this paper we present such an effort, the TOURBOT project (an acronym for TOUr-guide RoBOT), which emphasizes the development of alternative ways for interactive museum tele-presence, essentially through the use of robotic ""avatars"", and comment on the experience gained from its use in a museum setting.",Experiences from the use of a Robotic Avatar in a Museum Setting,"Proceedings VAST 2001 Virtual Reality, Archeology, and Cultural Heritage",Conference Paper,1/1/2001,Wolfram,Burgard,2001
10.1145/602382.602400,"The challenges and grand challenges for researchers doing computational intelligence, in the artificial intelligence area of computer science, are discussed. There is a need to facilitate the building of general and specific ontologies. It is suggested to give the multitude of web page creators a markup language in which each can do an extensive semantic markup of textual submission. To implement the Grand Vision, knowledge engineers must build a system of 'semantic scrapers' that will access the semantic markups, integrate them appropriately into the growing knowledge base.",Some challenges and grand challenges for computational intelligence,Journal of the ACM,Review,1/1/2003,Edward,Feigenbaum,2003
10.1145/800191.805529,"© 1976 ACM.In the effort to construct intelligent computer systems, a primary consideration is how to represent large amounts of knowledge in a fashion that permits their effective use and interaction . Indeed, many researchers in the field of artificial intelligence have come to believe that knowledge representation is the fundamental issue in the attempt to understand intelligence . The presentations in this session explore this issue and describe two current important knowledge representation methodologies, namely frames and semantic nets.",The role of representation in artificial intelligence,"Proceedings of the 1976 Annual conference, ACM 1976",Conference Paper,10/20/1976,Richard,Fikes,1976
10.1145/945645.945665,"Copyright 2003 ACM.Acquiring knowledge has long been the major bottleneck preventing the rapid spread of AI systems. Manual approaches are slow and costly. Machine-learning approaches have limitations in the depth and breadth of knowledge they can acquire. The spread of the Internet has made possible a third solution: building knowledge bases by mass collaboration, with thousands of volunteers contributing simultaneously. While this approach promises large improvements in the speed and cost of knowledge base development, it can only succeed if the problem of ensuring the quality, relevance and consistency of the knowledge is addressed, if contributors are properly motivated, and if the underlying algorithms scale. In this paper we propose an architecture that meets all these desiderata. It uses first-order probabilistic reasoning techniques to combine potentially inconsistent knowledge sources of varying quality, and it uses machine-learning techniques to estimate the quality of knowledge. We evaluate the approach using a series of synthetic knowledge bases and a pilot study in the domain of printer troubleshooting.",Building large knowledge bases by mass collaboration,"Proceedings of the 2nd International Conference on Knowledge Capture, K-CAP 2003",Conference Paper,10/23/2003,Pedro,Domingos,2003
10.1145/956750.956767,"As product prices become increasingly available on the World Wide Web, consumers attempt to understand how corporations vary these prices over time. However, corporations change prices based on proprietary algorithms and hidden variables (e.g., the number of unsold seats on a flight). Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions?This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period. When trained on this data, Hamlet - our multi-strategy data mining algorithm - generated a predictive model that saved 341 simulated passengers $198,074 by advising them when to buy and when to postpone ticket purchases. Remarkably, a clairvoyant algorithm with complete knowledge of future prices could save at most $320,572 in our simulation, thus HAMLET's savings were 61.8% of optimal. The algorithm's savings of $198,074 represents an average savings of 23.8% for the 341 passengers for whom savings are possible. Overall, HAMLET saved 4.4% of the ticket price averaged over the entire set of 4,488 simulated passengers. Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum. Copyright 2003 ACM.",To buy or not to buy: Mining airfare data to minimize ticket purchase price,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Conference Paper,12/1/2003,Oren,Etzioni,2003
10.1148/radiol.2018180694,"© RSNA, 2018.Purpose: To develop a deep learning (DL) algorithm to assess mammographic breast density. Materials and Methods: In this retrospective study, a deep convolutional neural network was trained to assess Breast Imaging Reporting and Data System (BI-RADS) breast density based on the original interpretation by an experienced radiologist of 41 479 digital screening mammograms obtained in 27 684 women from January 2009 to May 2011. The resulting algorithm was tested on a held-out test set of 8677 mammograms in 5741 women. In addition, five radiologists performed a reader study on 500 mammograms randomly selected from the test set. Finally, the algorithm was implemented in routine clinical practice, where eight radiologists reviewed 10 763 consecutive mammograms assessed with the model. Agreement on BI-RADS category for the DL model and for three sets of readings-(a) radiologists in the test set, (b) radiologists working in consensus in the reader study set, and (c) radiologists in the clinical implementation set-were estimated with linear-weighted k statistics and were compared across 5000 bootstrap samples to assess significance. Results: The DL model showed good agreement with radiologists in the test set (k = 0.67; 95% confidence interval [CI]: 0.66, 0.68) and with radiologists in consensus in the reader study set (k = 0.78; 95% CI: 0.73, 0.82). There was very good agreement (k = 0.85; 95% CI: 0.84, 0.86) with radiologists in the clinical implementation set; for binary categorization of dense or nondense breasts, 10 149 of 10 763 (94%; 95% CI: 94%, 95%) DL assessments were accepted by the interpreting radiologist. Conclusion: This DL model can be used to assess mammographic breast density at the level of an experienced mammographer.",Mammographic breast density assessment using deep learning: Clinical implementation,Radiology,Article,1/1/2019,Regina,Barzilay,2019
10.1148/radiol.2019182716,"© 2019 Radiological Society of North America Inc.. All rights reserved.Background: Mammographic density improves the accuracy of breast cancer risk models. However, the use of breast density is limited by subjective assessment, variation across radiologists, and restricted data. A mammography-based deep learning (DL) model may provide more accurate risk prediction. Purpose: To develop a mammography-based DL breast cancer risk model that is more accurate than established clinical breast cancer risk models. Materials and Methods: This retrospective study included 88 994 consecutive screening mammograms in 39 571 women between January 1, 2009, and December 31, 2012. For each patient, all examinations were assigned to either training, validation, or test sets, resulting in 71 689, 8554, and 8751 examinations, respectively. Cancer outcomes were obtained through linkage to a regional tumor registry. By using risk factor information from patient questionnaires and electronic medical records review, three models were developed to assess breast cancer risk within 5 years: a risk-factor-based logistic regression model (RF-LR) that used traditional risk factors, a DL model (image-only DL) that used mammograms alone, and a hybrid DL model that used both traditional risk factors and mammograms. Comparisons were made to an established breast cancer risk model that included breast density (Tyrer-Cuzick model, version 8 [TC]). Model performance was compared by using areas under the receiver operating characteristic curve (AUCs) with DeLong test (P < .05). Results: The test set included 3937 women, aged 56.20 years ± 10.04. Hybrid DL and image-only DL showed AUCs of 0.70 (95% confidence interval [CI]: 0.66, 0.75) and 0.68 (95% CI: 0.64, 0.73), respectively. RF-LR and TC showed AUCs of 0.67 (95% CI: 0.62, 0.72) and 0.62 (95% CI: 0.57, 0.66), respectively. Hybrid DL showed a significantly higher AUC (0.70) than TC (0.62; P < .001) and RF-LR (0.67; P = .01). Conclusion: Deep learning models that use full-field mammograms yield substantially improved risk discrimination compared with the Tyrer-Cuzick (version 8) model.",A deep learning mammography-based model for improved breast cancer risk prediction,Radiology,Article,1/1/2019,Regina,Barzilay,2019
10.1148/radiol.2019182908,"© RSNA, 2019Background: Recent deep learning (DL) approaches have shown promise in improving sensitivity but have not addressed limitations in radiologist specificity or efficiency. Purpose: To develop a DL model to triage a portion of mammograms as cancer free, improving performance and workflow efficiency. Materials and Methods: In this retrospective study, 223 109 consecutive screening mammograms performed in 66 661 women from January 2009 to December 2016 were collected with cancer outcomes obtained through linkage to a regional tumor registry. This cohort was split by patient into 212 272, 25 999, and 26 540 mammograms from 56 831, 7021, and 7176 patients for training, validation, and testing, respectively. A DL model was developed to triage mammograms as cancer free and evaluated on the test set. A DL-triage workflow was simulated in which radiologists skipped mammograms triaged as cancer free (interpreting them as negative for cancer) and read mammograms not triaged as cancer free by using the original interpreting radiologists’ assessments. Sensitivities, specificities, and percentage of mammograms read were calculated, with and without the DL-triage–simulated workflow. Statistics were computed across 5000 bootstrap samples to assess confidence intervals (CIs). Specificities were compared by using a two-tailed t test (P , .05) and sensitivities were compared by using a one-sided t test with a noninferiority margin of 5% (P , .05). Results: The test set included 7176 women (mean age, 57.8 years 6 10.9 [standard deviation]). When reading all mammograms, radiologists obtained a sensitivity and specificity of 90.6% (173 of 191; 95% CI: 86.6%, 94.7%) and 93.5% (24 625 of 26 349; 95% CI: 93.3%, 93.9%). In the DL-simulated workflow, the radiologists obtained a sensitivity and specificity of 90.1% (172 of 191; 95% CI: 86.0%, 94.3%) and 94.2% (24 814 of 26 349; 95% CI: 94.0%, 94.6%) while reading 80.7% (21 420 of 26 540) of the mammograms. The simulated workflow improved specificity (P = .002) and obtained a noninferior sensitivity with a margin of 5% (P , .001). Conclusion: This deep learning model has the potential to reduce radiologist workload and significantly improve specificity without harming sensitivity.",A deep learning model to triage screening mammograms: A simulation study,Radiology,Article,1/1/2019,Regina,Barzilay,2019
10.1155/2019/7206096,"© 2019 Moisés Lodeiro-Santiago et al.This work presents a system to detect small boats (pateras) to help tackle the problem of this type of perilous immigration. The proposal makes extensive use of emerging technologies like Unmanned Aerial Vehicles (UAV) combined with a top-performing algorithm from the field of artificial intelligence known as Deep Learning through Convolutional Neural Networks. The use of this algorithm improves current detection systems based on image processing through the application of filters thanks to the fact that the network learns to distinguish the aforementioned objects through patterns without depending on where they are located. The main result of the proposal has been a classifier that works in real time, allowing the detection of pateras and people (who may need to be rescued), kilometres away from the coast. This could be very useful for Search and Rescue teams in order to plan a rescue before an emergency occurs. Given the high sensitivity of the managed information, the proposed system includes cryptographic protocols to protect the security of communications.",Secure UAV-based system to detect small boats using neural networks,Complexity,Article,1/1/2019,Pino Caballero,Gil,2019
10.1155/2021/9977751,"© 2021 Fatmah Abdulrahman Baothman.Artificial intelligence (AI) is progressively changing techniques of teaching and learning. In the past, the objective was to provide an intelligent tutoring system without intervention from a human teacher to enhance skills, control, knowledge construction, and intellectual engagement. This paper proposes a definition of AI focusing on enhancing the humanoid agent Nao's learning capabilities and interactions. The aim is to increase Nao intelligence using big data by activating multisensory perceptions such as visual and auditory stimuli modules and speech-related stimuli, as well as being in various movements. The method is to develop a toolkit by enabling Arabic speech recognition and implementing the Haar algorithm for robust image recognition to improve the capabilities of Nao during interactions with a child in a mixed reality system using big data. The experiment design and testing processes were conducted by implementing an AI principle design, namely, the three-constituent principle. Four experiments were conducted to boost Nao's intelligence level using 100 children, different environments (class, lab, home, and mixed reality Leap Motion Controller (LMC). An objective function and an operational time cost function are developed to improve Nao's learning experience in different environments accomplishing the best results in 4.2 seconds for each number recognition. The experiments' results showed an increase in Nao's intelligence from 3 to 7 years old compared with a child's intelligence in learning simple mathematics with the best communication using a kappa ratio value of 90.8%, having a corpus that exceeded 390,000 segments, and scoring 93% of success rate when activating both auditory and vision modules for the agent Nao. The developed toolkit uses Arabic speech recognition and the Haar algorithm in a mixed reality system using big data enabling Nao to achieve a 94% success learning rate at a distance of 0.09 m; when using LMC in mixed reality, the hand sign gestures recorded the highest accuracy of 98.50% using Haar algorithm. The work shows that the current work enabled Nao to gradually achieve a higher learning success rate as the environment changes and multisensory perception increases. This paper also proposes a cutting-edge research work direction for fostering child-robots education in real time.",An Intelligent Big Data Management System Using Haar Algorithm-Based Nao Agent Multisensory Communication,Wireless Communications and Mobile Computing,Article,1/1/2021,Fatmah,Baothman,2021
10.1162/artl.2010.Bongard.024,"Embodied artificial intelligence argues that the body and brain play equally important roles in the generation of adaptive behavior. An increasingly common approach therefore is to evolve an agent's morphology along with its control in the hope that evolution will find a good coupled system. In order for embodied artificial intelligence to gain credibility within the robotics and cognitive science communities, however, it is necessary to amass evidence not only for how to co-optimize morphology and control of adaptive machines, but why. This work provides two new lines of evidence for why this co-optimization is useful: Here we show that for an object manipulation task in which a simulated robot must accomplish one, two, or three objectives simultaneously, subjugating more aspects of the robot's morphology to selective pressure allows for the evolution of better robots as the number of objectives increases. In addition, for robots that successfully evolved to accomplish all of their objectives, those composed of evolved rather than fixed morphologies generalized better to previously unseen environmental conditions. © 2010 Massachusetts Institute of Technology.",The utility of evolving simulated robot morphology increases with task complexity for object manipulation,Artificial Life,Article,1/1/2010,Josh,Bongard,2010
10.1162/artl_a_00336,"© 2021 Massachusetts Institute of Technology.On the one hand, complexity science and enactive and embodied cognitive science approaches emphasize that people, as complex adaptive systems, are ambiguous, indeterminable, and inherently unpredictable. On the other, Machine Learning (ML) systems that claim to predict human behaviour are becoming ubiquitous in all spheres of social life. I contend that ubiquitous Artificial Intelligence (AI) and ML systems are close descendants of the Cartesian and Newtonian worldview in so far as they are tools that fundamentally sort, categorize, and classify the world, and forecast the future. Through the practice of clustering, sorting, and predicting human behaviour and action, these systems impose order, equilibrium, and stability to the active, fluid, messy, and unpredictable nature of human behaviour and the social world at large. Grounded in complexity science and enactive and embodied cognitive science approaches, this article emphasizes why people, embedded in social systems, are indeterminable and unpredictable. When ML systems “pick up” patterns and clusters, this often amounts to identifying historically and socially held norms, conventions, and stereotypes. Machine prediction of social behaviour, I argue, is not only erroneous but also presents real harm to those at the margins of society.",The impossibility of automating ambiguity,Artificial Life,Article,3/17/2021,Abeba,Birhane,2021
10.1167/tvst.11.2.23,"© 2022 The Authors.Purpose: The purpose of this study was to develop a deep learning model for automatic binarization of the choroidal tissue, separating choroidal blood vessels from nonvascu-lar stromal tissue, in optical coherence tomography (OCT) images from healthy young subjects. Methods: OCT images from an observational longitudinal study of 100 children were used for training, validation, and testing of 5 fully semantic networks, which provided a binarized output of the choroid. These outputs were compared with ground truth images, generated from a local binarization technique after manually optimizing the analysis window size for each individual image. The performance was evaluated using accuracy and repeatability metrics. The methods were also compared with a fixed window size local binarization technique, which has been commonly used previously. Results: The tested deep learning methods provided a good performance in terms of accuracy and repeatability. With the U-Net and SegNet networks showing >96% accuracy. All methods displayed a high level of repeatability relative to the ground truth. For analysis of the choroidal vascularity index (a commonly used metric derived from the binarized image), SegNet showed the closest agreement with the ground truth and high repeatability. The fixed window size showed a reduced accuracy compared to other methods. Conclusions: Fully semantic networks such as U-Net and SegNet displayed excellent performance for the binarization task. These methods provide a useful approach for clinical and research applications of deep learning tools for the binarization of the choroid in OCT images. Translational Relevance: Deep learning models provide a novel, robust solution to automatically binarize the choroidal tissue in OCT images.",Application of Deep Learning Methods for Binarization of the Choroid in Optical Coherence Tomography Images,Translational Vision Science and Technology,Article,2/1/2022,Michael,Collins,2022
10.1167/tvst.8.6.10,"© 2019 The Authors.Purpose: To develop a fully automatic method, based on deep learning algorithms, for determining the locations of cone photoreceptors within adaptive optics scanning laser ophthalmoscope images and evaluate its performance against a dataset of manually segmented images. Methods: A fully convolutional network (FCN) based on U-Net architecture was used to generate prediction probability maps and then used a localization algorithm to reduce the prediction map to a collection of points. The proposed method was trained and tested on two publicly available datasets of different imaging modalities, with Dice overlap, false discovery rate, and true positive reported to assess performance. Results: The proposed method achieves a Dice coefficient of 0.989, true positive rate of 0.987, and false discovery rate of 0.009 on the first confocal dataset; and a Dice coefficient of 0.926, true positive rate of 0.909, and false discovery rate of 0.051 on the second split detector dataset. Results compare favorably with a previously proposed method, but this method provides quicker (25 times faster) evaluation performance. Conclusions: The proposed FCN-based method demonstrates that deep learning algorithms can achieve accurate cone localizations, almost comparable to a human expert, while labeling the images. Translational Relevance: Manual cone photoreceptor identification is a time-consuming task due to the large number of cones present within a single image; using the proposed FCN-based method could support the image analysis task, drastically reducing the need for manual assessment of the photoreceptor mosaic.",Automatic detection of cone photoreceptors with fully convolutional networks,Translational Vision Science and Technology,Article,11/1/2019,Michael,Collins,2019
10.1167/tvst.9.11.12,"© 2020 The Authors.Purpose: To use a deep learning model to develop a fully automated method (fully semantic network and graph search [FS-GS]) of retinal segmentation for optical coherence tomography (OCT) images from patients with Stargardt disease. Methods: Eighty-seven manually segmented (ground truth) OCT volume scan sets (5171 B-scans) from 22 patients with Stargardt disease were used for training, validation and testing of a novel retinal boundary detection approach (FS-GS) that combines a fully semantic deep learning segmentation method, which generates a per-pixel class predic-tion map with a graph-search method to extract retinal boundary positions. The performance was evaluated using the mean absolute boundary error and the differences in two clinical metrics (retinal thickness and volume) compared with the ground truth. The performance of a separate deep learning method and two publicly available software algorithms were also evaluated against the ground truth. Results: FS-GS showed an excellent agreement with the ground truth, with a boundary mean absolute error of 0.23 and 1.12 pixels for the internal limiting membrane and the base of retinal pigment epithelium or Bruch’s membrane, respectively. The mean difference in thickness and volume across the central 6 mm zone were 2.10 µm and 0.059 mm3. The performance of the proposed method was more accurate and consistent than the publicly available OCTExplorer and AURA tools. Conclusions: The FS-GS method delivers good performance in segmentation of OCT images of pathologic retina in Stargardt disease. Translational Relevance: Deep learning models can provide a robust method for retinal segmentation and support a high-throughput analysis pipeline for measuring retinal thickness and volume in Stargardt disease.",Retinal boundary segmentation in stargardt disease optical coherence tomography images using automated deep learning,Translational Vision Science and Technology,Article,10/1/2020,Michael,Collins,2020
10.1177/0278364918770733,"© 2018, © The Author(s) 2018.The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.",The limits and potentials of deep learning for robotics,International Journal of Robotics Research,Article,4/1/2018,Wolfram,Burgard,2018
10.1177/0278364918770733,"© 2018, © The Author(s) 2018.The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.",The limits and potentials of deep learning for robotics,International Journal of Robotics Research,Article,4/1/2018,Dieter,Fox,2018
10.1177/095646249200300103,"The following topics are reviewed: condom use, attitudes toward condoms, reasons for failure to use condoms, personality and attitudes toward condoms, attitudes toward condoms and behavior, and changing attitudes toward condoms. The use of condoms varies across cultures. Prevention which emphasizes group subjective norms, personality, and interpersonal and situational barriers may influence condom use; intervention studies have not yet fully tested this hypothesis, however. What is known is that attitudes and use of condoms can be changed. Negative attitudes toward condom use include the perception of impaired pleasure, lack of availability at the appropriate time, coitus interruptus, and unnaturalness or unreliability. Positive perceptions may counterbalance the negative attitudes. In several military studies, it was found that lack of availability, perceived lack of risk, and influence of alcohol were the major reasons for failure to use. In many studies the reduction of pleasure is a dominant theme. Pharmacies were the preferred place of purchase. Among college students, condoms were viewed as minimizing a health risk but associated use with there being something wrong with their partner. A Dutch study found a reduced belief in the efficacy of condoms as a contraceptive but increased trust in the protection against AIDS. In a pre- and posttest of attitudes toward condom use among heterosexual couples, the findings were that attitudes could be modified and that information needs to relate to the context of sexual activity and sensorimotor arousal. The condom film with sexually explicit activity and instructions on condom placement was successful in generating a significant increase in positive attitudes toward condoms. Examining link between attitudes and behavior, however, is a necessary next step.",Attitudes toward condoms and condom use: A review,International Journal of STD and AIDS,Review,1/1/1992,W,Ross,1992
10.1177/1049732310392385,"HIV-related stigma continues to be a significant barrier to HIV testing, treatment, and care. Understanding the factors that underlie this stigma could help remove barriers to HIV/AIDS intervention. We identified these factors among nurses as well as community leaders in Lui, Southern Sudan. Participants included health workers at a local hospital, a women's group, local market traders, religious leaders, and teachers. We categorized the responses generated from group interaction forums as concerns, fears, and perceptions. We found that stigma persisted not only toward people with conspicuous signs of full-blown AIDS, but also toward community programs, like voluntary counseling and testing centers. Future interventions, including delabeling the counseling and testing centers and demonstrating the efficacy of highly active antiretroviral therapy, will be critical in reducing the stigma of HIV/AIDS in communities. © 2011 The Author(s).",Issues of expressed stigma of HIV/AIDS among professionals in Southern Sudan,Qualitative Health Research,Article,8/1/2011,W,Ross,2011
10.1177/107834580301000203,"The following evaluates a pilot HIV education program conducted in five Texas prison facilities that used male and female prisoners as peer educators. Quantitative data, collected via pre- and post-surveys from 242 prisoners, indicated statistically significant increases in knowledge about HIV/AIDS and nonsignificant improvements in attitudes and beliefs about HIV/AIDS prevention. Qualitative data were collected via interviews with wardens and program coordinators and through focus groups with trained peer educators and their students. These data indicated that program implementation and effectiveness were influenced by a range of common factors including the roles of program coordinators and security staff in program implementation and maintenance, the selection of peer educators and students, curricular content, program promotion and benefits, and logistical considerations (i.e., space, time, and scheduling). Qualitative data also suggested a diffusion of knowledge to other prisoners and facility staff, as well as to family members and friends outside the facility. These data suggest that such a peer-based, risk-reduction education program is both feasible and beneficial in a prison population. © 2004, National Commission on Correctional Health Care. All rights reserved.",Evaluation of an HIV Peer Education Program in Texas Prisons,Journal of Correctional Health Care,Article,1/1/2004,W,Ross,2004
10.1177/1466424008092793,"The HIV/AIDS pandemic has become one of the most important public health problems in recent times and it is having a profound impact on the lives of infected people and their families. There is an acknowledged burden of HIV/AIDS in Nigeria. As the prevalence of HIV/AIDS infection rises, health care professionals worldwide can expect greater clinical exposure to infected patients. The care of people living with AIDS presents a significant challenge to the health care sector. This study seeks to explore the relationship between sources of HIV/AIDS information and knowledge, and the relationship between knowledge of HIV/AIDS and care for people with AIDS among health care providers in three different levels of health care institutions in the southern region of Nigeria. Health care workers from two states in southern Nigeria completed a questionnaire that was designed to assess knowledge, attitudes and practices about HIV/AIDS. The sample was composed of 277 (65%) females and 135 (31.7%) males. The results showed a fair level of knowledge among all health care professionals, with the highest level of knowledge among the doctors and the lowest among laboratory workers. There was a significant gender difference in the level of knowledge but the data suggested that knowledge did not differ by hospital settings. There were generally negative feelings and views about the care of HIV/AIDS patients among the professionals, these views being worst at the community health centers and best at the government hospital. The greatest source of information for the majority of professionals was health talks/seminars, and those respondents who got their information from school scored the highest on the items on general knowledge of HIV/AIDS incidence, cause, transmission, and clinical treatment. This has important implications for future interventions designed for health care professionals including doctors, nurses and laboratory workers. © The Journal of The Royal Society for the Promotion of Health 2008.","Knowledge, beliefs and attitudes about HIV/AIDS-related issues, and the sources of knowledge among health care professionals in southern Nigeria",Journal of The Royal Society for the Promotion of Health,Article,9/1/2008,W,Ross,2008
10.1177/17456916221091833,"© The Author(s) 2022.Scientific discovery is a driving force for progress involving creative problem-solving processes to further our understanding of the world. The process of scientific discovery has historically been intensive and time-consuming; however, advances in computational power and algorithms have provided an efficient route to make new discoveries. Complex tools using artificial intelligence (AI) can efficiently analyze data as well as generate new hypotheses and theories. Along with AI becoming increasingly prevalent in our daily lives and the services we access, its application to different scientific domains is becoming more widespread. For example, AI has been used for the early detection of medical conditions, identifying treatments and vaccines (e.g., against COVID-19), and predicting protein structure. The application of AI in psychological science has started to become popular. AI can assist in new discoveries both as a tool that allows more freedom to scientists to generate new theories and by making creative discoveries autonomously. Conversely, psychological concepts such as heuristics have refined and improved artificial systems. With such powerful systems, however, there are key ethical and practical issues to consider. This article addresses the current and future directions of computational scientific discovery generally and its applications in psychological science more specifically.",Computational Scientific Discovery in Psychology,Perspectives on Psychological Science,Article,1/1/2023,Fernand,Gobet,2023
10.1186/s12859-022-05032-y,"© 2022, The Author(s).Over the last few years, dozens of healthcare surveys have shown a shortage of doctors and an alarming doctor-population ratio. With the motivation of assisting doctors and utilizing their time efficiently, automatic disease diagnosis using artificial intelligence is experiencing an ever-growing demand and popularity. Humans are known by the company they keep; similarly, symptoms also exhibit the association property, i.e., one symptom may strongly suggest another symptom’s existence/non-existence, and their association provides crucial information about the suffering condition. The work investigates the role of symptom association in symptom investigation and disease diagnosis process. We propose and build a virtual assistant called Association guided Symptom Investigation and Diagnosis Assistant (A-SIDA) using hierarchical reinforcement learning. The proposed A-SIDDA converses with patients and extracts signs and symptoms as per patients’ chief complaints and ongoing dialogue context. We infused association-based recommendations and critic into the assistant, which reinforces the assistant for conducting context-aware, symptom-association guided symptom investigation. Following the symptom investigation, the assistant diagnoses a disease based on the extracted signs and symptoms. The assistant then diagnoses a disease based on the extracted signs and symptoms. In addition to diagnosis accuracy, the relevance of inspected symptoms is critical to the usefulness of a diagnosis framework. We also propose a novel evaluation metric called Investigation Relevance Score (IReS), which measures the relevance of symptoms inspected during symptom investigation. The obtained improvements (Diagnosis success rate-5.36%, Dialogue length-1.16, Match rate-2.19%, Disease classifier-6.36%, IReS-0.3501, and Human score-0.66) over state-of-the-art methods firmly establish the crucial role of symptom association that gets uncovered by the virtual agent. Furthermore, we found that the association guided symptom investigation greatly increases human satisfaction, owing to its seamless topic (symptom) transition.",Symptoms are known by their companies: towards association guided disease diagnosis assistant,BMC Bioinformatics,Article,12/1/2022,Pushpak,Bhattacharyya,2022
10.1200/CCI.20.00028,"Copyright © 2020 American Society of Clinical Oncology. All rights reserved.PURPOSE Literature on clinical note mining has highlighted the superiority of machine learning (ML) over handcrafted rules. Nevertheless, most studies assume the availability of large training sets, which is rarely the case. For this reason, in the clinical setting, rules are still common. We suggest 2 methods to leverage the knowledge encoded in pre-existing rules to inform ML decisions and obtain high performance, even with scarce annotations. METHODS We collected 501 prostate pathology reports from 6 American hospitals. Reports were split into 2,711 core segments, annotated with 20 attributes describing the histology, grade, extension, and location of tumors. The data set was split by institutions to generate a cross-institutional evaluation setting. We assessed 4 systems, namely a rule-based approach, an ML model, and 2 hybrid systems integrating the previous methods: a Rule as Feature model and a Classifier Confidence model. Several ML algorithms were tested, including logistic regression (LR), support vector machine (SVM), and eXtreme gradient boosting (XGB). RESULTS When training on data from a single institution, LR lags behind the rules by 3.5% (F1 score: 92.2% v 95.7%). Hybrid models, instead, obtain competitive results, with Classifier Confidence outperforming the rules by +0.5% (96.2%). When a larger amount of data from multiple institutions is used, LR improves by +1.5% over the rules (97.2%), whereas hybrid systems obtain +2.2% for Rule as Feature (97.7%) and +2.6% for Classifier Confidence (98.3%). Replacing LR with SVM or XGB yielded similar performance gains. CONCLUSION We developed methods to use pre-existing handcrafted rules to inform ML algorithms. These hybrid systems obtain better performance than either rules or ML models alone, even when training data are limited.",Exploiting rules to enhance machine learning in extracting information from multi-institutional prostate pathology reports,JCO Clinical Cancer Informatics,Article,1/1/2020,Regina,Barzilay,2020
10.1200/JCO.21.01337,"© American Society of Clinical Oncology.PURPOSEAccurate risk assessment is essential for the success of population screening programs in breast cancer. Models with high sensitivity and specificity would enable programs to target more elaborate screening efforts to high-risk populations, while minimizing overtreatment for the rest. Artificial intelligence (AI)-based risk models have demonstrated a significant advance over risk models used today in clinical practice. However, the responsible deployment of novel AI requires careful validation across diverse populations. To this end, we validate our AI-based model, Mirai, across globally diverse screening populations.METHODSWe collected screening mammograms and pathology-confirmed breast cancer outcomes from Massachusetts General Hospital, USA; Novant, USA; Emory, USA; Maccabi-Assuta, Israel; Karolinska, Sweden; Chang Gung Memorial Hospital, Taiwan; and Barretos, Brazil. We evaluated Uno's concordance index for Mirai in predicting risk of breast cancer at one to five years from the mammogram.RESULTSA total of 128,793 mammograms from 62,185 patients were collected across the seven sites, of which 3,815 were followed by a cancer diagnosis within 5 years. Mirai obtained concordance indices of 0.75 (95% CI, 0.72 to 0.78), 0.75 (95% CI, 0.70 to 0.80), 0.77 (95% CI, 0.75 to 0.79), 0.77 (95% CI, 0.73 to 0.81), 0.81 (95% CI, 0.79 to 0.82), 0.79 (95% CI, 0.76 to 0.83), and 0.84 (95% CI, 0.81 to 0.88) at Massachusetts General Hospital, Novant, Emory, Maccabi-Assuta, Karolinska, Chang Gung Memorial Hospital, and Barretos, respectively.CONCLUSIONMirai, a mammography-based risk model, maintained its accuracy across globally diverse test sets from seven hospitals across five countries. This is the broadest validation to date of an AI-based breast cancer model and suggests that the technology can offer broad and equitable improvements in care.",Multi-Institutional Validation of a Mammography-Based Breast Cancer Risk Model,Journal of Clinical Oncology,Article,6/1/2022,Regina,Barzilay,2022
10.1200/JCO.22.01345,"© American Society of Clinical Oncology.PURPOSELow-dose computed tomography (LDCT) for lung cancer screening is effective, although most eligible people are not being screened. Tools that provide personalized future cancer risk assessment could focus approaches toward those most likely to benefit. We hypothesized that a deep learning model assessing the entire volumetric LDCT data could be built to predict individual risk without requiring additional demographic or clinical data.METHODSWe developed a model called Sybil using LDCTs from the National Lung Screening Trial (NLST). Sybil requires only one LDCT and does not require clinical data or radiologist annotations; it can run in real time in the background on a radiology reading station. Sybil was validated on three independent data sets: a heldout set of 6,282 LDCTs from NLST participants, 8,821 LDCTs from Massachusetts General Hospital (MGH), and 12,280 LDCTs from Chang Gung Memorial Hospital (CGMH, which included people with a range of smoking history including nonsmokers).RESULTSSybil achieved area under the receiver-operator curves for lung cancer prediction at 1 year of 0.92 (95% CI, 0.88 to 0.95) on NLST, 0.86 (95% CI, 0.82 to 0.90) on MGH, and 0.94 (95% CI, 0.91 to 1.00) on CGMH external validation sets. Concordance indices over 6 years were 0.75 (95% CI, 0.72 to 0.78), 0.81 (95% CI, 0.77 to 0.85), and 0.80 (95% CI, 0.75 to 0.86) for NLST, MGH, and CGMH, respectively.CONCLUSIONSybil can accurately predict an individual's future lung cancer risk from a single LDCT scan to further enable personalized screening. Future study is required to understand Sybil's clinical applications. Our model and annotations are publicly available.",Sybil: A Validated Deep Learning Model to Predict Future Lung Cancer Risk from a Single Low-Dose Chest Computed Tomography,Journal of Clinical Oncology,Article,4/20/2023,Regina,Barzilay,2023
10.1201/9781315546377-17,"© 2009 by C.A.P. Smith, Kenneth W. Kisiel and Jeffrey G. Morrison.Current brain imaging technology is not able to detect the dynamics of thoughts and feelings as they pass through the brain. Artificial Intelligence (AI) experts agree that this sort of Artificial General Intelligence (AGI) will eventually be possible, but estimates of the timeline and the most likely path to this goal differ widely. motion capture should be supplemented with emotion capture: a variety of physical sensors capturing parameters such as heart rate, galvanic skin response, and so forth, giving a physiological indication of the user's emotional state. There is significant ongoing work in this direction as well, though not specifically directed at virtual agent control. Robot simulators exceed virtual worlds and game engines in terms of allowing detailed control of simulated robot joints, and physically realistic interactions between objects (as needed for instance to enable tool use), but don't allow large numbers of individuals to log on and help teach robots.",Mirror Man: A Speculative Case Study of the Synergetic Potential of Data Visualization and Virtual Worlds,Working Through Synthetic Worlds,Book Chapter,1/1/2018,Ben,Goertzel,2018
10.1207/s15516709cog0000_47,"In this study we use a computational model of language learning called model of syntax acquisition in children (MOSAIC) to investigate the extent to which the optional infinitive (OI) phenomenon in Dutch and English can be explained in terms of a resource-limited distributional analysis of Dutch and English child-directed speech. The results show that the same version of MOSAIC is able to simulate changes in the pattern of finiteness marking in 2 children learning Dutch and 2 children learning English as the average length of their utterances increases. These results suggest that it is possible to explain the key features of the OI phenomenon in both Dutch and English in terms of the interaction between an utterance-final bias in learning and the distributional characteristics of child-directed speech in the 2 languages. They also show how computational modeling techniques can be used to investigate the extent to which cross-linguistic similarities in the developmental data can be explained in terms of common processing constraints as opposed to innate knowledge of universal grammar. Copyright © 2006 Cognitive Science Society, Inc. All rights reserved.",Modeling the development of children's use of optional infinitives in Dutch and English using MOSAIC,Cognitive Science,Article,3/1/2006,Fernand,Gobet,2006
10.1212/WNL.0000000000000780,"Objective: To estimate the prevalence of topographical memory impairment following posterior cerebral artery infarctions (PCAI) and define its anatomical correlations. Methods: We recruited 15 patients (mean duration of 4months postinfarct).We administered 2 sets of experimental tests to assess topographical memory: one set included 5 computerized tasks (CompT) and the other set consisted of one ecological topographical orientation test (EcolT) that included 4 tasks (i.e., map drawing, picture recognition and ordering, backward path). Fifteen healthy participants served as controls. Patients and controls underwent a volumetric T1 MRI brain scan. Brain lesions in patients were segmented, normalized, and correlated with performance. Results: Topographical memory impairments were evidenced in patients with PCAI using both group and individual analyses (50%), with more severe outcomes in patients with PCAI in the right hemisphere. CompT and EcolT were highly correlated, but the ecological test was more sensitive in revealing topographical memory impairments. Voxel-based lesion-symptom mapping demonstrated that 2 regions located in the cuneus and the calcarine sulcus correlated significantly with behavioral performance. Conclusions: Topographical memory disorders following PCAI are reported in 50%of the patient population. Our results demonstrate the importance of developing and using dedicated batteries of topographical memory tests, in particular real-life tests, to identify such deficits.",A systematic study of topographical memory and posterior cerebral artery infarctions,Neurology,Article,9/1/2014,François,Chollet,2014
10.1214/06-BA126,"A key problem in statistics and machine learning is inferring suitable structure of a model given some observed data. A Bayesian approach to model comparison makes use of the marginal likelihood of each candidate model to form a posterior distribution over models; unfortunately for most models of interest, notably those containing hidden or latent variables, the marginal likelihood is intractable to compute. We present the variational Bayesian (VB) algorithm for directed graphical models, which optimises a lower bound approximation to the marginal likelihood in a procedure similar to the standard EM algorithm. We show that for a large class of models, which we call conjugate exponential, the VB algorithm is a straightforward generalisation of the EM algorithm that incorporates uncertainty over model parameters. In a thorough case study using a small class of bipartite DAGs containing hidden variables, we compare the accuracy of the VB approximation to existing asymptoticdata approximations such as the Bayesian Information Criterion (BIC) and the Cheeseman-Stutz (CS) criterion, and also to a sampling based gold standard, Annealed Importance Sampling (AIS). We find that the VB algorithm is empirically superior to CS and BIC, and much faster than AIS. Moreover, we prove that a VB approximation can always be constructed in such a way that guarantees it to be more accurate than the CS approximation. © 2006 International Society for Bayesian Analysis.",Variational Bayesian learning of directed graphical models with hidden variables,Bayesian Analysis,Article,12/1/2006,Zoubin,Ghahramani,2006
10.1258/0956462001914797,"The purpose of this report was to present findings from a pilot study conducted to explore the associations between sociodemographic, drug use, and health belief factors and perceived compliance with zidovudine (AZT) among African-American drug users. Data were collected in Washington, DC, USA from individuals who were African-American; were recent or current drug injectors or crack smokers; were HIV-seropositive, and were receiving treatment for HIV infection. Participants were recruited through local organizations that provide services to HIV-infected persons. Participants were interviewed using a questionnaire that solicited sociodemographic, lifetime and current drug use, current sexual behaviours, health status, HIV and drug treatment history, and health belief data. Analyses were limited to individuals currently using an illicit substance and who had received AZT during their medical treatment. Parametric (Pearson's r) and nonparametric (Spearman's rho) statistics were used to assess correlations between perceived compliance with AZT dosing and independent variables. As the study was intended to be both descriptive and exploratory, the level of statistical significance was set at 0.10, rather than the customary 0.05. Antiretroviral medications recognized and recalled by participants are presented. The most commonly recalled medication was AZT. Slightly less than one-third of participants reported being completely compliant with an AZT regimen. Perceived compliance was found to be negatively associated with 5 variables: age, homelessness, number of injections in the previous 30 days, trading sex for drugs, and the perception that AIDS is no longer a serious disease since the development of new antiretroviral medications. Intensity of feelings of joy, fear, and the belief that taking more anti-HIV medications would result in better health were found to be positively correlated. Bivariate associations between perceived compliance and sociodemographic, drug use, sexual behaviour, and health belief variables suggest further avenues of study and potential points for intervention to increase compliance with antiretroviral medications among racial/ethnic minority drug users receiving treatment for HIV infection.",Perceived compliance with AZT dosing among a sample of African-American drug users,International Journal of STD and AIDS,Article,1/31/2000,W,Ross,2000
10.1258/0956462001916416,"Homeless people are one of the most vulnerable with regard to HIV transmission. However, most research on this population has been carried out on samples from health clinics. We surveyed 390 homeless people in Houston at a day shelter with regard to their HIV/AIDS knowledge and risk behaviours. The sample was 76% African-American, 11% Euro-American, with small numbers of Latin-Americans, Native-Americans and Asian-Americans: half were born in Texas, and 92% were male. Data indicated that HIV/AIDS knowledge was higher in those who were at higher behavioural risk, although the direction of causality in these cross-sectional data cannot be inferred. African-Americans were at slightly higher risk. Compared with previous clinic samples, this sample was older and a higher number (one-third) slept the last night outside. Eighty per cent had had an HIV test. Condom use was low with both males and females most commonly not reporting using condoms although more than half had had sexual contact in the past month. Multivariate analysis indicated that ethnicity and HIV/AIDS knowledge were independent predictors of risk behaviour. Lifetime risks included one-third who had injected drugs land shared needles), and nearly 10% had had sex with someone they knew to be HIV seropositive. Lack of future time perspective rather than level of knowledge may be a barrier to reducing HIV risks, and the data are discussed in terms of policy implications and homelessness.",HIV risks in a homeless population,International Journal of STD and AIDS,Article,8/22/2000,W,Ross,2000
10.1258/095646205774763135,"The epidemiology of the HIV/AIDS epidemic in the United States has focused research attention on lesbian, gay, bisexual and transgendered communities as well as on racial and ethnic minorities. Much of that attention has, however, been focused on specific racial and ethnic groups, and specific sexual minorities. We report on the results of a study that examined the association between condom use and partnership types among men from four major racial/ethnic groups. Self-reported data on sexual identity (homosexual, bisexual, and heterosexual) and condom use in the past three months were collected from 806 African Americans, Hispanic, Asian, and white men intercepted in public places in Houston, TX. Data indicated that condom use was lowest in African Americans and Hispanic men, bisexual men reported the highest levels of use, with heterosexual men reporting the lowest use. African Americans and Hispanic men reported generally that it was very difficult to use a condom during sexual contact, although the patterns for self-identified homosexual, heterosexual, and bisexual men varied across race/ethnicity. Homosexual African American men reported the least difficulty, and white homosexual men the most difficulty compared with heterosexual and bisexual peers. For homosexually identified men, there were considerable differences across race/ethnicity in the proportion of partners who never or rarely disagreed to use condoms, with Asians disagreeing least, and African Americans most. Within racial/ethnic groups, the levels of condom use and difficulty were similar for male and female partners, suggesting that it is sexual identity, rather than partner gender, that has impacted condom-use messages. These data suggest that racial/ethnic targeting of condom use is likely to be most efficacious in increasing condom use in men.",Reported condom use and condom use difficulties in street outreach samples of men of four racial and ethnic backgrounds,International Journal of STD and AIDS,Article,11/1/2005,W,Ross,2005
10.1258/095646207781024757,"One hundred and ninety-three adults with HIV taking antiretroviral therapy completed a questionnaire on demographics, health beliefs, medication side-effects, and adherence to dose, schedule, and dietary instructions. Three health beliefs indices were identified: antiretroviral therapy (ART) benefits, ART adherence self-efficacy, and beliefs about future HIV-related health concerns. Patients who experienced medication side-effects reported strong beliefs that HIV infection would cause them future health problems or distrust in the benefits of ART. AIDS diagnosis obtained through medical records or medication side-effects were not related to any of the three types of adherence. Beliefs about future HIV-related health concerns were associated with suboptimal dose adherence. Beliefs about ART benefits were associated with suboptimal schedule and dietary instructions adherence. Older age and partner were protective factors of schedule adherence. Data suggest that health beliefs may vary across type of adherence and that adherence behaviours may be a coping strategy to adjust antiretroviral therapy to one's daily living.","Three types of adherence to HIV antiretroviral therapy and their association with AIDS diagnosis, medication side-effects, beliefs about antiretroviral therapy, and beliefs about HIV disease",International Journal of STD and AIDS,Article,6/1/2007,W,Ross,2007
10.1258/0956462971919246,"Fears about occupational transmission of HIV may have a significant impact on the behaviour of health care workers and on infection control practices. We investigated the relationships between fear of AIDS and infection control practices in health care workers in major university teaching hospitals in Nigeria and the USA. Data from the fear of AIDS scale and on a measure of infection control practices and beliefs showed that knowledge of whether the patient was HIV-infected determined infection control practices in Calabar but not Texas. Where the patient was known to be infected there were no differences between the 2 countries. Fears of AIDS were related to infection control practices significantly more in the USA than in Nigeria where there was almost no relationship. These data may be influenced by the greater availability of disposable equipment in the USA compared with Nigeria.HIV infection control practices and fear of AIDS were compared in 388 health care workers (doctors, nurses, nursing aides, and laboratory technologists) at major university teaching hospitals in Houston, Texas (US), and Calabar, Nigeria, in 1994. The mean duration of hospital employment was 9.2 years in Nigeria and 9.1 years in the US sample. The mean probability of treating a patient with AIDS in the next 12 months was estimated at 29.3% in Nigeria and 54.3% in the US. Nigerian hospital workers were as likely as their US counterparts to observe infection control practices such as carrying sharps in puncture-proof containers when they knew a patient had AIDS, but were not as compliant if the patient's serostatus was unknown. Differences in infection control practices were related to cross-national and not demographic or occupational factors. In the US, but not in Nigeria, there was a significant correlation between practices such as recapping of syringes by hand and fear of AIDS, regardless of prior knowledge of HIV status. The significantly stronger association between fear of AIDS and compliance with infection control practices in the US compared to Nigeria may reflect the higher HIV prevalence, more widespread availability of disposable equipment, stricter enforcement of infection control laws, and lower provider-patient ratios in the US setting.",Cross-national HIV infection control practices and fear of AIDS: A comparison between Nigeria and the USA,International Journal of STD and AIDS,Article,12/22/1997,W,Ross,1997
10.1287/ijoc.2021.1091,"© 2021 INFORMS.This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.",Predicting Tactical Solutions to Operational Planning Problems under Imperfect Information,INFORMS Journal on Computing,Article,1/1/2022,Yoshua,Bengio,2022
10.1300/J077V12N01_08,"This article examines the responses of 16 partners and close family members who were bereaved when a partner or family member died of AIDS. Respondents were interviewed and their responses were categorized. Special emphasis was placed on the patterns of bereavement and possible signs of posttraumatic stress disorder in the two groups of respondents. The authors found that respondents who had lost lovers or family members to AIDS focused on the positive aspects of death and dying. Most of them described loneliness and feelings of emotiness as their worst encounters with bereavement. The format of funerals appeared to encapsulate their feelings about the dead loved one and reactions to the loss. Self-euthanasia also generated ambivalent feelings. Interestingly, respondents received the most support from friends, support groups, or professional counselors rather than from family members. Most respondents showed little or no concern about their own future with regard to HIV or AIDS. © 1993 by The Haworth Press, Inc. All rights reserved.",Responses to AIDS-related bereavement,Journal of Psychosocial Oncology,Article,7/21/1994,W,Ross,1994
10.1300/J129v03n03_03,"The objective of this study was to determine the relationships between social skills/anxieties in HIV/STD prevention and actual and anticipated sexual behaviors in year 11 and 12 Indian college students. A quantitative questionnaire examining HIV and STD risk behaviors, knowledge, attitudes and beliefs, and the AIDS Social Asser-tiveness Scale (ASAS) were administered to 1230 year 11 and 12 Indian college students. The 5 scales of the ASAS were scored and compared between three groups: those who had had sexual experience (HS), those who anticipated being unable to refuse sex (AS), and those who did not anticipate problems in refusing sex (DS). Those in the AS group had significantly greater anxieties about refusing sexual or other risk behaviors than the HS and DS groups, and there were also significantly greater anxieties about dealing with condoms in the AS and DS groups compared with the HS group. Confiding sexual or HIV/STD related problems to significant others was considered more anxiety-provoking for the AS group compared with the HS group, and the AS group were more anxious about interactions with people with HIV. Factor analysis produced the same 5 dimensions as those found in previous studies. Condom interactions and confiding in significant others were most anxiety provoking. It is concluded that social skills training in sexual negotiations, condom negotiations, and confiding HIV/STD-related concerns to significant others should reduce the risks of Indian college students having unwanted or unprotected sex. © 1999 by The Haworth Press, Inc. All rights reserved.","Aids-related social anxieties, social skills and sexual activities in indian college students",Journal of HIV/AIDS Prevention and Education for Adolescents and Children,Article,2/4/2000,W,Ross,2000
10.1300/J236v01n02_07,"This study describes the development, psychometric characteristics and norms of a Gay Life Events Scale (GALES) for homosexual men, standardized in two Western countries. The scale has been developed to determine the stresses of life events, including those related to sexual orientation and AIDS, and to assess the impact of stigmatization on mental and physical health. The scale exhibits high validity when compared with common items in its parent instrument and exhibits cross-cultural stability. Data suggest that the relationship between life events and the emotional distress and life change associated with them is closer in homosexual than in heterosexual men. © 1989 Taylor & Francis Group, LLC.",A gay life events scale (Gales) for homosexual men,Journal of Gay and Lesbian Psychotherapy,Article,4/28/1989,W,Ross,1989
10.1300/J236v01n04_06,"The effect of psychological resistance on AIDS counselling is explored both theoretically and practically. The theoretical implications of differing cognitive paradigms on HIV counselling emphasizes the need for health professionals to adopt and model a ""risk behavior"" as opposed to a ""risk group"" or ""phobic"" paradigm of AIDS virus transmission.Practical issues affecting the level of resistance were then explored chronologically. Pre-test resistance may manifest as reluctance to seek testing. Factors influencing pre-test resistance included type of testing available, degree of confidentiality, and general resistance common in all areas of medical testing. Resistance at a test site is commonly resistance specifically to counselling. Such resistance was examined and a Rational Emotive Therapy model (RET) for managing resistance outlined. Resistance following testing examines clients’ failure to engage in safer sex and safer needle practices, and the issue of subsequent referral. Finally resistance in the counsellor and as a process between counsellor and client was addressed. © 1991 Taylor & Francis Group, LLC.",Psychological resistance and hiv counselling,Journal of Gay and Lesbian Psychotherapy,Article,10/11/1991,W,Ross,1991
10.1364/OSAC.403102,"© 2020 Optical Society of America under the terms of the OSA Open Access Publishing AgreementAutomated segmentation of the eye's morphological features in OCT datasets is fundamental to support rapid clinical decision making and to avoid time-consuming manual segmentation of the images. In recent years, deep learning (DL) techniques have become a commonly employed approach to tackle image analysis problems. This study provides a description of the development of automated DL segmentation methods of the Bruch's membrane opening (BMO) from a series of OCT cross-sectional scans. A range of DL techniques are systematically evaluated, with the secondary goal to understand the effect of the network input size on the model performance. The results indicate that a fully semantic approach, in which the whole B-scan is considered with data augmentation, results in the best performance, achieving high levels of similarity metrics with a dice coefficient of 0.995 and BMO boundary localization with a mean absolute error of 1.15 pixels. The work further highlights the importance of fully semantic methods over patch-based techniques in the classification of OCT regions.",Deep learning approaches for segmenting Bruch's membrane opening from OCT volumes,OSA Continuum,Article,12/15/2020,Michael,Collins,2020
10.1371/journal.pone.0195024,"© 2018 Xiao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Objective Hospital readmission costs a lot of money every year. Many hospital readmissions are avoidable, and excessive hospital readmissions could also be harmful to the patients. Accurate prediction of hospital readmission can effectively help reduce the readmission risk. However, the complex relationship between readmission and potential risk factors makes readmission prediction a difficult task. The main goal of this paper is to explore deep learning models to distill such complex relationships and make accurate predictions. Materials and methods We propose CONTENT, a deep model that predicts hospital readmissions via learning interpretable patient representations by capturing both local and global contexts from patient Electronic Health Records (EHR) through a hybrid Topic Recurrent Neural Network (TopicRNN) model. The experiment was conducted using the EHR of a real world Congestive Heart Failure (CHF) cohort of 5,393 patients. Results The proposed model outperforms state-of-the-art methods in readmission prediction (e.g. 0.6103 ± 0.0130 vs. second best 0.5998 ± 0.0124 in terms of ROC-AUC). The derived patient representations were further utilized for patient phenotyping. The learned phenotypes provide more precise understanding of readmission risks. Discussion Embedding both local and global context in patient representation not only improves prediction performance, but also brings interpretable insights of understanding readmission risks for heterogeneous chronic clinical conditions. Conclusion This is the first of its kind model that integrates the power of both conventional deep neural network and the probabilistic generative models for highly interpretable deep patient representation learning. Experimental results and case studies demonstrate the improved performance and interpretability of the model.",Readmission prediction via deep contextual embedding of clinical concepts,PLoS ONE,Article,4/1/2018,Adji Bousso,Dieng,2018
10.1371/journal.pone.0195024,"© 2018 Xiao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Objective Hospital readmission costs a lot of money every year. Many hospital readmissions are avoidable, and excessive hospital readmissions could also be harmful to the patients. Accurate prediction of hospital readmission can effectively help reduce the readmission risk. However, the complex relationship between readmission and potential risk factors makes readmission prediction a difficult task. The main goal of this paper is to explore deep learning models to distill such complex relationships and make accurate predictions. Materials and methods We propose CONTENT, a deep model that predicts hospital readmissions via learning interpretable patient representations by capturing both local and global contexts from patient Electronic Health Records (EHR) through a hybrid Topic Recurrent Neural Network (TopicRNN) model. The experiment was conducted using the EHR of a real world Congestive Heart Failure (CHF) cohort of 5,393 patients. Results The proposed model outperforms state-of-the-art methods in readmission prediction (e.g. 0.6103 ± 0.0130 vs. second best 0.5998 ± 0.0124 in terms of ROC-AUC). The derived patient representations were further utilized for patient phenotyping. The learned phenotypes provide more precise understanding of readmission risks. Discussion Embedding both local and global context in patient representation not only improves prediction performance, but also brings interpretable insights of understanding readmission risks for heterogeneous chronic clinical conditions. Conclusion This is the first of its kind model that integrates the power of both conventional deep neural network and the probabilistic generative models for highly interpretable deep patient representation learning. Experimental results and case studies demonstrate the improved performance and interpretability of the model.",Readmission prediction via deep contextual embedding of clinical concepts,PLoS ONE,Article,4/1/2018,David,Blei,2018
10.1422/101896,"© 2022. All Rights Reserved.Computational scientific discovery is an important area of research in cognitive science. Not only can a scientific theory be represented by a computer program, but it can also be discovered by computers, using techniques and methodologies from artificial intelligence. Here, we present a new methodology currently under development, called GEMS, that has been successfully applied in cognitive science in order to semi-automatically generate scientific theories. GEMS is an application of genetic programming that, given the protocol of an experiment, a set of experimental data and elementary operations (in our case, elementary psychological processes), evolves programs that ‘behave' like an experimental subject. From one generation to the next, the programs are improved thanks to evolutionary plausible mechanisms that aim to minimise the discrepancy between model predictions and data. Interestingly, GEMS makes it possible to perform an efficient search in the combinatorial model space; the output of GEMS is a formal scientific theory of a specific dataset, expressed as a computer program. In this paper, we present the main features of GEMS with an example of how it could be applied in a hypothetical scenario; we discuss the theoretical implications of this approach for scientific discovery; and we present ideas for future research in cognitive science using GEMS.",GEMS: GENETICALLY EVOLVING MODELS IN SCIENCE,Sistemi Intelligenti,Article,4/1/2022,Fernand,Gobet,2022
10.14324/LRE.19.1.01,"© 2021 Dignum.Artificial intelligence (AI) is impacting education in many different ways. From virtual assistants for personalized education, to student or teacher tracking systems, the potential benefits of AI for education often come with a discussion of its impact on privacy and well-being. At the same time, the social transformation brought about by AI requires reform of traditional education systems. This article discusses what a responsible, trustworthy vision for AI is and how this relates to and affects education.",The role and challenges of education for responsible ai,London Review of Education,Article,1/13/2021,Virginia,Dignum,2021
10.1515/pjbr-2021-0009,"© 2021 Selmer Bringsjord et al., published by De Gruyter.Suppose an artificial agent aadj{a}_{\text{adj}}, as time unfolds, (i) receives from multiple artificial agents (which may, in turn, themselves have received from yet other such agents...) propositional content, and (ii) must solve an ethical problem on the basis of what it has received. How should aadj{a}_{\text{adj}} adjudicate what it has received in order to produce such a solution? We consider an environment infused with logicist artificial agents a1,a2,...,an{a}_{1},{a}_{2},\ldots,{a}_{n} that sense and report their findings to ""adjudicator""agents who must solve ethical problems. (Many if not most of these agents may be robots.) In such an environment, inconsistency is a virtual guarantee: aadj{a}_{\text{adj}} may, for instance, receive a report from a1{a}_{1} that proposition f\phi holds, then from a2{a}_{2} that ¬f\neg \phi holds, and then from a3{a}_{3} that neither f\phi nor ¬f\neg \phi should be believed, but rather <U+03C8>\psi instead, at some level of likelihood. We further assume that agents receiving such incompatible reports will nonetheless sometimes simply need, before long, to make decisions on the basis of these reports, in order to try to solve ethical problems. We provide a solution to such a quandary: AI capable of adjudicating competing reports from subsidiary agents through time, and delivering to humans a rational, ethically correct (relative to underlying ethical principles) recommendation based upon such adjudication. To illuminate our solution, we anchor it to a particular scenario.",Automated argument adjudication to solve ethical problems in multi-agent environments,Paladyn,Article,1/1/2021,Selmer,Bringsjord,2021
10.1521/aeap.14.6.361.24081,"In a study to determine sampling differences between Internet sites, we obtained data on 353 men who have sex with men in Chinese gay chat rooms and through e-mail web sites. Respondents were approached by the investigator and agreed to fill out an anonymous questionnaire on their Internet use and sexual activity. All materials and contacts were in Chinese characters. Data indicated that there were few differences between the chat room and Internet samples, but that those using e-mail appear to be more isolated, more homosexually-identified (rather than bisexual), have more experience with casual partners on a number of sexual activities, and were less likely to carry condoms and to have safe sex. E-mail respondents were more likely to want to discuss HIV/AIDS prevention on a web site or other site. These data suggest that the two recruiting methods are largely comparable in respondent characteristics, but that e-mail respondents are likely to be more isolated and at higher HIV risk than chat room participants.",Differences between chat room and e-mail sampling approaches in Chinese men who have sex with men,AIDS Education and Prevention,Article,1/1/2002,W,Ross,2002
10.1521/aeap.15.7.570.24047,"This study investigated HIV/AIDS knowledge, risk behaviors and perceptions, and access to services among Black immigrants from more than 20 African nations to Houston, Texas, United States. Three hundred nine respondents completed a 98-item self-administered questionnaire on HIV/AIDS knowledge, risk behaviors, access to services, and stigma. Data analysis revealed this population to be highly educated (70.9% had educational attainment levels beyond high school), with a plurality motivated to immigrate to the United States for academic reasons (45.0%). As a group they displayed a high level of knowledge about modes of HIV transmission. Generally, Christian background respondents had higher knowledge than those of Muslim background. Nevertheless, 36.3% reported that they had never used a condom, with the overwhelming majority of respondents reporting low self-perceived risk for contracting HIV (79.5%). These findings, together with the persistent practice of traditional rituals such as body scarring/tattooing by a significant minority (20.1%), a lack of awareness about vertical transmission (16.3% of women; 29.9% of men), and discouraging scores on an HIV stigma perception scale, suggest that a targeted campaign to raise awareness in this population is warranted.",Assessing the HIV/AIDS health services needs of African immigrants to Houston,AIDS Education and Prevention,Article,12/1/2003,W,Ross,2003
10.1521/aeap.2008.20.6.547,"We studied internalized homonegativity (IH) in 675 HIV-positive men who have sex with men (MSM) from six epicenters across the United States who attended an HIV prevention workshop. Participants included 300 African American and over 150 Hispanic White and White non-Hispanic men. Higher IH was signifcantly associated with African American race. Compulsive sexual behavior, openness as MSM, sexual comfort, depression, education level, and importance of religion also were associated with IH and independently predicted a third of this outcome's variance. For those with higher IH, two signifcant paths led to unsafe sexual behavior: frst, to serodiscordant unprotected anal intercourse (SDUAI) through being less ""out""-thus disclosing serostatus to secondary partners less frequently, and second, to lower condom self-effcacy and SDUAI through lower sexual comfort. These data provide information on the demographic, sexual and mental health variables associated with IH. They offer an indication of the paths through which IH is associated with serodiscordant risk behavior in HIV-positive MSM. © 2008 The Guilford Press.",The relationship of internalized homonegativity to unsafe sexual behavior in HIV-seropositive men who have sex with men,AIDS Education and Prevention,Article,12/1/2008,W,Ross,2008
10.1521/aeap.2009.21.3.251,"Rapid socioeconomic transformation in Vietnam in last 15 years has been followed by more liberation of sexual expression and representation of sexual identity among young people. There has been an increase in the visibility of homosexual men in major cities of Vietnam who were largely an unknown population until the emergence of the HIV epidemic. Men who have sex with men (MSM) are now considered as one of the target groups in many HIV prevention programs. This qualitative study examines local identities, relationships, and sexual practices among young MSM aged 15-24 in the cities of Hanoi and Ho Chi Minh City. Our analyses were based on 26 in-depth interviews and 10 focus group discussions with young MSM recruited through public place intercepts and cruising areas. Data document the linguistic classification, sexual relationships and behaviors, identity and process of homosexual identification, and the potential linkage between sexual identity and sexual behaviors of MSM in Vietnam. Data also highlight the stages of homosexual community development in urban Vietnam and important differences between Vietnam and the West in the representation of homosexual identity, relationships, and practices. In light of the findings, we suggest that the continuing development and elaboration of a homosexual community in Hanoi and Ho Chi Minh City offers significant opportunities for targeted HIV/AIDS prevention activities in the Vietnamese MSM population. © 2009 The Guilford Press.","Male homosexual identities, relationships, and practices among young men WHO have sex with men in Vietnam: implications for HIV prevention",AIDS Education and Prevention,Article,1/1/2009,W,Ross,2009
10.1521/aeap.2010.22.2.126,"This study was designed to examine the impact of HIV treatment optimism on sexual risk among a racially diverse sample of HIV-positive MSM. Survey data were collected from 346 racially diverse HIV-positive MSM. Inclusion criteria: 18 years of age, male, at least one incident of unprotected anal intercourse (UAI) in the last year, currently on treatment. Other variables included demographics, sexual risk, depression, internalized homonegativity, HIV treatment history, alcohol/drug use and beliefs about HIV treatments (Susceptibility to transmit HIV, Severity of HIV infection and Condom Motivation). Those with lower income were more likely to report that HIV was less transmissible. A self-reported decrease in condom motivation was associated with being White, well-educated and increased alcohol/drug use. A decrease in Severity of HIV was associated with better mental health, being non-White and undetectable viral load. Sexual risk appears related to beliefs about how treatment affects the transmissibility of HIV. Race, socioeconomic status, alcohol/drug use, mental health and viral load were also associated with treatment beliefs. © 2010 The Guilford Press.",HIV treatment optimism and unsafe anal intercourse among HIV-positive men who have sex with men: Findings from the positive connections study,AIDS Education and Prevention,Article,4/21/2010,W,Ross,2010
10.1524/itit.47.3.163.65606,"© Oldenbourg Verlag 2005.The Transregional Collaborative Research Center SFB/TR 8 Spatial Cognition was established by the German Science Foundation (DFG) at the Universities of Bremen and Freiburg in January 2003. 13 Research projects pursue interdisciplinary research on intelligent spatial information processing. This article introduces the research field of spatial cognition and reports on aspects from cognitive psychology, cognitive robotics, linguistics, and artificial intelligence.","Transregional collaborative research center SFB/TR 8 spatial cognition: Reasoning, action, interaction",IT - Information Technology,Article,3/1/2005,Wolfram,Burgard,2005
10.1557/mrc.2019.50,"Copyright © Materials Research Society 2019.We introduce CRYSTAL, a multi-agent AI system for crystal-structure phase mapping. CRYSTAL is the first system that can automatically generate a portfolio of physically meaningful phase diagrams for expert-user exploration and selection. CRYSTAL outperforms previous methods to solve the example Pd-Rh-Ta phase diagram, enabling the discovery of a mixed-intermetallic methanol oxidation electrocatalyst. The integration of multiple data-knowledge sources and learning and reasoning algorithms, combined with the exploitation of problem decompositions, relaxations, and parallelism, empowers AI to supersede human scientific data interpretation capabilities and enable otherwise inaccessible scientific discovery in materials science and beyond.",CRYSTAL: A multi-agent AI system for automated mapping of materials' crystal structures,MRS Communications,Article,6/1/2019,Carla,Gomes,2019
10.1557/mrc.2020.55,"Copyright © The Author(s), 2020, published on behalf of Materials Research Society by Cambridge University Press.Abstract Obtaining a good statistical representation of material microstructures is crucial for establishing robust process-structure-property linkages and machine learning techniques can bridge this gap. One major difficulty in leveraging recent advances in deep learning for this purpose is the scarcity of good quality data with enough metadata. In machine learning, similarity metric learning using Siamese networks has been used to deal with sparse data. Inspired by this, the authors propose a Siamese architecture to learn microstructure representations. The authors show that analysis tasks such as the classification of microstructures can be done more efficiently in the learned representation space.",Microstructure representation learning using Siamese networks,MRS Communications,Article,12/1/2020,Pushpak,Bhattacharyya,2020
10.1557/proc-0955-i16-02,"The de and rf performance of AlGaN/GaN High Electron Mobility Transistors (HEMTs) grown by Molecular Beam Epitaxy on Si-on-poly (SopSiC) substrates is reported. The HEMT structure incorporated a 7 period GaN/AIN superlattice between the AlGaN barrier and GaN channel for improved carrier confinement. The knee voltage of devices with 2 µm gate-drain spacing was 2.12 V and increased to 3 V at 8 µm spacing. The maximum frequency of oscillation, f MAx, was -40 GHz for devices with 0.5 µm gate length and 2 µm gate-drain spacing. Parameter extraction from the measured rf characteristics showed a maximum intrinsic transconductance of 143 mS.mm-1. © 2007 Materials Research Society.",AlGaN/GaN high electron mobility transistors on Si/SiO2/poly-SiC substrates,Materials Research Society Symposium Proceedings,Conference Paper,1/1/2006,I,J.,2006
10.15607/RSS.2021.XVII.072,"© 2021, MIT Press Journals, All rights reserved.Robots will be expected to manipulate a wide variety of objects in complex and arbitrary ways as they become more widely used in human environments. As such, the rearrangement of objects has been noted to be an important benchmark for AI capabilities in recent years. We propose NeRP (Neural Rearrangement Planning), a deep learning based approach for multi-step neural object rearrangement planning which works with never-before-seen objects, that is trained on simulation data, and generalizes to the real world. We compare NeRP to several naive and model-based baselines, demonstrating that our approach is measurably better and can efficiently arrange unseen objects in fewer steps and with less planning time. Finally, we demonstrate it on several challenging rearrangement problems in the real world1 .",NeRP: Neural Rearrangement Planning for Unknown Objects,Robotics: Science and Systems,Conference Paper,1/1/2021,Dieter,Fox,2021
10.1561/2200000006,"Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks. © 2009 Y. Bengio.",Learning deep architectures for AI,Foundations and Trends in Machine Learning,Article,12/1/2009,Yoshua,Bengio,2009
10.1590/S1020-49892009001200005,"Objective. To assess and analyze the associations between adherence to treatment and social position in women living with HIV/AIDS. Method. A cross-sectional, descriptive, correlational study among 269 Colombian women was conducted. Participants completed three questionnaires: a socio-demographic and clinical characteristics survey, a treatment adherence scale, and a social position survey. Results. Women of low social position had a significantly higher probability of low treatment adherence (OR = 5.651, P < 0.0001), and the majority of social position variables measured had a significant effect on adherence. A general model considering the variables ""type of national health care plan"" (""contributive, "" ""subsidized, "" or, in the case of vinculadas or the uninsured, ""none""); ""having HIV-positive children""; and ""level of viral load"" was statistically reliable in predicting study participants' treatment adherence. Membership in the subsidized plan or being uninsured had a greater effect on the probability of low adherence than membership in the contributive plan (OR = 3.478, P < 0.0001). Univariate regression analyses confirmed that women with HIV-positive children and a viral load = 400 copies/ml were more likely to have low adherence than women without those characteristics (OR = 2.395, P = 0.0274 and OR = 2.178, P = 0.0050, respectively). Conclusions. Improving women's adherence to HIV/AIDS treatment in Colombia would require eliminating barriers to national health care system and comprehensive health care services and implementing programs that take into account women's role as maternal caregivers The findings underscore the need to integrate variables related to gender inequality and social position in treatment adherence analysis, as advocated in the social determinants of health approach.","Social position, gender role, and treatment adherence among Colombian women living with HIV/AIDS: Social determinants of health approach",Revista Panamericana de Salud Publica/Pan American Journal of Public Health,Article,1/1/2009,W,Ross,2009
10.1609/aimag.v30i1.2151,"The First Conference on Artificial General Intelligence (AGI-08) was held on March 1-3, 2008, at the University of Memphis. The overall goal of the conference was to work toward a common understanding of the most promising paths toward creating AI systems with general intelligence at the human level and beyond and to share interim results and ideas achieved by researchers actively working toward powerful artificial general intelligence. Copyright © 2009, Association for the Advancement of Artificial Intelligence. All rights reserved.",Report on the first conference on artificial general intelligence (AGI-08),AI Magazine,Conference Paper,1/1/2009,Ben,Goertzel,2009
10.1609/aimag.v30i3.2254,"Creativity isn't magical. It's an aspect of normal human intelligence, not a special faculty granted to a tiny elite. There are three forms: combinational, exploratory, and transformational. All three can be modeled by AI - in some cases, with impressive results. AI techniques underlie various types of computer art. Whether computers could ""really"" be creative isn't a scientific question but a philosophical one, to which there's no clear answer. But we do have the beginnings of a scientific understanding of creativity. Copyright © 2009, Association for the Advancement of Artificial Intelligence. All rights reserved.",Computer models of creativity,AI Magazine,Article,1/1/2009,Margaret,Boden,2009
10.1609/aimag.v34i1.2431,"EBird is a citizen-science project that takes advantage of the human observational capacity to identify birds to species, and uses these observations to accurately represent patterns of bird occurrences across broad spatial and temporal extents. eBird employs artificial intelligence techniques such as machine learning to improve data quality by taking advantage of the synergies between human computation and mechanical computation. We call this a human/computer learning network, whose core is an active learning feedback loop between humans and machines that dramatically improves the quality of both and thereby continually improves the effectiveness of the network as a whole. In this article we explore how human/computer learning networks can leverage the contributions of human observers and process their contributed data with artificial intelligence algorithms leading to a computational power that far exceeds the sum of the individual parts. Copyright © 2013, Association for the Advancement of Artificial Intelligence.",E Bird: A human/computer learning network to improve biodiversity conservation and research,AI Magazine,Conference Paper,1/1/2013,Carla,Gomes,2013
10.1609/aimag.v35i2.2525,"Computational sustainability problems, which exist in dynamic environments with high amounts of uncertainty, provide a variety of unique chal - lenges to artificial intelligence research and the opportunity for significant impact upon our collective future. This editorial introduction provides an overview of artificial intelligence for computational sustainability, and introduces the next two special issue articles that will appear in AI Magazine. Copyright © 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.",Computational sustainability,AI Magazine,Review,1/1/2014,Carla,Gomes,2014
10.1609/aimag.v36i1.2576,"© 2015, Association for the Advancement of Artificial Intelligence. All rights reserved.The AIIDE-14 Workshop program was held Friday and Saturday, October 3-4, 2014, at North Carolina State University in Raleigh, North Carolina. The workshop program included five workshops covering a wide range of topics. The titles of the workshops held Friday were Artificial Intelligence for Adversarial Real-Time Games and Games and Natural Language Processing The titles of the workshops held Saturday were Diversity in Games Research, Experimental AI in Games, and Musical Metacreation. This article presents short summaries of those events.",Reports of the workshops held at the tenth AAAI conference on artificial intelligence and interactive digital entertainment (AIIDE),AI Magazine,Conference Paper,3/1/2015,Arne,Eigenfeldt,2015
10.1609/aimag.v37i1.2636,"Copyright © 2016, Association for the Advancement of Artificial Intelligence. All rights reserved.Given the well-known limitations of the Turing test, there is a need for objective tests to both focus attention on, and measure progress toward, the goals of AI. In this paper we argue that machine performance on standardized tests should be a key component of any new measure of AI, because attaining a high level of performance requires solving sig-nificant AI problems involving language understanding and world modeling - critical skills for any machine that lays claim to intelligence. In addition, standardized tests have all the basic requirements of a practical test: they are accessible, easily comprehensible, clearly measurable, and offer a graduated progression from simple tasks to those requiring deep understanding of the world. Here we propose this task as a challenge problem for the community, summarize our state-of-the-art results on math and science tests, and provide supporting data sets (www.allenai.org).",My computer is an honor student - But how intelligent is it? Standardized tests as a measure of AI,AI Magazine,Article,1/1/2016,Oren,Etzioni,2016
10.1609/aimag.v37i1.2648,"Copyright © 2016, Association for the Advancement of Artificial Intelligence. All rights reserved.I have argued that the goal of human-level AI can be equivalently expressed as creating sufficiently smart software social organisms. This equivalence is useful because the latter formulation makes strong suggestions about how such systems should be evaluated. No single test is enough, something which has become very apparent from the limitations of Turing's test, which brought about the workshop that motivated the talk that this article was based on. More positively, it provides a framework for organizing a battery of tests, namely the apprenticeship trajectory. An apprentice is initially a student, learning from instructors through carefully designed exercises. Apprentices start working as assistants to a mentor, with increasing responsibility as they learn. Eventually they start working autonomously, communicating with others at their same level, and even taking on their own apprentices. If we can learn how to build AI systems with these capabilities, it would be revolutionary. I hope the substrate capabilities for social organisms proposed here will encourage others to undertake this kind of research. The fantasy of the Turing test, and many of its proposed replacements, is that a single simple test can be found for measuring progress toward human-level AI. Part of the attraction of this view is that the alternative is both difficult and expensive. Many tests, involving multiple capabilities and interactions over time with people, all require substantial investments in research, engineering, and evaluation. But given that we are tackling one of the deepest questions ever asked by humanity, that is, what is mind, this should not be too surprising. And I believe it will be an extraordinarily productive investment.",Software social organisms: Implications for measuring AI progress,AI Magazine,Article,1/1/2016,Ken,Forbus,2016
10.1609/aimag.v37i2.2661,"Copyright © 2016, Association for the Advancement of Artificial Intelligence. All rights reserved.This article contains the reports of the AI for Human-Robot Interaction, Cognitive Assistance in Government and Public Sector Applications, Deceptive and Counter-Deceptive Machines, Self-Confdence in Autonomous Systems, and Sequential Decision Making for Intelligent Agents symposia, which were held November 12-14, 2015 in Arlington, Virginia.",The 2015 AAAI fall symposium series reports,AI Magazine,Conference Paper,6/1/2016,Selmer,Bringsjord,2016
10.1609/aimag.v38i4.2743,"© Copyright 2017, Association for the Advancement of Artificial Intelligence. All rights reserved.The Companion cognitive architecture is aimed at reaching human-level AI by creating software social organisms - systems that interact with people using natural modalities, working and learning over extended periods of time as collaborators rather than tools. Our two central hypotheses about how to achieve this are (1) analogical reasoning and learning are central to cognition, and (2) qualitative representations provide a level of description that facilitates reasoning, learning, and communication. This article discusses the evidence we have gathered supporting these hypotheses from our experiments with the Companion architecture. Although we are far from our ultimate goals, these experiments provide strong evidence for the utility of analogy and qualitative representation across a range of tasks. We also discuss three lessons learned and highlight three important open problems for cognitive systems research more broadly.",Analogy and qualitative representations in the companion cognitive architecture,AI Magazine,Article,12/1/2017,Ken,Forbus,2017
10.1609/aimag.v39i1.2785,"Copyright © 2018, Association for the Advancement of Artificial Intelligence. All rights reserved.From the stone age to the bronze, iron, and modern silicon ages, the discovery and characterization of new materials has always been instrumental to humanity's development and progress. With the current pressing need to address sustainability challenges and find alternatives to fossil fuels, we look for solutions in the development of new materials that will allow for renewable energy. To discover materials with the required properties, materials scientists can perform high-throughput materials discovery, which includes rapid synthesis and characterization via X-ray diffraction (XRD) of thousands of materials. A central problem in materials discovery, the phase map identification problem, involves the determination of the crystal structure of materials from materials composition and structural characterization data. This analysis is traditionally performed mainly by hand, which can take days for a single material system. In this work we present Phase-Mapper, a solution platform that tightly integrates XRD experimentation, AI problem solving, and human intelligence for interpreting XRD patterns and inferring the crystal structures of the underlying materials. Phase-Mapper is compatible with any spectral demixing algorithm, including our novel solver, AgileFD, which is based on convolutive nonnegative matrix factorization (NMF). AgileFD allows materials scientists to rapidly interpret XRD patterns, and incorporates constraints to capture prior knowledge about the physics of the materials as well as human feedback. With our system, materials scientists have been able to interpret previously unsolvable systems of XRD data at the Department of Energy's Joint Center for Artificial Photosynthesis, including the Nb-Mn-V oxide system, which led to the discovery of new solar light absorbers and is provided as an illustrative example of AI-enabled high-throughput materials discovery.",Phase-mapper: Accelerating materials discovery with AI,AI Magazine,Article,3/1/2018,Carla,Gomes,2018
10.1609/aimag.v40i1.2844,"Copyright © 2019, Association for the Advancement of Artificial Intelligence. All rights reserved.n The idea of implementing reinforcement learning in a computer was one of the earliest ideas about the possibility of AI, but reinforcement learning remained on the margin of AI until relatively recently. Today we see reinforcement learning playing essential roles in some of the most impressive AI applications. This article presents observations from the author’s personal experience with reinforcement learning over the most recent 40 years of its history in AI, focusing on striking connections that emerged between largely separate disciplines and on some of the findings that surprised him along the way. These connections and surprises place reinforcement learning in a historical context, and they help explain the success it is finding in modern AI. The article concludes by discussing some of the challenges that need to be faced as reinforcement learning moves out into real world.","Reinforcement learning: Connections, surprises, challenges",AI Magazine,Review,3/1/2019,Andrew,Barto,2019
10.1609/aimag.v41i1.5189,"© 2020 AI Access Foundation. All rights reserved.Sketching is a valuable but underutilized tool for science education. Sketch worksheets were developed to help change this, by using artificial intelligence technology to give students immediate feedback and to give instructors assistance in grading. Sketch worksheets use automatically computed visual representations combined with conceptual information to give feedback to students, by computing analogies between students' sketches and an instructor's solution sketch. This enables domain experts to develop sketch worksheets, to facilitate dissemination. We describe our experiences in deploying them in geoscience and artificial intelligence classes. The geoscience worksheets, authored by geoscientists at University of Wisconsin- Madison, were used at both Wisconsin and Northwestern University. The artificial intelligence worksheets were developed and used at Northwestern. Our experience indicates that sketch worksheets can provide helpful on-thespot feedback to students, and significantly improve grading efficiency, to the point where sketching assignments can be more practical to use broadly in science, technology, engineering, and mathematics education.","Sketch worksheets in science, technology, engineering, and mathematics classrooms: Two deployments",AI Magazine,Article,3/1/2020,Ken,Forbus,2020
10.1609/aimag.v41i2.5295,"© 2020 AI Access Foundation. All rights reserved.A fundamental goal of artificial intelligence research and development is the creation of machines that demonstrate what humans consider to be intelligent behavior. Effective knowledge representation and reasoning methods are a foundational requirement for intelligent machines. The development of these methods remains a rich and active area of artificial intelligence research in which advances have been motivated by many factors, including interest in new challenge problems, interest in more complex domains, shortcomings of current methods, improved computational support, increases in requirements to interact effectively with humans, and ongoing funding from the Defense Advanced Research Projects Agency and other agencies. This article highlights several decades of advances in knowledge representation and reasoning methods, paying particular attention to research on planning and on the impact of the Defense Advanced Research Projects Agency's support.",Knowledge representation and reasoning - a history of DARPA leadership,AI Magazine,Review,6/1/2020,Richard,Fikes,2020
10.1609/aimag.v41i4.5304,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602Artificial intelligence has achieved remarkable mastery over games such as Chess, Go, and poker, and even Jeopardy!, but the rich variety of standardized exams has remained a landmark challenge. Even as recently as 2016, the best artificial intelligence system could only achieve 59.3 percent on an eighth-grade science exam (Schoenick et al. 2017). This article reports success on the Grade 8 New York Regents Science Exam, where, for the first time, a system scores more than ninety percent on the exam's nondiagram, multiple-choice questions. In addition, our Aristo system, building upon the success of recent language models, exceeded eighty-three percent on the corresponding Grade 12 Science Exam's non-diagram, multiple-choice questions. The results, on unseen test questions, are robust across different test years and different variations of this kind of test. They demonstrate that modern natural language processing methods can result in mastery on this task. While not a full solution to general question answering (the questions are limited to eighth-grade multiple-choice science), it represents a significant milestone for the field.",From F to A on the New York regents science exams - an overview of the aristo project,AI Magazine,Article,12/1/2020,Oren,Etzioni,2020
10.1613/jair.1.11263,"© 2018 AI Access Foundation. All rights reserved.In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.",Grounding language for transfer in deep reinforcement learning,Journal of Artificial Intelligence Research,Article,12/1/2018,Regina,Barzilay,2018
10.1613/jair.1625,"Stochastic processes that involve the creation of objects and relations over time are widespread, but relatively poorly studied. For example, accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations, but doing this efficiently and accurately is difficult. Modeled as dynamic Bayesian networks, these processes have discrete variables with very large domains and extremely high dimensionality. In this paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a generalization of dynamic probabilistic relational models (DPRMs), which we had proposed in our previous work to model dynamic uncertain domains. We first extend the Rao-Blackwellised particle filtering described in our earlier work to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in RDBNs and propose two new forms of particle filtering. The first one uses abstraction hierarchies over the predicates to smooth the particle filter's estimates. The second employs kernel density estimation with a kernel function specifically designed for relational domains. Experiments show these two methods greatly outperform standard particle filtering on the task of assembly plan execution monitoring. ©2005 AI Access Foundation. All rights reserved.",Relational dynamic bayesian networks,Journal of Artificial Intelligence Research,Article,1/1/2005,Pedro,Domingos,2005
10.1613/jair.2243,"Reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard. Future clients guide their business decisions by considering the feedback from past transactions; if truthfully exposed, cheating behavior is sanctioned and thus becomes irrational. It therefore becomes important to ensure that rational clients have the right incentives to report honestly. As an alternative to side-payment schemes that explicitly reward truthful reports, we show that honesty can emerge as a rational behavior when clients have a repeated presence in the market. To this end we describe a mechanism that supports an equilibrium where truthful feedback is obtained. Then we characterize the set of paretooptimal equilibria of the mechanism, and derive an upper bound on the percentage of false reports that can be recorded by the mechanism. An important role in the existence of this bound is played by the fact that rational clients can establish a reputation for reporting honestly. © 2007 AI Access Foundation. AU rights reserved.",Obtaining reliable feedback for sanctioning reputation mechanisms,Journal of Artificial Intelligence Research,Article,1/1/2007,Boi,Faltings,2007
10.1613/jair.2290,"Entity resolution is the problem of reconciling database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of query-time entity resolution: quick and accurate resolution for answering queries over such 'unclean' databases at query-time. Since collective entity resolution approaches -where related references are resolved jointly - have been shown to be more accurate than independent attribute-based resolution for off-line entity resolution, we focus on developing new algorithms for collective resolution for answering entity resolution queries at query-time. For this purpose, we first formally show that, for collective resolution, precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered. Unfolding this progression leads naturally to a two stage 'expand and resolve' query processing strategy. In this strategy, we first extract the related records for a query using two novel expansion operators, and then resolve the extracted records collectively. We then show how the same strategy can be adapted for query-time entity resolution by identifying and resolving only those database references that are the most helpful for processing the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution. In addition to experiments on real datasets, we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data. © 2007 AI Access Foundation. All rights reserved.",Query-time entity resolution,Journal of Artificial Intelligence Research,Article,1/1/2007,Lise,Getoor,2007
10.1613/jair.2450,"Gesture is a non-verbal modality that can contribute crucial information to the understanding of natural language. But not all gestures are informative, and non-communicative hand motions may confuse natural language processing (NLP) and impede learning. People have little difficulty ignoring irrelevant hand movements and focusing on meaningful gestures, suggesting that an automatic system could also be trained to perform this task. However, the informativeness of a gesture is context-dependent and labeling enough data to cover all cases would be expensive. We present conditional modality fusion, a conditional hidden-variable model that learns to predict which gestures are salient for coreference resolution, the task of determining whether two noun phrases refer to the same semantic entity. Moreover, our approach uses only coreference annotations, and not annotations of gesture salience itself. We show that gesture features improve performance on coreference resolution, and that by attending only to gestures that are salient, our method achieves further significant gains. In addition, we show that the model of gesture salience learned in the context of coreference accords with human intuition, by demonstrating that gestures judged to be salient by our model can be used successfully to create multimedia keyframe summaries of video. These summaries are similar to those created by human raters, and significantly outperform summaries produced by baselines from the literature. © 2008 AI Access Foundation. All rights reserved.",Gesture salience as a hidden variable for coreference resolution and keyframe extraction,Journal of Artificial Intelligence Research,Article,1/1/2008,Regina,Barzilay,2008
10.1613/jair.2500,"In the efficient social choice problem, the goal is to assign values, subject to side constraints, to a set of variables to maximize the total utility across a population of agents, where each agent has private information about its utility function. In this paper we model the social choice problem as a distributed constraint optimization problem (DCOP), in which each agent can communicate with other agents that share an interest in one or more variables. Whereas existing DCOP algorithms can be easily manipulated by an agent, either by misreporting private information or deviating from the algorithm, we introduce M-DPOP, the first DCOP algorithm that provides a faithful distributed implementation for efficient social choice. This provides a concrete example of how the methods of mechanism design can be unified with those of distributed optimization. Faithfulness ensures that no agent can benefit by unilaterally deviating from any aspect of the protocol, neither informationrevelation, computation, nor communication, and whatever the private information of other agents. We allow for payments by agents to a central bank, which is the only central authority that we require. To achieve faithfulness, we carefully integrate the Vickrey-Clarke-Groves (VCG) mechanism with the DPOP algorithm, such that each agent is only asked to perform computation, report information, and send messages that is in its own best interest. Determining agent i's payment requires solving the social choice problem without agent i. Here, we present a method to reuse computation performed in solving the main problem in a way that is robust against manipulation by the excluded agent. Experimental results on structured problems show that as much as 87% of the computation required for solving the marginal problems can be avoided by re-use, providing very good scalability in the number of agents. On unstructured problems, we observe a sensitivity of M-DPOP to the density of the problem, and we show that reusability decreases from almost 100% for very sparse problems to around 20% for highly connected problems. We close with a discussion of the features of DCOP that enable faithful implementations in this problem, the challenge of reusing computation from the main problem to marginal problems in other algorithms such as ADOPT and OptAPO, and the prospect of methods to avoid the welfare loss that can occur because of the transfer of payments to the bank. ©2008 AI Access Foundation. All rights reserved.",M-DPOP: Faithful distributed implementation of efficient social choice problems,Journal of Artificial Intelligence Research,Article,1/1/2008,Boi,Faltings,2008
10.1613/jair.2621,"We consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational, self-interested agents. One example are online feedback mechanisms, where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect. However, (i) providing such feedback is costly, and (ii) there are many motivations for providing incorrect feedback. Both problems can be addressed by reward schemes which (i) cover the cost of obtaining and reporting feedback, and (ii) maximize the expected reward of a rational agent who reports truthfully. We address the design of such incentive-compatible rewards for feedback generated in environments with pure adverse selection. Here, the correlation between the true knowledge of an agent and her beliefs regarding the likelihoods of reports of other agents can be exploited to make honest reporting a Nash equilibrium. In this paper we extend existing methods for designing incentive-compatible rewards by also considering collusion. We analyze different scenarios, where, for example, some or all of the agents collude. For each scenario we investigate whether a collusion-resistant, incentive-compatible reward scheme exists, and use automated mechanism, design to specify an algorithm for deriving an efficient reward mechanism. &copy;2009 AI Access Foundation. All rights reserved.",Mechanisms for making crowds truthful,Journal of Artificial Intelligence Research,Article,1/1/2009,Boi,Faltings,2009
10.1613/jair.2633,"© 2009 AI Access Foundation. All rights reserved.This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as “a real bargain” or “good value.” These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.",Learning document-level semantic properties from free-text annotations,Journal of Artificial Intelligence Research,Article,4/1/2009,Regina,Barzilay,2009
10.1613/jair.2772,"The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully- implemented system that runs in 0(KN log N) time in the number of extractions. N, and the maximum number of synonyms per word, K. The system, called resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of Resolver.'s probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve Fl by 5%. An extension to the basic Resolver. system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus. &copy;2009 AI Access Foundation. All rights reserved.",Unsupervised methods for determining object and relation synonyms on the web,Journal of Artificial Intelligence Research,Article,1/1/2009,Oren,Etzioni,2009
10.1613/jair.2830,"We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods. © 2009 AI Access Foundation. All right reserved.",Content modeling using latent permutations,Journal of Artificial Intelligence Research,Article,12/1/2009,Regina,Barzilay,2009
10.1613/jair.2843,"We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases. © 2009 AI Access Foundation.",Multilingual part-of-speech tagging: Two unsupervised approaches,Journal of Artificial Intelligence Research,Article,12/1/2009,Regina,Barzilay,2009
10.1613/jair.3198,"In this paper we apply Conformal Prediction (CP) to the <U+03BA>-Nearest Neighbours Regression (<U+03BA>-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures. © 2011 AI Access Foundation. All rights reserved.",Regression conformal prediction with nearest neighbours,Journal of Artificial Intelligence Research,Article,1/1/2011,Alexander,Gammerman,2011
10.1613/jair.3200,"We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. We describe the Value of Information Lattice (VOILA), an optimal and efficient feature subset acquisition framework. Unlike the common practice, which is to acquire features greedily, VOILA can reason with subsets of features. VOILA efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination. © 2011 AI Access Foundation. All rights reserved.",Value of information lattice: Exploiting probabilistic independence for effective feature subset acquisition,Journal of Artificial Intelligence Research,Article,5/1/2011,Lise,Getoor,2011
10.1613/jair.3229,"Robots operating in domestic environments generally need to interact with articulated objects, such as doors, cabinets, dishwashers or fridges. In this work, we present a novel, probabilistic framework for modeling articulated objects as kinematic graphs. Vertices in this graph correspond to object parts, while edges between them model their kinematic relationship. In particular, we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations. We furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks. We finally present how the learned models can be generalized to new and previously unseen objects. In various experiments using real robots with di erent camera systems as well as in simulation, we show that our approach is valid, accurate and efficient. Further, we demonstrate that our approach has a broad set of applications, in particular for the emerging fields of mobile manipulation and service robotics. © 2011 AI Access Foundation. All rights reserved.",A probabilistic framework for learning kinematic models of articulated objects,Journal of Artificial Intelligence Research,Article,5/1/2011,Wolfram,Burgard,2011
10.1613/jair.3560,"Domain knowledge is crucial for e ective performance in autonomous control systems. Typically, human e ort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To e ectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the ocial game manual as the text guide. Our results show that a linguistically-informed game-playing agent signicantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization. © 2012 AI Access Foundation.",Learning to win by reading manuals in a monte-carlo framework,Journal of Artificial Intelligence Research,Article,1/1/2012,Regina,Barzilay,2012
10.1613/jair.3647,"We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis. © 2013 AI Access Foundation. All rights reserved.",Automatic aggregation by joint modeling of aspects and values,Journal of Artificial Intelligence Research,Article,1/1/2013,Regina,Barzilay,2013
10.1613/jair.3904,"To achieve an optimal outcome in many situations, agents need to choose distinct actions from one another. This is the case notably in many resource allocation problems, where a single resource can only be used by one agent at a time. How shall a designer of a multi-agent system program its identical agents to behave each in a different way? From a game theoretic perspective, such situations lead to undesirable Nash equilibria. For example consider a resource allocation game in that two players compete for an exclusive access to a single resource. It has three Nash equilibria. The two pure-strategy NE are efficient, but not fair. The one mixed-strategy NE is fair, but not efficient. Aumann's notion of correlated equilibrium fixes this problem: It assumes a correlation device that suggests each agent an action to take. However, such a ""smart"" coordination device might not be available. We propose using a randomly chosen, ""stupid"" integer coordination signal. ""Smart"" agents learn which action they should use for each value of the coordination signal. We present a multi-agent learning algorithm that converges in polynomial number of steps to a correlated equilibrium of a channel allocation game, a variant of the resource allocation game. We show that the agents learn to play for each coordination signal value a randomly chosen pure-strategy Nash equilibrium of the game. Therefore, the outcome is an efficient correlated equilibrium. This CE becomes more fair as the number of the available coordination signal values increases. © 2013 AI Access Foundation.",Decentralized anti-coordination through multi-agent learning,Journal of Artificial Intelligence Research,Article,1/1/2013,Boi,Faltings,2013
10.1613/jair.3983,"As large-scale theft of data from corporate servers is becoming increasingly common, it becomes interesting to examine alternatives to the paradigm of centralizing sensitive data into large databases. Instead, one could use cryptography and distributed computation so that sensitive data can be supplied and processed in encrypted form, and only the final result is made known. In this paper, we examine how such a paradigm can be used to implement constraint satisfaction, a technique that can solve a broad class of AI problems such as resource allocation, planning, scheduling, and diagnosis. Most previous work on privacy in constraint satisfaction only attempted to protect specific types of information, in particular the feasibility of particular combinations of decisions. We formalize and extend these restricted notions of privacy by introducing four types of private information, including the feasibility of decisions and the final decisions made, but also the identities of the participants and the topology of the problem. We present distributed algorithms that allow computing solutions to constraint satisfaction problems while maintaining these four types of privacy. We formally prove the privacy properties of these algorithms, and show experiments that compare their respective performance on benchmark problems. © 2013 AI Access Foundation. All rights reserved.",Protecting privacy through distributed computation in multi-agent decision making,Journal of Artificial Intelligence Research,Article,1/1/2013,Boi,Faltings,2013
10.1613/jair.4166,"We analyze symmetric protocols to rationally coordinate on an asymmetric, eficient allocation in an infinitely repeated N-agent, C-resource allocation problems, where the resources are all homogeneous. Bhaskar proposed one way to achieve this in 2-agent, 1- resource games: Agents start by symmetrically randomizing their actions, and as soon as they each choose different actions, they start to follow a potentially asymmetric \conven- tion"" that prescribes their actions from then on. We extend the concept of convention to the general case of infinitely repeated resource allocation games with N agents and C resources. We show that for any convention, there exists a symmetric subgame-perfect equilibrium which implements it. We present two conventions: bourgeois, where agents stick to the first allocation; and market, where agents pay for the use of resources, and observe a global coordination signal which allows them to alternate between different al- locations. We define price of anonymity of a convention as a ratio between the maximum social payoff of any (asymmetric) strategy profile and the expected social payoff of the subgame-perfect equilibrium which implements the convention. We show that while the price of anonymity of the bourgeois convention is infinite, the market convention decreases this price by reducing the con ict between the agents.© 2014 AI Access Foundation.",Symmetric subgame-perfect equilibria in resource allocation,Journal of Artificial Intelligence Research,Review,1/1/2014,Boi,Faltings,2014
10.18564/jasss.2166,"In this paper we introduce and motivate a conceptualization framework for agent-based social simulation, MAIA: Modelling Agent systems based on Institutional Analysis. The MAIA framework is based on Ostrom's Institutional Analysis and Development framework, and provides an extensive set of modelling concepts that is rich enough to capture a large range of complex social phenomena. Developing advanced agent-based models requires substantial experience and knowledge of software development knowledge and skills. MAIA has been developed to help modellers who are unfamiliar with software development to conceptualize and implement agent-based models. It provides the foundation for a conceptualization procedure that guides modellers to adequately capture, analyse, and understand the domain of application, and helps them report explicitly on the motivations behind modelling choices. A web-based application supports conceptualization with MAIA, and outputs an XML file which is used to generate Java code for an executable simulation. © JASSS.",MAIA: A framework for developing agent-based social simulations,JASSS,Article,1/1/2013,Virginia,Dignum,2013
10.18653/v1/2020.coling-demos.12,"© COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of System Demonstrations.We demonstrate TrainX, a system for Named Entity Linking for medical experts. It combines state-of-the-art entity recognition and linking architectures, such as Flair and fine-tuned Bi-Encoders based on BERT, with an easy-to-use interface for healthcare professionals. We support medical experts in annotating training data by using active sampling strategies to forward informative samples to the annotator. We demonstrate that our model is capable of linking against large knowledge bases, such as UMLS (3.6 million entities), and supporting zero-shot cases, where the linker has never seen the entity before. Those zero-shot capabilities help to mitigate the problem of rare and expensive training data that is a common issue in the medical domain.",TrainX – Named Entity Linking with Active Sampling and Bi-Encoders,"COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of System Demonstrations",Conference Paper,1/1/2020,Felix,Gers,2020
10.18653/v1/d15-1080,"© 2015 Association for Computational Linguistics.Elementary-level science exams pose significant knowledge acquisition and reasoning challenges for automatic question answering. We develop a system that reasons with knowledge derived from textbooks, represented in a subset of firstorder logic. Automatic extraction, while scalable, often results in knowledge that is incomplete and noisy, motivating use of reasoning mechanisms that handle uncertainty. Markov Logic Networks (MLNs) seem a natural model for expressing such knowledge, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. First, we simply use the extracted science rules directly as MLN clauses and exploit the structure present in hard constraints to improve tractability. Second, we interpret science rules as describing prototypical entities, resulting in a drastically simplified but brittle network. Our third approach, called Praline, uses MLNs to align lexical elements as well as define and control how inference should be performed in this task. Praline demonstrates a 15% accuracy boost and a 107times; reduction in runtime as compared to other MLN-based methods, and comparable accuracy to word-based baseline approaches.",Exploring Markov logic networks for question answering,Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing,Conference Paper,1/1/2015,Oren,Etzioni,2015
10.18653/v1/d15-1180,"© 2015 Association for Computational Linguistics.The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of lowrank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.1.","Molding CNNs for text: Non-linear, non-consecutive convolutions",Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing,Conference Paper,1/1/2015,Regina,Barzilay,2015
10.18653/v1/d17-1057,"© 2017 Association for Computational Linguistics.In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi-Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval-2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state-of-the-art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.",A Multilayer perceptron based ensemble technique for fine-grained financial sentiment analysis,"EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings",Conference Paper,1/1/2017,Pushpak,Bhattacharyya,2017
10.18653/v1/e17-1077,"© 2017 Association for Computational Linguistics.End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a ""pipeline"" fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, entity type classification and relation type classification. This model is referred to as ""All Word Pairs"" model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.",End-to-end relation extraction using neural networks and markov logic networks,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",Conference Paper,1/1/2017,Pushpak,Bhattacharyya,2017
10.18653/v1/e17-1109,"© 2017 Association for Computational Linguistics.Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for biomedical text mining. In this paper, we propose a novel approach of feature selection for entity extraction that exploits the concept of deep learning and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other features extracted by studying the properties of the datasets. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86%, 5.27% and 7.25% F-measure points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.",Entity extraction in biomedical corpora: An approach to evaluateword embedding features with PSO based feature selection,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",Conference Paper,1/1/2017,Pushpak,Bhattacharyya,2017
10.18653/v1/p16-1014,"© 2016 Association for Computational Linguistics.The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including traditional count-based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each timestep, the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.",Pointing the unknown words,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",Conference Paper,1/1/2016,Yoshua,Bengio,2016
10.18653/v1/p18-2011,"© 2018 Association for Computational LinguisticsIdentification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity as well as newswire subset of ACE 2005 dataset. Our approach performs better than the state-of-the-art.",Identification of alias links among participants in narratives,"ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",Conference Paper,1/1/2018,Pushpak,Bhattacharyya,2018
10.19173/IRRODL.V22I4.5474,"© 2022. All Rights Reserved.The purpose of this study was to design a curriculum of artificial intelligence (AI) application for secondary schools. The learning objective of the curriculum was to allow students to learn the application of conversational AI on a block-based programming platform. Moreover, the empirical study actually implemented the curriculum in the formal learning of a secondary school for a period of six weeks. The study evaluated the learning performance of students who were taught with the cycle of experiential learning in one class, while also evaluating the learning performance of students who were taught with the conventional instruction, which was called the cycle of doing projects. Two factors, learning approach and gender, were taken into account. The results showed that females' learning effectiveness was significantly better than that of males regardless of whether they used experiential learning or the conventional projects approach. Most of the males tended to be distracted from the conversational AI curriculum because they misbehaved during the conversational AI process. In particular, in their performance using the Voice User Interface with the conventional learning approach, the females outperformed the males significantly. The results of two-way ANCOVA revealed a significant interaction between gender and learning approach on computational thinking concepts. Females with the conventional learning approach of doing projects had the best computational thinking concepts in comparison with the other groups.",The Effects on Secondary School Students of Applying Experiential Learning to the Conversational AI Learning Curriculum,International Review of Research in Open and Distance Learning,Article,2/1/2022,Hal,Abelson,2022
10.21437/Interspeech.2017-775,"Copyright © 2017 ISCA.Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates. This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.",Improving speech recognition by revising gated recurrent units,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",Conference Paper,1/1/2017,Yoshua,Bengio,2017
10.21437/Interspeech.2019-2380,"Copyright © 2019 ISCALearning good representations is of crucial importance in deep learning. Mutual Information (MI) or similar measures of statistical dependence are promising tools for learning these representations in an unsupervised way. Even though the mutual information between two random variables is hard to measure directly in high dimensional spaces, some recent studies have shown that an implicit optimization of MI can be achieved with an encoder-discriminator architecture similar to that of Generative Adversarial Networks (GANs). In this work, we learn representations that capture speaker identities by maximizing the mutual information between the encoded representations of chunks of speech randomly sampled from the same sentence. The proposed encoder relies on the SincNet architecture and transforms raw speech waveform into a compact feature vector. The discriminator is fed by either positive samples (of the joint distribution of encoded chunks) or negative samples (from the product of the marginals) and is trained to separate them. We report experiments showing that this approach effectively learns useful speaker representations, leading to promising results on speaker identification and verification tasks. Our experiments consider both unsupervised and semi-supervised settings and compare the performance achieved with different objective functions.",Learning speaker representations with mutual information,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",Conference Paper,1/1/2019,Yoshua,Bengio,2019
10.2174/1568026611313050003,"Recent years have seen a dramatic increase in the amount and availability of data in the diverse areas of medicinal chemistry, making it possible to achieve significant advances in fields such as the design, synthesis and biological evaluation of compounds. However, with this data explosion, the storage, management and analysis of available data to extract relevant information has become even a more complex task that offers challenging research issues to Artificial Intelligence (AI) scientists. Ontologies have emerged in AI as a key tool to formally represent and semantically organize aspects of the real world. Beyond glossaries or thesauri, ontologies facilitate communication between experts and allow the application of computational techniques to extract useful information from available data. In medicinal chemistry, multiple ontologies have been developed during the last years which contain knowledge about chemical compounds and processes of synthesis of pharmaceutical products. This article reviews the principal standards and ontologies in medicinal chemistry, analyzes their main applications and suggests future directions. © 2013 Bentham Science Publishers.",Ontologies in medicinal chemistry: Current status and future challenges,Current Topics in Medicinal Chemistry,Review,6/10/2013,Asunción Gómez,Pérez,2013
10.2190/PUWY-N3AL-KK3T-B89L,"Objective: This study compares the psychological symptoms and bereavement distress of individuals bereaved by AIDS with a group bereaved by a cancer death, and addresses the question of whether an AIDS death is associated with a higher rate of adverse psychosocial factors that may increase risk of psychological morbidity in the bereaved individuals. Method: AIDS (n = 28) and cancer (n = 30) bereaved individuals (all within 3 months of the bereavement) completed measures of psychological morbidity and measures addressing a range of other adverse factors, e.g., number of losses, levels of social support and stigma. Results: The cancer and AIDS bereaved were essentially similar on all psychological symptom measures. The AIDS group reported lower levels of social support in response to the bereavement than cancer bereaved individuals; a greater number of bereavements, were more likely to conceal the cause of death from significant others including their own family and perceived, in some instances, a greater level of rejection from others. The AIDS group reported higher levels of social support from friends than from family. Conclusions: At three months following bereavement, AIDS and cancer bereaved were similar in levels of distress. While this may change with the progress of grief over time, it suggests essentially similar early bereavement responses. Those bereaved by AIDS reported a range of other adverse factors such as a greater number of losses, lower social support, stigma, and less open disclosure of the cause of death.",A comparison of the psychosocial aspects of AIDS and cancer-related bereavement,International Journal of Psychiatry in Medicine,Article,1/1/1996,W,Ross,1996
10.2196/27868,"© Alison Darcy, Jade Daniels, David Salinger, Paul Wicks, Athena Robinson.Background: There are far more patients in mental distress than there is time available for mental health professionals to support them. Although digital tools may help mitigate this issue, critics have suggested that technological solutions that lack human empathy will prevent a bond or therapeutic alliance from being formed, thereby narrowing these solutions’ efficacy. Objective: We aimed to investigate whether users of a cognitive behavioral therapy (CBT)–based conversational agent would report therapeutic bond levels that are similar to those in literature about other CBT modalities, including face-to-face therapy, group CBT, and other digital interventions that do not use a conversational agent. Methods: A cross-sectional, retrospective study design was used to analyze aggregate, deidentified data from adult users who self-referred to a CBT-based, fully automated conversational agent (Woebot) between November 2019 and August 2020. Working alliance was measured with the Working Alliance Inventory-Short Revised (WAI-SR), and depression symptom status was assessed by using the 2-item Patient Health Questionnaire (PHQ-2). All measures were administered by the conversational agent in the mobile app. WAI-SR scores were compared to those in scientific literature abstracted from recent reviews. Results: Data from 36,070 Woebot users were included in the analysis. Participants ranged in age from 18 to 78 years, and 57.48% (20,734/36,070) of participants reported that they were female. The mean PHQ-2 score was 3.03 (SD 1.79), and 54.67% (19,719/36,070) of users scored over the cutoff score of 3 for depression screening. Within 5 days of initial app use, the mean WAI-SR score was 3.36 (SD 0.8) and the mean bond subscale score was 3.8 (SD 1.0), which was comparable to those in recent studies from the literature on traditional, outpatient, individual CBT and group CBT (mean bond subscale scores of 4 and 3.8, respectively). PHQ-2 scores at baseline weakly correlated with bond scores (r=-0.04; P<.001); however, users with depression and those without depression had high bond scores of 3.45. Conclusions: Although bonds are often presumed to be the exclusive domain of human therapeutic relationships, our findings challenge the notion that digital therapeutics are incapable of establishing a therapeutic bond with users. Future research might investigate the role of bonds as mediators of clinical outcomes, since boosting the engagement and efficacy of digital therapeutics could have major public health benefits.","Evidence of human-level bonds established with a digital conversational agent: Cross-sectional, retrospective observational study",JMIR Formative Research,Article,5/1/2021,Alison,Darcy,2021
10.2200/S00206ED1V01Y200907AIM007,"Most subfields of computer science have an interface layer via which applications communicate with the infrastructure, and this is key to their success (e.g., the Internet in networking, the relational model in databases, etc.). So far this interface layer has been missing in AI. First-order logic and probabilistic graphical models each have some of the necessary features, but a viable interface layer requires combining both. Markov logic is a powerful new language that accomplishes this by attaching weights to first-order formulas and treating them as templates for features of Markov random fields. Most statistical models in wide use are special cases of Markov logic, and first-order logic is its infinite-weight limit. Artificial intelligence needs an interface layer, a language linking applications to their common infrastructure needs. AI applications involve high degrees of complexity and uncertainty. First-order logic handles complexity well and probabilistic graphical models do the same for uncertainty, but neither can cope effectively with both. Thus neither is sufficient for general AI. Markov logic is a powerful new language that seamlessly combines the two. Statements in Markov logic are simply weighted formulas in first-order logic, interpreted as templates for features of Markov random fields. Most statistical models in wide use are special cases of Markov logic, and first-order logic is its infinite-weight limit. Inference algorithms for Markov logic combine ideas from satisfiability, Markov chain Monte Carlo, belief propagation, and resolution. Learning algorithms make use of conditional likelihood, convex optimization, and inductive logic programming. Markov logic has been successfully applied to problems in information extraction and integration, natural language processing, robot mapping, social networks, computational biology, and others, and is the basis of the open-source Alchemy system. © 2009 by Morgan & Claypool.",Markov logic: An interface layer for artificial intelligence,Synthesis Lectures on Artificial Intelligence and Machine Learning,Article,6/19/2009,Pedro,Domingos,2009
10.2214/AJR.18.20813,"© American Roentgen Ray SocietyOBJECTIVE. The purpose of this study is to develop an image-based deep learning (DL) model to predict the 5-year risk of breast cancer on the basis of a single breast MR image from a screening examination. MATERIALS AND METHODS. We collected 1656 consecutive breast MR images from screening examinations performed for 1183 high-risk women from January 2011 to June 2013, to predict the risk of cancer developing within 5 years of the screening. Women who lacked a 5-year screening follow-up examination and women who had cancer other than primary breast cancer develop in their breast were excluded from the study. We developed a logistic regression model based on traditional risk factors (the risk factor logistic regression [RF-LR] model) and a DL model based on the MR image alone (the Image-DL model). Examinations occurring within 6 months of a cancer diagnosis were excluded from the testing sets in each fold of cross-validation. We compared our models against the Tyrer-Cuzick (TC) model. All models were evaluated using mean (± SD) AUC values and observed-to-expected (OE) ratios across 10-fold cross-validation. RESULTS. The RF-LR and Image-DL models achieved mean AUC values of 0.558 ± 0.108 and 0.638 ± 0.094, respectively. In contrast, the TC model achieved an AUC value of 0.493 ± 0.092. The Image-DL and RF-LR models achieved mean OE ratios of 0.993 ± 0.658 and 0.828 ± 0.181, compared with the mean OE ratio of 1.091 ± 0.255 obtained using the TC model. CONCLUSION. Our DL model can assess the 5-year cancer risk on the basis of a breast MR image alone, and it showed improved individual risk discrimination when compared with a state-of-the-art risk assessment model. These results offer promising preliminary data regarding the potential of image-based risk assessment models to support more personalized care.",Deep learning model to assess cancer risk on the basis of a breast MR image alone,American Journal of Roentgenology,Article,1/1/2019,Regina,Barzilay,2019
10.2307/25470648,"How accurately condoms are being used vary across populations and knowledge of the factors determining its proper use remains unclear. Knowledge of such differentials and determinants would aid in evaluating the contributions of condom use to HIV epidemic reduction. Baseline data from the Situationally Focused Individual HIV/AIDS intervention to promote HIV protective behavior among 2,213 Nigerian Military Personnel were analyzed. Educational status as a predictor variable was assessed using univariable and multivariable logistic regression model. Compared to those with less than high school education, those with high school and some college education were two times more likely to demonstrate knowledge of condom use and modeling, prevalence odds ratio (POR), 2.32, 95% Confidence Interval (CI) = 1.60-3.37. After adjustment for the relevant covariates, higher education attainment was associated with nonsignificant 62% increase in knowledge and modeling, POR, 1.62, 95% CI = 0.78-3.38. This study is indicative of low knowledge of condom use and modeling among the Nigerian military personnel; as well as a direct correlation between education attainment and knowledge of condom use and modeling.",Epidemiologic and behavioral characterization of knowledge of condom use and modeling among military personnel.,African journal of reproductive health,Article,1/1/2008,W,Ross,2008
10.2307/3583273,"This study examined the association between HIV/AIDS knowledge, attitudes, and perceptions among high school students age 14 years and older who attended school in two large urban towns, Bo and Freetown, Sierra Leone, West Africa. All subjects in the survey were Africans in grade two to ten in schools in Sierra Leone (n = 137). The mean age of the respondents was 18 years; 55 percent were female and 45 percent were male. The AIDS Social Assertiveness Scale (ASAS) was used. There were five factors derived from the ASAS in the study, which are similar to those used in previous studies. The factors derived were: Disclosure-Help Seeking, Contact with Infected People, Sexual Negotiation, Difficult Social Interaction, and Refusal of Risk Behaviours. These data indicate that the sub-scale structure of the ASAS in students from Sierra Leone is almost identical to the sub-scale structure of the ASAS reported by Venier, Akande, and Ross for Kenya, Nigeria and Zimbabwe, and by Ross, Caudle, and Taylor for Australia. Some 52 percent of the subjects who had intercourse reported that they used condoms. Factors related to condom use were age, knowledge about HIV/AIDS, and anxiety about disclosure of HIV/STD problems. Data indicate that intercourse was common and that greater condom use was associated with less anxiety over sexual-negotiation and greater anxiety over disclosure of having HIV or an STD.","HIV/AIDS prevention-related social skills and knowledge among adolescents in Sierra Leone, West Africa.",African journal of reproductive health,Article,1/1/1997,W,Ross,1997
10.2466/pr0.1992.70.3.771,"The role of bias in assessments of personal susceptibility to threat is a central concept in research on perception of risk. The current study aimed to clarify the association between perceived personal susceptibility to infection with HIV/AIDS and injecting risk behaviour with injecting drug-users' perception of the baseline rate of infection with HIV/AIDS. 1262 injecting drug-users from Australian cities were interviewed. The injecting drug-users were divided into high- and low-risk groups depending on the HIV/AIDS risk associated with their injecting behaviour. Subjects were subdivided into low-, medium-, and high-perceived personal susceptibility groups. Analysis indicated that injecting drug-users in the high-risk group underestimated the prevalence of HIV/AIDS infection relative to those in the low-risk group and that perceived personal susceptibility was rationally related to estimates of the baseline rate of infection.",Sources of bias in perception of HIV risk by injecting drug-users.,Psychological reports,Article,1/1/1992,W,Ross,1992
10.2478/jdis-2021-0024,"© 2021 2021 Sahand Vahidnia et al., published by Sciendo.Detection of research fields or topics and understanding the dynamics help the scientific community in their decisions regarding the establishment of scientific fields. This also helps in having a better collaboration with governments and businesses. This study aims to investigate the development of research fields over time, translating it into a topic detection problem. To achieve the objectives, we propose a modified deep clustering method to detect research trends from the abstracts and titles of academic documents. Document embedding approaches are utilized to transform documents into vector-based representations. The proposed method is evaluated by comparing it with a combination of different embedding and clustering approaches and the classical topic modeling algorithms (i.e. LDA) against a benchmark dataset. A case study is also conducted exploring the evolution of Artificial Intelligence (AI) detecting the research topics or sub-fields in related AI publications. Evaluating the performance of the proposed method using clustering performance indicators reflects that our proposed method outperforms similar approaches against the benchmark dataset. Using the proposed method, we also show how the topics have evolved in the period of the recent 30 years, taking advantage of a keyword extraction method for cluster tagging and labeling, demonstrating the context of the topics. We noticed that it is not possible to generalize one solution for all downstream tasks. Hence, it is required to fine-tune or optimize the solutions for each task and even datasets. In addition, interpretation of cluster labels can be subjective and vary based on the readers' opinions. It is also very difficult to evaluate the labeling techniques, rendering the explanation of the clusters further limited. As demonstrated in the case study, we show that in a real-world example, how the proposed method would enable the researchers and reviewers of the academic research to detect, summarize, analyze, and visualize research topics from decades of academic documents. This helps the scientific community and all related organizations in fast and effective analysis of the fields, by establishing and explaining the topics. In this study, we introduce a modified and tuned deep embedding clustering coupled with Doc2Vec representations for topic extraction. We also use a concept extraction method as a labeling approach in this study. The effectiveness of the method has been evaluated in a case study of AI publications, where we analyze the AI topics during the past three decades.",Embedding-based Detection and Extraction of Research Topics from Academic Documents Using Deep Clustering,Journal of Data and Information Science,Article,6/1/2021,Hussein,Abbass,2021
10.24963/ijcai.2017/655,"As intelligent systems are increasingly making decisions that directly affect society, perhaps the most important upcoming research direction in AI is to rethink the ethical implications of their actions. Means are needed to integrate moral, societal and legal values with technological developments in AI, both during the design process as well as part of the deliberation algorithms employed by these systems. In this paper, we describe leading ethics theories and propose alternative ways to ensure ethical behavior by artificial systems. Given that ethics are dependent on the socio-cultural context and are often only implicit in deliberation processes, methodologies are needed to elicit the values held by designers and stakeholders, and to make these explicit leading to better understanding and trust on artificial autonomous systems.",Responsible autonomy,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,1/1/2017,Virginia,Dignum,2017
10.24963/ijcai.2017/658,"The doctrine of double effect (DDE) is a longstudied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate DDE. We briefly present DDE, and use a firstorder modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to successfully simulate scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build DDE-compliant autonomous systems from scratch; or one can use it to verify that a given AI system is DDE-compliant, by applying a DDE layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the DDE layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by sketching initial work on how one can apply our DDE layer to the STRIPS-style planning model, and to a modified POMDP model. This is preliminary work to illustrate the feasibility of the second mode, and we hope that our initial sketches can be useful for other researchers in incorporating DDE in their own frameworks.",On automating the doctrine of double effect,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,1/1/2017,Selmer,Bringsjord,2017
10.24963/ijcai.2018/718,"© 2018 International Joint Conferences on Artificial Intelligence.All right reserved.We present nine facets for the analysis of the past and future evolution of AI. Each facet has also a set of edges that can summarise different trends and contours in AI. With them, we first conduct a quantitative analysis using the information from two decades of AAAI/IJCAI conferences and around 50 years of documents from AI topics, an official database from the AAAI, illustrated by several plots. We then perform a qualitative analysis using the facets and edges, locating AI systems in the intelligence landscape and the discipline as a whole. This analytical framework provides a more structured and systematic way of looking at the shape and boundaries of AI.",The facets of artificial intelligence: A framework to track the evolution of AI,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,1/1/2018,Peter,Flach,2018
10.24963/ijcai.2019/437,"© 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.Natural language generation (NLG) is an essential component of task-oriented dialogue systems. Despite the recent success of neural approaches for NLG, they are typically developed for particular domains with rich annotated training examples. In this paper, we study NLG in a low-resource setting to generate sentences in new scenarios with handful training examples. We formulate the problem from a meta-learning perspective, and propose a generalized optimization-based approach (Meta-NLG) based on the well-recognized model-agnostic metalearning (MAML) algorithm. Meta-NLG defines a set of meta tasks, and directly incorporates the objective of adapting to new low-resource NLG tasks into the meta-learning optimization process. Extensive experiments are conducted on a large multi-domain dataset (MultiWoz) with diverse linguistic variations. We show that Meta-NLG significantly outperforms other training procedures in various low-resource configurations. We analyze the results, and demonstrate that Meta-NLG adapts extremely fast and well to low-resource situations.",Meta-learning for low-resource natural language generation in task-oriented dialogue systems,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,1/1/2019,Boi,Faltings,2019
10.24963/ijcai.2019/802,"© 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.Artificial Intelligence (AI) applications are being used to predict and assess behaviour in multiple domains which directly affect human well-being. However, if AI is to improve people's lives, then people must be able to trust it, by being able to understand what the system is doing and why. Although transparency is often seen as the requirement in this case, realistically it might not always be possible, whereas the need to ensure that the system operates within set moral bounds remains. In this paper, we present an approach to evaluate the moral bounds of an AI system based on the monitoring of its inputs and outputs. We place a 'Glass-Box' around the system by mapping moral values into explicit verifiable norms that constrain inputs and outputs, in such a way that if these remain within the box we can guarantee that the system adheres to the value. The focus on inputs and outputs allows for the verification and comparison of vastly different intelligent systems; from deep neural networks to agent-based systems. The explicit transformation of abstract moral values into concrete norms brings great benefits in terms of explainability; stakeholders know exactly how the system is interpreting and employing relevant abstract moral human values and calibrate their trust accordingly. Moreover, by operating at a higher level we can check the compliance of the system with different interpretations of the same value.",Governance by glass-box: Implementing transparent moral bounds for AI behaviour,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,1/1/2019,Virginia,Dignum,2019
10.2514/6.2017-5143,"© 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.The Vegetable Production System (Veggie) is a scientific payload designed to support plant growth for food production under microgravity conditions. The configuration of Veggie consists of an LED lighting system with modular rooting “pillows” designed to contain substrate media and time-release fertilizer. The pillows were designed to be watered passively using capillary principles but have typically been watered manually by the astronauts in low-Earth orbit (LEO). The design of Veggie allows cabin air to be drawn through the plant enclosure for thermal and humidity control and for supplying CO2 to the plants. Since its delivery to the International Space Station (ISS) in 2014, Veggie has undergone several experimental trials by various crews. Ground unit testing of Veggie was conducted during an 8-month Mars analog study in a semi-contained environment of a simulated habitat located at approximately 8,200 feet (2,500 m) elevation on the Mauna Loa volcano on the Island of Hawai’i. The Hawai’i Space Exploration Analog and Simulation (HI-SEAS) offered conditions (habitat, mission, communications, etc.) intended to simulate a planetary exploration mission. This paper provides data and analyses to show the prospect for optimized use of the current Veggie design for human habitats. Lessons learned during the study may provide opportunities for updating the system design and operational parameters for current Veggie experiments being conducted onboard the ISS and for payloads on future deep space missions.",Plant growth optimization by vegetable production system in HI-SEAS analog habitat,"AIAA SPACE and Astronautics Forum and Exposition, SPACE 2017",Conference Paper,1/1/2017,Kim,Binsted,2017
10.2991/agi.2010.17,"Two new formal definitions of intelligence are presented, the ""pragmatic general intelligence"" and ""efficient pragmatic general intelligence."" Largely inspired by Legg and Hutter's formal definition of ""universal intelligence,"" the goal of these definitions is to capture a notion of general intelligence that more closely models that possessed by humans and practical AI systems, which combine an element of universality with a certain degree of specialization to particular environments and goals. Pragmatic general intelligence measures the capability of an agent to achieve goals in environments, relative to prior distributions over goal and environment space. Efficient pragmatic general intelligences measures this same capability, but normalized by the amount of computational resources utilized in the course of the goal-achievement. A methodology is described for estimating these theoretical quantities based on observations of a real biological or artificial system operating in a real environment. Finally, a measure of the ""degree of generality"" of an intelligent system is presented, allowing a rigorous distinction between ""general AI"" and ""narrow AI."".",Toward a formal characterization of real-world general intelligence,"Artificial General Intelligence - Proceedings of the Third Conference on Artificial General Intelligence, AGI 2010",Conference Paper,1/1/2010,Ben,Goertzel,2010
10.3109/10826089509048753,"Survey data were used to study the association of methadone maintenance and needle-sharing. An ordinal scale of HIV risk was derived from the number of persons from whom subjects reported accepting a used needle and syringe in the 6 months prior to interview. The odds of respondents in methadone maintenance being in the higher risk group were half those of daily heroin users not in treatment for all three transitions in a four-level ordinal scale of risk (OR 0.55, 95% CL 0.33 to 0.90, ordinal logistic regression). This association disappeared when methadone patients who had not injected in the month prior to interview were excluded from the analysis. Subjects' knowledge concerning HIV and AIDS had no measurable association with the outcome. It is concluded that methadone maintenance reduces heroin addicts' risk of infection with HIV by reducing the likelihood of their injecting drugs rather than by changing their injecting behavior. © 1995 Informa UK Ltd All rights reserved: reproduction in whole or part not permitted.",Methadone maintenance and the likelihood of risky needle-sharing,Substance Use and Misuse,Article,1/1/1995,W,Ross,1995
10.3166/ria.16.165-190,"This paper first gives an overview of Simon's research in psychology, and then focuses on his work on the psychology of expertise. The roles of search and pattern recognition, as well as their interaction, are described, in particular with respect to chess players. Implications for artificial intelligence are drawn.",Research and pattern recognition among experts,Revue d'Intelligence Artificielle,Article,1/1/2002,Fernand,Gobet,2002
10.3233/978-1-60750-959-2-104,"The human mind can do many amazing things. Of particular interest are a set of cognitive abilities such as simple inference and recognition that are computationally very demanding, but that we humans perform without any perceptible delay or any sense of mental effort- this despite the fact that our brains use slow, millisecond-speed components. In this position paper, I present a brief inventory of human mental operations that exhibit this kind of surprising efficiency. I suggest that we humans accomplish these feats by (a) avoiding the computationally intractable forms of these problems and (b) by applying massively parallel processing, though perhaps of a very simple kind. This is not a new suggestion- some of these ideas were first developed in my mid-1970s work on NETL [1]. However, I believe that a renewed focus on the parallel vs. serial components of mental processing can help us both in understanding human intelligence and in achieving human-like performance in our AI systems. © 2011 The authors and IOS Press. All rights reserved.",Parallel and serial components in human-like intelligence,Frontiers in Artificial Intelligence and Applications,Conference Paper,1/1/2011,Scott,Fahlman,2011
10.3233/978-1-61499-672-9-3,"© 2016 The Authors and IOS Press.Institutions regulate societies. Comprising Searle's constitutive counts-as rules, ""A counts-as B in context C"", an institution ascribes from brute and institutional facts (As), a social reality comprising institutional facts (Bs) conditional on the social reality (contexts Cs). When brute facts change an institution evolves from one social reality to the next. Rule changes are also regulated by rule-modifying counts-as rules ascribing rule change in the past/present/future (e.g. a majority rule change vote counts-as a rule change). Determining rule change legality is difficult, since changing counts-as rules both alters and is conditional on the social reality, and in some cases hypothetical rule-change effects (e.g. not retroactively criminalising people). However, without a rigorous account of rule change ascriptions, AI agents cannot support humans in understanding the laws imposed on them. Moreover, advances in automated governance design for socio-technical systems, are limited by agents' ability to understand how and when to enact institutional changes. Consequently, we answer ""when do rule changes count-as legal rule changes?"" in a temporal setting with a novel formal framework.",When do rule changes count-as legal rule changes?,Frontiers in Artificial Intelligence and Applications,Conference Paper,1/1/2016,Virginia,Dignum,2016
10.3233/AIC-130576,The paper tells the story of the beginnings of Artificial Intelligence (AI) as a scientific venture from a European perspective. The story is embedded into three main steps of history. In view of a sustainable future of our globe it is deemed necessary to vigorously advance the initiated third step. Assuming AI's due role in this process would mean a change in its long-term goal: enhancing rather than simulating human intelligence. © 2014 - IOS Press and the authors.,Artificial intelligence in a historical perspective,AI Communications,Review,1/1/2014,Wolfgang,Bibel,2014
10.3233/AIC-1987-0103,"ECCAI, the European Coordinating Committee for Artificial Intelligence, is the legitimate representative body of Artificial Intelligence (AI) in Europe. ECCAI has, among many other things, initiated the foundation of an AI-periodical which has finally been realised. This first issue's publication appears to be a suitable occasion for a brief review of ECCAI's role from the viewpoint of the past Chairman of ECCAI. © 1987 IOS Press.",ECCAI and the european malaise,AI Communications,Article,1/1/1987,Wolfgang,Bibel,1987
10.3233/aic-1996-9204,"Classical artificial intelligence techniques such as expert systems have often been found to be too brittle for largescale applications. Model-based reasoning is a technique for making artificial intelligence software applicable to problems of realistic size. In this working group, we have investigated some fundamental issues in model-based reasoning and various applications in diagnosis, control and design. This paper reports on the results in basic techniques as well as diagnosis applications obtained at EPFL, ETHZ, Landis & Gyr and Nestlé. The results of the part dealing with design are reported in another paper in this issue, pp. 65-73.",Working group in model-based design and reasoning. Part I: Modeling and diagnosis,AI Communications,Article,1/1/1996,Boi,Faltings,1996
10.3233/aic-1996-9205,"Conventional artificial intelligence techniques such as expert systems have often been found to be too brittle for large-scale applications. Model-based reasoning is a technique for making artificial intelligence software applicable to problems of realistic size. In this working group, we have investigated some fundamental issues in model-based reasoning and various applications in diagnosis, control and design. This paper reports on the work at the Artificial Intelligence Laboratory, EPFL and the Chair for Computer-aided Architectural Design, ETHZ on applying model-based reasoning to design problems. The results concerning basic techniques and diagnosis applications are reported in another paper in this issue, pp. 59-64.",Working group in model-based design and reasoning. Part II: Design,AI Communications,Article,1/1/1996,Boi,Faltings,1996
10.3233/FAIA200376,"© 2020 The authors and IOS Press.The demand for insights into how artificial intelligent systems work is rapidly growing. This has arisen as AI systems are being integrated into almost every aspect of our lives from finance to health, security and our social lives. Current techniques for generating explanations focus on explaining opaque algorithms such as neural network models. However, considering the fact that these models do not work in isolation, but are combined, either manually or automatically, with other inference operations, local explanations of individual components are simply not enough to give the user adequate insights into how an intelligent system works. It is not unusual for a system made up of fairly intuitive components to become opaque when it is combined with others to build an intelligent agent. In this paper we argue that there is the need to combine diverse forms of reasoning in order to generate explanations that span the entire chain of reasoning: Not just explanations for the, so called, black-box models. Our hypothesis is that: A hybrid approach using statistical and deductive reasoning makes possible a richer form of explanation not available to purely statistical ML approaches. We explore the concepts of alocal' and aglobal' explanations and show how to give users a wide range of insights, using what we term an aexplanation blanket'. We tackle this challenge using the FRANK query answering system and show that its hybrid approach facilitates this kind of reasoning with explanations. It is important to note that the evaluation of user preferences for explanation is outside the scope of this work.",Explainable inference in the frank query answering system,Frontiers in Artificial Intelligence and Applications,Conference Paper,8/24/2020,Alan,Bundy,2020
10.3233/FAIA200941,"© 2020 The authors and IOS Press. All rights reserved.After taking note of the conceptual fact that robots may well carry humans inside them, and more specifically that modern AI-infused cars, jets, spaceships, etc. can be viewed as such robots, we present a case study in which inconsistent attitude measurements resulted in the tragic crash in Sweden of such a jet and the death of both pilots. After setting out desiderata for an automated defeasible inductive reasoner able to suitably prevent such tragedies, we formalize the scenario in a first-order defeasible reasoner-OSCAR- A nd find that it can quickly generate a partial solution to the dilemma the pilots couldn't conquer. But we then note and address the shortcomings of OSCAR relative to the desiderata, and adumbrate a solution supplied by a more expressive reasoner based on an inductive defeasible multi-operator cognitive calculus (â.žâ.,°?'ž) that is inspired by a merely deductive (monotonic) precursor (žâ.,°?'ž). Our solution in this calculus exploits both the social and cultural aspects of of the jet/robot we suggest be engineered in the future. After describing our solution, some remarks about related prior work follow, we present and rebut two objections, and then wrap up with a brief conclusion.","Culturally Aware Social Robots That Carry Humans Inside Them, Protected by Defeasible Argumentation Systems",Frontiers in Artificial Intelligence and Applications,Conference Paper,12/15/2020,Selmer,Bringsjord,2020
10.3233/FAIA220186,"© 2022 The authors and IOS Press.When mathematical modelling is applied to capture a complex system, multiple models are often created that characterise different aspects of that system. Often, a model at one level will produce a prediction which is contradictory at another level but both models are accepted because they are both useful. Rather than aiming to build a single unified model of a complex system, the modeller acknowledges the infinity of ways of capturing the system of interest, while offering their own specific insight. We refer to this pragmatic applied approach to complex systems - one which acknowledges that they are incompressible, dynamic, nonlinear, historical, contextual, and value-laden - as Open Machine Learning (Open ML). In this paper we define Open ML and contrast it with some of the grand narratives of ML of two forms: 1) Closed ML, ML which emphasizes learning with minimal human input (e.g. Google's Alpha Zero) and 2) Partially Open ML, ML which is used to parameterize existing models. To achieve this, we use theories of critical complexity to both evaluate these grand narratives and contrast them with the Open ML approach. Specifically, we deconstruct grand ML ‘theories' by identifying thirteen ‘games' played in the ML community. These games lend false legitimacy to models, contribute to over-promise and hype about the capabilities of artificial intelligence, reduce wider participation in the subject, lead to models that exacerbate inequality and cause discrimination and ultimately stifle creativity in research. We argue that best practice in ML should be more consistent with critical complexity perspectives than with rationalist, grand narratives.",The Games We Play: Critical Complexity Improves Machine Learning,Frontiers in Artificial Intelligence and Applications,Conference Paper,9/19/2022,Abeba,Birhane,2022
10.3233/FAIA220459,"© 2022 The authors and IOS Press.We introduce the Illinois Intentional Tort Qualitative Dataset, a set of Illinois Common Law cases in Assault, Battery, Trespass, and Self-Defense, machine-translated into qualitative predicate representations. We discuss the cases involved, the natural language understanding system used to translate the cases into predicate logic, and validation measures that serve as performance baselines for future AI research using the dataset.",The Illinois Intentional Tort Qualitative Dataset,Frontiers in Artificial Intelligence and Applications,Conference Paper,12/5/2022,Ken,Forbus,2022
10.32473/flairs.v35i.130630,"© 2022 by the authors. All rights reserved.Qualitative mechanical problem-solving (QMPS) is central to human-level intelligence. Human agents use their capacity for such problem-solving to succeed in tasks as routine as opening the tap to drink or hanging a picture on the wall, as well as for more sophisticated tasks in demanding jobs in today’s economy (e.g., emergency medicine, plumbing, hydraulic machinery, & driving). Unfortunately, artificial agents (in-cluding specifically robots) of today lack the capacity in question. Our work takes QMPS to fall under the general, longstanding AI area of qualitative reasoning (QR), historically an intensely logic-based affair. We embrace this history, and take new, further steps to advance QMPS. The Bennett Mechanical Comprehension Tests (BMCT-I and BMCT-II) assess a human’s ability to solve QMPS problems, and are used in the real world by many employers to evaluate job candidates. Building on the work of others who have attacked BMCT under the rubric of Psychometric AI (PAI), we introduce one of our novel algorithms (AB1) in a family (AB) of such for QMPS as required by BMCT, illustrate via case studies, report time-based performance of AB1, and assess our progress with an eye to future work in which our approach is extended to a sub-class of algorithms in AB that exploit the power of argument-based nonmonotonic logic, and leverage the success of transformer models to enhance their efficiency.","Qualitative Mechanical Problem-Solving by Artificial Agents: Further Progress, Under Psychometric AI","Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS",Conference Paper,1/1/2022,Selmer,Bringsjord,2022
10.32604/iasc.2021.014828,"© 2021, Tech Science Press. All rights reserved.The National Center for Education Statistics study reported that 80% of students change their major or institution at least once before getting a degree, which requires a course equivalency process. This error-prone process varies among disciplines, institutions, regions, and countries and requires effort and time. Therefore, this study aims to overcome these issues by developing a decision support tool called TiMELY for automatic Arabic text recognition using artificial intelligence techniques. The developed tool can process a complete document analysis for several course descriptions in multiple file formats, such as Word, Text, Pages, JPEG, GIF, and JPG. We applied a comparative approach in selecting the highest score using three Arabic text extraction algorithms: term frequency-inverse document frequency measure algorithm, Cortical.io tool with Retina Database, and keyword extraction using word co-occurrence algorithm. The data repository consisted of 1000 datasets built from five different faculties at King Abdul-Aziz University and King Faisal University. It was followed by a discussion of the evaluation techniques using precision and recall measure-ments, which indicated that the keyword extraction using word co-occurrence algorithm scored 90% for the English language and 80% for the Arabic language in terms of the F1 measure that focuses on the linguistic relation between words.",Decision support system tool for arabic text recognition,Intelligent Automation and Soft Computing,Article,1/1/2021,Fatmah,Baothman,2021
10.3366/E174236000800052X,"The paper develops some of the conclusions, reached in Floridi (2007), concerning the future developments of Information and Communication Technologies (ICTs) and their impact on our lives. The two main theses supported in that article were that, as the information society develops, the threshold between online and offline is becoming increasingly blurred, and that once there won't be any significant difference, we shall gradually re-conceptualise ourselves not as cyborgs but rather as inforgs, i.e. socially connected, informational organisms. In this paper, I look at the development of the so-called Semantic Web and Web 2.0 from this perspective and try to forecast their future. Regarding the Semantic Web, I argue that it is a clear and well-defined project, which, despite some authoritative views to the contrary, is not a promising reality and will probably fail in the same way AI has failed in the past. Regarding Web 2.0, I argue that, although it is a rather ill-defined project, which lacks a clear explanation of its nature and scope, it does have the potentiality of becoming a success (and indeed it is already, as part of the new phenomenon of Cloud Computing) because it leverages the only semantic engines available so far in nature, us. I conclude by suggesting what other changes might be expected in the future of our digital environment. © 2009, Cambridge University Press. All rights reserved.",Web 2.0 vs. the Semantic Web: A Philosophical Assessment,Episteme,Article,1/1/2009,Luciano,Floridi,2009
10.3389/fcomp.2022.1068361,"Copyright © 2022 Mökander, Sheth, Gersbro-Sundler, Blomgren and Floridi.While the use of artificial intelligence (AI) systems promises to bring significant economic and social benefits, it is also coupled with ethical, legal, and technical challenges. Business leaders thus face the question of how to best reap the benefits of automation whilst managing the associated risks. As a first step, many companies have committed themselves to various sets of ethics principles aimed at guiding the design and use of AI systems. So far so good. But how can well-intentioned ethical principles be translated into effective practice? And what challenges await companies that attempt to operationalize AI governance? In this article, we address these questions by drawing on our first-hand experience of shaping and driving the roll-out of AI governance within AstraZeneca, a biopharmaceutical company. The examples we discuss highlight challenges that any organization attempting to operationalize AI governance will have to face. These include questions concerning how to define the material scope of AI governance, how to harmonize standards across decentralized organizations, and how to measure the impact of specific AI governance initiatives. By showcasing how AstraZeneca managed these operational questions, we hope to provide project managers, CIOs, AI practitioners, and data privacy officers responsible for designing and implementing AI governance frameworks within other organizations with generalizable best practices. In essence, companies seeking to operationalize AI governance are encouraged to build on existing policies and governance structures, use pragmatic and action-oriented terminology, focus on risk management in development and procurement, and empower employees through continuous education and change management.",Challenges and best practices in corporate AI governance: Lessons from the biopharmaceutical industry,Frontiers in Computer Science,Article,11/10/2022,Luciano,Floridi,2022
10.3389/fmars.2021.768668,"Copyright © 2021 Graham, Aoki, Stephens, Stokes, Dayal, Rappazzo, Gomes and Harvell.Seagrass meadows provide valuable ecosystem benefits but are at risk from disease. Eelgrass (Zostera marina) is a temperate species threatened by seagrass wasting disease (SWD), caused by the protist Labyrinthula zosterae. The pathogen is sensitive to warming ocean temperatures, prompting a need for greater understanding of the impacts on host health under climate change. Previous work demonstrates pathogen cultures grow faster under warmer laboratory conditions and documents positive correlations between warmer ocean temperatures and disease levels in nature. However, the consequences of disease outbreaks on eelgrass growth remain poorly understood. Here, we examined the effect of disease on eelgrass productivity in the field. We coupled in situ shoot marking with high-resolution imagery of eelgrass blades and used an artificial intelligence application to determine disease prevalence and severity from digital images. Comparisons of eelgrass growth and disease metrics showed that SWD impaired eelgrass growth and accumulation of non-structural carbon in the field. Blades with more severe disease had reduced growth rates, indicating that disease severity can limit plant growth. Disease severity and rhizome sugar content were also inversely related, suggesting that disease reduced belowground carbon accumulation. Finally, repeated measurements of diseased blades indicated that lesions can grow faster than healthy tissue in situ. This is the first study to demonstrate the negative impact of wasting disease on eelgrass health in a natural meadow. These results emphasize the importance of considering disease alongside other stressors to better predict the health and functioning of seagrass meadows in the Anthropocene.",Effects of Seagrass Wasting Disease on Eelgrass Growth and Belowground Sugar in Natural Meadows,Frontiers in Marine Science,Article,11/10/2021,Carla,Gomes,2021
10.3389/fphy.2022.944064,"Copyright © 2022 Abbass, Petraki and Hunjet.Bi-directional communication between humans and swarm systems begs for efficient languages to communicate information between the humans and the Artificial Intelligence (AI)-enabled agents in a manner that is most appropriate for the context. We discuss the criteria for effective teaming and functional bi-directional communication between humans and AI, and the design choices required to create effective languages. We then present a human-AI-teaming communication language inspired by the Australian Aboriginal language of Jingulu, which we call JSwarm. We present the motivation and structure of the language. An example is used to demonstrate how the language operates for a shepherding swarm guidance task.",JSwarm: A Jingulu-Inspired Human-AI-Teaming Language for Context-Aware Swarm Guidance,Frontiers in Physics,Article,7/14/2022,Hussein,Abbass,2022
10.3389/fpsyg.2019.01401,"© 2019 Gobet and Sala.Recent years have been marked by important developments in artificial intelligence (AI). These developments have highlighted serious limitations in human rationality and shown that computers can be highly creative. There are also important positive outcomes for psychologists studying creativity. It is now possible to design entirely new classes of experiments that are more promising than the simple tasks typically used for studying creativity in psychology. In addition, given the current and future AI algorithms for developing new data structures and programs, novel theories of creativity are on the horizon. Thus, AI opens up entire new avenues for studying human creativity in psychology.",How artificial intelligence can help us understand human creativity,Frontiers in Psychology,Article,1/1/2019,Fernand,Gobet,2019
10.3389/frai.2021.737072,"© Copyright © 2021 Methnani, Aler Tubella, Dignum and Theodorou.As Artificial Intelligence (AI) continues to expand its reach, the demand for human control and the development of AI systems that adhere to our legal, ethical, and social values also grows. Many (international and national) institutions have taken steps in this direction and published guidelines for the development and deployment of responsible AI systems. These guidelines, however, rely heavily on high-level statements that provide no clear criteria for system assessment, making the effective control over systems a challenge. “Human oversight” is one of the requirements being put forward as a means to support human autonomy and agency. In this paper, we argue that human presence alone does not meet this requirement and that such a misconception may limit the use of automation where it can otherwise provide so much benefit across industries. We therefore propose the development of systems with variable autonomy—dynamically adjustable levels of autonomy—as a means of ensuring meaningful human control over an artefact by satisfying all three core values commonly advocated in ethical guidelines: accountability, responsibility, and transparency.",Let Me Take Over: Variable Autonomy for Meaningful Human Control,Frontiers in Artificial Intelligence,Article,9/14/2021,Virginia,Dignum,2021
10.3389/frobt.2018.00021,"© 2018 Linson, Clark, Ramamoorthy and Friston.The emerging neurocomputational vision of humans as embodied, ecologically embedded, social agents-who shape and are shaped by their environment-offers a golden opportunity to revisit and revise ideas about the physical and information-theoretic underpinnings of life, mind, and consciousness itself. In particular, the active inference framework (AIF) makes it possible to bridge connections from computational neuroscience and robotics/AI to ecological psychology and phenomenology, revealing common underpinnings and overcoming key limitations. AIF opposes the mechanistic to the reductive, while staying fully grounded in a naturalistic and information-theoretic foundation, using the principle of free energy minimization. The latter provides a theoretical basis for a unified treatment of particles, organisms, and interactive machines, spanning from the inorganic to organic, non-life to life, and natural to artificial agents. We provide a brief introduction to AIF, then explore its implications for evolutionary theory, ecological psychology, embodied phenomenology, and robotics/AI research. We conclude the paper by considering implications for machine consciousness.",The active inference approach to ecological perception: General information dynamics for natural and artificial embodied cognition,Frontiers Robotics AI,Article,1/1/2018,Andy,Clark,2018
10.3389/frobt.2022.937825,"Copyright © 2022 Clavel, Labeau and Cassell.Socio-conversational systems are dialogue systems, including what are sometimes referred to as chatbots, vocal assistants, social robots, and embodied conversational agents, that are capable of interacting with humans in a way that treats both the specifically social nature of the interaction and the content of a task. The aim of this paper is twofold: 1) to uncover some places where the compartmentalized nature of research conducted around socio-conversational systems creates problems for the field as a whole, and 2) to propose a way to overcome this compartmentalization and thus strengthen the capabilities of socio-conversational systems by defining common challenges. Specifically, we examine research carried out by the signal processing, natural language processing and dialogue, machine/deep learning, social/affective computing and social sciences communities. We focus on three major challenges for the development of effective socio-conversational systems, and describe ways to tackle them.",Socio-conversational systems: Three challenges at the crossroads of fields,Frontiers in Robotics and AI,Review,12/15/2022,Justine,Cassell,2022
10.3390/electronics11050779,"© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Argumentation-based dialogue models have shown to be appropriate for decision contexts in which it is intended to overcome the lack of interaction between decision-makers, either because they are dispersed, they are too many, or they are simply not even known. However, to support decision processes with argumentation-based dialogue models, it is necessary to have knowledge of certain aspects that are specific to each decision-maker, such as preferences, interests, and limitations, among others. Failure to obtain this knowledge could ruin the model’s success. In this work, we sought to facilitate the information acquisition process by studying strategies to automatically predict the tourists’ preferences (ratings) in relation to points of interest based on their reviews. We explored different Machine Learning methods to predict users’ ratings. We used Natural Language Processing strategies to predict whether a review is positive or negative and the rating assigned by users on a scale of 1 to 5. We then applied supervised methods such as Logistic Regression, Random Forest, Decision Trees, K-Nearest Neighbors, and Recurrent Neural Networks to determine whether a tourist likes/dislikes a given point of interest. We also used a distinctive approach in this field through unsupervised techniques for anomaly detection problems. The goal was to improve the supervised model in identifying only those tourists who truly like or dislike a particular point of interest, in which the main objective is not to identify everyone, but fundamentally not to fail those who are identified in those conditions. The experiments carried out showed that the developed models could predict with high accuracy whether a review is positive or negative but have some difficulty in accurately predicting the rating assigned by users. Unsupervised method Local Outlier Factor improved the results, reducing Logistic Regression false positives with an associated cost of increasing false negatives.",Anomaly Detection on Natural Language Processing to Improve Predictions on Tourist Preferences,Electronics (Switzerland),Article,3/1/2022,Amparo Alonso,Betanzos,2022
10.3390/philosophies7030057,"© 2022 by the author. Licensee MDPI, Basel, Switzerland.In the period between Turing’s 1950 “Computing Machinery and Intelligence” and the current considerable public exposure to the term “artificial intelligence (AI)”, Turing’s question “Can a machine think?” has become a topic of daily debate in the media, the home, and, indeed, the pub. However, “Can a machine think?” is sliding towards a more controversial issue: “Can a machine be conscious?” Of course, the two issues are linked. It is held here that consciousness is a pre-requisite to thought. In Turing’s imitation game, a conscious human player is replaced by a machine, which, in the first place, is assumed not to be conscious, and which may fool an interlocutor, as consciousness cannot be perceived from an individual’s speech or action. Here, the developing paradigm of machine consciousness is examined and combined with an extant analysis of living consciousness to argue that a conscious machine is feasible, and capable of thinking. The route to this utilizes learning in a “neural state machine”, which brings into play Turing’s view of neural “unorganized” machines. The conclusion is that a machine of the “unorganized” kind could have an artificial form of consciousness that resembles the natural form and that throws some light on its nature.",From Turing to Conscious Machines,Philosophies,Article,6/1/2022,Igor,Aleksander,2022
10.3390/s22052016,"© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Optical coherence tomography (OCT) of the posterior segment of the eye provides high-resolution cross-sectional images that allow visualization of individual layers of the posterior eye tissue (the retina and choroid), facilitating the diagnosis and monitoring of ocular diseases and abnormalities. The manual analysis of retinal OCT images is a time-consuming task; therefore, the development of automatic image analysis methods is important for both research and clinical applications. In recent years, deep learning methods have emerged as an alternative method to perform this segmentation task. A large number of the proposed segmentation methods in the literature focus on the use of encoder–decoder architectures, such as U-Net, while other architectural modalities have not received as much attention. In this study, the application of an instance segmentation method based on region proposal architecture, called the Mask R-CNN, is explored in depth in the context of retinal OCT image segmentation. The importance of adequate hyper-parameter selection is examined, and the performance is compared with commonly used techniques. The Mask R-CNN provides a suitable method for the segmentation of OCT images with low segmentation boundary errors and high Dice coefficients, with segmentation performance comparable with the commonly used U-Net method. The Mask R-CNN has the advantage of a simpler extraction of the boundary positions, especially avoiding the need for a time-consuming graph search method to extract boundaries, which reduces the inference time by 2.5 times compared to U-Net, while segmenting seven retinal layers.",OCT Retinal and Choroidal Layer Instance Segmentation Using Mask R-CNN,Sensors,Article,3/1/2022,Michael,Collins,2022
10.3390/su131911114,"© 2021 by the authors. Licensee MDPI, Basel, Switzerland.This study attempted to evaluate the learning effectiveness of using the MIT App Inventor platform and its Personal Image Classifier (PIC) tool in the interdisciplinary application. The instructional design was focused on applying PIC in the integration of STEAM (i.e., Science, Technology, Engineering, Art, and Mathematics) interdisciplinary learning, so as to provide sustainable and suit-able teaching content based on the experiential learning theory for 7th grader students. Accordingly, the sustainable AI-STEAM course with the experiential learning framework has been implemented and verified, so as to confirm that the AI-STEAM course is not too difficult for young students. Many basic concepts involved in the AI-STEAM course, regarding programming logic, electromechanical concepts, interface design, and the application of image recognition, were measured in this study. The results showed that the students not only made significant progress in learning effectiveness, but also in particular made significant improvements in two parts: electromechanical concepts and image recognition knowledge. In the end, this study further provides some advice on the sustainable AI-STEAM course based on the survey of some important factors including active learning, and self-efficacy after confirming that it is not a barrier for the young students to learn the sustainable AI-STEAM course developed in this study.",Is it possible for young students to learn the Ai-STEAM application with experiential learning?,Sustainability (Switzerland),Article,10/1/2021,Hal,Abelson,2021
10.3390/su14074019,"© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Climate change is a global priority. In 2015, the United Nations (UN) outlined its Sustainable Development Goals (SDGs), which stated that taking urgent action to tackle climate change and its impacts was a key priority. The 2021 World Climate Summit finished with calls for governments to take tougher measures towards reducing their carbon footprints. However, it is not obvious how governments can make practical implementations to achieve this goal. One challenge towards achieving a reduced carbon footprint is gaining awareness of how energy exhaustive a system or mechanism is. Artificial Intelligence (AI) is increasingly being used to solve global problems, and its use could potentially solve challenges relating to climate change, but the creation of AI systems often requires vast amounts of, up front, computing power, and, thereby, it can be a significant contributor to greenhouse gas emissions. If governments are to take the SDGs and calls to reduce carbon footprints seriously, they need to find a management and governance mechanism to (i) audit how much their AI system ‘costs’ in terms of energy consumption and (ii) incentivise individuals to act based upon the auditing outcomes, in order to avoid or justify politically controversial restrictions that may be seen as bypassing the creativity of developers. The idea is thus to find a practical solution that can be implemented in software design that incentivises and rewards and that respects the autonomy of developers and designers to come up with smart solutions. This paper proposes such a sustainability management mechanism by introducing the notion of ‘Sustainability Budgets’—akin to Privacy Budgets used in Differential Privacy—and by using these to introduce a ‘Game’ where participants are rewarded for designing systems that are ‘energy efficient’. Participants in this game are, among others, the Machine Learning developers themselves, which is a new focus for this problem that this text introduces. The paper later expands this notion to sustainability management in general and outlines how it might fit into a wider governance framework.",Sustainability Budgets: A Practical Management and Governance Method for Achieving Goal 13 of the Sustainable Development Goals for AI Development,Sustainability (Switzerland),Article,4/1/2022,Mark,Coeckelbergh,2022
10.3917/futur.433.0051,"© 2019 Futuribles. All rights reserved.After a first dossier on ""the Brain and Learning"" (Futuribles 428), followed by a second on the plasticity of the brain (issue 431), Futuribles is opening a third strand in the ""Brain"" series, this time on humanmachine interactions and the impact of screens on the development of young people. As a specialist in human-machine interaction, Laurence Devillers presents the issues inherent in the development of ""conversational agents"" and other robots endowed with artificial intelligence that are increasingly found interacting with individuals in various contexts. After reminding us how these (self-)learning systems operate, she stresses how vigilant we must be about the possible manipulation of individuals by these types of interface (particularly through ""nudge"" techniques). She also shows how emotions are used in human-machine interactions (emotional triggers, humour etc.) and outlines the tools available today to evaluate artificial intelligence - and even to compare it to human intelligence (particularly the Turing test and its limitations). Given the rapid advances in machine learning, Devillers calls for the development of new tests for assessing machine capabilities, aimed in particular at monitoring their ability to manipulate individuals. Though technical progress is exponential, responsibility for the way its application is regulated in society and in the real world still falls - at least for the moment - upon citizens: it is up to individuals, as of now, to determine the ethical, regulatory and other frameworks within which such human-machine interfaces should be embedded.",The human-machine dialogue. Artificial intelligence/human intelligence: Manipulation and evaluation,Futuribles: Analyse et Prospective,Article,11/1/2019,Laurence,Devillers,2019
10.4018/978-1-59904-063-9.ch004,"We describe a research project investigating symbol grounding, the dynamics by which psychological connections are made between abstract symbols and concrete physical phenomena observed via sense perception and motor action. The project involves the use of a 3D simulated environment (AGI-SIM) as a medium for training, teaching, and developing an integrative, general-intelligence-oriented AI software system (the Novamente AI Engine). Via acting in AGI-SIM, Novamente forms concrete sensorimotor groundings for abstract relationships, such as those expressed in English by prepositions and subject-argument relationships. We describe results obtained using probabilistic-evolutionary learning within Novamente to learn groundings for the concept near, the use of these groundings in practical procedure learning (e.g., learning to play fetch in a simulated world), and then we discuss the correlation of these groundings with linguistic usage of related words and the use of these groundings within analogical and other sorts of inference. © 2007, Idea Group Inc.",Toward a pragmatic understanding of the cognitive underpinnings of symbol grounding,Semiotics and Intelligent Systems Development,Book Chapter,12/1/2006,Ben,Goertzel,2006
10.4018/978-1-61692-014-2.ch006,"I'm a dualist; in fact, a substance dualist. Why? Myriad arguments compel me to believe as I do, some going back to Descartes. But some sound arguments for substance dualism are recent; and one of these, a new argument so far as I know, is given herein: one that exploits both the contemporary computational scene, and a long-established continuum of increasingly powerful computation, ranging from varieties ""beneath"" Turing machines to varieties well beyond them. This argument shows that the hypercomputational nature of human cognition implies that Descartes was right all along. Encapsulated, the implication runs as follows: If human persons are physical, then they are their brains (plus, perhaps, other central-nervous-system machinery; denote the composite object by 'brains+'). But brains +, as most in AI and related fields correctly maintain, are information processors no more powerful than Turing machines. Since human persons hypercompute (i.e., they process information in ways beyond the reach of Turing machines), it follows that they aren't physical, (i.e., that substance dualism holds). Needless to say, objections to this argument are considered and rebutted. © 2010, IGI Global.",The hypercomputational case for substance dualism,Thinking Machines and the Philosophy of Computer Science: Concepts and Principles,Book Chapter,12/1/2010,Selmer,Bringsjord,2010
10.4102/phcfm.v14i1.3434,"© 2022. The Authors. Licensee: AOSIS. This work is licensed under the Creative Commons Attribution License.Background: Tanzania is a country experiencing multiple sexual health challenges, but providers receive no formal training in sexual health. Aim: This study aimed to assess (1) what sexual health challenges are commonly seen in clinics in Tanzania, (2) which are raised by patients, (3) which are not addressed and (4) which topics to prioritise for a sexual health curriculum. Setting: Healthcare settings in Tanzania. Methods: Participants were 60 experienced and 61 student doctors, nurses and midwives working in Dar es Salaam. The authors conducted 18 focus groups stratified by profession (midwifery, nursing or medicine) and experience (practitioners vs. students). Results: Providers identified six common sexual health concerns: (1) Human immunodeficiency virus (HIV) and acquired immunodeficiency syndrome (AIDS) and sexually transmissible infection (STI) (especially syphilis and gonorrhoea), (2) sexual violence (including intimate partner violence and female genital mutilation), (3) early and unwanted pregnancy (including early sexual debut and complications from abortion), (4) sexual dysfunctions, (5) key population concerns (e.g. lesbian, gay, bisexual, transgender (LGBT); sex work) and (6) nonprocreative sexual behaviour (including pornography and masturbation in males and oral and anal sex practices in heterosexual couples). Across professions, few differences were observed. Homosexuality, sex work, masturbation and pornography were identified as taboo topics rarely discussed. Most participants (81%) wanted one comprehensive sexual health curriculum delivered across disciplines. Conclusion: A sexual health curriculum for health students in Tanzania needs to address the most common sexual health concerns of patients. In addition to teaching sexual science and clinical care, skills training in how to address taboo topics is recommended. Students endorsed almost all sexual health topics, which suggests that a comprehensive curriculum is appropriate.","Tailoring a sexual health curriculum to the sexual health challenges seen by midwifery, nursing and medical providers and students in Tanzania",African Journal of Primary Health Care and Family Medicine,Article,1/1/2022,W,Ross,2022
10.4137/CMC.S18746,"© the authors, publisher and licensee Libertas Academica Limited.Background: Heart failure (HF) manifests as at least two subtypes. The current paradigm distinguishes the two by using both the metric ejection fraction (EF) and a constraint for end-diastolic volume. About half of all HF patients exhibit preserved EF. In contrast, the classical type of HF shows a reduced EF. Common practice sets the cut-off point often at or near EF = 50%, thus defining a linear divider. However, a rationale for this safe choice is lacking, while the assumption regarding applicability of strict linearity has not been justified. Additionally, some studies opt for eliminating patients from consideration for HF if 40, EF, 50% (gray zone). Thus, there is a need for documented classification guidelines, solving gray zone ambiguity and formu-lating crisp delineation of transitions between phenotypes.Methods: Machine learning (ML) models are applied to classify HF subtypes within the ventricular volume domain, rather than by the single use of EF. Various ML models, both unsupervised and supervised, are employed to establish a foundation for classification. Data regarding 48 HF patients are employed as training set for subsequent classification of Monte Carlo–generated surrogate HF patients (n = 403). Next, we map consequences when EF cut-off differs from 50% (as proposed for women) and analyze HF candidates not covered by current rules.Results: The training set yields best results for the Support Vector Machine method (test error 4.06%), covers the gray zone, and other clinically relevant HF candidates. End-systolic volume (ESV) emerges as a logical discriminator rather than EF as in the prevailing paradigm.Conclusions: Selected ML models offer promise for classifying HF patients (including the gray zone), when driven by ventricular volume data. ML analysis indicates that ESV has a role in the development of guidelines to parse HF subtypes. The documented curvilinear relationship between EF and ESV suggests that the assumption concerning a linear EF divider may not be of general utility over the complete clinically relevant range.",Exploring guidelines for classification of major heart failure subtypes by using machine learning,Clinical Medicine Insights: Cardiology,Article,3/3/2015,Amparo Alonso,Betanzos,2015
10.4230/LIPIcs.CP.2021.17,"© Yiwei Bai, Di Chen, and Carla P. Gomes.We introduce a curriculum learning framework for challenging tasks that require a combination of pattern recognition and combinatorial reasoning, such as single-player visual combinatorial games. Our work harnesses Deep Reasoning Nets (DRNets) [4], a framework that combines deep learning with constraint reasoning for unsupervised pattern demixing. We propose CLR-DRNets (pronounced Clear-DRNets), a curriculum-learning-with-restarts framework to boost the performance of DRNets. CLR-DRNets incrementally increase the difficulty of the training instances and use restarts, a new model selection method that selects multiple models from the same training trajectory to learn a set of diverse heuristics and apply them at inference time. An enhanced reasoning module is also proposed for CLR-DRNets to improve the ability of reasoning and generalize to unseen instances. We consider Visual Sudoku, i.e., Sudoku with hand-written digits or letters, and Visual Mixed Sudoku, a substantially more challenging task that requires the demixing and completion of two overlapping Visual Sudokus. We propose an enhanced reasoning module for the DRNets framework for encoding these visual games We show how CLR-DRNets considerably outperform DRNets and other approaches on these visual combinatorial games.",CLR-DRNets: Curriculum learning with restarts to solve visual combinatorial games,"Leibniz International Proceedings in Informatics, LIPIcs",Conference Paper,10/1/2021,Carla,Gomes,2021
10.4269/ajtmh.2008.79.338,"Injection drug use in sub-Saharan Africa is a relatively new phenomenon that expands the repertoire of human immunodeficiency virus (HIV)-associated risk behaviors in Africa. We carried out a study of 537 injection drug users (56% men and 44% women) in Dar es Salaam, Tanzania, to examine their HIV risk behaviors and their drug-using careers that had culminated in injecting heroin. Data were collected in 2005-2006 using the Swahili version of the Tanzanian AIDS Prevention Project questionnaire. Marijuana, alcohol, and heroin were the first drugs reported for both men and women. Most drug milestones appeared in a similar order for men and women. Mandrax, khat, and injecting appeared close to one another in chronological time for both men and women, suggesting they were introduced into the country and appeared on the drug scene at about the same (real) time. Drug careers for women were shorter than for men, and time from first use of heroin to first injection was shorter for women. Years of injecting suggested that injecting had increased in males approximately five years prior to data collection, with males injecting earlier, but females being increasingly introduced to injecting in the previous two years. Injecting appears at a mean of five years (men) and three years (women) into their heroin-using career. Heroin use appears to occur in binges, with women being more likely to have sex during a binge. In this sample, more than 90% of women but only 2% of men reported ever trading sex for money. More than 90% of men and women reported using new needles for injection. These data confirm that heroin injecting is well established in large cities in east Africa, and that HIV prevention in the region must now include drug injectors and other drug users. Copyright © 2008 by The American Society of Tropical Medicine and Hygiene.",Drug use careers and blood-borne pathogen risk behavior in male and female Tanzanian heroin injectors,American Journal of Tropical Medicine and Hygiene,Article,1/1/2008,W,Ross,2008
10.4324/9780203055045,"© 1998 Andy Clark and Josefa Toribio. All rights reserved.Summarizes and illuminates two decades of research Gathering important papers by both philosophers and scientists, this collection illuminates the central themes that have arisen during the last two decades of work on the conceptual foundations of artificial intelligence and cognitive science. Each volume begins with a comprehensive introduction that places the coverage in a broader perspective and links it with material in the companion volumes. The collection is of interest in many disciplines including computer science, linguistics, biology, information science, psychology, neuroscience, iconography, and philosophy. Examines initial efforts and the latest controversies The topics covered range from the bedrock assumptions of the computational approach to understanding the mind, to the more recent debates concerning cognitive architectures, all the way to the latest developments in robotics, artificial life, and dynamical systems theory. The collection first examines the lineage of major research programs, beginning with the basic idea of machine intelligence itself, then focuses on specific aspects of thought and intelligence, highlighting the much-discussed issue of consciousness, the equally important, but less densely researched issue of emotional response, and the more traditionally philosophical topic of language and meaning. Provides a gamut of perspectives The editors have included several articles that challenge crucial elements of the familiar research program of cognitive science, as well as important writings whose previous circulation has been limited. Within each volume the papers are organized to reflect a variety of research programs and issues. The substantive introductions that accompany each volume further organize the material and provide readers with a working sense of the issues and the connection between articles.",Machine intelligence: Perspectives on the computational model,Machine Intelligence: Perspectives on the Computational Model,Book,1/1/2012,Andy,Clark,2012
10.4324/9780203055069,"© 1998 Josefa Toribio and Andy Clark. All rights reserved.Summarizes and illuminates two decades of research Gathering important papers by both philosophers and scientists, this collection illuminates the central themes that have arisen during the last two decades of work on the conceptual foundations of artificial intelligence and cognitive science. Each volume begins with a comprehensive introduction that places the coverage in a broader perspective and links it with material in the companion volumes. The collection is of interest in many disciplines including computer science, linguistics, biology, information science, psychology, neuroscience, iconography, and philosophy. Examines initial efforts and the latest controversies The topics covered range from the bedrock assumptions of the computational approach to understanding the mind, to the more recent debates concerning cognitive architectures, all the way to the latest developments in robotics, artificial life, and dynamical systems theory. The collection first examines the lineage of major research programs, beginning with the basic idea of machine intelligence itself, then focuses on specific aspects of thought and intelligence, highlighting the much-discussed issue of consciousness, the equally important, but less densely researched issue of emotional response, and the more traditionally philosophical topic of language and meaning. Provides a gamut of perspectives The editors have included several articles that challenge crucial elements of the familiar research program of cognitive science, as well as important writings whose previous circulation has been limited. Within each volume the papers are organized to reflect a variety of research programs and issues. The substantive introductions that accompany each volume further organize the material and provide readers with a working sense of the issues and the connection between articles.",Language and meaning in cognitive science: Cognitive issues and semantic theory,Language and Meaning in Cognitive Science: Cognitive Issues and Semantic theory,Book,1/1/2012,Andy,Clark,2012
10.4324/9780203214015-18,"© World Health Organization, 1998.In many areas, the spread of HIV-1 among injecting drug users (IDUs) due to the multi-person use of drug injection equipment has occurred with extreme rapidity. In New York City, for example, HIV-1 seroprevalence among IDUs increased from under 10 per cent to over 50 per cent in a period of five years (Des Jarlais et al., 1989); in Edinburgh, HIV-1 seroprevalence among IDUs increased from zero to over 40 per cent in one year (Robertson et al., 1986); in Bangkok, HIV-1 seroprevalence increased from 2 per cent to over 40 per cent in two years (Vanichseni and Sakuntanaga, 1990); and in the state of Manipur, India, levels increased from zero to approximately 50 per cent in one year (Naik et al., 1991). HIV-1 has spread rapidly among populations where there has been a lack of awareness of AIDS as a local threat and mechanisms such as ‘shooting galleries’, ‘dealer’s works’ and professional injectors that provide rapid and efficient mixing among large numbers of IDUs (Friedman and Des Jarlais, 1991).",Preventing Epidemics of HIV-1 among Injecting Drug Users,World Health Organization: Drug Injecting and HIV Infection: Global Dimensions and Local Responses,Book Chapter,1/1/2003,W,Ross,2003
10.4324/9780203503638,"© 2004 Psychology Press. All rights reserved.Board games have long fascinated as mirrors of intelligence, skill, cunning, and wisdom. While board games have been the topic of many scientific studies, and have been studied for more than a century by psychologists, there was until now no single volume summarizing psychological research into board games. This book, which is the first systematic study of psychology and board games, covers topics such as perception, memory, problem solving and decision making, development, intelligence, emotions, motivation, education, and neuroscience. It also briefly summarizes current research in artificial intelligence aiming at developing computers playing board games, and critically discusses how current theories of expertise fare with board games. Finally, it shows that the information provided by board game research, both data and theories, have a wider relevance for the understanding of human psychology in general.",Moves in mind: The psychology of board games,Moves in Mind: The Psychology of Board Games,Book,8/5/2004,Fernand,Gobet,2004
10.4324/9780203508527,"© 1990, 2004 Margaret A. Boden. All rights reserved.How is it possible to think new thoughts? What is creativity and can science explain it? And just how did Coleridge dream up the creatures of The Ancient Mariner? When The Creative Mind: Myths and Mechanisms was first published, Margaret A. Boden's bold and provocative exploration of creativity broke new ground. Boden uses examples such as jazz improvisation, chess, story writing, physics, and the music of Mozart, together with computing models from the field of artificial intelligence to uncover the nature of human creativity in the arts. The second edition of The Creative Mind has been updated to include recent developments in artificial intelligence, with a new preface, introduction and conclusion by the author. It is an essential work for anyone interested in the creativity of the human mind.",The creative mind: Myths and mechanisms: Second edition,The Creative Mind: Myths and Mechanisms: Second Edition,Book,9/17/2003,Margaret,Boden,2003
10.4324/9780203769621-11,"© 1993 Pergamon Press Ltd.Predicting AIDS-preventive behaviour, particularly condom use, with a model such as the theory of reasoned action (Ajzen & Fishbein, 1980) must be seen in the context of related research into attitudes and beliefs about condoms. This chapter attempts to review the previous evidence on the influence of attitudes towards condoms, in order to locate the data on the theory of reasoned action within work on determinants of condom use and to conceptualise the application of the theory of reasoned action as part of an attempt to better explain HIV and SID-preventive behaviour. It is important to note that a substantial portion of this work has involved attitudes toward condoms as contraceptives (although a significant amount of work fails to distinguish the use of the condom as SID prophylaxis with contraceptive use). This work is included in the historical perspective as attitudes toward, and beliefs about, condoms have been located within previous discourses and built upon previous perceptions about condoms. Indeed, with heterosexual condom use, it may not be possible to separate SID prophylactic and contraceptive elements. While much of the research cited is atheoretical, it does contribute substantially to an understanding of attitudes and beliefs regarding condoms and condom use which the theory of reasoned action later develops. Prior to reviewing the previous literature on condoms, it must be noted that the term ‘attitude’ as used in these studies may not necessarily be similar to the definition of attitude adopted by the theory of reasoned action. Nevertheless, an understanding One of the most comprehensive studies examining attitudes towards condoms was carried out by Darrow (1974), who studied 2325 individuals at a California SID clinic with regard to their attitudes toward, and utilisation of, condoms as a prophylactic. His sample had a mean age of 22 years, and two thirds were unmarried. The objections of Darrow’s respondents (percentages for each objection in parentheses) were that condoms interfered with sex (25.9%), were unnatural (17.9%), were unsatisfying (16.3%), were messy and uncomfortable (15.7%), irritating (8.2%), unreliable (7.2%), were forgotten (5.2%), unsafe (3.1%), hard to buy (1.6%), didn’t work (1.3%), reduced pleasure (1.2%) and against people’s religion (1.2%). It can be seen that the problem with condoms was perceived by most respondents in this study as being one of interference and discomfort, rather than of access to condoms.",Attitudes towards condoms and the theory of reasoned action,The Theory of Reasoned Action: Its application to AIDS-Preventive Behaviour,Book Chapter,1/1/2015,W,Ross,2015
10.4324/9780203769621-16,"© 1993 Pergamon Press Ltd.It is well known that the most important HIV-preventive behaviours are related to the use of condoms and safe injecting of drugs. These will remain critical behaviours, even after the introduction of an HIV -preventive vaccine (Fishbein & Middlestadt, 1989). Along with the proliferation of research on condom use and safe needle use in response to the AIDS epidemic, there has been a reliance on the use of a plethora of scales and questionnaires, which have neither been subjected to proper scale development based on qualitative research with appropriate target groups nor developed within the framework of appropriate theories of human behaviour. This chapter describes the development of a scale to measure intentions to use condoms, which is based on the components of the theory of reasoned action (TRA). Using the TRA as part of such an approach has the potential to increase our understanding of sexual behaviour and those determinants which are associated with an increased risk of HIV infection (Parker & Carballo, 1990).",Application of the theory of reasoned action to the measurement of condom use among gay men,The Theory of Reasoned Action: Its application to AIDS-Preventive Behaviour,Book Chapter,1/1/2015,W,Ross,2015
10.4324/9780429283130,"© 2019 Taylor & Francis.Given the rapid development of new technologies such as smart devices, robots, and artificial intelligence and their impact on the lives of people and on society, it is important and urgent to construct conceptual frameworks that help us to understand and evaluate them. Benefiting from tendencies towards a performative turn in the humanities and social sciences, drawing on thinking about the performing arts, and responding to gaps in contemporary artefact-oriented philosophy of technology, this book moves thinking about technology forward by using performance as a metaphor to understand and evaluate what we do with technology and what technology does with us. Focusing on the themes of knowledge/experience, agency, and power, and discussing some pertinent ethical issues such as deception, the narrative of the book moves through a number of performance practices: Dance, theatre, music, stage magic, and (perhaps surprisingly) philosophy. These are used as sources for metaphors to think about technology—in particular contemporary devices and machines—and as interfaces to bring in various theories that are not usually employed in philosophy of technology. The result is a sequence of gestures and movements towards a performance-oriented conceptual framework for a thinking about technology which, liberated from the static, vision-centred, and dualistic metaphors offered by traditional philosophy, can do more justice to the phenomenology of our daily embodied, social, kinetic, temporal, and narrative performances with technology, our technoperformances. This book will appeal to scholars of philosophy of technology and performance studies who are interested in reconceptualizing the roles and impact of modern technology.",Moved by machines: Performance metaphors and philosophy of technology,Moved by Machines: Performance Metaphors and Philosophy of Technology,Book,1/1/2019,Mark,Coeckelbergh,2019
10.4324/9781003159490,"© 2021 Mark Coeckelbergh. All rights reserved.This book discusses the problem of freedom and the limits of liberalism considering the challenges of governing climate change and artificial intelligence (AI). It mobilizes resources from political philosophy to make an original argument about the future of technology and the environment. Can artificial intelligence save the planet? And does that mean we will have to give up our political freedom? Stretching the meaning of freedom but steering away from authoritarian options, this book proposes that, next to using other principles such as justice and equality and taking collective action and cooperating at a global level, we adopt a positive and relational conception of freedom that creates better conditions for human and non-human flourishing. In contrast to easy libertarianism and arrogant techno-solutionism, this offers a less symptomatic treatment of the global crises we face and gives technologies such as AI a role in the gathering of a new, more inclusive political collective and the ongoing participative making of new common worlds. Written in a clear and accessible style, Green Leviathan or the Poetics of Political Liberty will appeal to researchers and students working in political philosophy, environmental philosophy, and the philosophy of technology.",Green Leviathan or the poetics of political liberty: Navigating freedom in the age of climate change and artificial intelligence,Green Leviathan or the Poetics of Political Liberty: Navigating Freedom in the Age of Climate Change and Artificial Intelligence,Book,5/31/2021,Mark,Coeckelbergh,2021
10.4324/9781315441887,"© 2019 Fernand Gobet.Do you need to be a genius to be good at chess? What does it take to become a Grandmaster? Can computer programmes beat human intuition in gameplay? The Psychology of Chess is an insightful overview of the roles of intelligence, expertise, and human intuition in playing this complex and ancient game. The book explores the idea of ‘practice makes perfect’, alongside accounts of why men perform better than women in international rankings, and why chess has become synonymous with extreme intelligence as well as madness. When artificial intelligence researchers are increasingly studying chess to develop machine learning, The Psychology of Chess shows us how much it has already taught us about the human mind.",The psychology of chess,The Psychology of Chess,Book,1/1/2018,Fernand,Gobet,2018
10.5220/0004807703800386,"Lately, derived from the Big Data problem, researchers in Machine Learning became also interested not only in accuracy, but also in scalability. Although scalability of learning methods is a trending issue, scalability of feature selection methods has not received the same amount of attention. In this research, an attempt to study scalability of both Feature Selection and Machine Learning on microarray datasets will be done. For this sake, the minimum redundancy maximum relevance (mRMR) filter method has been chosen, since it claims to be very adequate for this type of datasets. Three synthetic databases which reflect the problematics of microarray will be evaluated with new measures, based not only in an accurate selection but also in execution time. The results obtained are presented and discussed.",Scalability analysis of mRMR for microarray data,ICAART 2014 - Proceedings of the 6th International Conference on Agents and Artificial Intelligence,Conference Paper,1/1/2014,Amparo Alonso,Betanzos,2014
10.5220/0005215901310142,"The dual-process theory of human cognition proposes the existence of two systems for decision-making: a slower, deliberative, ""problem-solving"" system and a quicker, reactive, ""pattern-recognition"" system. The aim of this work is to explore the effect on agent performance of altering the balance of these systems in an environment of varying complexity. This is an important question, both in the realm of explanations of expert behaviour and to AI in general. To achieve this, we implement three distinct types of agent, embodying different balances of their problem-solving and pattern-recognition systems, using a novel, hybrid, humanlike cognitive architecture. These agents are then situated in the virtual, stochastic, multi-agent ""Tileworld"" domain, whose intrinsic and extrinsic environmental complexity can be precisely controlled and widely varied. This domain provides an adequate test-bed to analyse the research question posed. A number of computational simulations are run. Our results indicate that there is a definite performance benefit for agents which use a mixture of problem-solving and pattern-recognition systems, especially in highly complex environments.",The art of balance: Problem-solving vs. pattern-recognition,"ICAART 2015 - 7th International Conference on Agents and Artificial Intelligence, Proceedings",Conference Paper,1/1/2015,Fernand,Gobet,2015
10.5591/978-1-57735-516-8/IJCAI11-154,"Learning concepts via instruction and expository texts is an important problem for modeling human learning and for making autonomous AI systems. This paper describes a computational model of the self-explanation effect, whereby conceptual knowledge is repaired by integrating and explaining new material. Our model represents conceptual knowledge with compositional model fragments, which are used to explain new material via model formulation. Preferences are computed over explanations and conceptual knowledge, along several dimensions. These preferences guide knowledge integration and question-answering. Our simulation learns about the human circulatory system, using facts from a circulatory system passage used in a previous cognitive psychology experiment. We analyze the simulation's perfo rmance, showing that individual differences in sequences of models learned by students can be explained by different parameter settings in our model.",Repairing incorrect knowledge with model formulation and metareasoning,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,12/1/2011,Ken,Forbus,2011
10.5591/978-1-57735-516-8/IJCAI11-401,"This paper presents a new Monte-Carlo search algorithm for very large sequential decision-making problems. We apply non-linear regression within Monte-Carlo search, online, to estimate a stateaction value function from the outcomes of random roll-outs. This value function generalizes between related states and actions, and can therefore provide more accurate evaluations after fewer rollouts. A further significant advantage of this approach is its ability to automatically extract and leverage domain knowledge from external sources such as game manuals. We apply our algorithm to the game of Civilization II, a challenging multiagent strategy game with an enormous state space and around 1021joint actions. We approximate the value function by a neural network, augmented by linguistic knowledge that is extracted automatically from the official game manual. We show that this non-linear value function is significantly more efficient than a linear value function, which is itself more efficient than Monte-Carlo tree search. Our non-linear Monte-Carlo search wins over 78% of games against the built-in AI of Civilization II.",Non-linear Monte-Carlo search in civilization II,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,12/1/2011,Regina,Barzilay,2011
10.5591/978-1-57735-516-8/IJCAI11-413,"Several logistics service providers serve a certain number of customers, geographically spread over an area of operations. They would like to coordinate their operations so as to minimize overall cost. At the same time, they would like to keep information about their costs, constraints and preferences private, thus precluding conventional negotiation. We show how AI techniques, in particular Distributed Constraint Optimization (DCOP), can be integrated with cryptographic techniques to allow such coordination without revealing agents' private information. The problem of assigning customers to companies is formulated as a DCOP, for which we propose two novel, privacy-preserving algorithms. We compare their performances and privacy properties on a set of Vehicle Routing Problem benchmarks.",Coordinating logistics operations with privacy guarantees,IJCAI International Joint Conference on Artificial Intelligence,Conference Paper,12/1/2011,Boi,Faltings,2011
10.5694/j.1326-5377.1988.tb120669.x,"A random, stratified sample of 2601 adult Australians from all states and territories was interviewed about knowledge of the acquired immunodeficiency syndrome (AIDS). After the interview, an anonymous questionnaire on the prevalence of practices that are associated with risk of human immunodeficiency virus (HIV) infection was left with the respondents; 60.2% of these questionnaires were returned. Data from this survey suggest that the prevalences of male homosexual behaviour, prostitute contact and lesbian contact are substantially lower than were estimated previously. Men with homosexual experience were significantly more prevalent in the more populous states, but the majority of other risk factors - intravenous drug abuse, male respondents' contact with prostitutes, transfusion of blood or blood products during 1980-1985 and heterosexual contact - showed few significant associations with geographical, occupational or marital status. Intravenous drug abusers were significantly younger, and heterosexual contact was associated with age for both male and female respondents. No significant differences were found in the prevalence of homosexual contact among single, married and previously-married men, although the prevalence of homosexual contact was lower in married men. The results of the study are discussed in terms of targeting preventive campaigns and assessing the future potential for the spread of HIV infection.",Prevalence of risk factors for human immunodeficiency virus infection in the Australian population,Medical Journal of Australia,Article,1/1/1988,W,Ross,1988
10.5694/j.1326-5377.1993.tb121649.x,"The management of sexually transmissible diseases has long been a part of medical practice, but it has taken the AIDS epidemic to highlight the need for doctors to elicit an effective sexual history in relation to potential past exposure to HIV.",Lifestyle clues in the recognition of HIV infection. How to take a sexual history,Medical Journal of Australia,Note,1/1/1993,W,Ross,1993
10.7146/kkf.v29i2.124899,"© Europa Digital & Publishing 2022. All rights reserved.This article sets out our perspective on how to begin the journey of decolonising computational fields, such as data and cognitive sciences. We see this struggle as requiring two basic steps: a) realisation that the present-day system has inherited, and still enacts, hostile, conservative, and oppressive behaviours and principles towards women of colour; and b) rejection of the idea that centring individual people is a solution to system-level problems. The longer we ignore these two steps, the more “our” academic system maintains its toxic structure, excludes, and harms Black women and other minoritised groups. This also keeps the door open to discredited pseudoscience, like eugenics and physiognomy. We propose that grappling with our fields' histories and heritage holds the key to avoiding mistakes of the past. In contrast to, for example, initiatives such as “diversity boards”, which can be harmful because they superficially appear reformatory but nonetheless center whiteness and maintain the status quo. Building on the work of many women of colour, we hope to advance the dialogue required to build both a grass-roots and a top-down re-imagining of computational sciences - including but not limited to psychology, neuroscience, cognitive science, computer science, data science, statistics, machine learning, and artificial intelligence. We aspire to progress away from these fields' stagnant, sexist, and racist shared past into an ecosystem that welcomes and nurtures demographically diverse researchers and ideas that critically challenge the status quo.",Towards Decolonising Computational Sciences,"Women, Gender and Research",Article,1/1/2021,Abeba,Birhane,2021
